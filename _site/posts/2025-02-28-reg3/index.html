<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Lee Seungjun">
<meta name="dcterms.date" content="2025-02-28">
<meta name="description" content="Data가 Independent하지 않고 Clustered되어 있을 때 Regression Analysis를 수행하기 위해 GLM에서 발전된 두 가지 모델 GEE와 GLMM의 개념을 공부하고, M-estimator와 Robust (sandwich) estimation을 통해 지금까지의 Robust한 Covariance Matrix을 generalize하게 살펴봅니다.">
<title>Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation – 차라투 블로그</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/logo_favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2fa66d5285053e3ebee39b9a5638a87d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-135478021-2', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../styles.css">
</head>
<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/logo.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">차라투 블로그</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://openstat.ai/" target="_blank"> 
<span class="menu-text">Applications</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-r-packages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">R packages</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-r-packages">
<li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jstable/" target="_blank">
 <span class="dropdown-text">jstable</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jskm/" target="_blank">
 <span class="dropdown-text">jskm</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jsmodule/" target="_blank">
 <span class="dropdown-text">jsmodule</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="../../contributors.html"> 
<span class="menu-text">Contributors</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-partners" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Partners</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-partners">
<li>
    <a class="dropdown-item" href="https://www.r-bloggers.com/" target="_blank">
 <span class="dropdown-text"><img src="https://www.r-bloggers.com/favicon.ico"> R-bloggers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/shinykorea/" target="_blank">
 <span class="dropdown-text"><img width="16px" src="https://avatars.githubusercontent.com/u/46996346?s=200&amp;v=4"> Shinykorea</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zarathucorp" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/zarathu/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" target="_blank"> <i class="bi bi-rss" role="img" aria-label="RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-translate" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-translate" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-translate">
<li>
    <a class="dropdown-item" href="../../../index.html">
 <span class="dropdown-text">한국어</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../en/index.html">
 <span class="dropdown-text">English</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../jp/index.html">
 <span class="dropdown-text">日本語</span></a>
  </li>  
    </ul>
</li>
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation</h1>
                  <div>
        <div class="description">
          Data가 Independent하지 않고 Clustered되어 있을 때 Regression Analysis를 수행하기 위해 GLM에서 발전된 두 가지 모델 GEE와 GLMM의 개념을 공부하고, M-estimator와 Robust (sandwich) estimation을 통해 지금까지의 Robust한 Covariance Matrix을 generalize하게 살펴봅니다.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://github.com/aiseungjun">Lee Seungjun</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li><a href="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0" id="toc-들어가며" class="nav-link active" data-scroll-target="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0">들어가며</a></li>
  <li>
<a href="#m-estimation" id="toc-m-estimation" class="nav-link" data-scroll-target="#m-estimation">1. M-estimation</a>
  <ul class="collapse">
<li><a href="#m-estimation-%EC%A0%95%EC%9D%98" id="toc-m-estimation-정의" class="nav-link" data-scroll-target="#m-estimation-%EC%A0%95%EC%9D%98">1.1. M-estimation 정의</a></li>
  <li><a href="#m-estimation-%ED%8A%B9%EC%A7%95" id="toc-m-estimation-특징" class="nav-link" data-scroll-target="#m-estimation-%ED%8A%B9%EC%A7%95">1.2. M-estimation 특징</a></li>
  <li><a href="#m-estimation%EC%9D%98-asymptotic-normality-%EC%A6%9D%EB%AA%85" id="toc-m-estimation의-asymptotic-normality-증명" class="nav-link" data-scroll-target="#m-estimation%EC%9D%98-asymptotic-normality-%EC%A6%9D%EB%AA%85">1.3. M-estimation의 Asymptotic Normality 증명</a></li>
  <li><a href="#sandwichrobust-estimator" id="toc-sandwichrobust-estimator" class="nav-link" data-scroll-target="#sandwichrobust-estimator">1.4. Sandwich(Robust) Estimator</a></li>
  <li><a href="#glm-%EB%B3%B5%EC%8A%B5" id="toc-glm-복습" class="nav-link" data-scroll-target="#glm-%EB%B3%B5%EC%8A%B5">1.5. GLM 복습</a></li>
  <li><a href="#%EB%8B%A4%EC%8B%9C-%EB%8F%8C%EC%95%84%EC%99%80%EC%84%9C..-for-clustered-data" id="toc-다시-돌아와서..-for-clustered-data" class="nav-link" data-scroll-target="#%EB%8B%A4%EC%8B%9C-%EB%8F%8C%EC%95%84%EC%99%80%EC%84%9C..-for-clustered-data">다시 돌아와서.. (for clustered data)</a></li>
  </ul>
</li>
  <li>
<a href="#generalized-estimating-equation-gee" id="toc-generalized-estimating-equation-gee" class="nav-link" data-scroll-target="#generalized-estimating-equation-gee">2. Generalized Estimating Equation (GEE)</a>
  <ul class="collapse">
<li><a href="#gee-%EC%A0%95%EC%9D%98" id="toc-gee-정의" class="nav-link" data-scroll-target="#gee-%EC%A0%95%EC%9D%98">2.1. GEE 정의</a></li>
  <li><a href="#gee-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95" id="toc-gee-수학적-표현-및-추정" class="nav-link" data-scroll-target="#gee-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95">2.2. GEE 수학적 표현 및 추정</a></li>
  <li><a href="#gee-parameters-variance" id="toc-gee-parameters-variance" class="nav-link" data-scroll-target="#gee-parameters-variance">2.3. GEE parameter’s Variance</a></li>
  </ul>
</li>
  <li>
<a href="#generalized-linear-mixed-model-glmm" id="toc-generalized-linear-mixed-model-glmm" class="nav-link" data-scroll-target="#generalized-linear-mixed-model-glmm">3. Generalized Linear Mixed Model (GLMM)</a>
  <ul class="collapse">
<li><a href="#glmm-%EC%A0%95%EC%9D%98" id="toc-glmm-정의" class="nav-link" data-scroll-target="#glmm-%EC%A0%95%EC%9D%98">3.1. GLMM 정의</a></li>
  <li><a href="#lmm-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95" id="toc-lmm-수학적-표현-및-추정" class="nav-link" data-scroll-target="#lmm-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95">3.2. LMM 수학적 표현 및 추정</a></li>
  <li><a href="#glmm%EC%9D%98-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95" id="toc-glmm의-수학적-표현-및-추정" class="nav-link" data-scroll-target="#glmm%EC%9D%98-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95">3.3. GLMM의 수학적 표현 및 추정</a></li>
  <li><a href="#glmm-parameters-variance" id="toc-glmm-parameters-variance" class="nav-link" data-scroll-target="#glmm-parameters-variance">3.4. GLMM parameter’s Variance</a></li>
  </ul>
</li>
  <li><a href="#r-%EC%BD%94%EB%93%9C-%EC%98%88%EC%A0%9C-gee-glmm" id="toc-r-코드-예제-gee-glmm" class="nav-link" data-scroll-target="#r-%EC%BD%94%EB%93%9C-%EC%98%88%EC%A0%9C-gee-glmm">4. R 코드 예제: GEE, GLMM</a></li>
  <li><a href="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0" id="toc-마무리하며" class="nav-link" data-scroll-target="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0">마무리하며</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><!-- Google tag (gtag.js) --><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-L0DYYSH9KM"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L0DYYSH9KM');
</script><section id="들어가며" class="level2"><h2 class="anchored" data-anchor-id="들어가며">들어가며</h2>
<p>3장에서는 2장에서 다룬 <strong>Generalized Linear Model (GLM)</strong>에서 더 나아가, 데이터 내에 <strong>군집(Clustered)</strong> 구조가 존재하거나, <strong>반복측정(Repeated measures)</strong> 데이터로 인해 <strong>독립성 가정이 깨지는 경우</strong>를 다루는 방법론인 <strong>GEE (Generalized Estimating Equation)</strong>와 <strong>GLMM (Generalized Linear Mixed Model)</strong>을 다루며, 그 전에 중요한 추정 방법론 중 하나인 <strong>M-estimation</strong>과 <strong>Robust(Sandwich) estimation</strong>에 대해서 다루겠습니다.<br></p>
</section><section id="m-estimation" class="level2"><h2 class="anchored" data-anchor-id="m-estimation">1. M-estimation</h2>
<section id="m-estimation-정의" class="level3"><h3 class="anchored" data-anchor-id="m-estimation-정의">1.1. M-estimation 정의</h3>
<hr>
<p><strong>통계 분석</strong>에서 <strong>통계 모델</strong>이 비모수(non-parametric)가 아니라 <strong>모수(parametric)</strong>인 경우, 우리는 model의 모수, 즉 parameter (<span class="math inline">\(\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> 등)를 추정해야 합니다. 이는 생각보다 어려운 일이 될 수 있으며, 이전에 언급한 MLE, OLS, Method of Moments (MOM) 등 다양한 model의 estimation 방법이 제안되어 왔습니다. 그런데, 이러한 추정 방법들은 사실상 하나의 <strong>추정방정식(estimating equation, 통계 모델의 parameter 추정 방향을 제시하는 모든 식)</strong>을 세운 뒤, 그 방정식을 만족하는 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>를 찾는 과정으로 해석할 수 있습니다. 예를 들어, <strong>MLE에서는</strong> log likelihood를 parameter로 미분한 함수(score function)가 0이 되는 parameter point를 추정하는 과정이었고, <strong>OLS</strong>에서는 cost function (SSR, 오차 제곱합)을 parameter로 미분한 함수가 0이 되는 parameter point을 찾는 과정이었습니다. <strong>M-estimation은</strong> 이러한 공통된 개념을 일반화하여 공통된 parameter의 성질을 제시해줍니다.</p>
<p>즉, <strong>M-estimation</strong>은 다음 과 같은 형태를 지닌 추정 방정식을 세우고, 이를 만족하는 모수의 값을 해로 삼습니다:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} \psi_i(\boldsymbol{\theta}) = \mathbf{0},
\]</span></p>
<p>여기서 <span class="math inline">\(\psi_i(\boldsymbol{\theta})\)</span>는 (i)번째 관측치에 대해 정의된 <strong>estimating function (e.g.&nbsp;score function)</strong>, <span class="math inline">\(\boldsymbol{\theta}\)</span>는 추정하고자 하는 parameter(위 식에서는 우항이 scalar 0이 아닌 벡터 0이므로 <span class="math inline">\(\boldsymbol{\theta}\)</span> 또한 벡터.)입니다. 위에서 언급한 것처럼 MLS와 OLS, OLS의 일반화 버전인 Non-linear Least Squares 모두 <strong>M-estimation입니다.</strong> 1, 2장에 걸쳐 이미 익숙하시겠지만 다시 확인해보면, MLE를 M-estimation 형태로 작성해보면 다음과 같고,</p>
<p><span class="math display">\[
\psi_i(\boldsymbol{\theta}) = \frac{\partial}{\partial \boldsymbol{\theta}} \log f(Y_i; \boldsymbol{\theta}) \quad\Longrightarrow\quad \sum_{i=1}^n \psi_i(\boldsymbol{\theta}) = \sum_{i=1}^n \frac{\partial}{\partial \boldsymbol{\theta}} \log f(Y_i; \boldsymbol{\theta}) = \mathbf{0}.
\]</span></p>
<p>OLS는 다음과 같습니다. <span class="math display">\[
\psi_i(\boldsymbol{\beta}) = (Y_i - \mathbf{x}_i^\top \boldsymbol{\beta}) \mathbf{x}_i, \quad\Longrightarrow\quad \sum_{i=1}^n \mathbf{x}_i(Y_i - \mathbf{x}_i^\top \boldsymbol{\beta}) = \mathbf{0}.
\]</span></p>
<p>여기서 OLS의 estimation equation이 다음과 같은 이유는,<br><span class="math display">\[
\nabla_\beta J(\beta) = \nabla_\beta \frac{1}{2} (X\beta - y)^\top (X\beta - y) \\
= \frac{1}{2} \nabla_\beta \big( (X\beta)^\top X\beta - (X\beta)^\top y - y^\top (X\beta) + y^\top y \big)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \big( 2 X^\top X \beta - 2 X^\top y \big)
\]</span></p>
<p><span class="math display">\[
= X^\top X \beta - X^\top y
\]</span></p>
<p><span class="math display">\[
=\sum_{i=1}^n \mathbf{x}_i(\mathbf{x}_i^\top \boldsymbol{\beta} - Y_i) = \mathbf{0}
\]</span> 에서 유도된 식입니다.</p>
</section><section id="m-estimation-특징" class="level3"><h3 class="anchored" data-anchor-id="m-estimation-특징">1.2. M-estimation 특징</h3>
<hr>
<p><strong>M-estimation</strong>의 가장 중요한 특징은 <strong>일반성과 확장성</strong>입니다. 즉, parameter estimation 문제를</p>
<p><span class="math display">\[
\sum_{i=1}^{n} \psi_i(\boldsymbol{\theta}) = \mathbf{0}
\]</span></p>
<p>의 해로서 바라보면, 여러 기존 추정법들을 하나의 큰 이론적 틀에서 이해할 수 있고, 이로부터 발생하는 성질들은 해당되는 방법론 모두에 적용됩니다. 이렇게 M-estimation을 강조하여 설명하는 이유는, M-estimation은 아래 두 가지 <strong>수렴 이론(Asymptotic theory)</strong>을 제공하기 때문입니다.</p>
<p>(1) 적절한 <strong>정규성 조건</strong>(regularity conditions) 하에서(종속변수의 정규 분포 가정이 아니며, 언급드린 적이 없지만 아주 general한 조건이라고 생각해주시면 됩니다.), 위 M-estimation의 estimating equation의 추정해 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>가 참 모수 <span class="math inline">\(\boldsymbol{\theta}_0\)</span>에 대해 <strong>일치성(Consistency)</strong>과 <strong>점근정규성(Asymptotic Normality)</strong>을 가집니다.</p>
<p><br>
(2) 또한, 정규성을 갖는 모수의 점근분포가 <strong>중심극한정리</strong>(CLT)의 연장선상에 있다고 볼 수 있으며, 그 결과 위 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>의 asymptotic Normality에서 parameter의 분산은 Asymptotically&amp;robust하게 추정 가능하고, 이 Robust Estimator의 형태가 <strong>샌드위치(sandwich)</strong> 형태로 생겼기 때문에 Sandwich Estimation(or)이라고도 부릅니다.</p>
<p>즉, M-estimation으로부터 얻는 의의를 살펴보자면, 우리가 Regression Model의 parameters를 추정하는 과정에서 estimating equation이 위 M-estimation의 형태를 만족한다면, 어떠한 methods를 사용하든 이를 통해 추정한 parameter <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>는 참 모수 <span class="math inline">\(\boldsymbol{\theta}_0\)</span>에 대해 consistent함과, robust한 parameter의 분산을 얻을 수 있다는 것입니다. 이제 (1)과 (2)에 대한 수학적 증명을 걸친 뒤, 이들의 의미를 살펴보겠습니다.</p>
</section><section id="m-estimation의-asymptotic-normality-증명" class="level3"><h3 class="anchored" data-anchor-id="m-estimation의-asymptotic-normality-증명">1.3. M-estimation의 Asymptotic Normality 증명</h3>
<hr>
<p><strong>M-estimation</strong> 추정량 <span class="math inline">\(\hat{\theta}\)</span>의 점근적 성질을 유도하기 위해 <strong>1차 Taylor 전개</strong>를 사용합니다. 아래와 같은 M-estimation 추정 방정식 (증명의 편리를 위해 양변에 <span class="math inline">\(\frac{1}{n}\)</span>을 나누었으며, 나누지 않아도 똑같이 증명 가능하고, 등식에서 상수 term을 곱하고 나누는 것은 당연히 문제되지 않습니다. ) <span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n} \psi_i(\hat{\theta}) = 0
\]</span> 을 참 모수 <span class="math inline">\(\theta_0\)</span> Taylor 식으로 전개하면,</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n} \psi_i(\hat{\theta}) \approx \frac{1}{n} \sum_{i=1}^{n} \psi_i(\theta_0) + \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} (\hat{\theta} - \theta_0) = 0.
\]</span></p>
<p>가 됩니다. 이때 좌항 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n} \psi_i(\hat{\theta})\)</span>은 우리가 위 estimating equation에서 보았듯이, 이 항이 0이 되도록 하는 parameter를 추정한 결과가 <span class="math inline">\(\hat{\theta}\)</span>였기 때문에 당연히 0일 것이고, 따라서 중앙항 (<span class="math inline">\(\theta_0\)</span>에 대한 Taylor 1차 식 전개) 또한 0이 되는 것입니다. 이제 <span class="math inline">\(\theta\)</span>에 대한 식을 도출하기 위해 <span class="math inline">\(\frac{1}{n} \sum_{i=1}^{n} \psi_i(\theta_0)\)</span> term을 넘기고 양변에 <span class="math inline">\(\frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T}\)</span>을 inverse하여(Matrix 이므로) 곱해주고 <span class="math inline">\(\sqrt{n}\)</span>을 곱하면,</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta} - \theta_0) \approx - \left( \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right)^{-1} \cdot \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0).
\]</span></p>
<p>가 됩니다. 여기서 다음 두 Matrix들을 정의하겠습니다:</p>
<ul>
<li>
<span class="math display">\[ \mathbf{A} = \mathbb{E}
\left[ -\frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right]
\]</span> (2차 도함수 또는 score function의 미분의 기댓값)</li>
<li>
<span class="math display">\[ \mathbf{B} = \mathbb{E}
\left[ \psi_i(\theta_0) \psi_i(\theta_0)^T \right] \]</span> (score function의 분산의 기댓값)</li>
</ul>
<p>이제 <strong>대수의 법칙(LLN)</strong>과 <strong>중심극한정리(CLT)</strong>를 각각 적용하면 다음 두 식을 얻을 수 있습니다.</p>
<p><span class="math display">\[
-\frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \xrightarrow{p} \mathbf{A}
\]</span> <span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0) \xrightarrow{d} \mathcal{N}(0, \mathbf{B})
\]</span></p>
<p>각 정리를 간단하게 설명드리자면, <strong>대수의 법칙(LLN, Law of Large Numbers)</strong>은 표본 크기 <span class="math inline">\(n\)</span>이 충분히 크면, 표본 평균이 모평균에 점근적으로 수렴한다는 법칙으로, 확률 변수 <span class="math inline">\(X_i\)</span>가 동일 분포이고 기대값 <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span>를 가지면, 수학적으로 표현하면 아래 식과 같습니다.</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} X_i \xrightarrow{p} \mu.
\]</span></p>
<p>즉, 위 식에서는 <span class="math display">\[
- \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T}
\approx - \mathbb{E}\left[ \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right]
= \mathbf{A}
\]</span> 가 되는 것입니다. <strong>중심극한정리(CLT, Central Limit Theorem)</strong>는 독립이고 동일한 분포를 따르는 확률변수들의 표본 평균이 정규 분포를 따른다는 정리로, 분산이 <span class="math inline">\(\sigma^2\)</span>인 확률변수 <span class="math inline">\(X_i\)</span>들에 대해,</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} (X_i - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2).
\]</span> 입니다. (양변에 <span class="math inline">\(\sqrt{n}\)</span>이 나눠진 식이 더 친숙하실 겁니다.) 즉, 위 식에서는 <span class="math inline">\(\psi_i(\theta_0)=0\)</span>이고, 따라서 <span class="math display">\[
\mathbf{B} = \mathbb{E} \left[ \psi_i(\theta_0) \psi_i(\theta_0)^T \right] = Var(\psi_i(\theta_0))
\]</span> 이므로,</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0) \xrightarrow{d} \mathcal{N}(0, \mathbf{B})
\]</span> 임을 확인할 수 있습니다. 정리하자면, 대수의 법칙이 평균값으로의 수렴을 보장한다면, 중심극한정리는 표본 평균이 정규성을 띤다는 것을 보장하고, 이를 통해 우리가 고려하던 아래 식 <span class="math display">\[
\sqrt{n}(\hat{\theta} - \theta_0) \approx - \left( \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right)^{-1} \cdot \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0).
\]</span> 의 우항에 대한 두 정보를 얻을 수 있었습니다. 최종적으로 점근정규성을 보이기 위해선 이 두 수렴하는 분포의 곱을 나타낼 수 있는 <strong>Slutsky 정리</strong>를 보고, 최종적으로 식을 도출하겠습니다. <strong>Slutsky 정리</strong>는 두 개의 점근적 확률 분포를 결합하는 방법으로, 만약 <span class="math inline">\(X_n \xrightarrow{d} X\)</span> (약한 수렴)과, <span class="math inline">\(Y_n \xrightarrow{p} c\)</span> (확률적 수렴)이면,</p>
<p><span class="math display">\[
X_i Y_i \xrightarrow{d} Xc.
\]</span></p>
<p>입니다. 즉, 확률적으로 수렴하는 변수와 분포적으로 수렴하는 변수를 곱하면, 여전히 위 식과 같이 분포적으로 수렴한다는 것이 증명된 정리이고, 위 식에서는</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \xrightarrow{p} A
\]</span></p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0) \xrightarrow{d} \mathcal{N}(0, B)
\]</span> 이므로, Slutsky 정리를 사용하면 최종적으로</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta} - \theta_0) \approx - \left( \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right)^{-1} \cdot \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi_i(\theta_0).
\]</span></p>
<p><span class="math display">\[
\xrightarrow{d} A^{-1} \mathcal{N}(0, B) = \mathcal{N}(0, A^{-1} B A^{-1})
\]</span></p>
<p>입니다.(deteminant한 값은 분산 term에서 제곱된다는 것은 몇 번 보았었습니다.) 결국 M-estimation의 추정을 통해 얻은 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>가 참 모수 <span class="math inline">\(\boldsymbol{\theta}_0\)</span>에 대해 일치성(Consistency)을 갖고, <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>는 점근정규성(Asymptotic Normality)을 갖으며 그 식은 <span class="math inline">\(\mathcal{N}(0, A^{-1} B A^{-1})\)</span>입니다. 또한, 이 샌드위치(sandwich) 형태(<span class="math inline">\(A^{-1}\)</span> 빵 사이에 껴있는 고기 <span class="math inline">\(B\)</span>)처럼 생긴 점근적 분산 식이 바로 Sandwich estimator의 general version입니다. 이제 이 sandwich estimator에 대해 좀더 설명드리겠습니다.</p>
</section><section id="sandwichrobust-estimator" class="level3"><h3 class="anchored" data-anchor-id="sandwichrobust-estimator">1.4. Sandwich(Robust) Estimator</h3>
<hr>
<p>Sandwich(Robust) Estimator의 식은 써보면 다음과 같습니다: <span class="math display">\[
\widehat{\operatorname{Var}}(\hat{\boldsymbol{\theta}}) = A^{-1} B A^{-1}
\]</span> <span class="math display">\[
where, \; \mathbf{A} = - \mathbb{E}\left[ \frac{\partial \psi_i(\theta_0)}{\partial \theta^T} \right],
\; \mathbf{B} = \mathbb{E} \left[ \psi_i(\theta_0) \psi_i(\theta_0)^T \right]
\]</span></p>
<p>2장에서 GLM case에 대해 log likelihood의 1차 도함수를 score function, 이의 negative 2차 도함수를 Fisher Information matrix라고 언급한 적이 있습니다. 이의 general한 버전이 위와 같으며, 여기에서는 이 score function <span class="math inline">\(\psi_i(\theta_0)\)</span>의 분산을 <span class="math inline">\(\mathbf{B}\)</span>, 2차 도함수를 <span class="math inline">\(\mathbf{A}\)</span>로 표기하고 있습니다.</p>
<p><span class="math inline">\(\mathbf{A}\)</span>는 모형의 <strong>곡률(curvature)</strong>을, <span class="math inline">\(\mathbf{B}\)</span>은 모형의 분산을 반영합니다. 또한, Estimating equation이 log likelihood로부터 MLE 철학으로 나온 parameter라면, <span class="math inline">\(\mathbf{A} = \mathbf{B}\)</span>입니다. 이 이유는, 2장에서 2차 도함수가 정의되는 임의의 distribution을 따르는 종속변수 <span class="math inline">\(Y\)</span>와 그의 parameter <span class="math inline">\(\theta\)</span>에 대해서 <span class="math display">\[ \ell'' = \frac{d^2\ell}{d\theta^2} \]</span> 가 참임을 보였고, <span class="math inline">\(\ell''\)</span>은 <span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\frac{d^2\ell}{d\theta^2}\)</span>는 <span class="math inline">\(\mathbf{B}\)</span>와 같기 때문입니다. (<span class="math inline">\(\ell' = \psi\)</span> 이므로.)</p>
<p>즉 철학적으로 해석해보면, Regression Model의 selection이 정확한 경우 Fisher Information 행렬 동일성에 의해 <span class="math inline">\(\mathbf{A} = \mathbf{B}\)</span>가 성립하게 되고, 이에 따라 <strong>parameter의 분산은</strong> <span class="math inline">\(A^{-1}\)</span> <strong>만으로 추정</strong>될 수 있습니다. 그러나 <strong>Regression Model이 정확하지 않은 경우, consistent한 parameter estimation을 한다고 하더라도 이 모델의 추정 분산</strong> <span class="math inline">\(A^{-1}\)</span><strong>은 더이상 신뢰할 수 없으며, 이때 Sandwich estimtor는 경험적 분산(empirical variance)</strong> <span class="math inline">\(\mathbf{B}\)</span><strong>를 통해 robust하게 이를 추정할 수 있습니다.</strong> 즉, Regression Model의 몇 가지 가정이 의심될 때, 심지어는 의심되지 않더라도 Sandwich estimtor는 robust하게 parameter의 분산을 추정할 수 있는 것입니다.</p>
<p>또한 이전에 스포한대로, 이전 장들에서 다루어 왔던 robust한 parameter variance estimator인 Heteroskedasticity-Consistent SE, Cluster-robust SE는 모두 이 Sandwich estimator의 special한 case입니다.(생김새부터 짐작할 수 있으셨을 겁니다.) LM version에서만 이를 증명한 뒤(GLM 버전도 같습니다.), GLM을 복습하고 GEE, GLMM에 대해서 설명드리겠습니다.</p>
<ol type="1">
<li><strong>Prove HC0 is Sandwich estimator. (LM version)</strong></li>
</ol>
<p>OLS의 score function은은 위에서 보았듯 다음과 같습니다:</p>
<p><span class="math display">\[
\psi_i(\beta) = x_i (Y_i - x_i^T \beta).
\]</span> 그렇다면, <span class="math inline">\(A\)</span>는 베타로 미분 후 -를 씌워주면 다음과 같이 계산되며,</p>
<p><span class="math display">\[
A = \mathbb{E} \left[ -\frac{\partial \psi_i(\beta)}{\partial \beta^T} \right] = \mathbb{E} \left[ x_i x_i^T \right].
\]</span></p>
<p><span class="math inline">\(A\)</span>의 추정치는 결국</p>
<p><span class="math display">\[
\hat{A} = \frac{1}{n} \sum_{i=1}^{n} x_i x_i^T = \frac{1}{n} X^T X.
\]</span> 가 됩니다. 마찬가지로 <span class="math inline">\(B\)</span>를 계산하면,</p>
<p><span class="math display">\[
B = \mathbb{E} \left[ \psi_i(\beta) \psi_i(\beta)^T \right] = \mathbb{E} \left[ x_i x_i^T (Y_i - x_i^T \beta)^2 \right].
\]</span></p>
<p>이며, 추정치는</p>
<p><span class="math display">\[
\hat{B} = \frac{1}{n} \sum_{i=1}^{n} x_i x_i^T e_i^2 = \frac{1}{n} X^T \text{diag}(e_i^2) X.
\]</span> 입니다. 결국</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\hat{\beta}) = \hat{A}^{-1} \hat{B} \hat{A}^{-1} = \left( \frac{1}{n} X^T X \right)^{-1} \left( \frac{1}{n} X^T \text{diag}(e_i^2) X \right) \left( \frac{1}{n} X^T X \right)^{-1}.
\]</span></p>
<p><span class="math display">\[
= (X^T X)^{-1} X^T \text{diag}(e_i^2) X (X^T X)^{-1}.
\]</span></p>
<p>가 되고, 이 식은 1장에서 보았던 <strong>HC0</strong>의 식과 동일함을 확인할 수 있습니다.</p>
<ol start="2" type="1">
<li><strong>Prove Clustered-Robust SE is Sandwich estimator. (LM version)</strong></li>
</ol>
<p>이 또한 OLS와 같은 환경이므로(LM, cluster가 <span class="math inline">\(g\)</span>개로 구성되어 있다고 할 때, score function은 다음과 같습니다:</p>
<p><span class="math display">\[
\psi_g(\beta) = \sum_{i \in g} x_i (Y_i - x_i^T \beta).
\]</span></p>
<p><span class="math inline">\(A\)</span>의 식과 추정치 또한 비슷하게 구해지고,</p>
<p><span class="math display">\[
A = \mathbb{E} \left[ -\frac{\partial \psi_g(\beta)}{\partial \beta^T} \right] = \mathbb{E} \left[ \sum_{i \in g} x_i x_i^T \right].
\]</span> <span class="math display">\[
\hat{A} = \frac{1}{n} \sum_{g=1}^{G} \sum_{i \in g} x_i x_i^T = \frac{1}{n} X^T X.
\]</span></p>
<p><span class="math inline">\(B\)</span>도 비슷하게 계산되며, cluster간의 independent는 여전히 가정됩니다.</p>
<p><span class="math display">\[
B = \mathbb{E} \left[ \psi_g(\beta) \psi_g(\beta)^T \right].
\]</span></p>
<p><span class="math display">\[
\hat{B} = \frac{1}{n} \sum_{g=1}^{G} \left( \sum_{i \in g} x_i e_i \right) \left( \sum_{i \in g} x_i e_i \right)^T = \frac{1}{n} \sum_{g=1}^{G} X_g^T \hat{u}_g \hat{u}_g^T X_g.
\]</span></p>
<p>이에 따라 분산의 Sandwich estimator를 구하면</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\hat{\beta}) = \hat{A}^{-1} \hat{B} \hat{A}^{-1} = \left( \frac{1}{n} X^T X \right)^{-1} \left( \frac{1}{n} \sum_{g=1}^{G} X_g^T \hat{u}_g \hat{u}_g^T X_g \right) \left( \frac{1}{n} X^T X \right)^{-1}.
\]</span></p>
<p><span class="math display">\[
= (X^T X)^{-1} \left( \sum_{g=1}^{G} X_g^T \hat{u}_g \hat{u}_g^T X_g \right) (X^T X)^{-1}.
\]</span></p>
<p>이고, 이는 Cluster-robust SE의 식과 동일합니다.</p>
</section><section id="glm-복습" class="level3"><h3 class="anchored" data-anchor-id="glm-복습">1.5. GLM 복습</h3>
<hr>
<p><strong>Generalized Linear Model (GLM)</strong>의 모델 식은 다음과 같이 표현됩니다: <span class="math display">\[
g(\mathbb{E}[Y_i | X_i]) = g(\mu_i) =\eta_i = X_i^T \beta \\
where, Y_i \sim \text{Exponential Family}(\mu_i, \phi).
\]</span> 이때 <span class="math inline">\(g(\cdot)\)</span>은 링크 함수(link function)로 logit, log의 예시를 보았고, <span class="math inline">\(\mu_i = \mathbb{E}[Y_i | X_i]\)</span>는 반응 변수의 기대값으로 모델의 mapping의 목적이 되는 값(예측하고자 하는 값), <span class="math inline">\(\phi\)</span> 분산과 관련된 parameter(dispersion parameter) 로 정규 분포의 경우 <span class="math inline">\(\sigma^2\)</span>였습니다.</p>
<p>간략하게 복습하면 <strong>링크 함수(link function)</strong> <span class="math inline">\(g(\cdot)\)</span>를 통해 <span class="math inline">\(E(Y_i) = \mu_i\)</span>와 <span class="math inline">\(\eta_i = \mathbf{x}_i^\top \boldsymbol{\beta}\)</span>를 연결하고, <strong>분산 함수(variance function)</strong> <span class="math inline">\(V(\mu_i)\)</span>를 이용해 <span class="math inline">\(\operatorname{Var}(Y_i)\)</span>를 표현하며, <strong>추정방정식(estimating equation)</strong>을 세워</p>
<p><span class="math display">\[
\sum_{i=1}^n \frac{ \partial \mu_i}{\partial \boldsymbol{\beta}^\top} \frac{ (Y_i - \mu_i) }{\operatorname{Var}(Y_i)} = \mathbf{0}
\]</span></p>
<p>을 푸는 방식으로 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>를 구합니다.</p>
<p>이 해석 또한 M-estimation의 한 사례로 볼 수 있습니다. GLM에서 score 함수(추정방정식)는 <span class="math inline">\(\psi_i(\boldsymbol{\beta}) = \frac{ \partial \mu_i}{\partial \boldsymbol{\beta}^\top} \frac{ (Y_i - \mu_i) }{\operatorname{Var}(Y_i)}\)</span> 꼴로 정의되며, 이를 0으로 만드는 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>가 <strong>우리가 구하고자 하는 파라미터 추정치</strong>가 됩니다. 2장에서는 GLM의 parameter <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>를 추정하는 방법으로 IRLS(Iteratively Reweighted Least Squares)이나 Newton-Raphson/Fisher Scoring을 소개하였으며, 이는 결국 M-estimation에서 구체적으로 어떻게 “estimating equation을 수치적으로 풀어낼지” 알고리즘으로 구현한 예시 중에 하나였다고 이해할 수 있습니다. 또한, 2장에서 예고한대로, 왜 parameter <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>의 분산이</p>
<p><span class="math display">\[
Var(\hat{\boldsymbol{\beta}}) = -fisher
\]</span> 라고 했었는지 이제 살펴보면, GLM의 추정 또한 M-estimation에 해당하므로 GLM의 estimating equation을 만족하는 estimator에 대해서 위에서 확인한 점근정규성이 만족함을 알 수 있고, 때문에 consistent한 parameter estimator에 대해서 다음과 같은 robust한 Sandwich 분산을 갖습니다.<br><span class="math display">\[
\operatorname{Var}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0) \approx \left( \mathcal{I}(\boldsymbol{\beta}_0) \right)^{-1} \operatorname{Var}(\mathbf{S}(\boldsymbol{\beta}_0)) \left( \mathcal{I}(\boldsymbol{\beta}_0) \right)^{-1}
\]</span></p>
<p>그리고, 계속 보아왔던 것처럼 여기서 <span class="math inline">\(\operatorname{Var}(\mathbf{S}(\boldsymbol{\beta}_0)) = \mathcal{I}(\boldsymbol{\beta}_0)\)</span>가 만족(스코어 함수의 분산의 기댓값과 Fisher information matrix가 같습니다.) 하기 때문에 이를 대입하면 위 식이 아래 처럼 소거되고,<br><br><span class="math display">\[
\operatorname{Var}(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0) \approx \mathcal{I}(\boldsymbol{\beta}_0)^{-1}
\]</span></p>
<p>가 됩니다. 결국 GLM의 <strong>모형 기반 분산</strong>은 다음과 같습니다:</p>
<p><span class="math display">\[ \mathbb{V}ar_{\text{모형}}(\hat{\beta}) = \mathbf{A}^{-1} = \left( \sum_{i=1}^{n} \frac{\partial^2 \log f(Y_i; \beta)}{\partial \beta \partial \beta^T} \right)^{-1}. \]</span></p>
<p>또한, 이때 경험적 분포를 고려하여 Sandwich로 추정한 분산은,</p>
<p><span class="math display">\[ \mathbb{V}ar_{\text{robust}}(\hat{\beta}) = \mathbf{A}^{-1} \mathbf{B} \mathbf{A}^{-1}, \]</span></p>
<p><span class="math display">\[ where, \mathbf{B} = \sum_{i=1}^{n} \psi_i(\hat{\beta}) \psi_i(\hat{\beta})^T, \; \psi_i(\beta) = (Y_i - \mu_i) x_i / V(\mu_i). \]</span></p>
<p>로, 이는 이전에 확인한 HC0의 형태와도 이어집니다.</p>
</section><section id="다시-돌아와서..-for-clustered-data" class="level3"><h3 class="anchored" data-anchor-id="다시-돌아와서..-for-clustered-data">다시 돌아와서.. (for clustered data)</h3>
<p>일반적으로 <strong>선형 모델</strong>(Linear Model)과 <strong>일반화 선형 모델</strong>(Generalized Linear Model, GLM)은 <strong>독립 동일 분포(i.i.d.)</strong>를 가정합니다. 즉, 기존의 GLM은 관측치(observations, data points)들이 서로 독립이며(Independent)일 때 동일한 분산 구조에서 잘 작동합니다. 그러나 학교나 병원 등 군집(클러스터) 단위로 샘플이 묶여 있는, 비슷한 특성을 지닌 대상들을 <strong>클러스터(cluster)</strong>로 묶은 <strong>패널 데이터(panel data)</strong>나 동일한 실험 대상(피험자)에게서 <strong>반복 측정된 데이터(longitudinal data)</strong>의 경우, 같은 cluster(또는 group: 같은 피험자, 같은 단위 등)에 속한 data간에는 correlation이 존재합니다. 때문에 더이상 data들이 독립이 아니게 되며, GLM만으로는 이 상관구조를 모델 자체에서 고려할 수 없기에, <strong>GEE</strong>와 <strong>GLMM</strong> 와 같은, 더욱 general한 Regression Model이 개발되었습니다. 이제 아래에서 위 두 model에 대해서 살펴보겠습니다.<br></p>
</section></section><section id="generalized-estimating-equation-gee" class="level2"><h2 class="anchored" data-anchor-id="generalized-estimating-equation-gee">2. Generalized Estimating Equation (GEE)</h2>
<section id="gee-정의" class="level3"><h3 class="anchored" data-anchor-id="gee-정의">2.1. GEE 정의</h3>
<hr>
<p><strong>GEE (Generalized Estimating Equation)</strong>는 <strong>GLM</strong>이 독립성 가정을 전제로 하는 한계마저 뛰어넘어, <strong>군집(Clustered) 자료</strong>나 <strong>반복측정 자료</strong> 등 <strong>상관구조</strong>가 존재하는 데이터에 적용될 수 있도록 확장한 방법론입니다. 가장 critical하게 다른 점을 보면, GLM은 <span class="math inline">\(\operatorname{Var}(Y_i) = \phi V(\mu_i)\)</span>로 종속변수의 분산을 표현할 때 diagonal matrix로 두어 data points 간에는 correlation이 없음을 표현하였다면, GEE는 <span class="math inline">\(\operatorname{Var}(\mathbf{Y}_i) = \phi \mathbf{A}_i^{1/2}\mathbf{R}_i(\alpha)\mathbf{A}_i^{1/2}\)</span>와 같은 <strong>working correlation</strong> 행렬 <span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>를 사용하여, 관측치들 간의 <strong>상관관계</strong>를(반복 측정, 클러스터 내 상관) 모델에 반영합니다. 또한, 이러한 가정을 위해 종속변수의 확률 모델(공동 확률 분포)을 완전히 명시하지 않아도, <strong>Quasi-Likelihood(준 우도) 접근법</strong>을 통해 <strong>점근적(score) 방정식</strong>을 확장하여 모델을 적합합니다. 간단히 말하면, GEE는 “<strong>평균 모형</strong>은 GLM처럼 유지하되, <strong>상관 구조</strong>를 적절히 지정하여 <strong>군집성</strong>이나 <strong>반복측정</strong>을 고려하자”라는 접근입니다.</p>
<p><strong>LM</strong>이나 <strong>GLM</strong>은 서로 독립적인(i.i.d.) 표본을 가정하여 이를 기반으로 추정하는 반면, GEE에서는 <strong>상관 구조(correlation structure)</strong> <span class="math inline">\(R(\alpha)\)</span>를 추가하여 이러한 독립 가정을 완화하고, 평균 모형과 분산-공분산 구조에 대한 가정을 분리해서 Quasi 형태로 추정합니다. 이 때, Quasi-likelihood에 대한 적용을 짧게 설명하자면, <strong>GEE는 확률 모델을 직접 설정(완전한 공동 확률 밀도 함수 명시)하지 않고, GLM의 log likelihood function에 상관구조를 추가하는 형태로 접근</strong>합니다. 즉 GLM에서 <em>종속 변수의 Exponential family 가정 -&gt; 독립 가정 후 모든 data point의 확률(likelihood)를 곱해서 얻은 likelihood finction -&gt; 로그 씌워서 log likelihood -&gt; model parameter로 미분한 결과인 score function</em> 순으로 추정 과정을 설명했었다면, GEE는 <em>처음부터 직접적인 종속변수의 가정으로 시작하는 대신 log likelihood에서 시작</em>하고, 이 때 독립이 아님을 고려하기 위해 variance function에 상관 행렬 <span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>을 추가하여 <span class="math inline">\(\operatorname{Var}(\mathbf{Y}_i) = \phi \mathbf{A}_i^{1/2}\mathbf{R}_i(\alpha)\mathbf{A}_i^{1/2}\)</span>로 둔 후 추정하는 것입니다. 이러한 접근은, 실제로 종속변수의 완전한 확률적 기반(joint PDF)이 존재하지 않아도, 점근적 성질을 활용하여 <strong>일관된 추정량</strong>을 얻을 수 있고, <strong>오류 항이 독립적이지 않은 경우</strong>에도 GLM과 유사한 방식으로 추정할 수 있는 장점이 있습니다. 마지막으로, GEE는 상관행렬과 Quasi의 개념을 통해 GLM과 같이 data points들을 marginal하게 고려하여 fit하기 때문에 <strong>Population-Average GEE(or 모델)</strong> 이고, 이는 무작위 효과(Random Effect)를 통해 각 실험 단위(피험자)에 특화된 효과를 추정하는 G<strong>LMM(Generalized Linear Mixed Model)</strong>과 철학이 다르며,이 GLMM은 <strong>Subject-Specific GEE(or 모델)</strong>이라고도 부릅니다.</p>
</section><section id="gee-수학적-표현-및-추정" class="level3"><h3 class="anchored" data-anchor-id="gee-수학적-표현-및-추정">2.2. GEE 수학적 표현 및 추정</h3>
<hr>
<p>위에서 언급하였듯, GLM과 동일하게 GEE는 아래와 같은 marginal 모델입니다:<br><span class="math display">\[
g\bigl(\boldsymbol{\mu}_i\bigr) = \mathbf{X}_i \boldsymbol{\beta},
\]</span> 여기서 <span class="math inline">\(\mathbf{Y}_i\)</span>는 (i)번째 클러스터(또는 피험자)에서 나온 <span class="math inline">\(n_i\)</span>개의 관측치 벡터, <span class="math inline">\(\boldsymbol{\mu}_i = E(\mathbf{Y}_i) \; or \; E(\mathbf{Y}_i|X_i)\)</span>입니다. 또한, 언급한 대로 Working correlation을 아래와 같이 설정하며,</p>
<p><span class="math display">\[
\operatorname{Var}(\mathbf{Y}_i) = \phi \mathbf{A}_i^{1/2} \mathbf{R}_i(\alpha) \mathbf{A}_i^{1/2}.
\]</span> 이때 <span class="math inline">\(\mathbf{A}_i\)</span>가 기존 <span class="math inline">\(V(\mu_{ij})\)</span>의 역할 이었다면 이에 루트를 씌워 A라고 두고 (행렬에서의 square root, 또는 1/2 승은 기존 <span class="math inline">\(V\)</span>가 diagonal 이었으므로 이때는 단순히 각 대각 성분을 루트 씌운 값입니다.) 그 사이에 클러스터 당 상관관계를 <span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>로 표현합니다. 이때 상관행렬 <span class="math inline">\(\mathbf{R}(\alpha)\)</span>의 종류로는 크게 아래와 같은 예시들이 있습니다.</p>
<p>(1) Independent (기존 GLM)</p>
<p><span class="math display">\[
R(\alpha) = I, \quad V_k = V_k'
\]</span></p>
<p>(2) Exchangeable Correlation (동일 상관 구조)</p>
<p><span class="math display">\[
R(\alpha) = \begin{pmatrix}
1 &amp; \alpha &amp; \alpha \\
\alpha &amp; \ddots &amp; \alpha \\
\alpha &amp; \alpha &amp; 1
\end{pmatrix}
\]</span></p>
<p>(3) Autoregressive (AR-1)</p>
<p><span class="math display">\[
\text{Corr}(y_{ki}, y_{kj}) = \alpha^{|i-j|}
\]</span></p>
<p><span class="math display">\[
R(\alpha) = \begin{pmatrix}
1 &amp; \alpha &amp; \alpha^2 &amp; \dots &amp; \alpha^{n_k} \\
\alpha &amp; 1 &amp; \alpha &amp; \dots &amp; \alpha^{n_k-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\alpha^{n_k} &amp; \alpha^{n_k-1} &amp; \alpha^{n_k-2} &amp; \dots &amp; 1
\end{pmatrix}
\]</span></p>
<p>(4) Unstructured Form</p>
<p><span class="math display">\[
R(\alpha) = \begin{pmatrix}
1 &amp; \alpha_1 &amp; \alpha_2 &amp; \alpha_3 \\
\alpha_1 &amp; 1 &amp; \alpha_4 &amp; \alpha_5 \\
\alpha_2 &amp; \alpha_4 &amp; 1 &amp; \alpha_6 \\
\alpha_3 &amp; \alpha_5 &amp; \alpha_6 &amp; 1
\end{pmatrix}
\]</span></p>
<p>이러한 상관행렬 <span class="math inline">\(\mathbf{R}(\alpha)\)</span>는 사전에 정의되어야 하므로 분석 대상인 data의 성질에 따라 선정해야 하며, 이러한 관계의 구조를 어떻게 선택하는 지에 따라 분산이 다르게 나오므로, 위에서 다룬 Sandwich를 통한 robust한 추정이 GEE에서 대게 사용됩니다.</p>
<section id="gees-estimating-equation" class="level4"><h4 class="anchored" data-anchor-id="gees-estimating-equation">GEE’s Estimating Equation</h4>
<p>이전에 GLM에서는 다음과 같이 score functions로 부터 estimating equation을 세웠습니다:</p>
<p><span class="math display">\[
\Psi = \sum \psi_i = \sum_{i=1}^{N} \frac{y_i - \mu_i}{a(\phi)V(\mu_i)} \left( \frac{\partial \mu_i}{\partial \eta_i} \right) x_i = 0
\]</span></p>
<p>이제 이를 GLM 때와 다르게 각 cluster <span class="math inline">\(k\)</span>에 대해 <span class="math inline">\(D_k = diag(\left( \frac{\partial \mu_i}{\partial \eta_i} \right) x_i)\)</span>, <span class="math inline">\(V_k = \mathbf{A}_i^{1/2} \mathbf{R}_i(\alpha) \mathbf{A}_i^{1/2}\)</span>라고 하면,</p>
<p><span class="math display">\[
\sum_{k=1}^{K} \frac{1}{a(\phi)}  D_k V_k^{-1} (y_k - \mu_k) = 0
\]</span> 으로 식을 GLM의 score function 으로부터 변형해서 얻을 수 있고, 최종적으로 벡터와 행렬 연산으로 모든 클러스터 <span class="math inline">\(k\)</span>에 대해 block diagonal로 한 번에 표현하면(<span class="math inline">\(V\)</span>), <span class="math display">\[
\Psi = \frac{1}{a(\phi)} D V^{-1} (y - \mu) = 0
\]</span> 가 되며, <span class="math display">\[
\mathbf{U}(\boldsymbol{\beta}) = \sum_{i=1}^{n} \mathbf{D}_i^\top \mathbf{V}_i^{-1} \bigl(\mathbf{Y}_i - \boldsymbol{\mu}_i\bigr) = \mathbf{0},
\]</span> 로 표현할 수 있습니다. 이는 GLM의 score 함수와 같은 시작이지만, GEE는 <span class="math inline">\(\mathbf{V}_i\)</span>가 군집/반복측정 상관을 반영하도록 조작합니다. 결국 이 추정방정식을 품으로써 GEE의 추정이 가능할 것입니다.</p>
<p>가장 중요한 GEE에서 분산 term의 변형을 다시 한 번 강조하자면, <span class="math inline">\(V_k\)</span> 는 cluster 별 (Co)variance 행렬로, data가 independent하다면 <span class="math inline">\(V_k\)</span>와 이를 모두 합친 <span class="math inline">\(V\)</span>가 diagonal matrix가 되지만, 그룹 내 상관을 고려할 경우 <span class="math inline">\(V_k\)</span>가 더 이상 diagonal하지 않고, 이에 따라 <span class="math inline">\(V\)</span>는 block diagonal matrix 형태를 갖습니다. (block diagonal한 이유는 cluster끼리 독립이고 cluster안은 상관관계가 있는 1차 clustered data에서 다룬 2장의 cluster-robust를 떠올리면 좋을 것 같습니다.)</p>
<p><span class="math display">\[
V = \begin{pmatrix}
V_1 &amp; 0 &amp; 0 \\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; V_K
\end{pmatrix}
\]</span></p>
</section><section id="gee-parameter-추정irls" class="level4"><h4 class="anchored" data-anchor-id="gee-parameter-추정irls">GEE parameter 추정(IRLS)</h4>
<p>GEE의 parameter 추정 또한 GLM에서 비롯된 만큼, 이전에 다루었던 방식과 유사한 반복 알고리즘으로 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>를 추정할 수 있습니다. 하나의 스텝을 예시로 들어보면,</p>
<ol type="1">
<li><p>현재 추정치 <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(t)}\)</span>에서, 각 클러스터 <span class="math inline">\(i\)</span>에 대해 <span class="math inline">\(\mathbf{D}_i\)</span> (편미분 행렬), <span class="math inline">\(\boldsymbol{\mu}_i = \mu(\hat{\boldsymbol{\beta}}^{(t)})\)</span>, <span class="math inline">\(\mathbf{V}_i\)</span> (working correlation <span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>, dispersion parameter <span class="math inline">\(\phi\)</span> 포함)을 계산합니다. 이때, <strong>working correlation</strong> <span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>와 <span class="math inline">\(\phi\)</span>도 반복적으로 업데이트됩니다. 예컨대, <code>gee</code>나 <code>geepack</code> 패키지에서는 각 반복 단계에서 <strong>잔차(residual)</strong>를 기반으로 <span class="math inline">\(\alpha\)</span>와 <span class="math inline">\(\phi\)</span>를 재추정하여 새로운 <span class="math inline">\(\mathbf{V}_i\)</span>를 구합니다.</p></li>
<li>
<p>아래 식을 만족하도록 <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(t+1)}\)</span>를 업데이트합니다:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}^{(t+1)}
= \hat{\boldsymbol{\beta}}^{(t)}
- \left( \sum_{i=1}^n \mathbf{D}_i^\top \mathbf{V}_i^{-1} \mathbf{D}_i \right)^{-1}
\sum_{i=1}^n \mathbf{D}_i^\top \mathbf{V}_i^{-1} \bigl(\mathbf{Y}_i - \boldsymbol{\mu}_i\bigr).
\]</span></p>
</li>
<li><p>이전처럼 parameter의 변화량(distance between <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(t+1)}\)</span>and <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(t)}\)</span>)가 특정 threshold 아래로 수렴할 때까지 이 과정을 반복합니다.</p></li>
</ol>
<p>R에서는 <code>geepack</code>이나 <code>gee</code> 라이브러리에서 내부적으로 이러한 절차를 수행합니다. 1에서 어떻게 잔차로부터 <span class="math inline">\(\alpha\)</span>를 추정할 수 있는지 간단하게 예시를 보면 아래와 같습니다. 이는 이전과 원리는 같으며, 상관행렬에서 추정해야 하는 parameter 개수에 따른 자유도를 고려하기 때문에 식이 좀더 복잡해진 것입니다.</p>
<p>잔차는 아래와 같이 계산됩니다(Pearson): <span class="math display">\[
\hat{r}_{ki} = \frac{y_{ki} - \hat{\mu}_{ki}}{\sqrt{V(\hat{\mu}_{ki})}},
\]</span> where <span class="math inline">\(y_{ki}\)</span>는 observed response for cluster <span class="math inline">\(k\)</span> and observation <span class="math inline">\(i\)</span>, <span class="math inline">\(\hat{\mu}_{ki}\)</span>는 predicted mean for cluster <span class="math inline">\(k\)</span> and observation <span class="math inline">\(i\)</span>, <span class="math inline">\(V(\hat{\mu}_{ki})\)</span>는 variance function evaluated at <span class="math inline">\(\hat{\mu}_{ki}\)</span>. 이제 <span class="math inline">\(n_k = n\)</span>라고 가정하면 다음과 같습니다. (이는 클러스터 당 data point 개수가 같다는 가정이며, 이를 만족하지 않아도 식이 복잡해질 뿐 똑같이 계산됩니다.)</p>
<p><strong>(2) Exchangeable Correlation:</strong> <span class="math display">\[
\hat{\alpha} = \frac{1}{a(\phi)} \sum_{k=1}^K \sum_{i &gt; j} \frac{\hat{r}_{ki} \hat{r}_{kj}}{K \cdot \frac{1}{2}n(n-1) - p},
\]</span></p>
<p><strong>(3) Autoregressive (AR-1):</strong> <span class="math display">\[
\hat{\alpha} = \frac{1}{a(\phi)} \sum_{k=1}^K \sum_{i=1}^{n_k - 1} \frac{\hat{r}_{ki} \hat{r}_{k(i+1)}}{K(n-1) - p},
\]</span></p>
<p><strong>(4) Unstructured Form:</strong> <span class="math display">\[
\hat{a}_{ij} = \frac{1}{a(\phi)} \sum_{k=1}^K \frac{\hat{r}_{ki} \hat{r}_{kj}}{K - p},
\]</span></p>
<p>위 식들은 그저 잔차로부터 (co)variance를 추정하는 것일 뿐이고, 분산 term은 degree of freedom을 고려하기 때문에 그저 각각의 상관 행렬 속 미지수(parameter)의 개수에 따른 반영입니다.</p>
</section></section><section id="gee-parameters-variance" class="level3"><h3 class="anchored" data-anchor-id="gee-parameters-variance">2.3. GEE parameter’s Variance</h3>
<hr>
<p>GEE의 <strong>모수 추정치</strong> (<span class="math inline">\(\hat{\boldsymbol\beta}\)</span>)도 <strong>M-estimation</strong>의 범주에 속하므로, 점근 분산은 <strong>Sandwich</strong> 형태를 갖습니다.</p>
<p><span class="math display">\[
\widehat{\operatorname{Var}}(\hat{\boldsymbol\beta})_{\text{robust}}
= \left( \sum_{i=1}^n \mathbf{D}_i^\top \mathbf{V}_i^{-1}
\mathbf{D}_i \right)^{-1} \left( \sum_{i=1}^n
\mathbf{D}_i^\top \mathbf{V}_i^{-1} (\mathbf{Y}_i -
\boldsymbol\mu_i) (\mathbf{Y}_i - \boldsymbol\mu_i)^\top
\mathbf{V}_i^{-1} \mathbf{D}_i \right) \left( \sum_{i=1}^n
\mathbf{D}_i^\top \mathbf{V}_i^{-1} \mathbf{D}_i \right)^{-1}.
\]</span></p>
<p>이를 <strong>robust</strong> 또는 <strong>empirical</strong> 표준오차라고 하며, 실질적으로 <strong>상관구조</strong> (<span class="math inline">\(\mathbf{R}_i(\alpha)\)</span>)가 부정확하게 지정되었을지라도 일관성을 보장해 줍니다. 즉, 위 M-estimation으로 GLM을 해석할 때와 일치하게, <strong>Model-based SE</strong> 는 <span class="math inline">\(\mathbf{A}^{-1}\)</span>만을 사용해서 계산하는 것이고, 이는 설정한 working correlation(상관행렬) 가정이 정확하다고 믿을 때이기 때문에, 이를 신뢰할 수 없는 경우 거의 무조건 <strong>Robust SE</strong><span class="math inline">\(\mathbf{A}^{-1}\mathbf{B}\mathbf{A}^{-1}\)</span> 를 사용합니다. 이는 R에서 GEE를 계산할 때 <strong>기본 SE</strong>(model-based)와 <strong>robust SE</strong>(empirical) 두 가지가 함께 리포팅되는 이유이기도 합니다.<br></p>
</section></section><section id="generalized-linear-mixed-model-glmm" class="level2"><h2 class="anchored" data-anchor-id="generalized-linear-mixed-model-glmm">3. Generalized Linear Mixed Model (GLMM)</h2>
<section id="glmm-정의" class="level3"><h3 class="anchored" data-anchor-id="glmm-정의">3.1. GLMM 정의</h3>
<hr>
<p><strong>GLMM(Generalized Linear Mixed Model)</strong>은, 우리가 이미 익숙하게 다뤄온 <strong>GLM(Generalized Linear Model)</strong>을 GEE와는 다른 방식으로 (Mixed model) “군집(cluster) 또는 계층적 구조를 가지는 자료”에까지 확장하기 위한 방법론입니다. 즉, GLMM은 이러한 <strong>내재된 상관(또는 군집성)</strong>을 모델화하기 위해서 <strong>고정 효과 + 무작위 효과</strong>의 결합으로 모형을 설정합니다. 즉, GLMM은</p>
<ul>
<li>
<strong>고정 효과(fixed effects)</strong>: 전체 모집단에 공통적으로 적용되는 회귀계수(예: 전체 평균 경향)에 해당,</li>
<li>
<strong>무작위 효과(random effects)</strong>: 피험자(또는 군집, 클러스터)별로 달라지는 편차(“개인별 random intercept” 혹은 “개인별 random slope” 등)를 도입</li>
</ul>
<p>을 둘다 고려하는 모델이며, 즉 <strong>“Generalized Linear Model + Linear Mixed Model(Random Effects)”의 결합</strong>이라고 요약할 수 있습니다. GEE와 비교하여 이 GLMM은 각 cluster(또는 group)마다 직접적인 고려를 모델에 넣기 때문에(random effect) <strong>Subject-Specific 모델(또는 GEE)</strong>라고도 불리며, 이는 Population-Average GEE와 대비되는 특징입니다. 무작위 효과는 <strong>정규분포</strong>로 가정하는 것이 일반적이며, 경우에 따라서는 다른 분포(예: Gamma)로 가정하기도 하고, GLMM에서은 이러한 LMM을 GLM으로 확정한 것이기 때문에 종속변수의 분포를, <strong>Exponential Family</strong>로 확장합니다.<br></p>
</section><section id="lmm-수학적-표현-및-추정" class="level3"><h3 class="anchored" data-anchor-id="lmm-수학적-표현-및-추정">3.2. LMM 수학적 표현 및 추정</h3>
<hr>
<p>GLMM을 이해하기 위해서는 먼저 <strong>선형혼합모형(LMM; Linear Mixed Model)</strong>을 확실하게 이해할 필요가 있습니다. (이 LMM과 GLMM을 완벽하게 이전처럼 분석하려면 내용이 산만해지기 때문에 여기선 중요한 점을 위주로 짚고, 추가적인 공부가 필요하신 분들은 위키피디아에서 비롯되는 교재 및 논문 내용들을 집중적으로 살펴보시면 좋을 것 같습니다.) LMM은 종속변수 <span class="math inline">\(Y\)</span>의 분포가 정규분포라는 전제하에서, <strong>고정 효과(fixed effects)</strong>와 <strong>무작위 효과(random effects)</strong>가 동시에 존재한다고 보는 모형입니다.</p>
<section id="lmm-수학적-표현" class="level4"><h4 class="anchored" data-anchor-id="lmm-수학적-표현">LMM 수학적 표현</h4>
<hr>
<p>가장 단순한 형태의 LMM(임의절편 모형, random intercept model)을 생각해 보겠습니다. (이때 LMM에서 모형을 나누는 기준은 random effect, 즉 group을 어느 정도로 복잡하게 고려하는 지에 따른 설계의 차이입니다. random effect의 분포, 차원 등을 다양하게 고려할 수 있겠지요.) 예를 들어, <span class="math inline">\(i\)</span>번째 클러스터(또는 피험자) 내에서 <span class="math inline">\(j\)</span>번째 관측값을 나타내는 <span class="math inline">\(Y_{ij}\)</span>를 다음과 같이 모델링합니다:</p>
<p><span class="math display">\[
Y_{ij} = \beta_0 + \beta_1 X_{ij} + b_i + \varepsilon_{ij},
\quad i=1,\dots,K,\quad j=1,\dots,n_i.
\]</span></p>
<p>이때 <span class="math inline">\(\beta_0, \beta_1\)</span>은 <strong>고정 효과(fixed effects)</strong> parameter, 즉 모든 클러스터에 공통 적용되는 평균적인 효과)이고, <span class="math inline">\(b_i\)</span>는 <strong>무작위 효과(random effect)</strong> parameter로, 클러스터 <span class="math inline">\(i\)</span>마다 서로 다른 절편(intercept) 편차를 갖는 것을 모델링하고 있습니다. <span class="math inline">\(\varepsilon_{ij}\)</span>는 흔히 오차항(residual)으로 간주하고, 대게 <span class="math inline">\(\varepsilon_{ij} \sim N(0, \sigma^2)\)</span>로 가정합니다.</p>
<p>추가적으로, 무작위 효과 <span class="math inline">\(b_i\)</span>는 다음과 같은 분포로 가정합니다:</p>
<p><span class="math display">\[
b_i \sim N(0,\;\tau^2).
\]</span></p>
<p>이는 “(피험자마다) 임의로 달라지는 절편(intercept)”이 정규분포를 따른다는 것을 의미합니다. 모든 <span class="math inline">\(b_i\)</span>를 <strong>독립</strong>으로 가정하면,</p>
<p><span class="math display">\[
\operatorname{Var}(b_i) = \tau^2,\quad \operatorname{Var}(\varepsilon_{ij}) = \sigma^2.
\]</span></p>
<p>결국, 어떤 <span class="math inline">\(Y_{ij}\)</span>에 대해서는</p>
<p><span class="math display">\[
Y_{ij} = \beta_0 + b_i + \beta_1 X_{ij} + \varepsilon_{ij},
\]</span></p>
<p>이고,</p>
<p><span class="math display">\[
\operatorname{Var}(Y_{ij}) = \tau^2 + \sigma^2.
\]</span></p>
<p>이먀, 더 일반화 된 모델로 무작위 절편 + 무작위 기울기(random intercept + random slope)를 도입하여 독립변수 <span class="math inline">\(X\)</span>에 대해서도 개인별로 기울기가 달라지도록 만들 수 있습니다. 이 경우,</p>
<p><span class="math display">\[
Y_{ij}
= (\beta_0 + b_{0i}) + (\beta_1 + b_{1i}) X_{ij} + \varepsilon_{ij},
\quad
b_{0i} \sim N(0,\;\tau_{00}),\;
b_{1i} \sim N(0,\;\tau_{11}),\;
\operatorname{Cov}(b_{0i}, b_{1i}) = \tau_{01}.
\]</span></p>
<p>가 될 것입니다. 이처럼 무작위 효과를 하나 혹은 여러 개 갖는다는 것은, “클러스터마다 고유하게 발생하는 변동”을 모델에 포함하는 방식으로, LMM은 이러한 방식로 <strong>상관구조</strong>를 모델링 해낸다고 생각할 수 있습니다. 이를 벡터와 행렬 형태로 표현해보면, 각 클러스터(또는 피험자) <span class="math inline">\(i\)</span>에 대해</p>
<p><span class="math display">\[
\mathbf{Y}_i
= \mathbf{X}_i\,\boldsymbol{\beta}
\;+\; \mathbf{Z}_i\,\mathbf{b}_i
\;+\; \boldsymbol{\varepsilon}_i,
\]</span></p>
<ul>
<li>
<span class="math inline">\(\mathbf{Y}_i\)</span>: <span class="math inline">\(i\)</span>번째 클러스터에서의 <span class="math inline">\(n_i\)</span>차원 응답벡터</li>
<li>
<span class="math inline">\(\mathbf{X}_i\)</span>: <span class="math inline">\(n_i \times p\)</span> 차원의 <strong>고정 효과 설계 행렬</strong>(fixed effect parameter <span class="math inline">\(\boldsymbol{\beta}\)</span>와 매칭)</li>
<li>
<span class="math inline">\(\mathbf{Z}_i\)</span>: <span class="math inline">\(n_i \times q\)</span> 차원의 <strong>무작위 효과 설계 행렬</strong>(random effect parameter <span class="math inline">\(\mathbf{b}_i\)</span>와 매칭)</li>
<li>
<span class="math inline">\(\mathbf{b}_i \sim N(\mathbf{0}, \boldsymbol{G})\)</span> 이며 <span class="math inline">\(\boldsymbol{G}\)</span>는 <span class="math inline">\(q \times q\)</span> 공분산 행렬</li>
<li>
<span class="math inline">\(\boldsymbol{\varepsilon}_i \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_{n_i})\)</span>로 일반적으로 가정(독립 동일 분포)</li>
</ul>
<p>여기서 설계 행렬이란, 1장의 LM에서부터 사용하였지만, data point(observation)당 미리 input으로 지정되는 행렬로, 정확한 의미는 “일련의 개체에 대한 설명 변수 값을 나열한 행렬로 각 행은 개별 개체를 나타내며, 열은 해당 개체에 대한 변수 및 특정 값에 해당한다”입니다. X는 계속 봐왔지만 Z는 이번에 처음 나온 설계 행렬인데, 이는 각 data point마다 해당되는 cluster에는 1, 해당되지 않는 나머지 cluster는 0의 값을 갖는, cluster를 선택하는 스위치 느낌으로, input으로 정해지는 행렬이라고 생각하시면 됩니다.</p>
<p>이 LMM의 (Co) variance matrix는 단순하게 분산 term을 씌우면 random한 (determinant하지 않은) 항만 남아 다음과 같이 계산 될 것입니다:</p>
<p><span class="math display">\[
\operatorname{Var}(\mathbf{Y}_i)
= \mathbf{Z}_i\,\boldsymbol{G}\,\mathbf{Z}_i^\mathsf{T}
+ \sigma^2\,\mathbf{I}_{n_i}.
\]</span></p>
</section><section id="lmms-parameter-추정maximum-likelihood-reml" class="level4"><h4 class="anchored" data-anchor-id="lmms-parameter-추정maximum-likelihood-reml">LMM’s parameter 추정(Maximum Likelihood, REML)</h4>
<hr>
<p>이 LMM에서 <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\boldsymbol{G}\)</span> (또는 <span class="math inline">\(\tau^2\)</span> 등), <span class="math inline">\(\sigma^2\)</span> 모두 미지수입니다. 이를 추정하기 위해 주로 <strong>최대우도추정법(ML; Maximum Likelihood)</strong> 또는 <strong>제한최대우도추정법(REML; Restricted Maximum Likelihood)</strong>을 사용합니다. 각각이 어떻게 계산될지 설명드리면 다음과 같습니다.</p>
<p><strong>(1) With ML(MLE).</strong><br><span class="math inline">\(b_i\)</span>가 정규분포라는 가정 하에, <span class="math inline">\(\mathbf{Y}_i\)</span>의 <strong>joint distribution</strong>도 다변량 정규분포로 표현할 수 있습니다. 모든 <span class="math inline">\(i\)</span>에 대해 <span class="math inline">\(b_i\)</span> 또는 <span class="math inline">\(\mathbf{Y}_i\)</span>가 독립이라 가정하면(cluster간은 독립), 전체 자료의 joint density를 곱해서 <strong>likelihood 함수</strong>를 정의할 수 있고, 이를 최대화하는 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>와 <span class="math inline">\(\hat{\boldsymbol{G}}\)</span>, <span class="math inline">\(\hat{\sigma}^2\)</span>를 찾으면 됩니다. 단점으로는, ML은 <span class="math inline">\(\boldsymbol{\beta}\)</span> 추정에서 편향(bias)이 발생할 수 있어, 표본 크기가 작거나 모형 구조가 복잡해질 때 문제될 수 있습니다. 수식을 중요 부분만 전개해보면, LMM에서 모든 <span class="math inline">\(b_i\)</span>를 각각 적분하여(즉 클러스터 마다 적분) 얻은 <span class="math inline">\(\mathbf{Y}_i\)</span>의 분포는</p>
<p><span class="math display">\[
\mathbf{Y}_i \sim N(\mathbf{X}_i \beta, V_i) \quad \text{with} \quad V_i = \mathbf{Z}_i \mathbf{G} \mathbf{Z}_i^T + \sigma^2 I_{n_i}.
\]</span></p>
<p>입니다. 이제 <span class="math inline">\(i\)</span>번째 클러스터 자료 <span class="math inline">\(\mathbf{Y}_i\)</span>에 대한 log-likelihood를 계산해보면, 다차원 정규분포이므로 다음과 같이 나옵니다:</p>
<p><span class="math display">\[
\ell_i(\beta, \mathbf{G}, \sigma^2) = -\frac{1}{2} \left[ n_i \log(2\pi) + \log |V_i| + (\mathbf{Y}_i - \mathbf{X}_i \beta)^T V_i^{-1} (\mathbf{Y}_i - \mathbf{X}_i \beta) \right].
\]</span></p>
<p>이를 전체 <span class="math inline">\(K\)</span>개의 cluster에 대해 모두 합하면, 전체 자료에 대한 log-likelihood 함수 <span class="math inline">\(\ell(\beta, \mathbf{G}, \sigma^2)\)</span>가 도출될 것입니다.</p>
<p><span class="math display">\[
\ell(\beta, \mathbf{G}, \sigma^2) = \sum_{i=1}^{K} \ell_i(\beta, \mathbf{G}, \sigma^2).
\]</span></p>
<p>MLE(<span class="math inline">\(\hat{\beta}_{ML}, \hat{\mathbf{G}}_{ML}, \hat{\sigma}^2_{ML}\)</span>)를 구하기 위해서는 위의 log-likelihood를 <span class="math inline">\(\beta, \mathbf{G}, \sigma^2\)</span>에 대해 미분하여 0이 되게 하는 해를 찾으면 됩니다. 그러나 일반적으로 <span class="math inline">\(\mathbf{G}\)</span>와 <span class="math inline">\(\sigma^2\)</span>에 대한 미분은 해석적으로 단순화하기 어렵고, 또한 <span class="math inline">\(\mathbf{G}\)</span>가 양의 정부호(positive definite)가 되어야 하는 제약이 있으므로, 수치적 최적화(EM 알고리즘, Newton-Raphson, Fisher scoring 등)를 사용해야 합니다.</p>
<p><strong>(2) With REML.</strong><br>
이는 ML를 직접 바로 계산하는 대신, 고정 효과 <span class="math inline">\(\boldsymbol{\beta}\)</span>와 관련이 없는 term을 이용해 <span class="math inline">\(\boldsymbol{G}\)</span>와 <span class="math inline">\(\sigma^2\)</span>를 먼저 추정한 후 모델을 추정하는 방식입니다. 일반적으로 LMM 을 추정할 때는 REML이 <strong>고정 효과</strong> 추정에 대한 편의를 줄여주고, 분산 요소에 대한 추정이 좀 더 안정적이기 때문에 ML보다 선호됩니다. 이는 모델에서 <strong>무작위 효과를 적분(marginal likelihood)</strong>하는 접근을 통해 <span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{G}, \sigma^2\)</span>에 대한 우도 함수를 세우고(restricted likeli hood), 이 함수를 최대화하는 방식으로 진행됩니다. 실제 계산은 <strong>Iterative 알고리즘(EM 알고리즘, 또는 Fisher scoring, Newton-Raphson 등)</strong>을 사용합니다. 식을 보면,</p>
<p><span class="math display">\[
\ell_{REML} (\mathbf{G}, \sigma^2) = -\frac{1}{2} \left[ \sum_{i=1}^{K} \log |V_i| + \log |\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X}| + (\mathbf{Y} - \mathbf{X} \hat{\beta})^T \mathbf{V}^{-1} (\mathbf{Y} - \mathbf{X} \hat{\beta}) \right] + \text{const}.
\]</span></p>
<p>이고, 이때 <span class="math inline">\(\mathbf{V} = \text{blockdiag}(V_1, \dots, V_K)\)</span>, <span class="math inline">\(\hat{\beta}\)</span>는 <span class="math inline">\(\mathbf{G}, \sigma^2\)</span>가 주어졌을 때의 일반화최소제곱(GLS) 해입니다. 여기서 <span class="math inline">\(\log |\mathbf{X}^T \mathbf{V}^{-1} \mathbf{X}|\)</span>가 REML에서 추가로 나타나는 항으로, 이것이 <span class="math inline">\(\beta\)</span>를 제거(또는 <span class="math inline">\(\beta\)</span>에 무관한 부분만 모아놓음)하여 우선적으로 식을 전개한 효과라고 이해할 수 있습니다. REML에서는 이 <span class="math inline">\(\ell_{REML} (\mathbf{G}, \sigma^2)\)</span>를 <span class="math inline">\(\mathbf{G}, \sigma^2\)</span>에 대해 최대화한 뒤, 그 해 <span class="math inline">\(\hat{\mathbf{G}}_{REML}, \hat{\sigma}^2_{REML}\)</span>를 이용해 최종적으로 <span class="math inline">\(\hat{\beta}_{REML}\)</span> 을 구합니다. REML은 ML보다 fixed effect estimator의 편향 문제가 덜하며, 분산 성분 <span class="math inline">\(\mathbf{G}, \sigma^2\)</span>에 대해 좀 더 안정적인 추정을 제공합니다. 특히 샘플이 작거나 무작위효과 구조가 복잡할 때 일반적으로 더욱 안정적인 REML을 권장하는 편입니다.</p>
<p>이를 통해 얻은 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\boldsymbol{G}}\)</span>, <span class="math inline">\(\hat{\sigma}^2\)</span>는 <strong>LMM의 MLE(or REML) 추정치</strong>이며, R에서는 <code>lme4</code> 패키지 등에서 이 과정을 내부적으로 수행합니다.<br></p>
</section></section><section id="glmm의-수학적-표현-및-추정" class="level3"><h3 class="anchored" data-anchor-id="glmm의-수학적-표현-및-추정">3.3. GLMM의 수학적 표현 및 추정</h3>
<hr>
<p>이제 LMM에서 정규 오차항을 일반화하여, 종속변수가 이항, 포아송, 혹은 다른 지수분포족을 따를 수 있도록 확장하면, <strong>GLMM</strong>으로 이어집니다. GLMM은</p>
<p><span class="math display">\[
g\bigl(\mu_{ij}\bigr)
= \mathbf{x}_{ij}^\mathsf{T}\,\boldsymbol{\beta}
+ \mathbf{z}_{ij}^\mathsf{T}\,\mathbf{b}_i
\]</span></p>
<p><span class="math display">\[
where, \; \mathbf{b}_i \sim N(\mathbf{0}, \boldsymbol{G}),
\]</span></p>
<p><span class="math display">\[
Y_{ij} \mid \mathbf{b}_i \sim \text{Exponential Family}(\mu_{ij}, \phi),
\]</span></p>
<p>의 구조입니다. 직관적으로도 GLMM은 LMM+GLM임을 볼 수 있고, 당연히 <span class="math inline">\(g(\cdot)\)</span>는 link function으로, GLM과 마찬가지로 <span class="math inline">\(\mu_{ij} = E(Y_{ij} \mid \mathbf{b}_i)\)</span>를 <strong>적절한 링크 함수</strong> <span class="math inline">\(g\)</span>로 mapping하는 함수이며, 예시로 binomial case에서 로짓 링크(logit)를 사용하면 <span class="math inline">\(Y_{ij}\)</span>는 0 또는 1 값을 가지는 이항분포가 될 수 있고, 이는 <span class="math inline">\(\log\bigl(\mu_{ij}/(1-\mu_{ij})\bigr)\)</span>를 회귀식을 표현하는 것이었습니다.</p>
<p>이때, 위 식의 likelihood는</p>
<p><span class="math display">\[
\mathbf{Y}_i \mid \mathbf{b}_i
\sim \prod_{j=1}^{n_i} f\bigl(Y_{ij}\mid \mu_{ij}(\mathbf{b}_i)\bigr),
\]</span></p>
<p>로 쓸 수 있으며, <span class="math inline">\(\mathbf{b}_i\)</span>를 적분(marginalizing over <span class="math inline">\(\mathbf{b}_i\)</span>)하면,</p>
<p><span class="math display">\[
p(\mathbf{Y}_i)
= \int
\prod_{j=1}^{n_i}
f\bigl(Y_{ij}\mid \mu_{ij}(\mathbf{b}_i)\bigr)\,
\varphi\bigl(\mathbf{b}_i\bigr)\,
d\mathbf{b}_i,
\]</span></p>
<p>가 최종적으로 <strong>cluster</strong> <span class="math inline">\(i\)</span>에 대한 (marginal) 분포를 만들어냅니다.</p>
<p><span class="math display">\[
p(\mathbf{Y}_i) = \int \left[ \prod_{j=1}^{n_i} f(Y_{ij} | \mu_{ij} (b_i), \phi) \right] \varphi (b_i) db_i, \quad \varphi (b_i) = \frac{1}{\sqrt{(2\pi)^q |\mathbf{G}|}} \exp \left(-\frac{1}{2} b_i^T \mathbf{G}^{-1} b_i \right).
\]</span></p>
<p>모든 <span class="math inline">\(\mathbf{Y}_i\)</span>가 (조건부) 독립이라면, 전체 자료에 대한 marginal likelihood는</p>
<p><span class="math display">\[
L(\beta, \mathbf{G}, \phi) = \prod_{i=1}^{K} \int \prod_{j=1}^{n_i} f(Y_{ij} | \mu_{ij} (b_i), \phi) \varphi (b_i) db_i.
\]</span></p>
<p>입니다. 문제는 <span class="math inline">\(\mu_{ij} (b_i)\)</span>가 비선형이기 때문에 적분이 <strong>closed-form</strong>으로 풀리지 않는 경우가 대부분이라는 것이고, 따라서 실제로는 이 적분을 <strong>수치적(또는 근사적)</strong>으로 계산한 뒤, 그 결과(근사치)를 최대화하여 <span class="math inline">\(\hat{\beta}, \hat{\mathbf{G}}, \hat{\phi}\)</span>를 구하게 됩니다.</p>
<section id="glmms-parameter-추정marginal-likelihood-approximation" class="level4"><h4 class="anchored" data-anchor-id="glmms-parameter-추정marginal-likelihood-approximation">GLMM’s parameter 추정(Marginal Likelihood &amp; Approximation)</h4>
<hr>
<p>다시 한 번 언급하자면 문제는 <span class="math inline">\(\mathbf{b}_i\)</span>가 랜덤효과이므로 이를 적분해야 한다는 것인데, <span class="math inline">\(\mu_{ij}(\mathbf{b}_i)\)</span>가 <strong>비선형</strong>이기 때문에 이 적분이 <strong>closed-form</strong>으로 표현되지 않는 것이고, 다음과 같은 <strong>근사화</strong> 기법을 사용하여 계산합니다.</p>
<ul>
<li><p><strong>Laplace Approximation</strong><br><span class="math inline">\(\mathbf{b}_i\)</span> 주변에서 2차 근사를 수행하여 적분을 근사화하는 방법입니다. 한 번(1차) 또는 고차(AGQ, Adaptive Gauss-Hermite Quadrature) 버전으로 더 정확하게 시도할 수 있습니다.</p></li>
<li><p><strong>Gauss-Hermite Quadrature</strong><br>
적분을 수치적(Numerical)으로 가까운 근사값으로 계산합니다. 무작위 효과 차원이 높아질수록 계산량이 기하급수적으로 늘어날 수 있으므로, 실무에서는 차원이 작은 랜덤 효과 구조(예: 랜덤 인터셉트만)에서 자주 사용합니다.</p></li>
<li><p><strong>Penalized Quasi-Likelihood (PQL)</strong><br>
고전적으로 제안된 근사 기법으로, GLM의 IRLS 절차를 변형하여 무작위효과를 순차적으로 추정합니다. 데이터가 크거나, 근사 정밀도가 크게 중요하지 않은 상황에서 가볍게 쓰일 수 있습니다.</p></li>
</ul>
<p>최종적으로, (1) 적분으로 정의된 <strong>marginal likelihood</strong>를 (2) 수치적 근사화를 통해 (3) 최적화(예: Newton-Raphson, EM 등)하여, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\boldsymbol{G}}\)</span>, <span class="math inline">\(\hat{\phi}\)</span> 등을 찾습니다.</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}, \;\hat{\boldsymbol{G}}, \;\hat{\phi}
= \underset{\boldsymbol{\beta}, \boldsymbol{G}, \phi}{\mathrm{argmax}}
\;\;\Bigl\{
\prod_{i=1}^{K}
\int
\prod_{j=1}^{n_i} f\bigl(Y_{ij}\mid \mu_{ij}(\mathbf{b}_i), \phi\bigr)
\, \varphi(\mathbf{b}_i)\, d\mathbf{b}_i
\Bigr\}.
\]</span></p>
</section><section id="glmm-vs.-gee" class="level4"><h4 class="anchored" data-anchor-id="glmm-vs.-gee">GLMM vs.&nbsp;GEE</h4>
<hr>
<p>이 data간 상관관계를 고려하기 위해 개발된 두 모델을 짧게 정리해보면, <strong>GEE</strong>는 “Population-Average” 접근으로 군집 내 상관을 <strong>working correlation</strong> 방식으로 모델링하며, 완전한 joint PDF를 명시하지 않고 Quasi-likelihood처럼 추정하는 기법이었고, GLMM은 “Subject-Specific” 접근으로 군집/클러스터 효과를 <strong>무작위 효과</strong>로 모델링하여 종속변수를 (조건부) Exponential Family distribution으로 가정하고, 이 likelihood를 marginal하게 적분함으로써 추정합니다.<br></p>
</section></section><section id="glmm-parameters-variance" class="level3"><h3 class="anchored" data-anchor-id="glmm-parameters-variance">3.4. GLMM parameter’s Variance</h3>
<hr>
<p>마지막으로, GLMM에서의 추정된 파라미터(고정 효과 <span class="math inline">\(\boldsymbol{\beta}\)</span>, 무작위 효과 분산-공분산 행렬 <span class="math inline">\(\boldsymbol{G}\)</span> )의 분산 추정(표준 오차, 신뢰구간 등) 방법을 보겠습니다. GLMM의 경우, 근사화하여 최대화한 <strong>marginal log-likelihood</strong>에서의 <strong>헤시안 행렬(Hessian)</strong>을 기반으로 고정효과 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> 의 분산을 추정할 수 있습니다. 구체적으로, 아래와 같은 일반적 형식을 취합니다:</p>
<p><span class="math display">\[
\widehat{\operatorname{Var}}(\hat{\boldsymbol{\beta}})
=
\bigl[
  -\nabla^2_{\boldsymbol{\beta},\boldsymbol{\beta}}
   \,\ell(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{G}}, \hat{\phi})
\bigr]^{-1},
\]</span></p>
<p>여기서 <span class="math inline">\(\ell\)</span>은 GLMM의 (근사) marginal log-likelihood, <span class="math inline">\(\nabla^2_{\boldsymbol{\beta},\boldsymbol{\beta}}\)</span>는 고정효과 파라미터 <span class="math inline">\(\boldsymbol{\beta}\)</span>에 대한 2차 미분(Hessian)으로, 이 헤시안을 (적절한 수치 방법으로) 근사화하여 얻고, 그 역행렬이 분산 추정의 결과에 해당합니다. <strong>이는 여전히 log likelihood을 통한 추정이기 때문에 Fisher information matrix로 분산을 추정</strong>한다고 생각하면 될 것 같습니다. GEE(2장에서)와 마찬가지로, GLMM에서도 모델의 설계에서의 작은 misspecification이 있을 가능성을 고려하여 안정적으로 Sandwich estimator를 통해 추정할 수 있는지 고민할 수 있습니다. GLMM의 경우, 군집 간 독립 이나 군집 내 random effect의 정규성 가정과 같은 가정이 크게 벗어나지 않는다고 믿으면 위 <strong>모델 기반(model-based)</strong> 추정 분산을 사용하면 되고, 그렇지 않은 <em>“무작위 효과 분포가 정규가 아닐 가능성”</em> 혹은 <em>“link/variance function 형태가 부정확할 가능성”</em> 등을 고려하기 위해 적절한 <strong>샌드위치 추정(sandwich-type variance)</strong> 기법을 시도할 수도 있습니다. 다만, GEE와 달리 GLMM에서의 robust variance estimation은 쉽게 구현되지 않으며, <strong>근사기법, 부트스트랩(bootstrap)</strong> 등을 통해 대안적으로 접근하는 사례도 많습니다.</p>
<p>random effect의 분산-공분산 행렬 <span class="math inline">\(\boldsymbol{G}\)</span> 또한 우도(또는 제한 우도)에서 <strong>편미분이 0</strong> 조건을 이용하여 추정하지만, 그 표준 오차(불확실성)를 추정하는 과정 역시 (1) 2차 미분, (2) 프로파일(profile) likelihood, (3) 수치적 근사화 등을 거쳐야 합니다.<br></p>
</section></section><section id="r-코드-예제-gee-glmm" class="level2"><h2 class="anchored" data-anchor-id="r-코드-예제-gee-glmm">4. R 코드 예제: GEE, GLMM</h2>
<p>아래 R 코드를 복사하여 로컬 환경에서 돌려보세요.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">#library(nlme)</span></span>
<span><span class="co">#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)</span></span>
<span><span class="co">#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## GEE 모델 적합 (Exchangeable 상관 구조)</span></span>
<span><span class="co">#library(geepack)</span></span>
<span><span class="co">#gee_fit &lt;- geeglm(binary ~ age + Sex,</span></span>
<span><span class="co">#                 id = Subject,          # 클러스터 변수</span></span>
<span><span class="co">#                 data = Orthodont,</span></span>
<span><span class="co">#                 family = binomial,</span></span>
<span><span class="co">#                 corstr = "exchangeable")  # 동일 상관 가정</span></span>
<span><span class="co">#summary(gee_fit)  # 결과 출력</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## GLMM 모델 적합 (랜덤 절편 모델)</span></span>
<span><span class="co">#library(lme4)</span></span>
<span><span class="co">#glmm_fit &lt;- glmer(binary ~ age + Sex + (1|Subject),  # 랜덤 절편</span></span>
<span><span class="co">#                 data = Orthodont,</span></span>
<span><span class="co">#                 family = binomial)</span></span>
<span><span class="co">#summary(glmm_fit)  # 결과 출력</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="마무리하며" class="level2"><h2 class="anchored" data-anchor-id="마무리하며">마무리하며</h2>
<p>이번 장에서는 M-estimation 개념부터 시작하여, GLM이 어떻게 “estimating equation”의 한 사례로 해석되는지, GEE가 GLM을 확장하여 상관구조를 모델링하고, robust 분산을 제공함으로써 군집/반복측정 데이터를 다루는 과정을, GLMM이 임의효과를 통해 계층적 구조를 명시적으로 모델링하는 방식을 자세히 살펴보았습니다. 그리고 샌드위치 추정량(robust variance) 형태가 M-estimation의 일반 이론에서 비롯된다는 점도 수식과 함께 설명했습니다.</p>
<p>정리하자면, M-estimation은 MLE, OLS, GEE, GLMM 모두를 포괄하는 추정 이론적 틀로서, 샌드위치 분산은 그 점근 정규성(Asymptotic Normality)의 결과물이며, GEE는 marginal mean에 주목하고 robust한 표준오차를 산출해주는 반면, GLMM은 임의효과를 통해 개체별(군집별) 차이를 직접 모델링합니다. 실제 데이터 분석에서는 연구 목적(개체별 효과 추정 vs 전체 평균 효과 추정), 데이터 특성(정확한 상관 구조 가정 vs 모형 가정의 유연성) 등을 종합하여 GEE와 GLMM 중 적절한 접근을 택하거나 비교하는 것이 중요합니다. 사실 Regression Model에는 이번 블로그 “Exploring Regression Models for Regression Analysis”에서 다룬 모델들을 제외하고도 아주 다양한 철학과 수식을 가진 모델들이 있습니다. 다만 여기서는 의학 분석에서 자주 사용되는 모델을 다루었으며, 이를 어느 정도 이해하셨다면 이외의 모델을 이해하는 데에 부족함이 없을 것이라고 생각합니다.</p>


</section><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{seungjun2025,
  author = {Seungjun, Lee},
  title = {Exploring {Regression} {Models} for {Regression} {Analysis}
    (3): {GEE,} {GLMM,} {M-statistics,} {Robust} (Sandwich) Estimation},
  date = {2025-02-28},
  url = {https://blog.zarathu.com/posts/2025-02-28-reg3/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-seungjun2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Seungjun, Lee. 2025. <span>“Exploring Regression Models for Regression
Analysis (3): GEE, GLMM, M-Statistics, Robust (Sandwich)
Estimation.”</span> February 28, 2025. <a href="https://blog.zarathu.com/posts/2025-02-28-reg3/">https://blog.zarathu.com/posts/2025-02-28-reg3/</a>.
</div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/blog\.zarathu\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="zarathucorp/giscus-blog" data-repo-id="R_kgDOHztuxg" data-category="General" data-category-id="DIC_kwDOHztuxs4CQ6h5" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org">Quarto</a>.</p>
</div>   
    <div class="nav-footer-center">
<p>© 2019. <a href="https://www.zarathu.com">Zarathu Co.,Ltd.</a> All rights reserved. Licence: <a href="https://opensource.org/license/mit-0/">MIT</a>.</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>