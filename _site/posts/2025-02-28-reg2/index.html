<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Lee Seungjun">
<meta name="dcterms.date" content="2025-02-28">
<meta name="description" content="종속변수가 Non-Normal한 Data에서 Regression Analysis를 수행하기 위해 종속변수의 분포 조건을 Exponential Family로 확장하고 Link Function로 근사하는 Regression Model인 GLM의 수학적 원리에 대해서 공부합니다. 또, clustered data 버전의 robust 표준오차(Cluster-robust standard error)에 대해서 살펴봅니다.">
<title>Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error – 차라투 블로그</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/logo_favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-12f9b1590e9172db5cd2178fc043c68b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-135478021-2', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../styles.css">
</head>
<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/logo.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">차라투 블로그</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://openstat.ai/" target="_blank"> 
<span class="menu-text">Applications</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-r-packages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">R packages</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-r-packages">
<li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jstable/" target="_blank">
 <span class="dropdown-text">jstable</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jskm/" target="_blank">
 <span class="dropdown-text">jskm</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jsmodule/" target="_blank">
 <span class="dropdown-text">jsmodule</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="../../contributors.html"> 
<span class="menu-text">Contributors</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-partners" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Partners</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-partners">
<li>
    <a class="dropdown-item" href="https://www.r-bloggers.com/" target="_blank">
 <span class="dropdown-text"><img src="https://www.r-bloggers.com/favicon.ico"> R-bloggers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/shinykorea/" target="_blank">
 <span class="dropdown-text"><img width="16px" src="https://avatars.githubusercontent.com/u/46996346?s=200&amp;v=4"> Shinykorea</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zarathucorp" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/zarathu/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" target="_blank"> <i class="bi bi-rss" role="img" aria-label="RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-translate" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-translate" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-translate">
<li>
    <a class="dropdown-item" href="../../../index.html">
 <span class="dropdown-text">한국어</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../en/index.html">
 <span class="dropdown-text">English</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../jp/index.html">
 <span class="dropdown-text">日本語</span></a>
  </li>  
    </ul>
</li>
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error</h1>
                  <div>
        <div class="description">
          종속변수가 Non-Normal한 Data에서 Regression Analysis를 수행하기 위해 종속변수의 분포 조건을 Exponential Family로 확장하고 Link Function로 근사하는 Regression Model인 GLM의 수학적 원리에 대해서 공부합니다. 또, clustered data 버전의 robust 표준오차(Cluster-robust standard error)에 대해서 살펴봅니다.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://github.com/aiseungjun">Lee Seungjun</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li><a href="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0" id="toc-들어가며" class="nav-link active" data-scroll-target="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0">들어가며</a></li>
  <li>
<a href="#generalized-linear-models-glms" id="toc-generalized-linear-models-glms" class="nav-link" data-scroll-target="#generalized-linear-models-glms">1. Generalized Linear Models (GLMs)</a>
  <ul class="collapse">
<li><a href="#linear-model-%ED%95%9C%EA%B3%84" id="toc-linear-model-한계" class="nav-link" data-scroll-target="#linear-model-%ED%95%9C%EA%B3%84">1.1. Linear Model 한계</a></li>
  <li><a href="#glm-%EC%A0%95%EC%9D%98-%EB%B0%8F-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84" id="toc-glm-정의-및-수학적-표현" class="nav-link" data-scroll-target="#glm-%EC%A0%95%EC%9D%98-%EB%B0%8F-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84">1.2. GLM 정의 및 수학적 표현</a></li>
  <li><a href="#glm-%EC%98%88%EC%8B%9C" id="toc-glm-예시" class="nav-link" data-scroll-target="#glm-%EC%98%88%EC%8B%9C">1.3. GLM 예시</a></li>
  </ul>
</li>
  <li>
<a href="#glms-%EC%B6%94%EC%A0%95" id="toc-glms-추정" class="nav-link" data-scroll-target="#glms-%EC%B6%94%EC%A0%95">2. GLMs 추정</a>
  <ul class="collapse">
<li><a href="#derivatives-of-log-likelihoods-%EC%84%B1%EC%A7%88" id="toc-derivatives-of-log-likelihoods-성질" class="nav-link" data-scroll-target="#derivatives-of-log-likelihoods-%EC%84%B1%EC%A7%88">2.1. Derivatives of Log Likelihood’s 성질</a></li>
  <li><a href="#exponential-family-%EC%84%B1%EC%A7%88" id="toc-exponential-family-성질" class="nav-link" data-scroll-target="#exponential-family-%EC%84%B1%EC%A7%88">2.2 Exponential Family 성질</a></li>
  <li><a href="#glms-parameter-%EC%B6%94%EC%A0%95%EC%8B%9D-%EC%9C%A0%EB%8F%84" id="toc-glms-parameter-추정식-유도" class="nav-link" data-scroll-target="#glms-parameter-%EC%B6%94%EC%A0%95%EC%8B%9D-%EC%9C%A0%EB%8F%84">2.3. GLMs’ parameter 추정식 유도</a></li>
  <li><a href="#glms-parameter-%EC%B6%94%EC%A0%95-irls" id="toc-glms-parameter-추정-irls" class="nav-link" data-scroll-target="#glms-parameter-%EC%B6%94%EC%A0%95-irls">2.4. GLMs’ parameter 추정 (IRLS)</a></li>
  <li><a href="#glms-parameter-variance" id="toc-glms-parameter-variance" class="nav-link" data-scroll-target="#glms-parameter-variance">2.5. GLMs’ parameter Variance</a></li>
  </ul>
</li>
  <li>
<a href="#cluster-robust-standard-errors" id="toc-cluster-robust-standard-errors" class="nav-link" data-scroll-target="#cluster-robust-standard-errors">3. Cluster-Robust Standard Errors</a>
  <ul class="collapse">
<li><a href="#clustered-data-%EC%A0%95%EC%9D%98" id="toc-clustered-data-정의" class="nav-link" data-scroll-target="#clustered-data-%EC%A0%95%EC%9D%98">3.1. Clustered Data 정의</a></li>
  <li><a href="#cluster-robust-standard-errors-%EC%A0%95%EC%9D%98-%EB%B0%8F-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84" id="toc-cluster-robust-standard-errors-정의-및-수학적-표현" class="nav-link" data-scroll-target="#cluster-robust-standard-errors-%EC%A0%95%EC%9D%98-%EB%B0%8F-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84">3.2. Cluster-robust standard errors 정의 및 수학적 표현</a></li>
  <li><a href="#cluster-robust-standard-errors-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84" id="toc-cluster-robust-standard-errors-수학적-표현" class="nav-link" data-scroll-target="#cluster-robust-standard-errors-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84">3.3. Cluster-robust standard errors 수학적 표현</a></li>
  <li><a href="#in-glms.." id="toc-in-glms.." class="nav-link" data-scroll-target="#in-glms..">3.4. In GLMs..</a></li>
  </ul>
</li>
  <li><a href="#r-%EC%98%88%EC%8B%9C-glm-cluster-robust-se" id="toc-r-예시-glm-cluster-robust-se" class="nav-link" data-scroll-target="#r-%EC%98%88%EC%8B%9C-glm-cluster-robust-se">4. R 예시: GLM, Cluster-robust SE</a></li>
  <li><a href="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0" id="toc-마무리하며" class="nav-link" data-scroll-target="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0">마무리하며</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><!-- Google tag (gtag.js) --><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-L0DYYSH9KM"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L0DYYSH9KM');
</script><section id="들어가며" class="level2"><h2 class="anchored" data-anchor-id="들어가며">들어가며</h2>
<p>2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고 마치겠습니다.<br></p>
</section><section id="generalized-linear-models-glms" class="level2"><h2 class="anchored" data-anchor-id="generalized-linear-models-glms">1. Generalized Linear Models (GLMs)</h2>
<section id="linear-model-한계" class="level3"><h3 class="anchored" data-anchor-id="linear-model-한계">1.1. Linear Model 한계</h3>
<hr>
<p>1장에서 본 <strong>Linear Regression Model</strong>은 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (3) 오차항의 독립성(Independence) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정에서 비롯된 모델이었고, Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)를 통해 오차항의 등분산성(Homoscedasticity) 가정이 깨진 data에 대해서도 Linear model로부터 얻은 모델 parameter의 분산을 robust하게 추정할 수 있었습니다. 그러나, 위에서 언급하였듯 <strong>outcome of single yes/no, outcome of single K-way, count 등 많은 data는 반응 변수 Y가 정규분포를 따르지 않거나 등분산성 가정, 선형성에 위배</strong>됩니다. 각각에 대해서 좀 더 설명하자면, 어떤 사건이나 행동이 일어나거나 그렇지 않은 경우를 고려하는 <strong>이진 데이터(binary data, outcome of single yes/no)</strong>의 경우, <span class="math inline">\(Y \in \{0, 1\}\)</span>로 제한되며 이를 <span class="math inline">\(Y \in \mathbb{R}\)</span>인 정규분포를 가정하는 것은 옳지 않습니다. 특정 기간 동안 발생하는 사건의 횟수 등, 이진 분류처럼 discrete한 종속변수 값을 가지는 <strong>카운트 데이터(count data)</strong> 또한 discrete(정수) 값만 갖으며, 이 두 경우는 종종 분산이 평균(모델의 예측)에 비례하는 형태를 갖을 수 있고, 이는 당연하게도 등분산성 가정을 위배합니다.</p>
<p>이러한 데이터의 경우 단순히 독립변수의 선형결합 형태, 또는 기하학적으로는 Hyper plane 형태로 모델을 만들면, 비선형적인 (이진 데이터 등) 위 같은 경우에 대해서는 올바르게 고려하지 못할 것입니다. 이러한 기존의 Linear Regression 모델의 한계를 극복하고, (종속변수의) 다양한 형태의 데이터를 모델링하기 위해 여러 함수를 설계함으로써 유연성을 확장한 <strong>Generalized Linear Models</strong>이 개발되었습니다. <strong>Generalized Linear Models(GLMs)</strong>의 중요 구성 요소들과 원리를 간략히 설명해보자면, 선형 결합으로 바로 종속변수를 예측하는 대신, non-linear한 <strong>Link Function</strong>에 넣어 최종 예측을 함으로써 non-linear한 종속변수에도 fit 할 수 있고, 이에 따라 종속변수의 분포가 <strong>정규분포가 아닌 다른 분포(Exponential Family)</strong>도 포함할 수 있도록 하였으며, 이 <strong>Exponential Family와 Variance function</strong>구성은 종속변수의 분산이 모델의 예측값(종속변수의 mean)마다 다를 수 있도록 합니다. 이를 통해 <strong>Generalized Linear Models</strong>는 위 네 개의 Linear Regression 가정 중 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정을 깼으며, 위에서 Linear Model의 한계로 언급한 데이터들을 고려할 수 있는 모델입니다.</p>
</section><section id="glm-정의-및-수학적-표현" class="level3"><h3 class="anchored" data-anchor-id="glm-정의-및-수학적-표현">1.2. GLM 정의 및 수학적 표현</h3>
<hr>
<p><strong>GLM</strong>은 세 가지 구성 요소 (Random component, Systematic component, Link function)으로 정의 되며, 이때 Random component는 Y를 Exponential Family로, Systematic component는 Linear predictor와 Link function으로 구성됩니다. 어떻게 <strong>Generalized Linear Models</strong>가 설계되었는지 component들을 하나하나 자세히 다뤄보겠습니다.</p>
<section id="linear-predictor" class="level4"><h4 class="anchored" data-anchor-id="linear-predictor">Linear predictor</h4>
<hr>
<p>Linear predictor <span class="math inline">\(\eta\)</span>는 말그대로 Linear Model처럼 <strong>모델 parameter</strong> <span class="math inline">\(\boldsymbol{\beta}\)</span><strong>와 독립변수</strong> <span class="math inline">\(\mathbf{X}\)</span><strong>의 선형 결합</strong>으로, 기존에는 <span class="math inline">\(\eta\)</span>로 바로 <span class="math inline">\(\mathbf{y}\)</span>를 추정하여 non-linear한 종속변수를 고려하지 못하였었다면, GLM은 <span class="math inline">\(\eta\)</span>를 계산한 후, 이 값을 <em>non-linear한 Link function에 input하여 최종적으로 종속변수를 예측</em>합니다. 중요한 점은, 이는 단순히 종속변수를 transform한 뒤(로그 등) 이전처럼 선형적으로 추정하는 Transformation (with LM)과 다르다는 점입니다. 가장 큰 차이점은 Transformation을 함으로써 종속변수의 sample space에서 boundaries에 있는 값들은 정의가 되지 않고(로그는 0에서 정의되지 않음.), 이후 바로 Linear Model을 사용하기 위해선 종속변수가 변형 이후 반드시 linearity와 variance의 homogeneity가 거의 보장되어야 합니다.(기존 LM을 사용하기 때문에 이때 사용한 가정 또한 필요하게 됩니다.) GLM의 Linear predictor (선형 예측자) <span class="math inline">\(\eta\)</span>의 식은 다음과 같습니다: <span class="math display">\[\eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_p x_{pi}\]</span></p>
</section><section id="link-function-링크-함수" class="level4"><h4 class="anchored" data-anchor-id="link-function-링크-함수">Link function (링크 함수)</h4>
<hr>
<p>Link function <span class="math inline">\(g(\mu_i)\)</span>는 <strong>non-linear하고 미분 및 inverse(역)가 가능한 함수로 정의</strong>되며, 종속변수의 평균 <span class="math inline">\(E(Y_i) = \mu_i\)</span>를 선형 예측자 <span class="math inline">\(\eta_i\)</span>와 연결하여 간접적으로 <strong>독립변수 및 모델 parameter의 선형결합과 종속변수를 mapping하는 역할</strong>을 합니다.<br><span class="math display">\[g(\mu_i) = \eta_i\]</span></p>
</section><section id="variance-function-분산-함수" class="level4"><h4 class="anchored" data-anchor-id="variance-function-분산-함수">Variance function (분산 함수)</h4>
<hr>
<p>분산 함수는 <strong>평균</strong> <span class="math inline">\(\mu_i\)</span><strong>에 따라 종속변수의 분산이 어떻게 변하는지</strong>를 나타냅니다. 이를 통해 간접적으로 독립변수에 따라 분산이 다르게 나오는 것을 반영할 수 있으며 식은 아래와 같고,<br><span class="math display">\[\mathrm{Var}(Y_i) = \phi V(\mu_i)\]</span><br></p>
<p>여기서 <span class="math inline">\(\phi\)</span>는 <strong>dispersion parameter</strong>로, 일반적으로 특정 분포에 따라 다르게 정의됩니다. (예: Poisson 분포에서는 <span class="math inline">\(\phi = 1\)</span>).</p>
</section><section id="exponential-family" class="level4"><h4 class="anchored" data-anchor-id="exponential-family">Exponential Family</h4>
<hr>
<p>GLM은 종속변수의 분포로 Gaussian(또는 정규분포)를 포함한, 더욱 general한 <strong>Exponential Family</strong>을 고려하며, 이 분포는 다음과 같은 일반 형태를 가집니다.</p>
<p><span class="math display">\[
f(y; \theta, \phi) = \exp \left\{ \frac{y \theta - b(\theta)}{\phi} + c(y, \phi) \right\}
\]</span></p>
<p>즉 GLM은 LM과 다르게 linear predictor, link function, variance function을 설계함으로써 종속변수가 더욱 general한 분포인 exponential family distribution인 경우에도 잘 mapping할 수 있도록 하는 모델이라고 볼 수 있습니다. 위 식에서 의미론적으로 각 parameters를 해석하면 <span class="math inline">\(\theta\)</span>는 <strong>canonical parameter</strong>로 분포의 위치를 나타내는 파라미터, <span class="math inline">\(\phi\)</span>는 dispersion parameter로 분산과 관련된 파라미터, <span class="math inline">\(b(\theta)\)</span>는 평균과 분산 관계를 정의하는 함수입니다. 이 분포에 대해 <span class="math inline">\(E(Y) = b'(\theta) = \mu\)</span>, <span class="math inline">\(\operatorname{var}(Y) = \phi b''(\theta) = \phi V(\mu)\)</span>이라는 특성이 증명 가능하고, 이는 “2. GLMs 추정”에서 모델 <span class="math inline">\(\beta\)</span>를 추정하는 과정에 필요하기 때문에 아래에서 증명할 것입니다. 이보다 더욱 general한 분포로 (dispersion parameter 관련) exponential dispersion family가 있습니다.</p>
<p>다음으로 넘어가기 전에 간단하게 잘 알려져있는 Exponential Family의 예시인 정규분포, 이항분포, 포아송분포, 감마분포가 이에 포함됨을 확인해보겠습니다.</p>
<p><strong>(1) 정규분포 (Normal Distribution)</strong></p>
<p>정규분포의 확률밀도함수는 다음과 같습니다: <span class="math display">\[
    f(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{ -\frac{(y - \mu)^2}{2\sigma^2} \right\}.
    \]</span> 이를 Exponential Family 형태로 변환하면: <span class="math display">\[
    f(y; \mu, \sigma^2) = \exp\left\{ \frac{y\mu - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right\}.
    \]</span></p>
<ul>
<li>Canonical parameter: <span class="math inline">\(\theta = \mu\)</span>,</li>
<li>Dispersion parameter: <span class="math inline">\(\phi = \sigma^2\)</span>,</li>
<li>
<span class="math inline">\(b(\theta) = \frac{\theta^2}{2}\)</span>,</li>
<li>
<span class="math inline">\(b'(\theta) = \theta = \mu\)</span>,</li>
<li>
<span class="math inline">\(b''(\theta) = 1\)</span>,</li>
<li>
<span class="math inline">\(c(y, \phi) = -\frac{y^2}{2\phi} - \frac{1}{2} \log(2\pi\phi)\)</span>.</li>
</ul>
<p><strong>(2) 이항분포 (Binomial Distribution)</strong></p>
<p>이항분포의 확률질량함수는 다음과 같습니다: <span class="math display">\[
    f(y; n, p) = \binom{n}{y} p^y (1-p)^{n-y}.
    \]</span> 이를 Exponential Family 형태로 변환하면: <span class="math display">\[
    f(y; n, p) = \exp\left\{ y \log\left(\frac{p}{1-p}\right) + n \log(1-p) + \log\binom{n}{y} \right\}.
    \]</span></p>
<ul>
<li>Canonical parameter: <span class="math inline">\(\theta = \log\left(\frac{p}{1-p}\right)\)</span>,</li>
<li>Dispersion parameter: <span class="math inline">\(\phi = 1\)</span>,</li>
<li>
<span class="math inline">\(b(\theta) = n \log(1 + e^\theta)\)</span>,</li>
<li>
<span class="math inline">\(b'(\theta) = e^\theta = \lambda\)</span>,</li>
<li>
<span class="math inline">\(b''(\theta) = e^\theta = \lambda\)</span>,</li>
<li>
<span class="math inline">\(c(y, \phi) = \log\binom{n}{y}\)</span>.</li>
</ul>
<p><strong>(3) 포아송분포 (Poisson Distribution)</strong></p>
<p>포아송분포의 확률질량함수는 다음과 같습니다: <span class="math display">\[
    f(y; \lambda) = \frac{\lambda^y e^{-\lambda}}{y!}.
    \]</span> 이를 Exponential Family 형태로 변환하면: <span class="math display">\[
    f(y; \lambda) = \exp\left\{ y \log\lambda - \lambda - \log(y!) \right\}.
    \]</span></p>
<ul>
<li>Canonical parameter: <span class="math inline">\(\theta = \log\lambda\)</span>,</li>
<li>Dispersion parameter: <span class="math inline">\(\phi = 1\)</span>,</li>
<li>
<span class="math inline">\(b(\theta) = e^\theta\)</span>,</li>
<li>
<span class="math inline">\(b'(\theta) = \frac{n e^\theta}{1 + e^\theta} = np\)</span>,</li>
<li>
<span class="math inline">\(b''(\theta) = \frac{n e^\theta}{(1 + e^\theta)^2} = np(1-p)\)</span>,</li>
<li>
<span class="math inline">\(c(y, \phi) = -\log(y!)\)</span>.</li>
</ul>
<p><strong>(4) 감마분포 (Gamma Distribution)</strong></p>
<p>감마분포의 확률밀도함수는 다음과 같습니다: <span class="math display">\[
    f(y; k, \theta) = \frac{1}{\Gamma(k)\theta^k} y^{k-1} e^{-\frac{y}{\theta}}.
    \]</span> 이를 Exponential Family 형태로 변환하면: <span class="math display">\[
    f(y; k, \theta) = \exp\left\{ -\frac{y}{\theta} + (k-1)\log y - k\log\theta - \log\Gamma(k) \right\}.
    \]</span></p>
<ul>
<li>Canonical parameter: <span class="math inline">\(\theta = -\frac{1}{\theta}\)</span>,</li>
<li>Dispersion parameter: <span class="math inline">\(\phi = \frac{1}{k}\)</span>,</li>
<li>
<span class="math inline">\(b(\theta) = -\log(-\theta)\)</span>,</li>
<li>
<span class="math inline">\(b'(\theta) = \frac{1}{\theta} = \mu\)</span>,</li>
<li>
<span class="math inline">\(b''(\theta) = \frac{1}{\theta^2} = \mu^2\)</span>,</li>
<li>
<span class="math inline">\(c(y, \phi) = (k-1)\log y - \log\Gamma(k)\)</span>.</li>
</ul>
<p>위에서 <span class="math inline">\(b(\theta)\)</span>는 한 번 미분하면 mean, 두 번 미분하면 variance의 term과 관련됨을 언급하였고 위 4개의 분포에서 원래 알고 계신 mean, variance와 <span class="math inline">\(b'(\theta)\)</span>, <span class="math inline">\(b''(\theta)\)</span>가 dispersion parameter를 고려하면 일치한 것을 확인하실 수 있습니다. 이는 cumulant generating function의 일부이기 때문이며, 따라서 평균과 분산 관계를 정의하는 항이라고 언급하였던 것입니다.</p>
</section><section id="canonical-link" class="level4"><h4 class="anchored" data-anchor-id="canonical-link">Canonical Link</h4>
<hr>
<p><strong>Canonical link</strong>는 GLM에서 통계적 성질을 최적화하기 위해 사용되는 링크 함수(link function)로, 다음과 같이 정의됩니다:</p>
<p><span class="math display">\[
g(\mu_i) = g(b'(\theta_i)) = \theta_i = \eta_i
\]</span> 이 식의 의미는 결국 아래 식과 같습니다. <span class="math display">\[
g = (b')^{-1}
\]</span></p>
<p>아래에서 확인하겠지만, <strong>Binomial 분포</strong>의 경우 canonical link는 <strong>logit</strong> 함수이고, <strong>Poisson 분포</strong>의 경우 canonical link는 <strong>log</strong> 함수이며, Canonical link를 사용하면 MLE(Maximum Likelihood Estimation) 과정이 단순화되고, efficient한 추정치를 얻을 수 있기 때문에 link function은 거의 항상 Canonical link로 정의합니다. 또한, canonical하지 않은 link function의 경우에도 위 Exponential Family distribution에서 식 조작을 통해 canonical link 형태를 만들 수 있습니다.</p>
</section></section><section id="glm-예시" class="level3"><h3 class="anchored" data-anchor-id="glm-예시">1.3. GLM 예시</h3>
<hr>
<p>위 철학에 따라, data가 따르는 Exponential Family 중 특정 분포가 정해지면, 이에 해당하는 보통 사용하는 Link function(Canonical link), Variance function가 정해져 있고 결국 모델이 특정되며, GLM은 이렇게 특정될 수 있는 모든 모델에서 공통적으로 parameter와 그 variance를 추정해내는 general한 모델이라고 생각할 수 있습니다. 여기에서 다루지는 않겠지만, 사실 특정한 형태의 data에서 가능한 link function은 여러 개이며, 이에 따라 variance function도 여러 가지가 가능할 수 있습니다. 그러나 효율성과 computation cost를 고려하여 보통 사용되는 function forms는 정해져 있다고 알아두시면 좋을 것 같습니다. 아래 예시 중 대표적으로 Binomial 예시에서는 link function이 0 이상 1 이하의 정의역에서 실수 전체(for linear predictor)를 map할 수 있는 미분 및 역이 가능한 함수이면 되지만, 보통 logit function이 사용됩니다. 헷갈릴 수 있지만 아래 Exponential Family 중 친숙한 분포의 예시를 직관적인 관점에서 고려하여 위에서 얻은 Exponential Family의 form과 같은 결과가 나옴을 보시면 좋을 것 같습니다.<br></p>
<p>아래의 예시에서 linear predictor는 공통이므로 미리 정의하고 각각의 link function, variance function은 어떻게 특정되는지를 보겠습니다.<br></p>
<p><span class="math display">\[
\eta_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi}
\]</span></p>
<p>where <span class="math inline">\(\eta_i\)</span>는 linear predictor, <span class="math inline">\(\beta_0, \beta_1, ..., \beta_p\)</span>는 regression coefficients(parameters), <span class="math inline">\(x_{1i}, ..., x_{pi}\)</span>는 predictor variables(독립변수) 입니다.</p>
<section id="binomial-case" class="level4"><h4 class="anchored" data-anchor-id="binomial-case">Binomial Case</h4>
<hr>
<p>Binomial Data, 즉 종속변수가 <span class="math inline">\(Y_i \sim \text{Binomial}(n_i, p_i)\)</span>인 data의 경우, 종속변수의 기댓값의 sample space 또한 0~1이며, 우리가 모델링하고 싶은 값이 <span class="math inline">\(Y_i/n_i\)</span>인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, <span class="math inline">\(E(Y_i / n_i) = p_i\)</span>이고, 분산은 <span class="math inline">\(\frac{1}{n_i} p_i (1 -p_i)\)</span> 입니다. <span class="math inline">\(Y_i/n_i\)</span>의 variance 식에 <span class="math inline">\(Y_i/n_i\)</span>의 mean이 들어감을 알 수 있고, 기존 LM에서는 이렇게 관측치에 따라 다르게 variance를 고려할 수 없었지만, GLM에서는 이 관계를 variance function을 통해 고려할 수 있으며, 식은 다음과 같습니다: <span class="math display">\[
V(\mu_i) = \mu_i (1 - \mu_i)
\]</span> 또한, <span class="math inline">\(Y_i / n_i\)</span>와 linear predictor를 matching 해줄 수 있는 미분가능한 function을 link로 고려해야 하고, Binomial에서는 non-linear link function으로 대부분 <strong>logit 함수</strong>를 사용합니다. (이때, logit function의 inverse는 sigmoid function입니다.)</p>
<p><span class="math display">\[g(\mu_i) = \log \left( \frac{\mu_i}{1 - \mu_i} \right)\]</span></p>
<p>이 식은 위에서 확인한 이항분포의 canonical parameter와 같은 형태임을 알 수 있습니다.</p>
</section><section id="poisson-case" class="level4"><h4 class="anchored" data-anchor-id="poisson-case">Poisson Case</h4>
<hr>
<p>Poisson Data, 즉 종속변수가 <span class="math inline">\(Y_i \sim \text{Poisson}(\lambda_i)\)</span>인 data이고 우리가 모델링하고 싶은 값이 종속변수 <span class="math inline">\(Y_i\)</span>인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, <span class="math inline">\(E(Y_i) = \lambda_i\)</span>이고 분산은 <span class="math inline">\(\lambda_i\)</span> 이므로, 마찬가지로 <span class="math inline">\(Y_i\)</span>의 variance 식에 <span class="math inline">\(Y_i\)</span>의 mean이 들어감을 알 수 있고, variance function은 다음과 같습니다:</p>
<p><span class="math display">\[
V(\mu_i) = \mu_i
\]</span> 또한, <span class="math inline">\(Y_i\)</span>의 sample space는 0 이상의 실수로, 이와 linear predictor를 matching 해줄 수 있는 미분가능한 non-linear function을 link로 고려해야 하고, Binomial에서는 link function으로 대부분 <strong>log 함수</strong>를 사용합니다. (inverse는 지수 함수.) <span class="math display">\[
g(\mu_i) = \log(\mu_i)
\]</span>이 식은 위에서 확인한 포아송분포의 canonical parameter와 같은 형태임을 알 수 있습니다. 위 두 예시에서는 어떻게 GLMs의 구성 요소들이 선택되는지 직관적으로 보았고, 이는 이해를 돕기 위한 해석이었으며 이미 위에서 Exponential Family에 포함됨을 보일 때 같은 결과가 나왔다는 것을 보시면 됩니다. 위 Binomial Data의 GLM은 Logistic Regression, Poisson Data의 GLM은 Poisson Regression으로도 불립니다.<br></p>
</section></section></section><section id="glms-추정" class="level2"><h2 class="anchored" data-anchor-id="glms-추정">2. GLMs 추정</h2>
<p>위 내용들을 통해서 GLMs가 어떻게 비정규분포를 갖는 종속변수를 고려해서 잘 작동하며, 어떠한 함수(Link function, Variance function, Exponential Family, Canonical Link)가 어떠한 수식과 철학으로 GLM을 구성하고 있는지 확인할 수 있었습니다. GLMs은 거의 대부분의 고려 가능한 data 분포가 Exponential Family를 따르며, 이에 대해 일관적인 form과 parameter estimation이 가능하기 때문에 아주 powerful한 Regression Model입니다. 그러나 어떻게 Exponential Family를 따르는 data를 다룰 수 있는지는 확인할 수 있었지만, <em>어떻게 Regression Model의 parameter와 그 분산을 추정할 수 있는지는 다루지 않았습니다.</em> <strong>Linear Model에서는 closed-form solution을 쉽게 찾을 수 있었지만, GLM은 대부분의 경우(있는 경우도 있습니다.) 이러한 closed-form이 없어 컴퓨터 프로그램으로 여러 번에 걸쳐 추정할 수 있도록 알고리즘을 구현하여 이를 추정</strong>합니다. 실제로 이 estimation의 수식과 실제 구현 과정을 다루기 위해서는 긴 증명 과정을 거치는데, 최대한 중요한 부분은 빠지지 않으면서 증명해보겠습니다. 우선, <strong>MLE로 모델을 추정하는 과정을 증명하기 위해 필요한 두 가지 유용한 성질</strong>을 살펴보겠습니다. (Derivatives of Log Likelihood’s, Exponential Family 성질)<br></p>
<section id="derivatives-of-log-likelihoods-성질" class="level3"><h3 class="anchored" data-anchor-id="derivatives-of-log-likelihoods-성질">2.1. Derivatives of Log Likelihood’s 성질</h3>
<hr>
<p>확률변수 <span class="math inline">\(Y\)</span>의 밀도 함수 <span class="math inline">\(f(y; \theta)\)</span>가 주어지며, 이때 <span class="math inline">\(\theta\)</span>는 스칼라 매개변수라고 가정하겠습니다. 또, <span class="math inline">\(\ell\)</span>이 <span class="math inline">\(\theta\)</span>에 대해 최소 두 번 미분 가능하다고 가정하면, 단일 관측치 <span class="math inline">\(Y\)</span>에 대한 로그 가능도(log likelihood) 함수 <span class="math inline">\(\ell(\theta; Y)\)</span>에 대해서 함수의 첫 번째 및 두 번째 도함수는 다음과 같습니다.</p>
<ul>
<li>
<strong>첫 번째 도함수</strong>: <span class="math display">\[ \ell' = \frac{d\ell}{d\theta} \]</span>
</li>
<li>
<strong>두 번째 도함수</strong>: <span class="math display">\[ \ell'' = \frac{d^2\ell}{d\theta^2} \]</span>
</li>
</ul>
<p>이때, 이 두 함수들은 다음 관계식이 성립합니다.</p>
<p><span class="math display">\[
E \{ \ell'(\theta; Y) \} = 0
\]</span></p>
<p><span class="math display">\[
E \left[ \{ \ell'(\theta; Y) \}^2 \right] = -E \{ \ell''(\theta; Y) \}
\]</span></p>
<p>이를 증명해보겠습니다. 의지를 잃지 않기 위해 위 식들의 의미를 스포하자면, MLE를 통해 모델을 추정할 때 보통 l<strong>og likelihood를 모델의 parameter로 미분한 식이 0(또는 영벡터)이 되도록 하는 parameter를 찾음으로써 이를 수행</strong>하는데, 첫 번째 식은 이 <strong>미분한 식(score function)의 기댓값(평균)이 0이라는 의미</strong>이고, 두 번째 식은 첫 번째 식에서 mean이 0이었으므로 왼쪽항의 제곱 안에 -0을 넣어주면 <span class="math display">\[
E\left[ \{ \ell'(\theta; Y) - E [ \ell'(\theta; Y) ] \}^2 \right]
\]</span></p>
<p>가 되어 분산 term이 되고, 따라서 <strong>분산은 이차 도함수의 기댓값의 음수와 같다는 의미</strong>입니다. 이러한 성질들을 이용해서 앞으로 Likelihood 기반의 다양한 모델 추정을 수행할 수 있게 됩니다.<br></p>
<p><strong>(1) Prove</strong> <span class="math inline">\(E \{ \ell'(\theta; Y) \} = 0\)</span><strong>.</strong></p>
<p>우선, 확률 분포는 모든 범위에서의 적분 또는 누적합이 1이므로,</p>
<p><span class="math display">\[
1 = \int f(y; \theta) dy
\]</span></p>
<p>입니다. 이제 양변을 <span class="math inline">\(\theta\)</span>에 대해 미분한 후 미분과 적분의 순서를 바꾸면,</p>
<p><span class="math display">\[
0 = \frac{d}{d\theta} \int f(y; \theta) dy \\
=\int \frac{d}{d\theta} f(y; \theta) dy
\]</span></p>
<p>입니다. 여기서 수학적 증명 과정에서 굉장히 자주 사용되는 skill <span class="math inline">\(\frac{d}{d\theta} f(y; \theta) = \frac{d}{d\theta} \{ \log f(y; \theta) \} f(y; \theta)\)</span>를 사용하면</p>
<p><span class="math display">\[
0 = \int \frac{d}{d\theta} f(y; \theta) dy  \\
= \int \frac{d}{d\theta} \{ \log f(y; \theta) \} f(y; \theta) dy \\
= \int \ell'(\theta; Y) f(y; \theta) dy \\
= E \{ \ell'(\theta; Y) \}
\]</span> 입니다. 의미를 다시 해석해보면, 어떠한 분포를 따르는 <span class="math inline">\(Y\)</span>와 이의 매개변수 <span class="math inline">\(\theta\)</span>에 대해서, 우리는 MLE를 통해 log likelihood 함수 <span class="math inline">\(\ell(\theta; Y)\)</span>를 <span class="math inline">\(\theta\)</span>로 미분하였을 때 0이 나오도록 하는 <span class="math inline">\(\hat{\theta}\)</span>를 찾음으로써 parameter를 estimate합니다. 위 (1)은 이러한 <span class="math inline">\(\ell'(\theta; Y)\)</span>의 기댓값은 <span class="math inline">\(Y\)</span>의 분포가 이계도함수가 존재한다면 어떤 분포이건 관계 없이 0임을 보인 것입니다.</p>
<p><strong>(2) Prove</strong> <span class="math inline">\(\ell'' = \frac{d^2\ell}{d\theta^2}\)</span><strong>.</strong></p>
<p>동일한 논리를 따라 위 식을 한 번 더 미분하면,</p>
<p><span class="math display">\[
0 = \frac{d}{d\theta} \left[ \int \frac{d}{d\theta} \{ \log f(Y; \theta) \} f(y; \theta) dy \right]
\]</span></p>
<p>입니다. 두 함수의 곱 형태의 미분이며 둘 다 <span class="math inline">\(\theta\)</span>를 포함하므로 이를 전개하고 마찬가지로 기댓값으로 표기하면,</p>
<p><span class="math display">\[
0 = \int \frac{d^2}{d\theta^2} \{ \log f(y; \theta) \} f(y; \theta) dy + \int \frac{d}{d\theta} \{ \log f(y; \theta) \} \frac{d}{d\theta} f(y; \theta) dy \\
= E \{ \ell''(\theta; Y) \} + E \left[ \{ \ell'(\theta; Y) \}^2 \right]
\]</span></p>
<p>이고, 따라서 아래 식이 증명되었습니다.</p>
<p><span class="math display">\[
E \left[ \{ \ell'(\theta; Y) \}^2 \right] = -E \{ \ell''(\theta; Y) \}
\]</span></p>
<p>위 증명 과정에서 미분과 적분 연산자의 교환을 정당화하는 과정의 설명이 생략되었지만 exponential family에선 문제가 없고, Y가 discrete한 경우는 적분을 누적합으로 바꿔주면 된다고 얘기해두며 마무리 하겠습니다. 또한, 위 증명에서는 <span class="math inline">\(\theta\)</span>가 <strong>1차원 스칼라 변수</strong>라고 가정했지만, <strong>다차원 매개변수</strong>에 대해서도 동일한 결과가 성립됩니다. 식의 의미를 마지막으로 되짚어보면, log likelihood의 일차 도함수는 기대값이 0이고, 이 일차 도함수의 공분산 행렬은 <strong>이차 도함수 행렬의 기대값의 음수</strong>에 해당합니다. 이 값은 <strong>피셔 정보 행렬(Fisher Information Matrix)</strong>이라고도 불립니다. (함수의 기댓값이라는 말이 어색하게 들릴 수도 있는데, 애초에 모든 랜덤(확률)변수는 어떠한 관측치에 대해서 실수를 output하는 함수임을 되새기면 좋을 것 같습니다.)</p>
</section><section id="exponential-family-성질" class="level3"><h3 class="anchored" data-anchor-id="exponential-family-성질">2.2 Exponential Family 성질</h3>
<hr>
<p>이번에는 위에서 증명한 수식을 통해서 Exponential Family를 소개할 때 언급한 <span class="math inline">\(E(Y) = b'(\theta) = \mu\)</span>, <span class="math inline">\(\operatorname{var}(Y) = \phi b''(\theta) = \phi V(\mu)\)</span>을 증명할 것입니다. Exponential Family distribution은 다음과 같은 일반적인 형식으로 정의됩니다.</p>
<p><span class="math display">\[
f(y; \theta, \phi) = \exp \left\{ \frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi) \right\}
\]</span> 때문에 log likelihood는 단순하게 아래와 같이 도출됩니다.</p>
<p><span class="math display">\[
\ell(y; \theta, \phi) = \frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)
\]</span> 이제 이 log likelihood의 derivatives를 계산하면 다음과 같습니다:</p>
<ul>
<li><p><strong>첫 번째 도함수 (Score Function):</strong> <span class="math display">\[
\ell' (y; \theta, \phi) = \frac{y - b'(\theta)}{a(\phi)}
\]</span></p></li>
<li><p><strong>두 번째 도함수 (Observed Information):</strong> <span class="math display">\[
\ell'' (y; \theta, \phi) = \frac{-b''(\theta)}{a(\phi)}
\]</span></p></li>
</ul>
<p>이 두 함수를 통해서 위에서 유도한 두 공식을 활용하면 다음 두 수식을 얻을 수 있습니다.</p>
<p><span class="math display">\[
E \left\{ \frac{Y - b'(\theta)}{a(\phi)} \right\} = 0
\]</span></p>
<p><span class="math display">\[
E \left[ \left( \frac{Y - b'(\theta)}{a(\phi)} \right)^2 \right] = \frac{b''(\theta)}{a(\phi)}
\]</span> 이때, 식을 잘 보면 첫 번째 식은 결국 <span class="math display">\[E[Y - b'(\theta)] = E[Y] - b'(\theta) = 0\]</span> 이 되어 <span class="math inline">\(E\{ Y \} = b'(\theta)\)</span>을 얻을 수 있고, 두 번째 식에서 <span class="math display">\[
E \left[ \left( \frac{Y - b'(\theta)}{a(\phi)} \right)^2 \right] = \frac{E[(Y - b'(\theta))^2]}{E[a(\phi)^2]} = \frac{\operatorname{Var}(Y)}{a(\phi)^2} = \frac{b''(\theta)}{a(\phi)}
\]</span> 이므로, <span class="math inline">\(\text{Var}(Y) = b''(\theta) a(\phi)\)</span>임을 보일 수 있습니다.<br></p>
<p>증명한 수식을 다시 한 번 확인하면,</p>
<p><span class="math display">\[
E(Y) = b'(\theta) = \mu
\]</span></p>
<p><span class="math display">\[
Var(Y) = a(\phi) V(\mu_i) = a(\phi)b''(\theta)
\]</span></p>
</section><section id="glms-parameter-추정식-유도" class="level3"><h3 class="anchored" data-anchor-id="glms-parameter-추정식-유도">2.3. GLMs’ parameter 추정식 유도</h3>
<hr>
<p>이제 필요한 식이 준비되었으니, 위에서 계속 다루고 있는 log likelihood을 이용해서 MLE estimation으로 GLMs’ parameter을 추정하는 과정을 살펴볼 것입니다. 이때, 추정 과정은 계속 언급한대로 Exponential Family distribution을 따르는 종속변수에 대해서 log likelihood의 model parameter에 대해 미분한 식이 (parameter가 벡터이므로, 좀더 엄밀하게 정의해야 하지만, 의미는 같으니 이렇게 얘기하겠습니다.) 0이 되게 하는 parameter를 찾음으로써 수행되며, <strong>이 때의 함수 (log likelihood의 1차 도함수)를 앞으로는 score function</strong>이라고 부르겠습니다.</p>
<p>우리는 MLE estimation을 통해 여러 Exponential Family distributions에 대해 통일된 estimation algorithm으로 parameter를 추정할 수 있습니다. (이러한 분포 가정 마저 없다면, 3장 GEE에서 보겠지만 분포에 대한 직접적 가정없이&nbsp;cumulative generating function 등 몇 함수 만으로 Likelihood를 고려하는 Quasi-likelihood Estimation의 개념으로 이어집니다.)</p>
<p>주어진 data가 <span class="math inline">\((y_1, ... , y_n)\)</span>일 때, 위에서부터 계속 사용해왔던 log-likelihood function은 다음과 같습니다.</p>
<p><span class="math display">\[
l = \sum_{i=1}^{n} \left( \frac{y_i \theta_i - b(\theta_i)}{\phi_i} + c(y_i, \phi_i) \right)
\]</span></p>
<p><strong>지금까지 우리는</strong> <span class="math inline">\(\theta\)</span><strong>로 log likelihood를 다뤘지만, 추정해야 하는 parameter는</strong> <span class="math inline">\(\boldsymbol{\beta}\)</span><strong>입니다</strong>. <span class="math inline">\(\boldsymbol{\beta}\)</span>는 벡터이기 때문에 이 중 하나의 파라미터 <span class="math inline">\(\beta_j\)</span>에 대해 먼저 log likelihood를 미분해보면 아래와 같은 식이 나옵니다. (<span class="math inline">\(\text{Var}(y) = \phi_i V(\mu_i)\)</span>, <span class="math inline">\(\frac{\partial \mu}{\partial \eta} = \frac{1}{g'(\mu_i)}\)</span>)임은 위에서 보았습니다. g는 link function이었습니다.)</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \beta_j} = s(\beta_j) \\
= \left(\frac{\partial l}{\partial \theta} \right) \left(\frac{\partial \theta}{\partial \mu} \right) \left(\frac{\partial \mu}{\partial \eta} \right) \left(\frac{\partial \eta}{\partial \beta_j} \right) \\
= \frac{y_i - \mu_i}{\text{Var}(y_i)} \left( \frac{\partial \mu}{\partial \eta} \right) x_{ij}, \quad \text{or} \\
= \sum_{i=1}^{n} \frac{y_i - \mu_i}{\phi_i V(\mu_i)} \times \frac{x_{ij}}{g'(\mu_i)} = 0
\]</span></p>
<p>식이 혼란스러울 수 있는데, 이는 단지 chain rule을 이용해서 <span class="math inline">\(\beta\)</span>에 대한 <span class="math inline">\(\ell\)</span>의 기울기를 구하는 과정이며, 위에서 GLM을 구성하는 과정을 차근차근 복기하면 각각의 변화율은 다음과 같이 구할 수 있기 때문에 최종 식을 얻을 수 있었음을 알 수 있습니다.</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \theta} =
\frac{y - b'(\theta)}{a(\phi)} =
\frac{y - \mu}{a(\phi)} \\
\frac{\partial \theta}{\partial \mu} =
\frac{1}{b''(\theta)} =
\frac{1}{V(\mu)} =
\frac{a(\phi)}{\text{Var}(y)} \\
\frac{\partial \eta}{\partial \beta_j} = x_{ij}
\]</span></p>
<p>이제 이 score function의 음의 미분(또는 분산)의 기댓값을 전개하면 다음과 같습니다.</p>
<p><span class="math display">\[
- E \left( \frac{\partial^2 l}{\partial \beta_j \partial \beta_k}
\right) = E \left[ \left( \frac{\partial l}{\partial \beta_j} \right)
\left( \frac{\partial l}{\partial \beta_k} \right) \right]
\]</span></p>
<p><span class="math display">\[
= E \left[ \left( \frac{y - \mu}{\text{Var}(y)} \right)^2
\left( \frac{\partial \mu}{\partial \eta} \right)^2
x_{ij} x_{ik} \right]
\]</span></p>
<p><span class="math display">\[
= E \left[ \frac{\text{Var}(y)}{\text{Var}(y)^2}
\left( \frac{\partial \mu}{\partial \eta} \right)^2
x_{ij} x_{ik} \right]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{\text{Var}(y)} \left(\frac{\partial \mu}{\partial \eta}
\right)^2 x_{ij} x_{ik}
\]</span></p>
<p>위 term은 <strong>Fisher Information matrix</strong>라고도 부르며, 이 term이 분산의 기댓값인 이유를 생각해보면, 이전에 구한 derivatives of log likelihood의 성질들에 의해 <span class="math inline">\(\ell\)</span>의 negative 2차 도함수의 기댓값은 1차 도함수의 기댓값의 square와 같고, 이 1차 도함수의 기댓값이 0이므로 이는 분산과 같습니다. 정리하자면, <strong>score function의 미분식이 score function의 분산과 기댓값이 같으므로, 미분을 직접하는 대신 분산으로 근사 후 식을 전개한 것이며, 이 때 근사한 이 Matrix를 Fisher Information matrix라고 부릅니다.</strong></p>
<p>이들을 한 번에 벡터와 행렬 연산으로 표현하면 다음과 같습니다:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \beta} = X^\top A (y - \mu)
\]</span></p>
<p><span class="math display">\[
E \left( \frac{\partial^2 l}{\partial \beta \partial \beta^T} \right) =
-X^T W X
\]</span></p>
<p><span class="math display">\[
where, W \; \text{is diagonal matrix comes from }\frac{1}{\text{Var}(y)}
\left(\frac{\partial \mu}{\partial \eta} \right)^2,
\]</span></p>
<p><span class="math display">\[
\text{and }A \; \text{is diagonal matrix comes from }\frac{1}{\text{Var}(y)}
\left(\frac{\partial \mu}{\partial \eta} \right).
\]</span></p>
<p>이제 우리는 score function와 그 미분, 또는 log likelihood의 1차 도함수와 (negative) 2차 도함수의 추정식을 얻었습니다. 사실, <span class="math inline">\(X^\top A (y - \mu)\)</span><strong>가 0이 되도록 하는 parameter</strong> <span class="math inline">\(\beta\)</span><strong>만 찾으면 parameter 추정이 끝나지만, 이는 closed-form solution이 존재하지 않기 때문에 그 미분(2차 도함수)식을 이용해 근사적으로 구할 수 있는 알고리즘을 최종적으로 다룰 것입니다.</strong> (물론 후에 보겠지만 이 Fisher Information matrix는 분산과도 관련이 있습니다.) 즉, 우리는 <em>추정식을 유도하는 것은 완성했지만, 실제로 알고리즘을 설계하여 어떻게 이를 추정할 지에 대해서는 모르는 상태</em>이고, 때문에 최종적으로 GLM을 제안한 학자가 소개하였으며 대부분의 패키지에서 이 GLM을 estimate하기 위해 사용하고 있는 method인 IRLS(Iteratively Reweighted Least Squares) Algorithm을 살펴볼 것입니다. (negative) 2차 도함수의 추정식(or Fisher Matrix) <span class="math inline">\(X^T W X\)</span>을 유도한 이유는, 이 알고리즘에서 필요로 하기 때문이고, <strong>위 식에서</strong> <span class="math inline">\(W, A\)</span><strong>는 식이 복잡해보이지만, 그저 observations(data) 하나 당 GLM 모델의 구성요소를 통해 determinant하게 미리 계산되어 대각성분으로 각각 들어가는 term임</strong>을 명심하시면 좋을 것 같습니다. (이전 설명에서, 종속변수의 분포로 Exponential Family 중 특정 분포가 정해지면, <strong>이에 따라 Link function(Canonical link), Variance function가 정해져 모델이 특정된다고 설명드린 적이 있고, 위</strong> <span class="math inline">\(W, A\)</span><strong>모두 이 두 함수로 이루어진 식이기 때문에 관측치마다 각각 넣으면 determinant하게 하나의 값이 나오는 식인 것입니다.</strong>)</p>
</section><section id="glms-parameter-추정-irls" class="level3"><h3 class="anchored" data-anchor-id="glms-parameter-추정-irls">2.4. GLMs’ parameter 추정 (IRLS)</h3>
<hr>
<p>위에서 언급하였듯, GLM의 MLE estimation은 <strong>비선형 최적화 문제</strong>이기 때문에 공통된 framework에서 사용할 수 있는 closed-form solution이 존재하지 않으며, 대신 여러 최적화 방법을 사용할 수 있습니다. <strong>뉴턴-랩슨 방법(Newton-Raphson Method)</strong>은 2차 도함수(Hessian Matrix)를 사용하여 score function을 수렴시키지만, Hessian Matrix <span class="math inline">\(H =\frac{\partial^2l}{\partial \beta \partial \beta^T}\)</span>를 직접 구해야 하고, 이는 계산이 복잡하여 computation cost가 큽니다. <strong>Fisher Scoring</strong>은 Newton Method에서 Hessian Matrix 대신 이를 근사하는 <strong>Fisher Information Matrix를</strong> 사용합니다. 이는 이전에 Derivatives of log likelihood 의 성질이랑 추정식 유도에서 모두 보았던대로 2차 도함수가 1차 도함수의 분산(또는 제곱)와 기댓값이 같다는 수학적 성질을 토대로 <span class="math display">\[
E \left(
\frac{\partial^2 l}{\partial \beta \partial \beta^T} \right) = E
\left[ \left( \frac{\partial l}{\partial \beta_j} \right) \left( \frac{\partial l}{\partial \beta_k} \right) \right]
= X^T W X\
\]</span> 가 만족함을 확인하였기 때문에, MLE추정에서 <strong>Newton-Raphson Method</strong>보다 computation cost가 합리적인 method라고 생각해 볼 수 있습니다. 이외에도 <strong>경사 하강법(Gradient Descent)</strong> 알고리즘은 <strong>1차 도함수(Gradient)만 사용</strong>하여 특정 값만큼 조금씩 점진적으로 parameter를 움직여 최적점을 찾는 방법입니다. 모델이 매우 복잡해서 2차 도함수를 계산하기 힘든 딥러닝에서는 많은 경우 이를 발전시킨 여러 methods로 iterativaly하게 parameter를 추정합니다. (이렇게 iterative하게 model’s parameter를 움직이면서 추정하는 과정이 AI에서 얘기하는 learning입니다.)</p>
<p>위에서 얘기한 <strong>IRLS(Iteratively Reweighted Least Squares) Algorithm</strong>는 이 <strong>Fisher Scoring</strong>의 알고리즘적 변형으로, Fisher Scoring의 식을 <strong>가중 최소제곱(Weighted Least Squares)</strong> 문제로 치환하여, 이 문제에서 사용하는 IRLS 알고리즘으로 GLM의 parameter 해를 구하는 method입니다. 우선 Newton-Raphson Method, Fisher Scoring에 대한 이야기를 간단하게 하고, 자세하게 어떻게 IRLS가 GLM의 parameter를 추정하는지 보겠습니다.</p>
<section id="newton-raphson-method-fisher-scoring" class="level4"><h4 class="anchored" data-anchor-id="newton-raphson-method-fisher-scoring">Newton-Raphson Method &amp; Fisher Scoring</h4>
<hr>
<p>GLM (Generalized Linear Model)의 파라미터 추정을 위한 최적화 과정은 우선 log likelihood function의 최대화를 목적으로 합니다. 이때, <strong>Newton-Raphson Method</strong>와 그 변형인 <strong>Fisher Scoring</strong>은 모두 이를 위한 알고리즘입니다.</p>
<p>Newton-Raphson 방법은 다음과 같은 일반적인 업데이트 식을 갖습니다:</p>
<p><span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}\right]^{-1} \frac{\partial \ell}{\partial \boldsymbol{\beta}},
\]</span> 이후의 method들은 모두 이 method에서 기인하므로, 위 식의 의미를 이해하는 것은 매우 중요합니다. 위 식이 어떻게 안정적으로 <span class="math inline">\(\boldsymbol{\beta}\)</span>를 수렴시킬 수 있는지 2차 테일러 전개를 통해 수식적으로 좀 더 명확히 볼 수 있지만, 여기서는 좀더 직관적으로 가볍게 이해해보겠습니다.</p>
<p>Newton-Raphson 방법은 어떠한 함수 <span class="math inline">\(f(x)\)</span>에 대해서 함수의 해, 즉 <span class="math inline">\(f(x) = 0\)</span>을 만족하는 <span class="math inline">\(x\)</span>를 찾기 위한 반복적인 근사 방법입니다.(informal한 증명이므로 1-dimension case로 보겠습니다.) 이 방법은 현재 점에서 함수의 접선을 그려 <span class="math inline">\(x\)</span>축과 만나는 점을 다음 근사해로 사용합니다. 예를 들어, 초기값 <span class="math inline">\(x_0\)</span>에서 접선을 그리면 그 접선의 방정식은 <span class="math inline">\(y = f'(x_0)(x - x_0) + f(x_0)\)</span>입니다. 이 접선이 <span class="math inline">\(x\)</span> 축과 만나는 점은 <span class="math inline">\(y = 0\)</span>일 때이므로, 이를 대입하여 접선이 0이 되는 점을 구해보면 <span class="math inline">\(x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}\)</span>가 됩니다. 그러나 접선은 함수의 선형 approximation이므로 이 접선이 0이 되는 점 <span class="math inline">\(x_1\)</span>이 실제 함수에서도 곧바로 0이 되지는 않습니다. 따라서 이 과정을 반복하여 특정 단계에서 <span class="math inline">\(f(x_n)\)</span>이 0에 충분히 가까워지면, <span class="math inline">\(x_n\)</span>을 근사해로 채택합니다.</p>
<p>이 방법이 작동하는 이유는 접선의 기울기 <span class="math inline">\(f'(x_n)\)</span>이 함수의 곡률을 반영하기 때문입니다. 곡률이 클수록(기울기가 가파를수록) 업데이트의 크기가 작아지고, 곡률이 작을수록 업데이트의 크기가 커집니다. 또한, Newton-Raphson 방법은 <strong>2차 수렴(Quadratic Convergence)</strong> 속도를 가집니다. 이는 오차가 반복마다 제곱으로 줄어들기 때문에 매우 빠르게 해에 수렴한다는 의미입니다.</p>
<p>이정도로 간단하게 Newton-Raphson method를 이해할 수 있고, 다시 돌아와서 위 식에서는 multi-dimention 상황에서 해를 찾고 싶은 함수가 score function, 즉 <span class="math inline">\(\frac{\partial  \ell}{\partial \beta}\)</span>인 경우이기 때문에 위처럼 식이 구성되었다는 것을 알 수 있습니다.(f의 미분이 분모로 들어간 term은 행렬에서 역행렬 -1과 같은 의미라고 보시면 됩니다.) 첨언하자면, 이 경우 Newton-Raphson method는 두 가지 중요 조건이 붙는데, 언급만 하자면 <strong>Hessian matrix가 Positive-definite (볼록) 해야 하며, 초기값이 최적점에 충분히 가까워야 합니다.</strong></p>
<p><strong>Fisher Scoring</strong>은 Hessian 행렬 대신 Fisher Information 행렬 <span class="math inline">\(\mathcal{I}(\boldsymbol{\beta})\)</span>를 사용하여 다음과 같이 업데이트합니다:</p>
<p><span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \left( \mathcal{I}(\boldsymbol{\beta}^{(t)}) \right)^{-1} \mathbf{S}(\boldsymbol{\beta}^{(t)}),
\]</span></p>
<p>여기서 <span class="math display">\[
\mathbf{S}(\boldsymbol{\beta}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}}
\]</span> 는 Score function으로 구한 (Fisher) score 벡터이고, <span class="math display">\[
\mathcal{I}(\boldsymbol{\beta}) = E
\left[ \left( \frac{\partial l}{\partial \beta_j} \right) \left( \frac{\partial l}{\partial \beta_k} \right) \right] = E\left[-\frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}\right]
\]</span> 는 <strong>Fisher information matrix</strong>입니다.</p>
<p>즉, Fisher Scoring 방법은 <strong>뉴턴-랩슨 방법(Newton-Raphson Method)</strong>에서 Hessian Matrix <span class="math inline">\(H =\frac{\partial^2l}{\partial \beta \partial \beta^T}\)</span> 를 사용하는 대신, <strong>Fisher information matrix를 사용해서 업데이트를 수행함</strong>으로써 parameter를 estimation하는 매커니즘입니다. <strong>IRLS</strong>는 GLM에서 이 Fisher Scoring와 거의 일치하다 봐도 무방하며, <strong>단순히 위에서 추정한</strong> <span class="math inline">\(\mathbf{S}(\boldsymbol{\beta})\)</span><strong>와</strong> <span class="math inline">\(\mathcal{I}(\boldsymbol{\beta})\)</span><strong>를 Fisher Scoring 공식에 넣으면 weighted least squares problme(가중 최소제곱 문제)와 완전히 유사해지기 때문에</strong>, 이 문제를 해결하는 방식으로 parameter를 추정한다는 의미라고 생각하시면 될 것 같습니다. 좀 더 수식과 같이 자세하게 설명드리겠습니다.</p>
</section><section id="irls-iteratively-reweighted-least-squares-algorithm" class="level4"><h4 class="anchored" data-anchor-id="irls-iteratively-reweighted-least-squares-algorithm">IRLS (Iteratively Reweighted Least Squares) Algorithm</h4>
<hr>
<p>앞서, 2.3.에서 log likelihood의 <strong>gradient(1차 도함수, Score function)</strong>와 <strong>expected Hessian(Fisher Information matrix)</strong>가 각각</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \beta} = X^T A (y - \mu)
\]</span></p>
<p><span class="math display">\[
E \left( \frac{\partial^2 l}{\partial \beta \partial \beta^T} \right) = X^T W X
\]</span></p>
<p><span class="math display">\[
where, \quad A = \frac{1}{\text{Var}(y)} \left( \frac{\partial \mu}{\partial \eta} \right) \; and \quad W = \frac{1}{\text{Var}(y)} \left( \frac{\partial \mu}{\partial \eta} \right)^2
\]</span></p>
<p>임을 보았습니다. 이 결과들을 Fisher Scoring method의 식에 대입하면,</p>
<p><span class="math display">\[
\beta^{(t+1)} = \beta^{(t)} + \left( X^T W X \right)^{-1} X^T A (y - \mu)
\]</span></p>
<p>이고, 이 equation은 <strong>가중 최소제곱 문제(Weighted Least Squares, WLS)</strong>의 parameter 추정식과 일치하기 때문에 비선형 GLM의 parameter 추정을 WLS problem으로 치환할 수 있음을 알 수 있습니다.</p>
<p>위의 업데이트 식은 아래 <strong>working response</strong> <span class="math inline">\(z\)</span>를 정의하면</p>
<p><span class="math display">\[
z = X \beta^{(t)} + W^{-1} A (y - \mu),
\]</span></p>
<p>아래와 같이 표현할 수 있습니다.</p>
<p><span class="math display">\[
\beta^{(t+1)} = \beta^{(t)} + \left( X^T W X \right)^{-1} X^T A (y - \mu)
\]</span> <span class="math display">\[
= \left( X^T W X \right)^{-1}\left( X^T W X \right)\beta^{(t)} + \left( X^T W X \right)^{-1} X^T W W^{-1} A (y - \mu)
\]</span> <span class="math display">\[
= \left( X^T W X \right)^{-1}X^T W \left( X \beta^{(t)} + W^{-1} A (y - \mu) \right)
\]</span></p>
<p><span class="math display">\[
= \left( X^T W X \right)^{-1} X^T W z
\]</span></p>
<p>또 강조하지만, 이는 가중치 행렬 <span class="math inline">\(W\)</span>에 따라 각 관측치의 기여도를 달리하는 선형 회귀 문제(가중 최소제곱 문제)의 정규방정식과 동일합니다:</p>
<p><span class="math display">\[
(X^T W X) \beta = X^T W z.
\]</span></p>
<p>따라서 해당 정규방정식의 <span class="math inline">\(\beta\)</span>를 가중 최소제곱 문제 방식으로 풀어냄으로써 추정치 <span class="math inline">\(\beta^{(t+1)}\)</span>를 구할 수 있으며, 이는 현재 단계의 추정치 <span class="math inline">\(\beta^{(t)}\)</span>에서의 예측값과 오차를 반영한 새로운 업데이트가 됩니다.</p>
</section><section id="irls-구체적-절차" class="level4"><h4 class="anchored" data-anchor-id="irls-구체적-절차">IRLS 구체적 절차</h4>
<hr>
<p>정리하자면, <strong>IRLS(Iteratively Reweighted Least Squares)</strong> 알고리즘은 위의 아이디어를 바탕으로 GLM의 최대우도추정 문제를 반복적으로 가중 최소제곱 문제로 전환하여 해결합니다. 구체적인 단계는 다음과 같습니다:</p>
<p><strong>(1) 초기화</strong>: 초기 파라미터 <span class="math inline">\(\beta^{(0)}\)</span>를 설정합니다.</p>
<p><strong>(2) 현재 단계 계산</strong>:</p>
<p><strong>(2.1)</strong> <strong>예측값 계산</strong>: 현재 추정치 <span class="math inline">\(\beta^{(t)}\)</span>을 이용하여 선형 예측치 <span class="math inline">\(\eta^{(t)} = X \beta^{(t)}\)</span>를 구하고, link 함수의 역함수를 통해 <span class="math inline">\(\mu^{(t)} = g^{-1}(\eta^{(t)})\)</span>를 계산합니다.</p>
<p><strong>(2.2)</strong> <strong>가중치 및 보조 행렬 계산</strong>: 정의한 식에 따라</p>
<p><span class="math display">\[
A^{(t)} = \frac{1}{\text{Var}(y)} \left( \frac{\partial \mu}{\partial \eta} \right)^{(t)}
\]</span></p>
<p><span class="math display">\[
W^{(t)} = \frac{1}{\text{Var}(y)} \left( \frac{\partial \mu}{\partial \eta} \right)^{(t) 2}
\]</span></p>
<p>을 계산합니다.</p>
<p><strong>(2.3)</strong> <strong>Working Response 구성</strong>:</p>
<p><span class="math display">\[
z^{(t)} = X \beta^{(t)} + \left( W^{(t)} \right)^{-1} A^{(t)} (y - \mu^{(t)}).
\]</span></p>
<p><strong>(3) 가중 최소제곱 문제 해결</strong>:</p>
<p>위의 working response와 가중치 행렬을 사용하여 정규방정식</p>
<p><span class="math display">\[
(X^T W^{(t)} X) \beta^{(t+1)} = X^T W^{(t)} z^{(t)}
\]</span> 을 풀어 새로운 추정치 <span class="math inline">\(\beta^{(t+1)}\)</span>를 구합니다.</p>
<p><strong>(4) 수렴 판단 및 반복</strong>:</p>
<p><span class="math inline">\(||\beta^{(t+1)} - \beta^{(t)}||\)</span>(L1 norm, 쉽게는 절댓값)가 미리 설정한 임계값 이하가 될 때까지 2번과 3번의 단계를 반복하고, 수렴이 되었다면 이 <span class="math inline">\(\beta^{(t+1)}\)</span> 값이 GLM의 parameter에 대한 IRLS의 최종 Estimation 결과입니다. 결국, Fisher Scoring에서 대입한 수식이 결국 가중 최소제곱 문제로 귀착됨을 통해, <strong>IRLS 알고리즘</strong>은 각 반복마다 선형 회귀 문제와 유사한 방식으로 parameter를 업데이트합니다.</p>
<p>이 IRLS의 소프트웨어 구현에 대한 첨언을 하자면, 보통 <strong>IRLS</strong>에서는 각 단계마다 위 3단계와 같이 아래 정규방정식을 풉니다:</p>
<p><span class="math display">\[
(X^T W X) \beta = X^T W z.
\]</span></p>
<p>이때 직접 <span class="math inline">\((X^T W X)^{-1}\)</span>를 구해 업데이트하는 방법은 계산적으로 불안정할 수 있습니다. 특히, 데이터의 규모가 크거나 <span class="math inline">\(X\)</span> 행렬이 <strong>ill-conditioned(조건수가 열악한)</strong>인 경우에는 직접 역행렬을 계산하는 과정에서 수치적인 문제가 발생할 위험이 큽니다. 따라서, 구현의 영역이기 때문에 더이상 나열하지는 않겠지만 실제는 역행렬을 direct하게 구하는 대신, <strong>QR 분해</strong>나 <strong>Cholesky 분해</strong> 같은 선형대수 기법을 활용하여 안정적으로 선형 시스템을 풀 수 있습니다.</p>
</section></section><section id="glms-parameter-variance" class="level3"><h3 class="anchored" data-anchor-id="glms-parameter-variance">2.5. GLMs’ parameter Variance</h3>
<hr>
<p>앞서 <strong>IRLS(Iteratively Reweighted Least Squares)</strong> 알고리즘으로 GLM의 파라미터를 추정하는 과정을 살펴보았습니다. 이때 우리는 MLE(최대우도추정)을 IRLS(반복적으로 <strong>가중 최소제곱</strong>)문제로 전환하는 과정을 거쳤는데, 최종적으로 구해지는 추정치 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>의 분산에 대한 추정까지 마쳐야 유의성 검정 등의 분석을 수행할 수 있을 것입니다. GLM에서 최대우도추정(MLE)을 사용해 얻은 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>는,&nbsp;이론적으로 위에서 언급한 <strong>Fisher 정보 행렬</strong>(Fisher information matrix)의 역행렬로써 구할 수 있습니다:</p>
<p><span class="math display">\[ \widehat{\mathrm{Var}}\bigl(\hat{\boldsymbol{\beta}}\bigr) = \bigl(\mathbf{I}(\hat{\boldsymbol{\beta}})\bigr)^{-1}, \]</span></p>
<p>여기서 <span class="math inline">\(\mathbf{I}(\hat{\boldsymbol{\beta}})\)</span>는 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>에서의 (observed 혹은 expected) Fisher 정보 행렬입니다.(3장에서 이 이유에 대해 살펴볼 것입니다.) 모델의 parameter를 추정하는 과정에서, 각 관측치 <span class="math inline">\(i\)</span>에 대해 <span class="math inline">\(\mathrm{Var}(y_i)\)</span>와 <span class="math inline">\(\frac{\partial \mu_i}{\partial \eta_i}\)</span>가 포함된 특정 가중치 <span class="math inline">\(W^{(t)}\)</span>가 등장하였었고, 반복(step)마다 업데이트되는 정규방정식을 풀어감으로써 추정치 <span class="math inline">\(\beta^{(t+1)}\)</span>를 얻었습니다. 이후 최종 수렴 시점(<span class="math inline">\(t \to \infty\)</span>)에서, 우리는 <span class="math inline">\(\hat{\boldsymbol{\beta}} = \beta^{(\infty)}\)</span>에 도달하게 되고, 이 시점에서 계산된 <strong>Hessian(또는 Fisher information) matrix</strong> <span class="math inline">\(\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X}\)</span>에 대해서, 이 행렬의 역수가 분산이 되는 것입니다.</p>
<p>즉, 실제 계산 시에는 아래와 같은 모양이 됩니다.</p>
<p><span class="math display">\[ \widehat{\mathrm{Var}}\bigl(\hat{\boldsymbol{\beta}}\bigr) = (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1}, \]</span></p>
<p>왜 최종적으로 추정된 모델의 분산이 수렴된 time step에서의 Fisher information matrix로 추정할 수 있는지에 대해서는 3장의 M-estimation에서 증명할 것입니다. 여기서 미리 강조할 것은, 위에서 얻는 분산의 추정값은 GLM에서의 <strong>기본 가정들이었던</strong> 독립성, 분산 함수의 형태 등이 성립한다는 조건 위에서 도출된 것이므로, 이러한 기본 가정이 어느 정도 엄격히 맞아떨어지는 상황(정확한 포아송 분포를 따르는 count data, 명시적으로 독립적인 개별 관측치 등)이라면 괜찮지만, 현실의 data에서는 <strong>이질분산</strong>, <strong>클러스터 내 상관</strong>, <strong>과산포(overdispersion)</strong> 등으로 인해 이 기본 가정들이 깨질 수 있습니다. 다행히도, GLM에서도 모델이 consist할 때(GLM의 위로부터 추정된 parameter 자체는 consist합니다.) <strong>HC(Heteroskedasticity-Consistent) se</strong>와, 아래에서 clustered data에서 고려할 수 있는 버전인 <strong>Cluster-robust standard errors</strong>를 사용하여 더욱 robust하게 분산을 추정할 수 있습니다. 때문에 아래에서는 Cluster-robust se를 OLS 버전으로 소개드린 후, GLM에서 사용하기 위해 <span class="math inline">\(\hat{\mathbf{W}}\)</span> 행렬 <span class="math inline">\(\hat{\mathbf{A}}\)</span> 행렬 등의 구조가 어떻게 수식적으로 첨가되어 LM(Linear Model)에서의 식과는 살짝 다른 모습을 취하게 되는지 보고 마치겠습니다.<br></p>
</section></section><section id="cluster-robust-standard-errors" class="level2"><h2 class="anchored" data-anchor-id="cluster-robust-standard-errors">3. Cluster-Robust Standard Errors</h2>
<section id="clustered-data-정의" class="level3"><h3 class="anchored" data-anchor-id="clustered-data-정의">3.1. Clustered Data 정의</h3>
<hr>
<p><strong>Clustered data</strong>란 데이터 내에서 동일 그룹에 속하는 관측치들이 상관관계를 가지는 경우를 의미합니다. 예를 들어, 한 환자의 여러 진료 기록이 서로 상관되어 있을 수 있습니다. 이 때, cluster간에는 상관관계가 없고 cluster 내의 데이터들은 상관관계가 있다는 가정하에 Cluster-robust standard errors나 3장의 GEE, GLMM model이 개발되었습니다. 의료 분석 상황의 예시로는 대표적으로 데이터의 각 환자 당 여러 시간 또는 주기에 걸쳐 측정한 데이터, 여러 학교나 병원과 같은 단체에서 얻은 데이터들을 한 번에 고려하는 경우가 있을 것입니다. 또한, cluster간에 상관관계가 있거나 cluster 안에 cluster가 있는 hierarchical의 경우도 있지만, 이에 대한 공식들은 위에서 고려하는 1차적인 상황을 이해하면 쉽게 이해할 수 있으며, 의료 분석에서 고려하는 피험자 내 관측치 간 상관관계, 병원 내 관측치 간 상관관계 등을 고려해야 하는 상황은 이번 블로그에서 이야기 할 1차적인 clustered 상황임을 알아두시면 좋을 것 같습니다. 이 observations간의 상관관계에 대한 이야기와, 이때 사용해야 하는 Regression Models에 대한 설명은 3장에서 GEE, GLMM과 함께 더욱 자세하게 다뤄볼 예정입니다.</p>
<p>확실한 것은, 비선형 분포를 추정할 수 있는 GLM이나, OLS에서 안정적인 parameter 분산 추정 method였던 HC(Heteroskedasticity-Consistent) 표준오차는 관측치 간의 독립성을 가정하였었고, 이는 <strong>위와 같은 data를 다룰 때에는 깨져야 하는 가정</strong>이라는 것입니다. 이제 설명드릴 <strong>Cluster-robust standard errors</strong>는 HC se와 형태가 매우 비슷하며, 같은 철학으로 <strong>clustered data에서 robust한 모델 분산 추정 method</strong>입니다. 이때 기억하셔야 할 부분은, 1장에서는 HC se의 안정성을 Linear Regression에 대해서 고려하였고 R의 sandwich 패키지를 통해 구현할 수 있음을 보았는데, 이번 장에서 다루고 있는 <strong>GLM에서도 이 HC se, Cluster-robust se를 모두 사용할 수 있다</strong>는 것입니다. 이때 식이 LM과 GLM에서 살짝 다른데, 우선 Linear Model에서의 Cluster-robust se에 대해서 설명드리고, GLM에서는 무엇이 다른지 보겠습니다.</p>
<p>실제 소프트웨어의 구현에 대해서 첨언하자면, R의 sandwich 패키지나 대부분의 패캐지에서는 이 두 robust 분산 추정의 계산 및 검정을 LM, GLM 모두에 사용 가능하고, 이 패키지들은 들어오는 모델의 객체가 LM, GLM임을 분류한 뒤 각각에 맞는 살짝 변형된 식으로 추정한다고 생각하시면 될 것 같습니다.</p>
</section><section id="cluster-robust-standard-errors-정의-및-수학적-표현" class="level3"><h3 class="anchored" data-anchor-id="cluster-robust-standard-errors-정의-및-수학적-표현">3.2. Cluster-robust standard errors 정의 및 수학적 표현</h3>
<hr>
<p>Cluster-robust standard errors는 <strong>클러스터 내 상관관계</strong>를 고려하여 분산을 추정합니다. 이를 통해 클러스터 간 독립성은 유지하되, 클러스터 내 관측치 간 상관관계가 존재할 때도 일관된 추정치를 제공합니다. LM에서 Cluster-robust standard error를 구하는 식은 다음과 같습니다:</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left( \sum_{g=1}^{G} \mathbf{X}_g^\top \mathbf{\hat{u}}_g \mathbf{\hat{u}}_g^\top \mathbf{X}_g \right) (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>위 식에서 <span class="math inline">\(g\)</span>는 클러스터 인덱스, <span class="math inline">\(\mathbf{\hat{u}}_g\)</span>는 클러스터 <span class="math inline">\(g\)</span>의 잔차 벡터입니다.이 식은 1장에서의 HC0과 아주 유사하며, 가운데 meat항 (두 <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1}\)</span> 사이에 있는 항)만 달라졌음을 알 수 있습니다. 이는 실제로 HC0에서, 위에서 설명드린 가정인 1차적 clustered 구조(클러스터 간 독립성을 가정하지만, 클러스터 내 관측치들 간 상관관계는 허용)를 고려해서 <span class="math inline">\(\Phi\)</span> 항만 바뀌었음을 짐작해볼 수 있습니다. 이제 이 Cluster-robust standard errors의 철학에 대해서 구체적으로 살펴보겠습니다.</p>
</section><section id="cluster-robust-standard-errors-수학적-표현" class="level3"><h3 class="anchored" data-anchor-id="cluster-robust-standard-errors-수학적-표현">3.3. Cluster-robust standard errors 수학적 표현</h3>
<hr>
<p>LM에서는 이전에 봤던대로 parameter의 분산을 유도하면 다음과 같습니다:</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \Phi \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>여기서 <span class="math inline">\(\Phi\)</span>는 오차항의 공분산 행렬을 나타냈었습니다. HC0 (Heteroskedasticity-Consistent 0)에서는 모든 관측치가 서로 독립임을 가정하였기 때문에 (Heteroskedasticity를 고려하였지 dependent case를 고려하지는 않았었습니다.) 이에 따라 <span class="math inline">\(\Phi\)</span>는 대각행렬로 표현되며,</p>
<p><span class="math display">\[
\Phi_{\text{HC0}} = \operatorname{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2)
\]</span></p>
<p>결과적으로 분산 추정량은 개별 관측치에 대해아래와 같이 계산하였었습니다.</p>
<p><span class="math display">\[
\widehat{\text{Var}}_{\text{HC0}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left(\sum_{i=1}^n \mathbf{x}_i \hat{u}_i^2 \mathbf{x}_i^\top \right) (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span> 위 식은 1장의 HC0 식과 같은 식입니다. 표현이 어색하다고 느끼시는 분을 위해 이전에 사용한 식을 가져오면 <span class="math display">\[
\widehat{\text{Var}}_{\text{HC0}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{diag}(e_i^2) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>이며, 위에서부터 얘기하고 있는 <span class="math inline">\(\hat{u}\)</span>는 이 error term <span class="math inline">\(e\)</span>와 비슷한(잔차이기 때문에 사실 의미는 다릅니다) 의미입니다. HC0에서는 각 관측치만을 고려하기 때문에 <span class="math inline">\(\hat{u}_i\)</span>는 <span class="math inline">\(e_i\)</span>와 같고, 길이가 1인 벡터, 즉 scalar이기 때문에 제곱을 사용하였지만 위 cluster-robust 식에서 사용한 <span class="math inline">\(\hat{u}_g\)</span>는 클러스터 g에 해당하는 모든 관측치를 한 줄로 나열한 임의의 길이의 벡터이기 때문에 제곱이 아니라 <span class="math inline">\(\mathbf{\hat{u}}_g \mathbf{\hat{u}}_g^\top\)</span> term을 사용한 것입니다.</p>
<p>이에 대한 이해를 바탕으로 Cluster-robust se의 meat term을 생각해보면, <strong>Cluster-robust에서는 cluster간은 독립적이고, cluster안의 관측치들은 상관관계를 가질 수 있다고 가정하기 때문에 각 cluser에 대해서</strong> <span class="math inline">\(\Phi_i\)</span><strong>를</strong> <span class="math inline">\(\mathbf{\hat{u}}_g \mathbf{\hat{u}}_g^\top\)</span><strong>로 각각 구한 후, 전체</strong> <span class="math inline">\(\Phi\)</span><strong>는 아래와 같이 block diagonal 구조로 넣어준다고 이해</strong>할 수 있습니다. (block 행렬은 행렬을 특정한 block으로 나누었을 때 대각선 이외의 모든 행렬 블록이 영행렬인 행렬을 의미하며, cluster간의 독립을 block diagonal 구조로 고려하였다고 이해하면 됩니다.)</p>
<p><span class="math display">\[
\Phi_{\text{cluster}} =
\begin{pmatrix}
\Phi_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \Phi_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \Phi_G
\end{pmatrix}
\]</span></p>
<p>즉, 여기서 각 <span class="math inline">\(\Phi_g = E[\mathbf{u}_g \mathbf{u}_g^\top]\)</span>는 클러스터 <span class="math inline">\(g\)</span> 내의 오차의 공분산 행렬이고, 각 cluster에 대해 잔차<span class="math inline">\(\hat{\mathbf{u}}_g\)</span>를 사용하여</p>
<p><span class="math display">\[
\widehat{\text{Var}}_{\text{cluster}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left(\sum_{g=1}^{G} \mathbf{X}_g^\top \hat{\mathbf{u}}_g \hat{\mathbf{u}}_g^\top \mathbf{X}_g \right) (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span> 와 같이 추정합니다.</p>
<p>위 Cluster-robust의 meat항에 대한 이해는 비슷하게 3장에서도 필요하기 때문에 예시를 통해 좀더 직관적으로 보여드리겠습니다. 우선 이 중앙항은 다음과 같고,</p>
<p><span class="math display">\[
\mathbf{B} = \sum_{g=1}^{G} \mathbf{X}_g^\top \hat{\mathbf{u}}_g \hat{\mathbf{u}}_g^\top \mathbf{X}_g
\]</span></p>
<p>이는 행렬로 보면 위에서 보신 <span class="math inline">\(\Phi\)</span>항과 같이 block diagonal 형태를 갖습니다. 3개의 cluster가 있고, 각 cluster 내 관측치 수가 2, 3, 1개라고 가정하면 각각의 <span class="math inline">\(\Phi\)</span>는 다음과 같고,</p>
<p><span class="math display">\[
\Phi_1 = \mathbf{X}_1^\top \hat{\mathbf{u}}_1 \hat{\mathbf{u}}_1^\top \mathbf{X}_1 =
\begin{pmatrix}
\sigma_{11}^2 &amp; \sigma_{12} \\
\sigma_{12} &amp; \sigma_{22}^2
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
\Phi_2 = \mathbf{X}_2^\top \hat{\mathbf{u}}_2 \hat{\mathbf{u}}_2^\top \mathbf{X}_2 =
\begin{pmatrix}
\sigma_{33}^2 &amp; \sigma_{34} &amp; \sigma_{35} \\
\sigma_{34} &amp; \sigma_{44}^2 &amp; \sigma_{45} \\
\sigma_{35} &amp; \sigma_{45} &amp; \sigma_{55}^2
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
\Phi_3 = \mathbf{x}_6 \hat{u}_6^2 \mathbf{x}_6^\top
\]</span></p>
<p>로 표현될 수 있으며, 결국 Cluster-robust의 중앙 term은</p>
<p><span class="math display">\[
\mathbf{\Phi} =
\begin{pmatrix}
\Phi_1 &amp; 0 &amp; 0 \\
0 &amp; \Phi_2 &amp; 0 \\
0 &amp; 0 &amp; \Phi_3
\end{pmatrix}
\]</span> 가 될 것입니다.</p>
<p>정리하자면, <strong>HC0는</strong> <span class="math inline">\(\Phi\)</span>가 대각행렬인 경우로, 개별 관측치의 Heteroskedasticity 만을 고려하며</p>
<p><span class="math display">\[
\widehat{\text{Var}}_{\text{HC0}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left(\sum_{i=1}^n \mathbf{x}_i \hat{u}_i^2 \mathbf{x}_i^\top \right) (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>Cluster-Robust 분산 추정량은 clusr별 <span class="math inline">\(\Phi\)</span>가 block diagonal 구조로, Heteroskedasticity와 cluster 내의 상관관계를 반영합니다.</p>
<p><span class="math display">\[
E\left[\widehat{\text{Var}}(\hat{\boldsymbol{\beta}})\right] = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \Phi \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
</section><section id="in-glms.." class="level3"><h3 class="anchored" data-anchor-id="in-glms..">3.4. In GLMs..</h3>
<hr>
<p>이미 위(3장에서) <strong>Cluster-robust standard errors</strong>가 어떤 원리로부터 유도되며, OLS 환경(Linear Model)에서의 공식이 어떻게 생겼는지 살펴보았습니다. 또한 <strong>HC(Heteroskedasticity-Consistent) se</strong> 역시 기본 가정(등분산, 독립성 등)이 약화되었을 때도 일관된 추정을 제공하기 위해 <strong>Robust</strong>(샌드위치) 분산 추정량을 쓰게 된다는 것을 보았습니다. GLM에서도 LM과 같이 위 두 robust한 분산 추정치 식을 사용하여 Fisher information matrix의 역행렬로 분산을 추정하는 대신, 더욱 안정적으로 분산을 추정할 수 있습니다. GLM의 경우, 단순 OLS와 달리 <span class="math inline">\(\hat{\mathbf{W}}, \hat{\mathbf{A}}\)</span> 등 추가적인 항이 존재하고, 이 행렬들이 실제 분산 추정 과정에 반영됩니다. 이 때문에 <strong>“bread”</strong>(양쪽에 곱해지는 행렬)와 <strong>“meat”</strong>(중간에 오는 분산·잔차 구조) 부분이 LM에서의 표기와는 형태가 조금 달라집니다. 즉, 원리는 동일하되, <strong>link &amp; variance function</strong>으로 부터 비롯된 미분 항(<span class="math inline">\(\mathbf{A}\)</span>)과 가중치 항(<span class="math inline">\(\mathbf{W}\)</span>)이 반영되어야 한다는 점만 다릅니다. 1장에서 소개했던 HC0를 떠올리면, LM의 경우</p>
<p><span class="math display">\[ \widehat{\mathrm{Var}}_{\text{HC0}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left(\sum_{i=1}^n \mathbf{x}_i \hat{u}_i^2 \mathbf{x}_i^\top \right) (\mathbf{X}^\top \mathbf{X})^{-1}, \]</span></p>
<p>로 식이 구성되었습니다. <strong>GLM</strong>의 경우에는 <span class="math inline">\(\hat{\mathbf{W}}\)</span>가 <span class="math inline">\(\mathbf{X}\)</span>와 상호작용하여 분산 추정에 들어가므로, 실제로는 다음과 같은 형태를 가집니다. (식은 패키지나 저자별 표기 차이에 따라 다소 달라질 수 있습니다). <span class="math display">\[ \widehat{\mathrm{Var}}_{\text{HC0}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1} \left( \sum_{i=1}^n \mathbf{x}_i \Bigl(\hat{u}_i^2 \Bigr) \mathbf{x}_i^\top \right) (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1}. \]</span></p>
<p>여기서 <span class="math inline">\(\hat{u}_i\)</span>는 단순 잔차가 아니라, <strong>펄슨(pearson) 잔차</strong> 등 비선형적인 GLM 설정에 맞춰 적절히 조정된 형태일 수 있습니다. 구현별로 <strong>이탈도(deviance) 잔차</strong>를 사용할 수도 있고, 핵심은 “관측치별 잔차의 크기”를 통해 이질분산성을 추정하는 것입니다. 여기서는 앞뒤의 <span class="math inline">\((\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1}\)</span>이 <strong>bread</strong>(빵)이고, 가운데 잔차 <span class="math inline">\(\hat{u}_i\hat{u}_i^\top\)</span>가 <strong>meat</strong>(고기) 역할을 한다고 보면 됩니다. 철학적으로 해석하면, GLM에서도 LM에서와 동일하게 <strong>HC se</strong>는 “각 관측치별 오차분산”이 서로 다르더라도 일관된 추정을 제공하기 위하는 목적이며, 식은 (1) 잔차(오차항) 부분은 그대로 <strong>meat</strong>로 넣고, (2) 정보를 제공하는 <strong>bread</strong>에는 <span class="math inline">\(\mathbf{X}\)</span>에 가중치의 의미를 가진 <span class="math inline">\(\hat{\mathbf{W}}\)</span> 항을 추가하여 구성한 위 형태로 구성됩니다.</p>
<p><strong>클러스터링이 있는 데이터</strong>에 대하여, LM과 마찬가지로 <strong>GLM에서도 Cluster-robust se</strong>가 적용될 수 있습니다. 이미 섹션 3.2~3.3에서 보았듯, 클러스터 간에는 독립이지만 클러스터 내 관측치들 간에는 상관관계가 존재할 수 있으므로, <span class="math inline">\(\Phi\)</span> 행렬(오차의 공분산 구조)을 <strong>block diagonal</strong> 형태로 가정하고, 이를 샌드위치 가운데(meat)에 반영합니다.</p>
<p>LM에서의 일반적 식은 다음과 같았습니다.</p>
<p><span class="math display">\[ \widehat{\mathrm{Var}}_{\text{cluster}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \mathbf{X})^{-1} \left( \sum_{g=1}^G \mathbf{X}_g^\top \hat{\mathbf{u}}_g \hat{\mathbf{u}}_g^\top \mathbf{X}_g \right) (\mathbf{X}^\top \mathbf{X})^{-1}. \]</span></p>
<p><strong>GLM</strong>에서는 동일한 철학으로, 단순히 <span class="math inline">\(\mathbf{X}\)</span> 대신 가중치를 고려해 <span class="math inline">\(\hat{\mathbf{W}}^{1/2}\mathbf{X}\)</span>와 같은 형태(혹은 관련 도함수 항)가 곱해지게 됩니다. 즉,</p>
<p><span class="math display">\[ \widehat{\mathrm{Var}}_{\text{cluster}}(\hat{\boldsymbol{\beta}}) = (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1} \left( \sum_{g=1}^G (\mathbf{X}_g^\top \hat{\mathbf{W}}_g^{1/2}) \, \hat{\mathbf{u}}_g \hat{\mathbf{u}}_g^\top \, (\hat{\mathbf{W}}_g^{1/2} \mathbf{X}_g) \right) (\mathbf{X}^\top \hat{\mathbf{W}} \mathbf{X})^{-1}, \]</span></p>
<p>와 같은 꼴이 됩니다(마찬가지로 패키지마다 표기 방식이나 구현 세부가 약간씩 다를 수 있습니다).각 항들 또한 한 번 더 설명하자면, <span class="math inline">\(\hat{\mathbf{u}}_g\)</span>는 클러스터 <span class="math inline">\(g\)</span> 내 잔차 벡터(pearson 또는 deviance 잔차 등).<span class="math inline">\(\mathbf{X}_g\)</span>는 클러스터 <span class="math inline">\(g\)</span>에 해당하는 행만 추출한 <span class="math inline">\(\mathbf{X}\)</span>의 서브 행렬. <span class="math inline">\(\hat{\mathbf{W}}\)</span>는 클러스터 (<span class="math inline">\(g\)</span>)에 해당하는 마찬가지로 가중치 서브 행렬이고, 이때 1/2승을 한다는 의미는 이가 diagonal matrix이므로 이 경우에는 단순히 diagonal 성분들 각각을 루트 씌운 값입니다.<br></p>
</section></section><section id="r-예시-glm-cluster-robust-se" class="level2"><h2 class="anchored" data-anchor-id="r-예시-glm-cluster-robust-se">4. R 예시: GLM, Cluster-robust SE</h2>
<p>아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. GLM모델의 분산과 cluster-robust 분산을 비교하시면서 해석하면 됩니다.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## 필요한 패키지 설치 (필요시)</span></span>
<span><span class="co">## install.packages("sandwich")</span></span>
<span><span class="co">## install.packages("lmtest")</span></span>
<span><span class="co">## install.packages("nlme")</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 데이터 불러오기</span></span>
<span><span class="co">#library(nlme)</span></span>
<span><span class="co">#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)</span></span>
<span><span class="co">#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 기본 GLM (로지스틱 회귀)</span></span>
<span><span class="co">#glm_fit &lt;- glm(binary ~ age + Sex, </span></span>
<span><span class="co">#               data = Orthodont, </span></span>
<span><span class="co">#               family = binomial)</span></span>
<span><span class="co">#summary(glm_fit) </span></span>
<span><span class="co">## 클러스터-로버스트 표준오차 (Subject 기준)</span></span>
<span><span class="co">#library(sandwich)</span></span>
<span><span class="co">#library(lmtest)</span></span>
<span><span class="co">#cluster_se &lt;- vcovCL(glm_fit, cluster = ~ Subject)</span></span>
<span><span class="co">#coeftest(glm_fit, vcov = cluster_se)  # 결과 출력</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="마무리하며" class="level2"><h2 class="anchored" data-anchor-id="마무리하며">마무리하며</h2>
<hr>
<p>이번 2장에서는 1장에서 다룬 Linear Model을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수에서도 분석할 수 있도록 확장한 <strong>Generalized linear model</strong>의 기본 개념과, 실제로 패키지에서 이 GLM의 parameter를 estimate할 때 사용하는 대표적인 알고리즘인 IRLS(Fisher scoring)을 수학적으로 상당히 깊게 살펴보았습니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고, GLMs에서도 이 둘을 사용할 수 있다는 것을 밝힌 뒤 그 변형된 수식을 보았습니다. 다음 3장에서는 아직 깨지 못한 가정이었던 오차항의 독립, 즉 data(observations)간의 correlation이 존재하는 경우 자체를 모델에 반영하기 위해 개발된 모델들인 GEE, GLMM에 대하여 어느 정도 살펴보고 (GLMM의 내용은 너무 길어지기 때문에 얕게 다룰 것입니다.), 모델의 분산을 robust하게 추정하기 위한 가장 general한 형태의 Sandwich estimator를 M-estimation의 개념과 함께 공부할 것입니다.</p>


</section><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{seungjun2025,
  author = {Seungjun, Lee},
  title = {Exploring {Regression} {Models} for {Regression} {Analysis}
    (2): {GLM,} {Exponential} {Family,} {Link} {Function,} {IRLS(Fisher}
    Scoring), {Cluster-robust} Standard Error},
  date = {2025-02-28},
  url = {https://blog.zarathu.com/posts/2025-02-28-reg2/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-seungjun2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Seungjun, Lee. 2025. <span>“Exploring Regression Models for Regression
Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher
Scoring), Cluster-Robust Standard Error.”</span> February 28, 2025. <a href="https://blog.zarathu.com/posts/2025-02-28-reg2/">https://blog.zarathu.com/posts/2025-02-28-reg2/</a>.
</div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/blog\.zarathu\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="zarathucorp/giscus-blog" data-repo-id="R_kgDOHztuxg" data-category="General" data-category-id="DIC_kwDOHztuxs4CQ6h5" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org">Quarto</a>.</p>
</div>   
    <div class="nav-footer-center">
<p>© 2019. <a href="https://www.zarathu.com">Zarathu Co.,Ltd.</a> All rights reserved. Licence: <a href="https://opensource.org/license/mit-0/">MIT</a>.</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>