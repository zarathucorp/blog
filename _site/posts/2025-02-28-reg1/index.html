<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Lee Seungjun">
<meta name="dcterms.date" content="2025-02-28">
<meta name="description" content="Regression Analysis의 기본 Model인 Linear regression과 이의 robust한 (Co)variance Matrix Estimator인 HC Standard Errors(HCCCM), Wild Bootstrap에 대해서 공부합니다.">
<title>Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors – 차라투 블로그</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../img/logo_favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-12f9b1590e9172db5cd2178fc043c68b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-135478021-2', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../styles.css">
</head>
<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/logo.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">차라투 블로그</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://openstat.ai/" target="_blank"> 
<span class="menu-text">Applications</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-r-packages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">R packages</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-r-packages">
<li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jstable/" target="_blank">
 <span class="dropdown-text">jstable</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jskm/" target="_blank">
 <span class="dropdown-text">jskm</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://jinseob2kim.github.io/jsmodule/" target="_blank">
 <span class="dropdown-text">jsmodule</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="../../contributors.html"> 
<span class="menu-text">Contributors</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-partners" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Partners</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-partners">
<li>
    <a class="dropdown-item" href="https://www.r-bloggers.com/" target="_blank">
 <span class="dropdown-text"><img src="https://www.r-bloggers.com/favicon.ico"> R-bloggers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/shinykorea/" target="_blank">
 <span class="dropdown-text"><img width="16px" src="https://avatars.githubusercontent.com/u/46996346?s=200&amp;v=4"> Shinykorea</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zarathucorp" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/zarathu/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" target="_blank"> <i class="bi bi-rss" role="img" aria-label="RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-translate" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-translate" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-translate">
<li>
    <a class="dropdown-item" href="../../../index.html">
 <span class="dropdown-text">한국어</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../en/index.html">
 <span class="dropdown-text">English</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../jp/index.html">
 <span class="dropdown-text">日本語</span></a>
  </li>  
    </ul>
</li>
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors</h1>
                  <div>
        <div class="description">
          Regression Analysis의 기본 Model인 Linear regression과 이의 robust한 (Co)variance Matrix Estimator인 HC Standard Errors(HCCCM), Wild Bootstrap에 대해서 공부합니다.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://github.com/aiseungjun">Lee Seungjun</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li><a href="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0" id="toc-들어가며" class="nav-link active" data-scroll-target="#%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B0">들어가며</a></li>
  <li><a href="#regression-analysis" id="toc-regression-analysis" class="nav-link" data-scroll-target="#regression-analysis">1. Regression Analysis</a></li>
  <li>
<a href="#linear-regression-lm" id="toc-linear-regression-lm" class="nav-link" data-scroll-target="#linear-regression-lm">2. Linear Regression (LM)</a>
  <ul class="collapse">
<li><a href="#simple-multiple-linear-regression-%EC%A0%95%EC%9D%98" id="toc-simple-multiple-linear-regression-정의" class="nav-link" data-scroll-target="#simple-multiple-linear-regression-%EC%A0%95%EC%9D%98">2.1. (Simple, Multiple) Linear Regression 정의</a></li>
  <li><a href="#linear-regression-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95" id="toc-linear-regression-수학적-표현-및-추정" class="nav-link" data-scroll-target="#linear-regression-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84-%EB%B0%8F-%EC%B6%94%EC%A0%95">2.2. Linear Regression 수학적 표현 및 추정</a></li>
  </ul>
</li>
  <li><a href="#linear-regression-in-homo-vs.-heteroskedasticity" id="toc-linear-regression-in-homo-vs.-heteroskedasticity" class="nav-link" data-scroll-target="#linear-regression-in-homo-vs.-heteroskedasticity">3. Linear Regression in Homo vs.&nbsp;Heteroskedasticity</a></li>
  <li>
<a href="#heteroskedasticity-consistent-standard-errors-hc-standard-errors" id="toc-heteroskedasticity-consistent-standard-errors-hc-standard-errors" class="nav-link" data-scroll-target="#heteroskedasticity-consistent-standard-errors-hc-standard-errors">4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)</a>
  <ul class="collapse">
<li><a href="#hc-standard-errors-%EC%A0%95%EC%9D%98" id="toc-hc-standard-errors-정의" class="nav-link" data-scroll-target="#hc-standard-errors-%EC%A0%95%EC%9D%98">4.1. HC Standard Errors 정의</a></li>
  <li><a href="#hccme0-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84" id="toc-hccme0-수학적-표현" class="nav-link" data-scroll-target="#hccme0-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84">4.2. HC(CME)0 수학적 표현</a></li>
  <li><a href="#hccme-13-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84" id="toc-hccme-13-수학적-표현" class="nav-link" data-scroll-target="#hccme-13-%EC%88%98%ED%95%99%EC%A0%81-%ED%91%9C%ED%98%84">4.3. HC(CME) 1~3 수학적 표현</a></li>
  </ul>
</li>
  <li><a href="#wild-bootstrap" id="toc-wild-bootstrap" class="nav-link" data-scroll-target="#wild-bootstrap">5. (Wild) Bootstrap</a></li>
  <li><a href="#r-%EC%98%88%EC%8B%9C-hc03-%EB%B0%8F-%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9" id="toc-r-예시-hc03-및-부트스트랩" class="nav-link" data-scroll-target="#r-%EC%98%88%EC%8B%9C-hc03-%EB%B0%8F-%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9">6. R 예시: HC0~3 및 부트스트랩</a></li>
  <li><a href="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0" id="toc-마무리하며" class="nav-link" data-scroll-target="#%EB%A7%88%EB%AC%B4%EB%A6%AC%ED%95%98%EB%A9%B0">마무리하며</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><!-- Google tag (gtag.js) --><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-L0DYYSH9KM"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L0DYYSH9KM');
</script><section id="들어가며" class="level2"><h2 class="anchored" data-anchor-id="들어가며">들어가며</h2>
<p>차라투 블로그 “Exploring Regression Models for Regression Analysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한 여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는 Regression Analysis의 개념, (simple, multiple or general) linear regression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의 특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로 추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는 방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent standard errors (heteroskedasticity-robust standard errors)와 Wild Bootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의 clustered data 버전인 Cluster-robust standard error를 다룹니다. 3장에서는 GLM에서 여전히 남아있는 observations의 independent 조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를 고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust (sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다. <strong>{4장은 계획 중에 있습니다.}</strong></p>
<p>결국 “Exploring Regression Models for Regression Analysis”는 Regression Analysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며, 이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들 중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게 수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를 공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로 제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련 분석이 필요하시다면 해당 연구를 <a href="https://www.zarathu.com/en">차라투</a>로 문의해주시길 바랍니다.<br></p>
</section><section id="regression-analysis" class="level2"><h2 class="anchored" data-anchor-id="regression-analysis">1. Regression Analysis</h2>
<p><strong>Regression Analysis</strong>는 통계학에서 가장 널리 사용되는 방법론 중 하나로, 어떤 종속변수(dependent variable)와 하나 이상의 독립변수(independent variables) 간의 관계를 추정하고 해석하는 분석 기법입니다. (종속변수 또한 벡터, 즉 여러 개일 수 있습니다.)</p>
<p>의학 분야에서는 예후를 예측하는 모델을 만들거나(예: 특정 약물 투여 후 혈압, outcome 등의 변화를 예측), 특정 위험인자(risk factor)가 결과에 유의미한 영향을 미치는지(예: 어떤 치료법이 환자의 생존율에 유의미한 차이를 주는지) 등을 살펴보기 위해 <strong>Regression Analysis</strong>가 필수적으로 활용됩니다. 이러한 Regression Analysis을 완벽하게 수행하기 위해서는 본인의 <em>data가 어떤 특성을 갖고 있는지 파악하고</em>, 이에 대해 <em>어떠한 Regression Model을 고려해야 하는지</em> 알고, 이 모델이 <em>어떻게 data에 적합(fit)하는지</em>에 대한 대략적인 수학 또는 알고리즘의 원리와, 적합된 모델이 <em>통계적으로 유의미한지 검정(test)</em>하고 예측 성능이나 해석력을 평가하는 방법까지 전 과정을 이해해야 합니다. 본 1장에서는 가장 간단한, 대신 많은 가정이 필요한 Regression Model인 (General) Linear Regression에 대해서 위 과정과 함께 살펴볼 것이며, 이후의 장들에서는 data의 가정이 조금씩 깨질 때 (data의 특성이 변할 때) 어떤 Regression Model을 고려해야 하는지와 이 Model들의 parameters는 어떻게 수학적으로 추정하는지 살펴볼 것입니다.<br></p>
</section><section id="linear-regression-lm" class="level2"><h2 class="anchored" data-anchor-id="linear-regression-lm">2. Linear Regression (LM)</h2>
<section id="simple-multiple-linear-regression-정의" class="level3"><h3 class="anchored" data-anchor-id="simple-multiple-linear-regression-정의">2.1. (Simple, Multiple) Linear Regression 정의</h3>
<hr>
<p><strong>(General) Linear Regression</strong>은 의학뿐만 아니라 다양한 분야에서 가장 basic한 Regression Model입니다. <strong>Linear Regression</strong>은 독립변수 <span class="math inline">\(X\)</span> 와 종속변수 <span class="math inline">\(Y\)</span> 간의 <strong>선형(linear) 관계</strong>를 가정하고, 이를 통해 종속변수를 설명하거나 예측합니다. 독립변수가 1개일 때는 <strong>Simple Linear Regression</strong>, 2개 이상일 때는 <strong>Multiple Linear Regression</strong>라고 부릅니다. 의학 연구에서 예를 들면, 혈압(종속변수)을 나이, 체중, 성별(독립변수) 등으로 설명하거나 예측하는 과정을 생각할 수 있습니다. 이러한 Linear regression은 아래 네 가지 가정을 기반으로 추정하며, 이 가정들에 대해서 잘 이해하는 것은 매우 중요하고, 이들 중 특정 가정들이 위배될 경우 이후에 다룰 Regression Model들을 고려해야 함을 명심해주시면 좋을 것 같습니다.</p>
<ul>
<li>
<p><strong>선형성(Linearity) 가정</strong></p>
<p>독립변수와 종속변수가 선형 관계에 있다고 가정합니다. 산점도(scatter plot)를 통해 대략적인 선형 관계 여부를 확인할 수 있습니다.</p>
</li>
<li>
<p><strong>오차항의 정규성(Normality) 가정</strong></p>
<p>주어진 독립변수의 값에서 종속변수의 확률분포는 정규분포를 따른다고 가정합니다. 이는 오차항 ε이 정규분포를 따른다고도 표현할 수 있습니다.(종속변수가 정규분포를 따른다는 뜻은 Linear Model이 종속변수의 mean을 예측하므로, 이 둘의 차인 오차항은 평균이 0이고 분산은 종속변수와 같은 정규분포를 따르게 되기 때문입니다.) 이는 정규 P-P plot 혹은 Q-Q plot 등을 통해 가정 위배 여부를 대략적으로 확인할 수 있습니다.</p>
</li>
<li>
<p><strong>오차항의 독립성(Independence) 가정</strong></p>
<p>각 관측치(Observations, Data(set)) 또는 잔차(residual)가 서로 독립이라고 가정합니다. 즉, data간의 상관관계가 없다고 가정하는 것이고, 잔차산점도(residual plot), Durbin-Watson 통계량 등을 통해 자기상관(autocorrelation)이 있는지 살펴볼 수 있습니다.</p>
</li>
<li>
<p><strong>오차항의 등분산성(Homoscedasticity) 가정</strong></p>
<p>모든 독립변수의 값에서 종속변수의 분산이 동일하다고 가정합니다. 잔차산점도 등을 통해 잔차가 일정한 분산을 가지는지 대략적으로 확인할 수 있습니다.</p>
</li>
</ul>
<p>이후 Linear Regression Model은 다음과 같은 과정으로 분석을 수행하게 됩니다; (1) 최소제곱법(Least Squares)를 통해 model parameters를 추정(estimate), (2) 결정계수(<span class="math inline">\(R^2\)</span>)를 통해 모델이 종속변수를 얼마나 잘 설명하는지 확인, (3) F-검정(F-test)으로 전체 회귀식의 유의성을 검정, (4) 검정(t-test)으로 각 회귀계수(regression coefficient)가 유의미한지 확인. 특히, 독립변수 각각의 Model coefficient(parameter)를 검정 함으로써 종속변수와의 상관성을 분석하는 (4) 과정은 유의성을 판단하는 가장 중요한 검정 과정으로, 고전적으로 <strong>Wald Test, Likelihood Ratio Test, Score Test</strong>가 자주 사용됩니다.</p>
</section><section id="linear-regression-수학적-표현-및-추정" class="level3"><h3 class="anchored" data-anchor-id="linear-regression-수학적-표현-및-추정">2.2. Linear Regression 수학적 표현 및 추정</h3>
<hr>
<p>Linear Regression은 다음과 같은 형태를 가집니다:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_k X_{ik} + \varepsilon_i
\]</span></p>
<p>혹은 Matrix 형태로 간단히 쓰면,</p>
<p><span class="math display">\[
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} \\
    where, \quad \mathbf{y} \in \mathbb{R}^n, \mathbf{X} \in \mathbb{R}^{n \times p}, \boldsymbol{\beta} \in \mathbb{R}^p, \boldsymbol{\varepsilon} \in \mathbb{R}^n
\]</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>: 관측된 종속변수들의 벡터</p></li>
<li><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>: 설계 행렬, 각 행은 하나의 관측치(Observation)이며 (<span class="math inline">\(n\)</span>), 각 열은 하나의 독립변수에 (<span class="math inline">\(p\)</span>) 해당</p></li>
<li><p><span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span>: 추정하고자 하는 회귀 계수(or 모수 or parameters) 벡터 (여기선 intercept <span class="math inline">\(\beta_0\)</span> 제외)</p></li>
<li><p><span class="math inline">\(\boldsymbol{\varepsilon} \in \mathbb{R}^n\)</span>: 랜덤 오차 항 벡터, 위 <strong>네 가정</strong>에 따라 랜덤 오차항은 평균이 <span class="math inline">\(E(\boldsymbol{\varepsilon}) = 0\)</span>이고, 분산이 모든 관측치에서 같으며 독립인 정규분포</p></li>
</ul>
<p>입니다. Multiple Linear Regression의 모델 <span class="math inline">\(\boldsymbol{\beta}\)</span>의 분산은 scalar variance가 아니라 (co)variance matrix 형태가 될 것입니다. (simple도 intercept를 고려하면 matrix) 앞으로 세 장에 걸쳐 이를 분산 행렬이라고 부르겠지만, 정확히는 <strong>분산-공분산 행렬</strong>이라고도 부릅니다. 또한, 위 matrix form 수식처럼 intercept를 제외하는 경우도 많은데, 항상 있다고 가정하며 가독성을 위해 제외한 것이니 혼동할 필요 없이 자연스럽게 읽으시면 됩니다. Covariance matrix에서는 회귀계수 서로 간의 공분산이 들어있으므로, 각 <strong>회귀계수(독립변수의 유의성)에 대한 검정</strong>은 <strong>Covariance Matrix에 있는 각 diagonal 원소들을 사용하여 수행</strong>하게 됩니다. (<strong>회귀계수 각각에서 스스로의 대한 공분산 = 회귀계수의 분산</strong>) 예를 들어 <span class="math inline">\(\beta_2\)</span>의 분산은 <span class="math inline">\(p\)</span>가 5 (독립변수가 4개, intercept 포함)일 때 Covariance Matrix의 3행 3열 값이 될 것입니다. (intercept가 없다면 2행 2열) 여기서 확인할 수 있듯 Regression Model의 <strong>parameter 추정 역시 중요</strong>하지만, <strong>parameter의 Covariance Matrix를 추정하는 것 또한 유의성 판단에서 매우 중요</strong>하며, 이후의 장들 또한 자연스럽게 Model의 소개, paramater 추정법, parameter의 (Co)variance Matrix를 추정법으로 이루어지게 될 것입니다.</p>
<p>또한, Linear Model 식은 선형대수학에서 <strong>Hyper Plane</strong>으로 정의하는 형태가 되는데, 쉽게 설명하자면 독립변수가 하나일 경우 종속변수와의 2차원 평면에서 직선(2차원의 Hyper Plane)을 띄고, 독립변수가 두 개일 경우 종속변수와의 3차원 공간에서 평면을 띄는(3차원의 Hyper Plane), 선형적으로 공간을 두 부분으로 가르는 공간이라고 생각하시면 될 것 같습니다. (마찬가지로 쉽게 생각하면, 비선형적일 경우 직선이 아니라 곡선, 평면이 아니라 곡면일 것입니다.)</p>
<p>이제 어떻게 Linear Model을 regression, 즉 fit(parameter를 추정)할 것인지 설명하겠습니다. 가장 기본적으로 알려진 Model의 통계학적 parameter 추정법으로는 <strong>Ordinary Least Squares(OLS), Maximum Likelihood Estimation(MLE), Method of Moment(MOM)</strong>가 있습니다. 이후의 모델들은 대부분 MLE로 모델을 추정하지만, Linear Model은 특별하게도 MLE와 OLS의 추정 결과가 같고, OLS의 추정 결과는 BLUE(Best Linear Unbiased Estimator) 입니다. <strong>최소제곱법(OLS, Ordinary Least Squares) 추정량(estimator)</strong>은 <strong>오차 제곱합(SSR, Sum of Squared Residuals)</strong>, 즉 모든 데이터에서 실제 종속변수 값과 모델의 예측 값의 차이를 제곱한 수들의 합을 최소화하는 <span class="math inline">\(\boldsymbol{\beta}\)</span>를 찾는 것이며, 다음의 closed-form solution을 갖습니다: <span class="math display">\[
\hat{\boldsymbol{\beta}} = \left( \mathbf{X}^\top \mathbf{X} \right)^{-1} \mathbf{X}^\top \mathbf{y}
\]</span></p>
<p>그리고, 고전적 가정(특히 등분산성, 독립성, 정상성)이 모두 충족된다고 할 때, 이 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>의 추정오차의 공분산행렬(covariance matrix)은 <span class="math display">\[
\operatorname{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2 \left( \mathbf{X}^\top \mathbf{X} \right)^{-1}
\]</span> 이며, 실제 계산할 때는 데이터에서 추정한 <span class="math inline">\(\hat{\sigma}^2\)</span>를 통해 <span class="math display">\[
\widehat{\operatorname{Var}}(\hat{\boldsymbol{\beta}}) = \hat{\sigma}^2 \left( \mathbf{X}^\top \mathbf{X} \right)^{-1}, \quad where \; \hat{\sigma}^2 = \frac{\sum e_i^2}{N-K}, \; e_i = y_i - x_i \hat{\boldsymbol{\beta}}
\]</span></p>
<p>로 추정합니다. (분모에서 K를 뺀 이유는 degree of freedom에 의한 것이며, 에러 term이 정확히는 차원에 맞춰 <span class="math inline">\(x_i^\top \hat{\boldsymbol{\beta}}\)</span> 지만, 앞으로 이정도 표기는 가독성을 위해 넘어가겠습니다.) 이렇게 구한 모델 추정량과 추정오차의 공분산행렬을 통해 각 회귀계수에 대한 검정을 수행하거나 신뢰구간(CI)을 계산할 수 있게 됩니다.</p>
<p>다음으로 넘어가기 전에, Linear Regression에서 (1) <span class="math inline">\(\hat{\beta}\)</span>이 closed-form solution <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\)</span>을 갖게 되는 과정과, (2) 위에서 언급하였듯 model의 parameter (<span class="math inline">\(\hat{\beta}\)</span>) OLS 추정량이 MLE(Maximum likelihood estimation) 추정량과 같음을 증명하겠습니다.</p>
<p><strong>(1) Prove</strong> <span class="math inline">\(\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\)</span><strong>.</strong></p>
<p>우선, OLS 추정량은 오차 제곱합(SSR)을 최소화하는 식이므로 오차 SSR을 표현하는 식 <span class="math inline">\(J(\beta)\)</span>를 적으면 다음과 같습니다.</p>
<p><span class="math display">\[
J(\beta) = \frac{1}{2} (X\beta - y)^\top (X\beta - y) = \frac{1}{2} \sum_{i=1}^{n} \big( x_i \beta - y^{(i)} \big)^2
\]</span></p>
<p>이제 위 <span class="math inline">\(J(\beta)\)</span>를 <span class="math inline">\(\beta\)</span>에 대해 미분하였을 때 0이 나오는 <span class="math inline">\(\hat{\beta}\)</span>이 SSR이 최소가 되는 OLS 추정량입니다.(이에 대한 증명은 안하겠지만, 간단하게 볼록한 2차 함수에서는 미분값이 0인 점이 최소점인 것과 같은 원리라고 생각하시면 되겠습니다.) 식을 풀어보면,</p>
<p><span class="math display">\[ \nabla_\beta J(\beta) = \nabla_\beta \frac{1}{2} (X\beta - y)^\top (X\beta - y) \\
= \frac{1}{2} \nabla_\beta \big( (X\beta)^\top X\beta - (X\beta)^\top y - y^\top (X\beta) + y^\top y \big) \]</span> 입니다. <span class="math inline">\(\nabla\)</span>는 미분하는 변수가 scalar가 아니라 vector이기 때문에 사용되는 기호이며(gradient), 그냥 <span class="math inline">\(\beta\)</span>로 미분한다고만 생각하시면 됩니다. 이후 미분하는 과정 또한 크게 다르지 않습니다. 식을 계속 진행해보면, <span class="math inline">\(y^\top y\)</span>는 식에 <span class="math inline">\(\beta\)</span>가 없고 <span class="math inline">\((X\beta)^\top y\)</span>와 <span class="math inline">\(y^\top (X\beta)\)</span> 둘다 단순히 두 벡터의 내적인 똑같은 식이므로 <span class="math display">\[ = \frac{1}{2} \nabla_\beta \big( \beta^\top (X^\top X) \beta - 2 (X^\top y)^\top \beta \big) \]</span>이고, 이제 <span class="math inline">\(\beta\)</span>로 미분하면, 우항은 그냥 미분, 좌항은 두 번 나오므로 곱미분을 수행하여<br><span class="math display">\[ = \frac{1}{2} \big( 2 X^\top X \beta - 2 X^\top y \big) \]</span><br><span class="math display">\[ = X^\top X \beta - X^\top y = 0 \]</span></p>
<p>입니다. 결국 OLS 추정량 <span class="math inline">\(\hat{\beta}\)</span>는</p>
<p><span class="math display">\[ X^\top X \hat{\beta} = X^\top y \]</span> 이고, 양변 앞에 역행렬을 곱해주면<br><span class="math display">\[ \hat{\beta} = (X^\top X)^{-1} X^\top y \]</span>가 됩니다.</p>
<p><strong>(2) Prove OLS estimator is same as MLE estimator in Linear Model (Regression).</strong></p>
<p><br>
위 Linear Regression Model 식을 다음과 같이 작성할 수 있습니다. <span class="math display">\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})
\]</span></p>
<p>식이 어색할 수 있지만 정확히 이전의 수학적 표현과 동일하며, <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> term이 복잡해 보이는 이유는, 이전의 네 가지 조건들을 수식으로 반영하여 모델을 확률 기반으로 해석하였기 때문입니다. 식을 설명드리자면, 선형성(Linearity) 가정에 의해 error의 mean이 (기댓값이) 0이 되고, 오차항의 정규성 가정에 의해 정규 분포를 따르며, 오차항의 독립성 및 등분산성 가정에 의해 분산(Covariance Matrix)이 값이 모두 <span class="math inline">\(\sigma^2\)</span>인 대각행렬, 즉 다른 observations간의 correlation은 0이고(없고), 각 observations의 분산은 <span class="math inline">\(\sigma^2\)</span>로 일정함을 표현한 것입니다.</p>
<p><strong>최대가능도방법</strong> 또는 <strong>최대우도법</strong>(Maximum Likelihood Method)은 어떤 모수와 표집한 값들이 주어졌을 때, 표집한 값들이 나올 가능도(확률)을 최대로 만드는 모수를 선택하는 아주 general한 모델의 점 추정 방법이며, 처음 들어보셨다면 대표님께서 더욱 자세하게 설명하신 블로그 글을 읽으시면 좋을 것 같습니다. 철학적으로는 종속변수의 확률 분포를 데이터에 맞게 가정한 후, 모든 observations(data) 각각이 나올 확률(분포)을 모두 곱한 식이 최대가 되도록 하는 모수값이 바로 MLE를 통해 추정한 parameter라고 생각하시면 됩니다. 간단한 예시를 들어보자면, 동전을 던져 앞면이 7번, 뒷면이 3번 나온 데이터에서 동전이 앞면이 나올 확률을 모수로 두고 종속변수의 분포를 베르누이 분포로 가정하면, 각각의 동전을 던져 얻은 관측치들 10개의 data의 Likelihood(확률)를 모두 곱하면 모수가 하나였으니 변수가 하나인 하나의 식(함수)가 나오고, 이 식(함수)을 최대화 하는 모수를 찾는 method가 MLE라고 간단하게 정리할 수 있을 것 같습니다. (이때 추정된 결과는 0.7일 것이고, 모수로 식을 미분한 함수(score function)이 0인 값을 찾음으로써 모수를 추정하게 되며, MLE 이외에도 MAP, 베이지안 등 다른 추정 기법들도 있지만 여기에서는 MLE 정도를 이해하면 충분할 것 같습니다.)</p>
<p>이제 돌아와서 LM에서는 OLS estimator와 MLE estimator가 상응함을 보이겠습니다. 이후의 식 전개는 dim이 1일 때로 증명하겠습니다. Multi-dimension에서는 분포식과 term이 더 복잡하지만 meaning은 1차원과 정확히 동일하기 때문입니다. 오차 항 <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>는 정규분포를 따르므로, n개의 관측치에 대한 Likelihood는 <span class="math display">\[
\mathcal{L}(\boldsymbol{\beta}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{(y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2}{2 \sigma^2} \right)
\]</span></p>
<p>이며, 로그를 씌운 log Likelihood는 <span class="math display">\[
\ell(\boldsymbol{\beta}) = \log \mathcal{L}(\boldsymbol{\beta})
\]</span></p>
<p>입니다. 식이 복잡해 보이지만 데이터 n개에 대하여 정규(가우시안)분포를 따르는 종속변수가 모델의 예측값을 가질 확률을 단순히 모두 곱한 식입니다. 결국 <strong>얻어진 표본 data 자체가 나올 확률에 대한 식을 세운 후, 이를 최대화 하는 parameter를 찾는 과정이 MLE estimation</strong>이라고 직관적으로 해석할 수 있습니다.</p>
<p>돌아와서, log를 씌운 이유를 생각해보겠습니다. log의 그래프를 생각해보시면 log함수는 직관적으로도 monotonically increase하기 때문에 likelihood를 최대화 하는 parameter와 log likelihood를 최대화 하는 parameter는 같으며, log는 곱셈을 덧셈으로, 지수 term을 아래로 만들어주어 likelihood 식이 매우 간단해지기 때문에 (아래에서 실제로 그런지 확인해보세요.) MLE에서는 항상 log likelihood를 최대화하는 parameter를 찾는 방향으로 parameter를 추정합니다. 이어서 전개하면,<br><br><span class="math display">\[
= \log \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{(y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2}{2 \sigma^2} \right)
\]</span><br><span class="math display">\[
= \sum_{i=1}^{n} \log \frac{1}{\sqrt{2 \pi \sigma^2}} - \frac{(y_i - \mathbf{x_i^\top} \boldsymbol{\beta})^2}{2 \sigma^2}
\]</span><br><span class="math display">\[
= n \log \frac{1}{\sqrt{2 \pi \sigma^2}} - \frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y_i - \mathbf{x_i^\top} \boldsymbol{\beta})^2
\]</span></p>
<p>입니다. Log likelihood를 최대화 하는 것은 negative Log likelihood를 최소화 하는 것과 같으므로 양변에 -를 곱하면<br><span class="math display">\[
- \ell(\boldsymbol{\beta}) = -n \log \frac{1}{\sqrt{2 \pi \sigma^2}} + \frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y^{(i)} - \mathbf{x}^{(i)^\top} \boldsymbol{\beta})^2
\]</span></p>
<p>이고, 상수 <span class="math inline">\(-n \log \frac{1}{\sqrt{2 \pi \sigma^2}}\)</span>는 <span class="math inline">\(\boldsymbol{\beta}\)</span>에 영향을 주지 않으므로 제거하면, <span class="math display">\[
- \log \mathcal{L}(\boldsymbol{\beta}) \approx \frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y^{(i)} - \mathbf{x}^{(i)^\top} \boldsymbol{\beta})^2
\]</span></p>
<p>가 됩니다. 위에서 보았듯 OLS estimator는 SSR을 최소화하는 아래 문제로 정의되므로, <span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{OLS}} = \arg \min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y^{(i)} - \mathbf{x}^{(i)^\top} \boldsymbol{\beta})^2
\]</span></p>
<p>결국 위 두 식, OLS estimator와 MLE estimator의 목적식이 같고, 따라서 추정량 또한 같습니다. <span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{MLE}} = \hat{\boldsymbol{\beta}}_{\text{OLS}}
\]</span></p>
<p>위 증명은 MLE로 LM의 parameter를 추정한 것이 아니라 식 전개를 통해 OLS와 목적식이 같음을 보인 것이고, 실제 MLE로 parameter를 추정하는 과정은 2장부터 자세히 보게 될 것입니다. 한 가지 첨언하자면, Linear model에서는 이 두 추정량이 같지만, 정규성 가정이 깨진 경우 두 추정량은 같지 않을 수 있습니다. 이유를 생각해보면, 정규분포는 확률이 가장 큰, 즉 mode가 mean이며 mean을 기준으로 양 옆이 symmetric한 분포이기 때문에 두 추정량이 같으며, 정규분포가 아닐 경우 mean을 추정하려고 하는 MLE는 절대적인 거리를 좁히려는 OLS와 다른 값을 추정하게 된다고 직관적으로 생각해볼 수 있습니다.<br></p>
</section></section><section id="linear-regression-in-homo-vs.-heteroskedasticity" class="level2"><h2 class="anchored" data-anchor-id="linear-regression-in-homo-vs.-heteroskedasticity">3. Linear Regression in Homo vs.&nbsp;Heteroskedasticity</h2>
<p>앞서 언급한 Linear Regression 모형은 “오차항의 등분산성(homoscedasticity)”을 가정합니다. 그러나 실제 연구 상황에서는 이 가정이 위배되는 경우가 흔합니다. <strong>Heteroskedasticity</strong>는 이러한 오차항은 독립 변수에 따라 같지 않을 수 있다는 뜻으로, 등분산성 가정이 깨진 경우를 의미합니다. 즉,</p>
<ul>
<li>
<strong>Homoscedasticity</strong>: 모든 관측값에 대해 오차항(또는 종속변수)의 분산이 동일</li>
<li>
<strong>Heteroskedasticity</strong>: 오차항(또는 종속변수)의 분산이 관측값에 따라 다름</li>
</ul>
<p>입니다. 즉 이전 수식 중 오차 항을 <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>로 두었다면, observations의 분산은 더이상 <span class="math inline">\(\sigma^2\)</span>로 일정하지 않고 observations마다 다른 값을 갖을 수 있다는 뜻입니다. 예를 들어, 임상 연구에서 나이(age)에 따라 혈압(blood pressure)의 분산이 달라질 수 있습니다. 이런 상황에선 등분산성을 가정하는 것이 부적절해집니다. 등분산성 가정이 깨진 상황에서 Linear regression을 수행하면, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> 자체가 편향되진 않더라도(즉, 일관성(consistency)은 여전히 유지), <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> 의 분산 추정치 <span class="math inline">\(\hat{\sigma}^2 ( \mathbf{X}^\top \mathbf{X} )^{-1}\)</span> 가 bias를 갖게 되어 잘못된 표준오차, 잘못된 유의성 검정 결과로 이어질 수 있습니다. 따라서, <strong>Heteroskedasticity가 의심되는 상황에서</strong>는 모델의 분산을 <strong>안정적으로 추정</strong>해야 하며, 이때 1번으로 고려되는 방법 중 하나가 <strong>Heteroskedasticity-consistent(heteroskedasticity-robust) standard errors</strong>, 줄여서 HC standard errors를 통한 (Co)variance Matrix estimation입니다.<br></p>
</section><section id="heteroskedasticity-consistent-standard-errors-hc-standard-errors" class="level2"><h2 class="anchored" data-anchor-id="heteroskedasticity-consistent-standard-errors-hc-standard-errors">4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)</h2>
<section id="hc-standard-errors-정의" class="level3"><h3 class="anchored" data-anchor-id="hc-standard-errors-정의">4.1. HC Standard Errors 정의</h3>
<hr>
<p><strong>HC(Heteroskedasticity-Consistent) standard errors</strong>는, 고전적 선형회귀 가정 중 <strong>등분산성</strong>만 깨졌을 때(나머지 <em>선형성, 정규성, 독립성 가정 in LM</em> 은 그대로 유지) <strong>회귀계수 추정치의 분산</strong>을 “강건(robust)”하게 추정하기 위한 method입니다. 이는 heteroskedasticity-robust standard errors 또는 HCCME(Heteroskedasticity-Consistent Covariance Matrix Estimation) 라고도 부르며, 이전에로 구한 <span class="math inline">\(\hat{\beta}\)</span>를 그대로 사용하되, 그 공<strong>분산행렬 추정만 새롭게(robust하게) 구하는 방식</strong>입니다. 다만, 독립성(오차항들이 서로 독립), 선형성, 정규성 등의 다른 가정이 또 깨져 있다면 HC SE만으로는 대응할 수 없음을 명심하시길 바랍니다. 예를 들어, 오차항이 서로 상관되어 있는 clustered data에서는 더 이상 추정량이 consist하지 않기 때문에, 이보다 한 단계 더 나아간 cluster-robust standard errors, 혹은 GEE, GLMM 등의 모델을 고려해야 합니다. 즉, “등분산성 가정만 깨졌을 때” 쓰는 표준오차(or covariance matrix) 추정량이라고 생각하면 됩니다. 또한, 모델이 실제로 크게 잘못 설정되어 있다면(<em>선형성조차 안 맞는 경우, 모델의 estimator가 크게 편향, HC se가 모델에서 구한 se와 크게 차이가 나는 경우 등</em>), 그때는 variance(standard error)만 “robust standard errors”로 바꾼다고 해서 문제가 해결되지 않고, 모델을 수정하거나 data를 다시 확인하고 분석을 다른 방식으로 수행해야 합니다.</p>
<p>즉 Greene의 말처럼,</p>
<blockquote class="blockquote">
<p>“Simply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.”</p>
</blockquote>
<p>이라는 점을 늘 유의해야 합니다. 정리하자면, 모델을 잘 구성하고, 모델의 분산과 큰 차이가 없을 경우에는 <em>모델의 parameter의 분산 효율성이 최대가 아니거나 MLE 추정을 통해 얻은 모델의 분산이 더 이상 불편향이 아닐 때도</em> heteroskedasticity-robust standard errors를 통해서 모델의 분산을 robust하게 estimate할 수 있으며, 분석 결과 편향이 너무 크지 않다면 이후의 검정에서 이 HC se를 사용함으로써 이후의 통계 분석에서 설득력을 높일 수 있습니다. 추가로 스포하자면, 아래에서 다룰 Heteroskedasticity-robust Standard Errors 식은 Linear Model에서 robust하게 분산을 추정하는 <strong>Robust(or Sandwich) Estimation</strong>이며, 이후의 모델에서도 해당 모델에 대한 Sandwich Estimator를 고려함으로써 안정적인 분산 추정 방법에 대해 다룰 것입니다.</p>
</section><section id="hccme0-수학적-표현" class="level3"><h3 class="anchored" data-anchor-id="hccme0-수학적-표현">4.2. HC(CME)0 수학적 표현</h3>
<hr>
<p>이제 위에서 설명한 Heteroskedasticity한 data에서 robust하게 standard errors, 혹은 Covariance Matrix를 estimate 하는 heteroskedasticity-consistent standard errors의 수식을 유도하겠습니다.</p>
<p>이전에 선형 회귀 모델을 다음과 같이 정의한 적이 있습니다.<br><span class="math display">\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\]</span> 이때, 위에서는 등분산성을 가정하였기 때문에 <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>를 원소값이 모두 <span class="math inline">\(\sigma^2\)</span> 인 대각행렬로 가정하여 각 observations의 분산은 <span class="math inline">\(\sigma^2\)</span>로 일정함을 가정했다면, 이번에 정의한 <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>은 <span class="math inline">\(E(\boldsymbol{\varepsilon}) = 0\)</span>, <span class="math inline">\(E(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^\top) = \Phi\)</span> 입니다. 이때 <span class="math inline">\(\Phi\)</span>는 <span class="math inline">\(\operatorname{diag}(\varepsilon_i^2)\)</span>로 정의되는 대각행렬이며(대각 성분을 제외하면 모두 0인 행렬), 식을 통해서 여전히 error term의 mean이 0이고 observations들은 independent하지만(대각행렬이므로), 더 이상 observations의 분산이 서로 일치하지 않음을 표현한 것입니다. 즉, 이제 Homoscedasticity에서 Heteroskedasticity을 고려하기 시작했다는 것을 수식을 해석함으로써 알 수 있습니다.</p>
<section id="variance-estimatation" class="level4"><h4 class="anchored" data-anchor-id="variance-estimatation">Variance estimatation</h4>
<p>위 식의 OLS 추정량은 여전히 <span class="math display">\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
\]</span></p>
<p>입니다. (still consistent) 이제 모델의 variance 식을 전개해보면, <span class="math display">\[
\operatorname{Var}(\hat{\boldsymbol{\beta}}) = \operatorname{Var} \left ( (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \right )\]</span></p>
<p>이며, <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top\)</span>는 determinant한 값으로, variance는 식 내의 determinant한 salar는 Var 밖으로 제곱과 함께 나오는 것처럼 (<span class="math inline">\(\mathrm{Var}(aX) = a^2\,\mathrm{Var}(X)\)</span>), 행렬 또는 벡터에 대해도 비슷하게 <span class="math display">\[
= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{Var} ( \mathbf{y} ) \left ((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \right ) ^ \top \\
= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{Var} ( \mathbf{y} )  \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}\]</span> 가 됩니다. (벡터와 행렬을 다룰 때에는 위처럼 제곱한다고 생각하시면 될 것 같습니다.) 참고로, <span class="math display">\[\left ( (\mathbf{X}^\top \mathbf{X})^{-1} \right )^ \top = \left ( (\mathbf{X}^\top \mathbf{X})^\top \right )^ {-1} =  (\mathbf{X}^\top \mathbf{X})^ {-1}\]</span> 입니다.</p>
<p>따라서, 처음 설정한대로 <span class="math inline">\(\operatorname{Var} ( \mathbf{y} ) = \Phi\)</span>를 넣으면, 최종적으로 추정된 consistent model <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>의 (co)variance matrix <span class="math inline">\(\operatorname{Var}(\hat{\boldsymbol{\beta}})\)</span>는 다음과 같습니다. <span class="math display">\[
(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \Phi \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>하나 확인할 수 있는 사실은, 등분산성 가정(<span class="math inline">\(\Phi = \sigma^2 \mathbf{I}\)</span>) 하에선 위 식이 아래처럼 단순화되어 이전 (co)variance matrix가 나왔던, 가정하의 특수한 결과라는 것입니다. <span class="math display">\[ \operatorname{Var}(\hat{\boldsymbol{\beta}}) = (\mathbf{X} ^ \top \mathbf{X})^ {-1} \mathbf{X} ^ \top \Phi \mathbf{X} (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \\ = (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \mathbf{X} ^ \top \sigma ^ 2 \mathbf{I} \mathbf{X} (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \\ = \sigma ^ 2 (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \mathbf{X} ^ \top \mathbf{X} (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \\ = \sigma ^ 2 (\mathbf{X} ^ \top \mathbf{X}) ^ {-1} \]</span> 또한, 이 때의 (OLS 추정량)의 분산 <span class="math inline">\(\sigma^2\)</span>의 식도 그냥 보여드리고 넘어갔었는데, 이는 대각 성분이 모두 같기 때문에 단순히 <strong>잔차 제곱합</strong> <span class="math inline">\(\sum e_i^2\)</span>을 자유도(<span class="math inline">\(N-K\)</span>)로 나눈 값(<span class="math inline">\(s^2\)</span>)으로 추정하여 <span class="math inline">\(\hat{\sigma}^2 = \frac{\sum e_i^2}{N-K}\)</span> 로 한번에 표현한 식이었다는 것도 확인할 수 있습니다.</p>
</section><section id="hc0s-robustness" class="level4"><h4 class="anchored" data-anchor-id="hc0s-robustness">HC0’s Robustness</h4>
<p>이분산성이 존재할 때 <span class="math inline">\(\Phi\)</span>는 대각행렬이지만 <strong>대각원소가 서로 다릅니다</strong>. White는 1980년에 이 heteroskedasticity에 robust하게 분산을 추정하기 위해 위 식에서 <span class="math inline">\(\Phi = \sigma^2 \mathbf{I}\)</span>로 term이 소거되게 하는 대신, <span class="math inline">\(\Phi_{ii}\)</span>를 <strong>잔차 제곱</strong> <span class="math inline">\(e_i^2\)</span>로 추정하는 HC0을 제안했습니다: <span class="math display">\[
\hat{\Phi}_{ii} = e_i^2 \quad \Rightarrow \quad \hat{\Phi} = \operatorname{diag}(e_i^2)
\]</span></p>
<p>이를 위 (co)variance 식에 대입하면 HC0 분산 추정량이 얻어집니다: <span class="math display">\[
\operatorname{HC0} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{diag}(e_i^2) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>이는 variance 식으로부터 시작해서 얻은 식이고, 데이터로부터 얻은 matrix <span class="math inline">\(\hat{\Phi} = \operatorname{diag}(e_i^2)\)</span>를 사용하여 이분산성의 구체적 형태를 가정하지 않았기 때문에 안정적으로 heteroskedasticity를 고려합니다. 이처럼 <strong>앞 뒤가 같은 모양 사이에 데이터로부터 얻은 분산 추정 식이 들어간 형태가 샌드위치 같다고 해서 Sandwich Estimator</strong>라고도 부르며, 여전히 <strong>가운데 matrix 항이 diagonal 함</strong>을 통해 오차(또는 관측치)간의 <strong>상관관계는 없다</strong>는 것도 기억해야 합니다.</p>
</section></section><section id="hccme-13-수학적-표현" class="level3"><h3 class="anchored" data-anchor-id="hccme-13-수학적-표현">4.3. HC(CME) 1~3 수학적 표현</h3>
<hr>
<p>이 HC Standard Error가 나온 이후, 위 식으로부터 구한 분산은 소표본에서는 <strong>모델의 overfitting(과적합, 딥러닝과 머신러닝에서도 자주 대두되는 문제로 데이터가 적어 학습한 데이터에서는 오차가 적지만 보지 못한 데이터에 대해서는 오차가 크게 나오는 상황, 통계 분석은 test data를 따로 두지 않기 때문에 overfitting 대처가 더욱 중요.)</strong>에 의해 residual이 작게 나오고, 이에 따라 <strong>모델의 covariance가 과소평가</strong>될 가능성이 크다는 문제가 제시되었습니다. 이에, 소표본에서도 안정적으로 추정하기 위해 White(1980)의 HC Standard Errors를 HC0로 두고, 여러 버전의 HC Standard Errors가 최근까지 다양한 학자들에 의해 연구되고 있습니다. 보통 HC0~HC5까지를 HC Standard Errors로 부르며, HC 오른쪽 숫자가 버전을 뜻하여 이 수가 커질수록 최근에 나온 것이고, 각각은 simulation 실험을 통해 소표본에서 더욱 강건함을 보이는 식으로 논문이 나옵니다. 중요한 점은, 첫 번째로 이 <strong>HC Standard Errors들은 소표본에서는 값이 다르지만 표본의 크기가 무수히 커질수록 asymptotically하게 같으며</strong> 소표본에서의 고려를 위해 값을 더 크게 만드는 추가 term을 넣어준다는 점(물론 수식적으로 필요성을 증명하지만), 둘 째로 R의 sandwich 패키지에서는 4까지 지원하고, 3을 넘는 HC 시리즈가 쓰이는 경우는 거의 볼 수 없기 때문에 3까지의 식 정도만 다루어도 충분하다는 점, 마지막으로 위 직관적인 이유로나 수식적으로나 식을 해석해 보면 거의 모든 표본 cases에 대해 <strong>버전이 클수록 분산을 더욱 크게 추정한다는 점</strong>입니다. <del>따라서 유의성을 보이기 위해서는 버전이 작은게 유리할 것입니다.^^</del> 아래에서는 HC1부터 3까지의 수식을 살펴보겠습니다. 각각의 철학에 대한 자세한 증명 과정 또한 다룰까 했지만 중요하지 않기 때문에 간단히 소개한 후 로컬에서 돌려 보실 R 예시 코드를 보여드릴 것입니다.</p>
<section id="hc1-소표본-편향-보정" class="level4"><h4 class="anchored" data-anchor-id="hc1-소표본-편향-보정">HC1: 소표본 편향 보정</h4>
<p>HC1은 소표본에서 자유도 조정 인자 <span class="math inline">\(\frac{n}{n-k}\)</span>를 도입하여 편향을 줄인다는 철학에서 출발하여, 잔차 <span class="math inline">\(e_i^2\)</span>의 기대값이 <span class="math inline">\(\sigma_i^2(1-h_{ii})\)</span>이므로, <span class="math inline">\(E(e_i^2) \approx \sigma_i^2\)</span>이 되도록 HC0에 <span class="math inline">\(\frac{N}{N-K}\)</span>을 나누어서(1보다 크며, n이 커짐에 따라 1로 수렴하는 term 추가) 스케일링합니다. 즉, 식은 다음과 같습니다: <span class="math display">\[
\operatorname{HC1} = \frac{N}{N-K} \cdot \operatorname{HC0} \\
= \frac{N}{N-K} \cdot (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{diag}(e_i^2) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
</section><section id="hc2-leverage-보정" class="level4"><h4 class="anchored" data-anchor-id="hc2-leverage-보정">HC2: Leverage 보정</h4>
<p>위 HC1에서 <span class="math inline">\(E(e_i^2) = \sigma_i^2(1-h_{ii})\)</span> 라고 이야기 하였습니다. 일단 그렇다고 하면, 기존에는 <span class="math inline">\(\sigma_i^2\)</span>를 <span class="math inline">\(e_i^2\)</span>으로 추정하였다면, 사실 <span class="math inline">\(\frac{e_i^2}{1-h_{ii}}\)</span>의 기대값이 <span class="math inline">\(\sigma_i^2\)</span>이기 때문에 HC1처럼 앞에 scalar term을 추가하는 대신, HC2는 이를 직접적으로 반영하여 <span class="math inline">\(\hat{\Phi}\)</span>를 <span class="math inline">\(\operatorname{diag}(e_i^2)\)</span> 대신 <span class="math inline">\(\operatorname{diag}( \frac{e_i^2}{1-h_{ii}})\)</span>로 추정합니다. 이 때 <span class="math inline">\(h_{ii}\)</span>은 <strong>Leverage</strong>(래버리지), <span class="math inline">\(H(or \;h)\)</span>는 Hat Matrix라고 불리고, 식은 <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\)</span>이며, 위 <span class="math inline">\(h_{ii}\)</span>은 이 matrix의 대각원소를 뜻합니다. Leverage에 대한 이야기 또한 길지만 간단하게 설명하면, <span class="math inline">\(\mathbf{H}\)</span>가 Hat Matrix라고 부르는 이유는 이 <span class="math inline">\(\mathbf{H}\)</span> term에 <span class="math inline">\(\mathbf{y}\)</span>를 곱하면, <span class="math display">\[
\mathbf{Hy} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} \\
= \mathbf{X}\hat{\boldsymbol{\beta}}
=\hat{\mathbf{y}}
\]</span> 이 되어 <span class="math inline">\(\mathbf{y}\)</span>에 hat을(추정치) 씌운다는 의미에서 비롯되었으며, 이는 기하학적으로 더욱 복잡하게 설명될 수 있지만 간단한 의미는 <strong>각 관측치가 얼마나 모델의 estimation에 영향을 주었는지를 보여주는 matrix</strong>입니다. 이 matrix의 diagonal값인 레버리지를 보며 관측치 하나하나의 영향력을 볼 수 있고, <strong>이 값이 큰</strong> <strong>관측치에 대해서는 아래 식에서처럼 분모를 작게 함으로써 분산을 크게 추정</strong>한다고 직관적으로 생각할 수 있습니다. <span class="math display">\[
\operatorname{HC2} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{diag}\left( \frac{e_i^2}{1-h_{ii}} \right) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>넘어가기 전에, 증명 없이 사용해왔던 수식 <span class="math inline">\(E(e_i^2) = \sigma_i^2(1-h_{ii})\)</span> 를 간단하게만 증명해보겠습니다. 예측 오차는 실제값 <span class="math inline">\(y_i\)</span>와 예측값 <span class="math inline">\(\hat{y}_i\)</span>의 차이입니다: <span class="math display">\[
e_i = y_i - \hat{y}_i.
\]</span> 잔차 벡터 <span class="math inline">\(\mathbf{e}\)</span>는 다음과 같이 표현됩니다: <span class="math display">\[
\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = (\mathbf{I} - \mathbf{H}) \mathbf{y}.
\]</span></p>
<p>에러 제곱의 기댓값은 잔차의 분산을 의미합니다. 잔차 벡터 <span class="math inline">\(\mathbf{e}\)</span>의 공분산 행렬은 다음과 같이 계산됩니다: <span class="math display">\[
\text{Cov}(\mathbf{e}) = \text{Cov}((\mathbf{I} - \mathbf{H}) \mathbf{y}) = (\mathbf{I} - \mathbf{H}) \text{Cov}(\mathbf{y}) (\mathbf{I} - \mathbf{H})^T.
\]</span> <span class="math inline">\(\text{Cov}(\mathbf{y}) = \sigma_i^2 \mathbf{I}\)</span>이므로 (당연히 <span class="math inline">\(\sigma\)</span>는 관측치 <span class="math inline">\(i\)</span>마다 다를 수 있습니다.): <span class="math display">\[
\text{Cov}(\mathbf{e}) = (\mathbf{I} - \mathbf{H}) \sigma_i^2 \mathbf{I} (\mathbf{I} - \mathbf{H})^T = \sigma_i^2 (\mathbf{I} - \mathbf{H}).
\]</span> 여기서 <span class="math inline">\(\mathbf{H}\)</span>는 대칭 행렬(<span class="math inline">\(\mathbf{H} = \mathbf{H}^T\)</span>)이고 멱등 행렬(<span class="math inline">\(\mathbf{H}^2 = \mathbf{H}\)</span>)이므로 (이 부분은 기하학적인 설명이 많이 필요해서 증명하지 않겠다만, 이들은 단지 특정 행렬들의 성질이며 이를 만족한다고 하고 넘기겠습니다.): <span class="math display">\[
\text{Cov}(\mathbf{e}) = \sigma_i^2 (\mathbf{I} - \mathbf{H}).
\]</span></p>
<p><span class="math inline">\(i\)</span>번째 잔차 <span class="math inline">\(e_i\)</span>의 분산은 공분산 행렬의 <span class="math inline">\(i\)</span>번째 대각 성분이고, <span class="math display">\[
\text{Var}(e_i) = [\text{Cov}(\mathbf{e})]_{ii} = \sigma^2 (1 - h_{ii}).
\]</span> 에러 제곱의 기댓값은 잔차의 분산과 동일하므로 (에러의 mean이 0이므로), <span class="math display">\[
E[e_i^2] = \text{Var}(e_i) = \sigma^2 (1 - h_{ii}).
\]</span> 가 증명됩니다.</p>
</section><section id="hc3-강화된-leverage-보정" class="level4"><h4 class="anchored" data-anchor-id="hc3-강화된-leverage-보정">HC3: 강화된 Leverage 보정</h4>
<p>HC3은 <strong>Jackknife 접근법</strong>에서 영감을 받아 HC2에서 <span class="math inline">\(\Phi\)</span>의 분모 term을 더 강하게 <span class="math inline">\((1-h_{ii})^2\)</span>로 둡니다. 직관적으로, 같은 <span class="math inline">\(h_{ii}\)</span>에 대해 더 크게 분산을 추정 (<span class="math inline">\((1-h_{ii})^2 &lt; 1-h_{ii}\)</span>)하여, 고레버리지, 즉 outlier에 대해 더 안정적으로 소표본의 분산을 추정할 수 있습니다. <span class="math display">\[
\operatorname{HC3} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{diag}\left( \frac{e_i^2}{(1-h_{ii})^2} \right) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
</section><section id="hccmes-비교" class="level4"><h4 class="anchored" data-anchor-id="hccmes-비교">HC(CME)s 비교</h4>
<p>위 내용을 간단하게 표로 정리하면 다음과 같습니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 46%">
<col style="width: 18%">
<col style="width: 27%">
</colgroup>
<thead><tr class="header">
<th>유형</th>
<th>수식</th>
<th>보정 요소</th>
<th>사용 시나리오</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>HC0</td>
<td><span class="math inline">\(\operatorname{diag}(e_i^2)\)</span></td>
<td>없음</td>
<td>대표본</td>
</tr>
<tr class="even">
<td>HC1</td>
<td><span class="math inline">\(\frac{n}{n-k}\operatorname{HC0}\)</span></td>
<td>자유도</td>
<td>소표본 기본</td>
</tr>
<tr class="odd">
<td>HC2</td>
<td><span class="math inline">\(\frac{e_i^2}{1-h_{ii}}\)</span></td>
<td>레버리지 1차</td>
<td>고레버리지 존재 시</td>
</tr>
<tr class="even">
<td>HC3</td>
<td><span class="math inline">\(\frac{e_i^2}{(1-h_{ii})^2}\)</span></td>
<td>레버리지 2차</td>
<td>소표본 + 고레버리지</td>
</tr>
</tbody>
</table>
<p>정리하자면, 동분산성 검정을 검정하여 이분산성이 확인되면 HC se 시리즈의 적용을 고려해야 하고, 표본 크기가 <span class="math inline">\(n &lt; 50\)</span>인 경우는 HC3 또는 아래에서 다룰 bootstrap기반의 (Clustered) Bootstrap Covariance Matrix Estimation를, <span class="math inline">\(n &gt; 500\)</span>인 경우는 HC0/HC1로 충분하다고 간단하게 생각해볼 수 있습니다. (물론 이는 정해진 규칙이 아니고, 시작은 항상 HC0부터 검정해보는 것이 맞습니다.)<br></p>
</section></section></section><section id="wild-bootstrap" class="level2"><h2 class="anchored" data-anchor-id="wild-bootstrap">5. (Wild) Bootstrap</h2>
<p>마지막으로, sandwich estimator는 아니지만, 모든 모델에서 data로부터 안정적으로 분산을 추정하여 의학 연구에서 인정하는 기법 중 하나인 Bootstraping을 통한 분산 term 추정 기법을 간단하게 소개드리겠습니다. 이 method는 특정 가정으로부터 수학적인 추정식을 세워 분산을 추정하는 대신, Bootstraping 통계 기법을 사용하여 data 만으로 경험적인 분산을 추정합니다. 때문에 이번에 소개하는 (Wild) Bootstrap 은 어떤 가정이나 모델이든 사용할 수 있는, 심지어 2장에서 다룰 clustered(grouped) data 에서도 사용할 수 있는 방법입니다.(가정이 전혀 들어가지 않았기 떄문입니다.) 이후에 다룰 <strong>샌드위치 추정량(Sandwich Estimator)</strong>과, 그의 OLS 버전인 <strong>HC(Heteroskedasticity-Consistent)</strong> 시리즈가 개발되었지만, 이들은 결국 대표본 점근적 성질로부터 얻은 수식이고, 극단적인 <strong>쇼포본</strong> 같은 작은 클러스터 수에서는 이러한 부트스트랩이 더 강력한 대안이 될 수도 있습니다. 이를 소개하기 전에 간단히 중요한 통계학적 resampling 기법인 잭나이프(Jackknife)와 부트스트랩(Bootstrap)을 얘기해보겠습니다. 이 둘은 모두 표본을 계속해서 얻기 힘든 상황 (현실에서는 대부분 그럴 것입니다.)에서 모수를 추정하기 위해, 이미 갖고 있는 표본의 data로부터 resampling을 하여 추정하는 통계학적 방법론입니다.</p>
<section id="잭나이프jackknife" class="level4"><h4 class="anchored" data-anchor-id="잭나이프jackknife">잭나이프(Jackknife)</h4>
<p><strong>Jackknife</strong> method는 한 번에 하나의 클러스터를 제외하고 모델을 추정 하는 것을 모든 클러스터에 대해 반복하여 분산을 계산합니다:</p>
<p><span class="math display">\[
\mathrm{Var}(\hat{\beta}_c) = \frac{n-1}{n} \sum_{c=1}^C \left(\hat{\beta}_c - \bar{\hat{\beta}}\right)^2,
\]</span></p>
<p>여기서 <span class="math inline">\(\hat{\beta}_c\)</span>는 c번째 클러스터를 제외한 추정치, <span class="math inline">\(\bar{\hat{\beta}}\)</span>는 추정치의 평균이고, 이는 resampling 기법이지만 위에서 설명드렸던 HC3 추정량이 이 식으로부터 도출된 결과입니다. 물론, 이는 이론적인 결과이므로 각각이 서로 다른 결과를 도출할 수도 있을 것입니다. 단, resampling 기법들은 그 특성상 클러스터나 그 data point의 수가 많을 경우 계산 부하가 큽니다.</p>
</section><section id="부트스트랩bootstrap" class="level4"><h4 class="anchored" data-anchor-id="부트스트랩bootstrap">부트스트랩(Bootstrap)</h4>
<p><strong>Bootstrap</strong> method는 데이터를 무작위로 resampling하여 여러 버전의 데이터셋을 생성하고, 각각에서 모델을 추정한 후 추정값의 변동성을 분산으로 사용합니다. 대표적인 예시로 <strong>와일드 부트스트랩 (Wild Bootstrap)</strong>은 잔차(residual)에 랜덤 가중치를 곱해 인위적 으로 데이터의 분산을 크게 보정하고, <strong>케이스 기반 (XY/Pairs 부트스트랩)</strong>은 기본적인 방법으로 클러스터 자체를 복원 추출하여 새 data를 생성한 뒤 구하는 과정을 반복하며, <strong>잔차 기반 (Residual 부트스트랩)</strong>은 잔차 만을 리샘플링하고 재추정하는 방법입니다다.</p>
</section><section id="wild-bootstrap-1" class="level4"><h4 class="anchored" data-anchor-id="wild-bootstrap-1">Wild Bootstrap</h4>
<p>위에서 설명드렸듯, 잔차 <span class="math inline">\(e_i\)</span>에 “랜덤 가중치” <span class="math inline">\(w_i\)</span>를 곱해 새로운 반응 변수를 생성하는 방법으로 다음과 같을 것입니다:</p>
<p><span class="math display">\[
y^* = \hat{y} + w_i \hat{e}.
\]</span></p>
<p>이때 <strong>가중치 분포</strong> 로 사용되는 대표적인 예시는 Rademacher: <span class="math inline">\(w_i \in \{-1, 1\}\)</span>, Mammen: <span class="math inline">\(w_i = \frac{\sqrt{5} \pm 1}{2}\)</span>, Webb: 6점 분포(<span class="math inline">\(w_i \in \left\{ -\sqrt{\frac{3}{2}}, -\sqrt{\frac{2}{2}}, -\sqrt{\frac{1}{2}}, \sqrt{\frac{1}{2}}, \sqrt{\frac{2}{2}}, \sqrt{\frac{3}{2}} \right\}\)</span>)로 클러스터별 정규분포 등 다른 적용도 가능합니다. 위에서 언급하였듯, 이외에도 <strong>케이스 기반 (XY) 부트스트랩</strong>, <strong>잔차 부트스트랩</strong>, <strong>프랙셔널 부트스트랩(Fractional Bootstrap)</strong>등의 방법론이 있지만, data를 크게 왜곡하지 않는 Wild가 이러한 분산 추정에서 디폴트하게 고려됩니다.</p>
</section></section><section id="r-예시-hc03-및-부트스트랩" class="level2"><h2 class="anchored" data-anchor-id="r-예시-hc03-및-부트스트랩">6. R 예시: HC0~3 및 부트스트랩</h2>
<p>아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. 위 내용 중 “시리즈가 클 수록 강건하게 추정한다”는 점을 기억하시고 해석하면 됩니다.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">## mtcars 데이터에서 mpg(연비)를 wt(차량 무게), hp(마력)으로 설명하는 회귀모형</span></span>
<span><span class="co">#data(mtcars)</span></span>
<span><span class="co">#model &lt;- lm(mpg ~ wt + hp, data = mtcars)</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 기본 OLS 표준오차 (등분산 가정)</span></span>
<span><span class="co">#summary(model)  </span></span>
<span><span class="co">#</span></span>
<span><span class="co">## HC 표준오차 계산</span></span>
<span><span class="co">#library(sandwich)</span></span>
<span><span class="co">#library(lmtest)</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## HC 유형별 분산-공분산 행렬</span></span>
<span><span class="co">#cov_hc0 &lt;- vcovHC(model, type = "HC0")  # 기본 White 추정량</span></span>
<span><span class="co">#cov_hc1 &lt;- vcovHC(model, type = "HC1")  # 자유도 보정</span></span>
<span><span class="co">#cov_hc2 &lt;- vcovHC(model, type = "HC2")  # 잔차 스케일링</span></span>
<span><span class="co">#cov_hc3 &lt;- vcovHC(model, type = "HC3")  # 작은 표본 보정</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 계수 테스트 결과 비교</span></span>
<span><span class="co">#coeftest(model, vcov = cov_hc0)  </span></span>
<span><span class="co">#coeftest(model, vcov = cov_hc1)  </span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 부트스트랩 분산-공분산 행렬 (기본 100회 복제)</span></span>
<span><span class="co">#cov_wild &lt;- vcovBS(fit, cluster = ~cluster_id, type = "wild", R = 1000)</span></span>
<span><span class="co">#cov_xy &lt;- vcovBS(model, R = 200, type = "xy")  # xy-쌍 부트스트랩</span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 결과 비교</span></span>
<span><span class="co">#coeftest(model, vcov = cov_wild)  </span></span>
<span><span class="co">#coeftest(model, vcov = cov_xy)  </span></span>
<span><span class="co">#</span></span>
<span><span class="co">## 주의: 부트스트랩 결과는 실행 횟수 R, 실행 시마다 다를 수 있음</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="마무리하며" class="level2"><h2 class="anchored" data-anchor-id="마무리하며">마무리하며</h2>
<hr>
<p>이번 1장에서는 <strong>Regression Analysis</strong>의 기본 개념과 <strong>(simple, multiple or General) Linear Regression</strong>의 이론적 배경, 그리고 등분산성 가정이 깨진(Heteroskedasticity) 상황에서 유용하게 쓰이는 <strong>Heteroskedasticity-Consistent Standard Errors(HC standard errors)</strong>에 대해 살펴보았습니다.</p>
<p><strong>HC standard errors</strong>를 사용하면, Linear Regression 모델에서 등분산성 가정이 위배되더라도 Standard Errors(or Covariance Matrix)를 좀 더 타당하게 추정할 수 있습니다. 하지만, “<strong>오차항의 독립성</strong>, <strong>선형성</strong>, <strong>정규성</strong>” 등 나머지 가정이 크게 어긋난다면, 단순히 HC SE로 분산을 보정하는 것만으로는 해결되지 않습니다. HC SE가 기본 OLS 분산추정치와 크게 다르다면, “정말 모델이 맞는지”를 다시 고민하고, 필요하다면 모델을 재설정하거나 다른 방법을 모색해야 합니다.</p>
<p>어쨌든, Heteroskedasticity가 의심되는 상황에서 가장 먼저 고려할 만한 접근인 <strong>HC(Heteroskedasticity-Consistent) standard errors</strong>는 모델의 유의성 검정을 위해 안정적으로 분산을 추정할 때 널리 쓰이며, 의학 연구에서도 일상적으로 활용되고 있습니다. 다음 2장에서는 이런 Linear Regression을 더 확장한 <strong>Generalized Linear Model(GLM)</strong>의 개념을 본격적으로 다루고, HC standard errors의 <strong>clustered data</strong> 버전인 <strong>Cluster-robust standard error</strong>에 대해서도 다룰 예정입니다.</p>


</section><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{seungjun2025,
  author = {Seungjun, Lee},
  title = {Exploring {Regression} {Models} for {Regression} {Analysis}
    (1): {Regression} {Analysis,} {Linear} {Regression,} {HC} {Standard}
    {Errors}},
  date = {2025-02-28},
  url = {https://blog.zarathu.com/posts/2025-02-28-reg1/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-seungjun2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Seungjun, Lee. 2025. <span>“Exploring Regression Models for Regression
Analysis (1): Regression Analysis, Linear Regression, HC Standard
Errors.”</span> February 28, 2025. <a href="https://blog.zarathu.com/posts/2025-02-28-reg1/">https://blog.zarathu.com/posts/2025-02-28-reg1/</a>.
</div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/blog\.zarathu\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="zarathucorp/giscus-blog" data-repo-id="R_kgDOHztuxg" data-category="General" data-category-id="DIC_kwDOHztuxs4CQ6h5" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org">Quarto</a>.</p>
</div>   
    <div class="nav-footer-center">
<p>© 2019. <a href="https://www.zarathu.com">Zarathu Co.,Ltd.</a> All rights reserved. Licence: <a href="https://opensource.org/license/mit-0/">MIT</a>.</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>