[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "차라투 블로그",
    "section": "",
    "text": "목차\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Models for Competing Risk Analysis\n\n\n\n\n\nCompeting risk model의 C-index, AUC, Brier score를 계산하고 비교하는 R 코드 소개 \n\n\n\n\n\n2025/04/03\n\n\nSuhyun Han\n\n\n\n\n\n\n\n\n\n\n\n\nSankey Diagram 텍스트 삽입\n\n\n\n\n\nR로 생성한 Sankey plot에 JavaScript를 활용해 텍스트를 추가하는 방법을 알아보았습니다. \n\n\n\n\n\n2025/03/26\n\n\nYeJi Kang\n\n\n\n\n\n\n\n\n\n\n\n\nIntraclass Correlation Coefficient 공부하기\n\n\n\n\n\n\nstatistics\n\n\n\nIntraclass Correlation Coefficient에 대해 정리하였습니다. \n\n\n\n\n\n2025/03/24\n\n\nSungho Choi\n\n\n\n\n\n\n\n\n\n\n\n\nKappa 분석 이해하기\n\n\n\n\n\n여러 종류의 Kappa 분석 방법을 설명하고, R 코드 예제를 통해 실전에서 활용하는 방법을 정리하였습니다. \n\n\n\n\n\n2025/03/18\n\n\nYeJi Kang\n\n\n\n\n\n\n\n\n\n\n\n\nquarto 의 기초\n\n\n\n\n\nYAML Header, 마크다운(Markdown) 텍스트, R 코드 청크(chunk) 그리고 그림과 테이블을 중심으로, R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 정리하였습니다. \n\n\n\n\n\n2025/03/13\n\n\nYuJeong Yoon\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors\n\n\n\n\n\n\nstatistics\n\n\n\nRegression Analysis의 기본 Model인 Linear regression과 이의 robust한 (Co)variance Matrix Estimator인 HC Standard Errors(HCCCM), Wild Bootstrap에 대해서 공부합니다.\n\n\n\n\n\n2025/02/28\n\n\nLee Seungjun\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error\n\n\n\n\n\n\nstatistics\n\n\n\n종속변수가 Non-Normal한 Data에서 Regression Analysis를 수행하기 위해 종속변수의 분포 조건을 Exponential Family로 확장하고 Link Function로 근사하는 Regression Model인 GLM의 수학적 원리에 대해서 공부합니다. 또, clustered data 버전의 robust 표준오차(Cluster-robust standard error)에 대해서 살펴봅니다.\n\n\n\n\n\n2025/02/28\n\n\nLee Seungjun\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation\n\n\n\n\n\n\nstatistics\n\n\n\nData가 Independent하지 않고 Clustered되어 있을 때 Regression Analysis를 수행하기 위해 GLM에서 발전된 두 가지 모델 GEE와 GLMM의 개념을 공부하고, M-estimator와 Robust (sandwich) estimation을 통해 지금까지의 Robust한 Covariance Matrix을 generalize하게 살펴봅니다.\n\n\n\n\n\n2025/02/28\n\n\nLee Seungjun\n\n\n\n\n\n\n\n\n\n\n\n\nSample Size Estimation in MRMC\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\nrpackage\n\n\nRJafroc\n\n\n\nRJafroc Package를 통해 Multi-Reader Multi-Case(이하 MRMC) 연구에서 표본 크기를 추정하는 방법에 대해 알아보자. \n\n\n\n\n\n2025/02/14\n\n\nJunsik CHU\n\n\n\n\n\n\n\n\n\n\n\n\nStandardized Mean Difference\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\nEffect size 중 하나인 Standardized Mean Difference의 개념을 살펴보고, SMD를 활용해 baseline covariate의 balance 평가를 해보자. \n\n\n\n\n\n2025/01/23\n\n\nDonghyung Lee\n\n\n\n\n\n\n\n\n\n\n\n\nCompeting Risk Analysis\n\n\n\n\n\n\nstatistics\n\n\n\nCompeting Risk Analysis의 원리와 결과가 가지는 의미를 살펴봅니다\n\n\n\n\n\n2025/01/03\n\n\nHyungwoo Jo\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Reader Multi-Case Analysis\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\nrpackage\n\n\niMRMC\n\n\n\nMRMC(Multi-Reader Multi-Case) 분석을 이해하고, iMRMC package를 통해 예시를 살펴본다. \n\n\n\n\n\n2024/12/27\n\n\nDonghyung Lee\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table의 rolling join\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\n\ndata.table에서 제공하는 inequality join method 중 하나인 rolling merge에 대해 알아보자 \n\n\n\n\n\n2024/12/13\n\n\nJeongmin Seo\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots: 간편한 그래프 작성 라이브러리\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nGgplot2\n\n\ntidyplots\n\n\n\ntidyplots 패키지를 이용하여 주어진 데이터로 빠르게 그래프를 그려보자 \n\n\n\n\n\n2024/12/06\n\n\nJeongmin Seo\n\n\n\n\n\n\n\n\n\n\n\n\nDeLong’s Method; for ROC AUC\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\nROC AUC와 관련된 통계; SE 추정과 이의 CI 구하기. \n\n\n\n\n\n2024/11/20\n\n\nHojun LEE\n\n\n\n\n\n\n\n\n\n\n\n\ncollapse 패키지 소개 v2\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\ndata.table\n\n\n\ncollapse; fast, flexible, parsimonoius code package for R. \n\n\n\n\n\n2024/10/29\n\n\nHojun LEE\n\n\n\n\n\n\n\n\n\n\n\n\nRWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\nClone-Censor-Weight method를 이용한 Target trial emulation을 소개하고, 실습해 봅니다. \n\n\n\n\n\n2024/10/17\n\n\nJihyuk Lee\n\n\n\n\n\n\n\n\n\n\n\n\nLLM을 이용한 분석 report 생성 (1)\n\n\n\n\n\n\nR\n\n\nAPI\n\n\nDocumentation\n\n\n\nLLM을 이용하여 표 또는 그래프를 요약하는 Quarto 문서를 만들어 봅시다. \n\n\n\n\n\n2024/10/14\n\n\nYoung Sun Park\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey design 모델에서의 통계\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\nSurvey design 모델에 대해 알아보고, 이를 직관적으로 표시할 수 있는 table을 만들어봅니다. \n\n\n\n\n\n2024/10/08\n\n\nHyungwoo Jo\n\n\n\n\n\n\n\n\n\n\n\n\njstable을 이용한 분석 모형 table 만들기\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\nGLM, GLE, GLMM, COX 모델에 대해 알아보고, 이를 직관적으로 표시할 수 있는 table을 만들어봅니다. \n\n\n\n\n\n2024/09/13\n\n\nHyungwoo Jo\n\n\n\n\n\n\n\n\n\n\n\n\nWald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\nPaired data에서 wald 신뢰 구간을 구하는 방법을 통해 민감도와 특이도의 신뢰 구간을 구하는 방법을 소개합니다. \n\n\n\n\n\n2024/08/26\n\n\nHyojong Myung\n\n\n\n\n\n\n\n\n\n\n\n\ncompeting risk 생존분석, fine-and-gray method와 multi-state model\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\ncompeting risk가 존재할 때 생존 분석할 수 있는 fine-and-gray method와 multi-state model을 소개합니다 \n\n\n\n\n\n2024/08/22\n\n\nHyojong Myung\n\n\n\n\n\n\n\n\n\n\n\n\nthe Extended DLNM 소개\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\nR의 dlnm 패키지를 이용해 the Extended DLNM을 적합시키는 방법을 소개합니다.\n\n\n\n\n\n2024/06/05\n\n\nSuhyeon Kwon\n\n\n\n\n\n\n\n\n\n\n\n\nShiny app 꾸미기\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nbslib\n\n\n\nShiny app을 간단하게 꾸밀 수 있는 방법을 소개합니다.\n\n\n\n\n\n2024/05/24\n\n\nHeeseok Choi\n\n\n\n\n\n\n\n\n\n\n\n\npatchwork를 활용한 고급 시각화\n\n\n\n\n\n\nR\n\n\ncowpot\n\n\nggplot2\n\n\nofficer\n\n\npatchwork\n\n\npowerpoint\n\n\n\nggplot2을 활용한 R 시각화의 결과물을 MS 파워포인트로 만들어내는 과정에서 쓰이는 R 패키지와 방법을 소개합니다. \n\n\n\n\n\n2024/05/17\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nrhub와 Github action를 활용한 OS별 R 패키지 검증\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ngithub\n\n\ngithub action\n\n\nrhub\n\n\nrcmdcheck\n\n\n\nrhub 패키지와 Github action을 사용해 R 패키지를 다양한 OS에서 정상적으로 설치, 실행할 수 있도록 확인 하는 R CMD CHECK 방법을 소개합니다. \n\n\n\n\n\n2024/05/13\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nquarto 에서 다중 언어로 콘텐츠 제공하기\n\n\n\n\n\n\nquarto\n\n\n\n애옹애옹 \n\n\n\n\n\n2024/04/10\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nbslib: input_task_button 소개\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nbslib\n\n\nux\n\n\n\nbslib의 0.7.0에서 새롭게 추가된 input_task_button에 대해 소개합니다. \n\n\n\n\n\n2024/03/30\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nProcess macro 소개\n\n\n\n\n\n\nR\n\n\n\n조절효과, 매개효과 분석을 위한 도구인 process macro를 소개합니다.\n\n\n\n\n\n2024/03/14\n\n\nHeeseok Choi\n\n\n\n\n\n\n\n\n\n\n\n\nshinylive 를 활용한 quarto 블로그에 shiny 추가 방법\n\n\n\n\n\n\nR\n\n\nshiny\n\n\ngithubpage\n\n\ndocumentation\n\n\nwebsite\n\n\nquarto\n\n\n\nwebR의 개선 버전인 shinylive 패키지를 사용하여 정적 페이지에 shiny application 추가하기 \n\n\n\n\n\n2024/03/05\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR&D 시험인증 후기 및 개발, 행정 관련 느낀 점\n\n\n\n\n\n\nR&D\n\n\nreview\n\n\nIITP\n\n\n시험인증\n\n\n행정\n\n\n\nR&D 시험인증 및 R&D 관련 느낀점입니다. \n\n\n\n\n\n2024/01/23\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Server에 2FA(OTP) 도입하기\n\n\n\n\n\n\nR\n\n\nrstudio server\n\n\nsecurity\n\n\n\nRStudio Server에 2차 인증(OTP)를 도입한 후기입니다. \n\n\n\n\n\n2024/01/05\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Dashboard를 이용해 대시보드 개발하기\n\n\n\n\n\n\nR\n\n\npython\n\n\nquarto\n\n\nquarto dashboard\n\n\ndashboard\n\n\n\nQuarto를 사용하여 R과 Python, Julia로 인터랙티브한 대시보드를 만드는 방법을 소개합니다. \n\n\n\n\n\n2023/12/11\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRstudio에서 Copilot을 활용해 AI로 코딩하기\n\n\n\n\n\n\nR\n\n\ngithub\n\n\nrstudio\n\n\ncopilot\n\n\n\nGithub 의 AI 서비스 Copilot을 Rstudio에 연동하여 자동 완성으로 코딩하는 방법을 소개합니다. \n\n\n\n\n\n2023/11/21\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDiscourse 기반 커뮤니티 구축\n\n\n\n\n\n\ndocker\n\n\nwebsite\n\n\ndiscourse\n\n\ncommunity\n\n\n\n커뮤니티를 만들기 위한 오픈소스 무료 플랫폼인 Discourse와 만드는 과정을 소개합니다. \n\n\n\n\n\n2023/11/10\n\n\nJisoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Manuscripts를 이용해 학술 논문 작성하기\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nquarto manuscript\n\n\narticle\n\n\n\n코드 블록을 포함한 학술 논문을 웹에 발행하고, PDF, docx등 다양한 형식으로 다운받을 수 있는 Quarto Manuscript를 소개합니다. \n\n\n\n\n\n2023/10/18\n\n\nJihee Han\n\n\n\n\n\n\n\n\n\n\n\n\nR로 만든 PPT 슬라이드 고해상도로 저장하기\n\n\n\n\n\n\nR\n\n\npowerpoint\n\n\nvisualization\n\n\n\nofficer 패키지를 활용해 파워포인트로 저장한 이미지를 300DPI의 고해상도로 내보내기 (해상도 설정 변경 방법은 운영체제마다 다르며, 본 게시글은 Windows OS를 기준으로 합니다.) \n\n\n\n\n\n2023/09/27\n\n\nJihee Han\n\n\n\n\n\n\n\n\n\n\n\n\nelectron forge를 활용하여 Standalone Shiny Application 제작하기\n\n\n\n\n\n\nR\n\n\nelectron forge\n\n\nshiny\n\n\nquarto\n\n\nstandalone\n\n\nexe\n\n\nwindows\n\n\n\nelectron forge라는 기술을 활용하여 사용자의 PC에서 R과 Rstudio를 설치하지 않고도 Shiny App을 사용할 수 있게 하는 (exe) 프로그램을 만드는 방법을 소개합니다. 단, 개발 과정은 OS에 따라 조금씩 다르며, windows를 기준으로 합니다. \n\n\n\n\n\n2023/09/18\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nweb assembly를 이용하여 웹페이지에서 Shiny App 활용하기\n\n\n\n\n\n\nR\n\n\nwebR\n\n\nshiny\n\n\nwasm\n\n\ngithub page\n\n\nquarto\n\n\n\nwebR이라는 기술을 활용하여 별도의 웹 서버를 사용하지 않고도 유저의 웹 브라우저(크롬)에서 Shiny App을 사용할 수 있게 하는 방법을 소개합니다. \n\n\n\n\n\n2023/09/10\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nweb assembly를 이용하여 웹페이지에서 R 활용하기\n\n\n\n\n\n\nR\n\n\nwebR\n\n\nwasm\n\n\ngithub page\n\n\nquarto\n\n\n\nwebR이라는 기술을 활용하여 별도의 웹 서버를 사용하지 않고도 유저의 웹 브라우저(크롬)에서 R을 사용할 수 있게 하는 방법을 소개합니다. \n\n\n\n\n\n2023/09/09\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기\n\n\n\n\n\n\nR\n\n\nofficer\n\n\nvectorgraphics\n\n\nggplot2\n\n\npowerpoint\n\n\nvisualization\n\n\n\nR을 활용하여 만든 이미지를 PowerPoint에서 편집할 수 있도록, 벡터 그래픽을 만드는 법을 소개합니다. \n\n\n\n\n\n2023/07/01\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny 기반 방역관리 위험도 평가 대시보드\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\nRSQLite, shinyauthr 등 여러 R package를 이용한 웹 기반 방역관리 위험도 평가 대시보드 제작 과정 \n\n\n\n\n\n2023/06/20\n\n\nYeongho Kim\n\n\n\n\n\n\n\n\n\n\n\n\nADaM in CDISC and tidyCDISC\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\n\nAnalysis Data Model in CDISC, tidyCDISC 오픈소스 프로그램에 대해서 알아보자 \n\n\n\n\n\n2023/05/02\n\n\nSeoyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nsunburstr 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\n\nsunburstr 패키지의 이용한 계층적 데이터 표시 방법에 대해 알아보자 \n\n\n\n\n\n2023/04/21\n\n\nSeoyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\npkgdown을 활용한 R 패키지 문서화\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\npkgdown\n\n\ngithub page\n\n\ndocumentation\n\n\nwebsite\n\n\n\nR 패키지를 다른 사람들도 잘 활용할 수 있게 설명해주는 웹사이트를 pkgdown을 사용하여 만들어보자 \n\n\n\n\n\n2023/03/17\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nshiny.likert 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\nplotly\n\n\nshinyapps\n\n\nlikert\n\n\ndatavis\n\n\nnocode\n\n\n\nlikert 패키지를 사용하는 shiny apps 개발/ 배포 과정 \n\n\n\n\n\n2023/02/15\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython을 이용한 검색포털 API 활용\n\n\n\n\n\n\npython\n\n\nAPI\n\n\n\npython을 이용한 크롤링 검색결과 저장하기 \n\n\n\n\n\n2023/02/15\n\n\nWon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython과 Slack API를 이용한 대나무숲 앱 제작\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\n\nSlack API를 이용하여 Python 기반 대나무숲 앱 만들기 \n\n\n\n\n\n2023/02/07\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nlikert 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\nplotly\n\n\nquarto\n\n\n\n문항별 만족/불만족을 한번에 표현하는 likert chart를 그려보자 \n\n\n\n\n\n2023/02/05\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython Streamlit 패키지를 이용한 대시보드 만들기\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\n\nstreamlit 패키지의 사용방법을 알아보자 \n\n\n\n\n\n2023/02/01\n\n\nWon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의료데이터분석가 성장기\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의료데이터분석가 성장기를 다시 정리했습니다. 본 내용은 보건산업진흥원이 지원하고 성균관대학교 의과대학에서 주관하는 “융합형 의사과학자 심포지엄” 에서 발표 예정입니다. \n\n\n\n\n\n2022/10/17\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGAM(Generalized Additive Model) 소개\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n비선형모델인 GAM(Generalized Additive Model) 을 소개합니다. 본 강의는 성균관대 바이오헬스 규제과학과 강의자료로 쓰일 예정입니다. \n\n\n\n\n\n2022/09/30\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny for Python\n\n\n\n\n\n\npython\n\n\nshiny\n\n\n\nPython으로 반응형 웹 어플리케이션을 만들수 있는 Shiny for Python에 대해 소개합니다.\n\n\n\n\n\n2022/08/19\n\n\nChaehee Lee\n\n\n\n\n\n\n\n\n\n\n\n\ncollapse 패키지 소개\n\n\n\n\n\n\nR\n\n\ncollapse\n\n\ndata.table\n\n\n\ncollapse 패키지를 소개하고 data.table 패키지와 비교하여 파악해보겠습니다.\n\n\n\n\n\n2022/07/25\n\n\nBeomsu Park\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table 패키지 기초\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\nlecture\n\n\n\n데이터를 빠르게 가공할 수 있는 data.table에 대하여 패키지 설치부터, 기본 구조 및 데이터를 가공하여 재구조화 하는 방법에 대해서 소개합니다.\n\n\n\n\n\n2022/07/13\n\n\nJunhyuk Ko\n\n\n\n\n\n\n\n\n\n\n\n\n데이터과학자가 갖춰야할 기술\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nstatistics\n\n\n\n의료분야 데이터과학자에 필요한 역량을 정리하였습니다. 본 내용은 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 발표할 예정입니다. \n\n\n\n\n\n2022/06/27\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n22년 지원사업 후기\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n22년 각종 지원사업 선정, 탈락 후기를 공유합니다. \n\n\n\n\n\n2022/04/12\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR로 논문용 그래프 그리기\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\n\nR 기본 함수, ggplot2 패키지, ggpubr 패키지를 활용해 의학논문에 필요한 그래프를 만들어보자. \n\n\n\n\n\n2022/03/25\n\n\nYumin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nReviewer들을 위한 의학통계\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 로 발표할 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2022/03/19\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ndata.table\n\n\n\n대용량의 데이터를 분산 처리 시스템 없이 처리할 수 있는 data.table 데이터 구조와 이를 조작, 관리하는데 사용하는 data.table 패키지에 대해서 소개합니다. \n\n\n\n\n\n2022/02/11\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n\ndocker\n\n\nlecture\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n2022/02/08\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\ntableone 패키지 소개\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n효율적으로 의학 연구 논문에 들어갈 table1을 만들 수 있는 tableone 패키지에 대해 소개합니다.\n\n\n\n\n\n2022/02/07\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\ngtsummary 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ngtsummary\n\n\n\n데이터 셋의 변수를 하나의 테이블로 요약하여 효율적으로 논문에 들어갈 table1을 만들 수 있는 gtsummary 패키지에 대해 소개합니다.\n\n\n\n\n\n2022/02/04\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\n창업 경험 공유\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 창업 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2022/01/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용\n\n\n\n\n\n\ndocker\n\n\ngithub\n\n\n\nMySQL 기반 DockerContainer에서 GitHub-Repository로의 주기적인 DB 백업 구현\n\n\n\n\n\n2022/01/20\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\n인턴십 - Django로 게시판 만들고 기능 추가하기\n\n\n\n\n\n\ndjango\n\n\n\n숭실대학교 인턴십 프로그램을 통해 인턴으로 활동하게 된 차라투에서 1주차 동안 학습한 내용에 대해 공유합니다.\n\n\n\n\n\n2022/01/05\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\nNotion으로 홈페이지 제작후 oopy로 배포한 후기\n\n\n\n\n\n\nreview\n\n\n\nNotion과 Oopy를 사용해 개편한 당사 홈페이지의 구축 논의 사항과 장·단점을 설명하였습니다.\n\n\n\n\n\n2021/10/01\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 환자데이터 입력웹 개발(2)\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n4월에 이어 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이터 입력웹을 소개합니다. 본 내용은 차라투가 후원하는 Shinykorea 10월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2021/09/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Login\n\n\n\n\n\n\nreview\n\n\n\nZarathu 앱에 적용할 예정인 구글 로그인에 대해 소개합니다.\n\n\n\n\n\n2021/09/11\n\n\nHyunki Lee\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 공공빅데이터 분석지원\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\nR 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유합니다. 본 내용은 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에서 발표할 예정입니다.\n\n\n\n\n\n2021/08/21\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 의학연구지원\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\nR 활용 의학연구지원경험을 공유합니다. 본 내용은 “Be a data scientist - major actor in the future research” 라는 제목으로 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에서 발표할 예정입니다. \n\n\n\n\n\n2021/08/19\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\ngoogleAuth\n\n\n\n\n\n\nlecture\n\n\n\nFollow up assignment of ShinyProxy lecture\n\n\n\n\n\n2021/07/25\n\n\nHyunjun Ko\n\n\n\n\n\n\n\n\n\n\n\n\n창업지원사업 도전기\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n지금까지 창업지원사업 도전했던 경험을 공유합니다. 본 내용은 차라투가 후원하는 Shinykorea 7월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2021/07/11\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n채널톡(channel.io) 설치 후기\n\n\n\n\n\n\nreview\n\n\n\n최근 Zarathu 공식 홈페이지에 추가한 channel.io 서비스 관련 적용 후기입니다. \n\n\n\n\n\n2021/07/05\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 환자데이터 입력웹 개발\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 4월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2021/04/02\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n코로나 수리모델링: 서울시 감염병연구센터 자문\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 정리했습니다. 본 내용은 2월 Shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2021/01/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n생존분석 실습\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\nKaplan-meier curve, 비례위험가정 확인, Time-dependent analysis 그리고 모수적 생존분석을 중심으로 R 코드를 정리했습니다. 본 내용은 성균관의대 사회의학교실 특강에서 실습할 예정입니다.\n\n\n\n\n\n2020/10/31\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio & Shiny Docker 소개\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앴습니다. 본 내용은 Shinykorea 10월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2020/10/05\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학통계지원 소개\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n차라투 업무 소개입니다. 본 내용은 영남대학교 “의사과학자 역량 배가 프로젝트” 에서 발표할 예정입니다.\n\n\n\n\n\n2020/10/05\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n메타분석 웹 개발 후기\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\n메타분석 ShinyApps 만든 후기를 정리하였습니다. 본 내용은 Shinykorea 8월 밋업에서 발표할 예정입니다..\n\n\n\n\n\n2020/08/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n회귀분석 in 의학연구\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의학 연구에서 사용하는 선형/로지스틱 회귀분석과 Cox 비례위험모형을 소개합니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n2020/07/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학 연구에서의 기술통계\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의학 연구에서 Table 1 에 활용되는 기술통계를 정리하였습니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n2020/07/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n2020년 만들었던 ShinyApps\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\nR\n\n\n\n올해 만들었던 ShinyApps 를 간단히 정리하였습니다. 본 내용은 6월 shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2020/06/20\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\n %&gt;% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다.\n\n\n\n\n\n2020/04/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트: 기초\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\nR 기본 문법과, 보험공단 샘플 데이터를 이용한 데이터 매니지먼트 방법을 정리하였습니다. 본 내용은 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다. \n\n\n\n\n\n2020/03/10\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTokyoR 81회 리뷰\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n일본 R 밋업 중 하나인 TokyoR 중 shiny 특집을 리뷰하였습니다. 본 내용은 차라투가 후원하는 Shinykorea 3월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2020/02/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR로 만드는 웹 애플리케이션\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\nshiny\n\n\n\nR과 shiny로 웹 애플리케이션을 만든 경험을 소개합니다. 본 내용은 디시인사이드가 후원하는 프로그래밍 갤러리 컨퍼런스 2020 에서 발표할 예정입니다.\n\n\n\n\n\n2020/01/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRSelenium 이용 팁\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\nRSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 1월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2019/11/30\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n의료데이터 분석가가 되기까지의 경험을 슬라이드로 공유합니다. 본 내용은 동국대학교 의생명공학과 세미나에서 발표할 예정으로, 초청해주신 김진식 교수님께 감사드립니다.\n\n\n\n\n\n2019/11/04\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 워크샵: 서울IT직업전문학교 국비교육\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nshiny\n\n\nR\n\n\nworkshop\n\n\n\nShiny 기초학습을 위한 강의 슬라이드와 실습파일입니다. 본 내용은 “서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정” 에서 쓰일 예정입니다.\n\n\n\n\n\n2019/10/27\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRUCK2019 발표: From ShinyApps to CRAN\n\n\n\n\n\n\npresentation\n\n\nrpackage\n\n\nR\n\n\nshiny\n\n\n\n맞춤형 의학연구 앱을 만들고, 그것을 패키지로 만들어 CRAN에 배포한 경험을 슬라이드로 정리하였습니다. 본 내용은 R User Conference in Korea 2019(RUCK 2019)에서 발표하였습니다. \n\n\n\n\n\n2019/10/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 이용 공공빅데이터 분석 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nR\n\n\n\nR을 이용, 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의대 학술원 특강에서 발표할 예정입니다. \n\n\n\n\n\n2019/09/20\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShinyApps 에 로그인 기능 넣기\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\nShiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명하였습니다. 본 내용은 Zarathu가 후원하는 Shinykorea 9월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/08/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장(2): 허수축 도입\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nrpackage\n\n\n\n이전 글 “선형모형의 다차원공간으로의 확장” 의 추가 제안으로, 선형모형의 무대를 허수축(Imaginary Axis)을 포함한 휘어진 다차원공간으로 확장, Inverted U-shape 관계를 선형관계로 재해석하였습니다. \n\n\n\n\n\n2019/08/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n괴델의 불완전성 정리\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nmathematics\n\n\n\n괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명하였습니다. 본 내용은 “제주대학교 경영정보학과 산업·직무 특화 전문가 특강” 에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 맞춤형 통계지원 소개\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n의학연구를 지원하면서 다양하게 R을 활용했던 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/13\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 활용 의학연구지원 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\nShiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 슬라이드로 정리하였습니다. 본 내용은 차라투(주)가 후원하는 Shinykorea 5월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/11\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n세무기장마법사 머니핀(MoneyPin) 리뷰\n\n\n\n\n\n\nreview\n\n\nfinance\n\n\n\n법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다. \n\n\n\n\n\n2019/04/04\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShinyApps를 R 패키지로 만들기\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록, RStudio Addins을 포함한 R 패키지를 만들어 CRAN에 배포신청했으나 실패한 경험을 정리하였습니다. 본 내용은 Anpanman이 후원하는 Shinykorea 2월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/02/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-controlled case series\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nsccs\n\n\n\n성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2019/02/06\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown 기초\n\n\n\n\n\n\nlecture\n\n\nmarkdown\n\n\n\nYAML Header, 마크다운(Markdown) 텍스트, R 코드 청크(chunk) 그리고 그림과 테이블을 중심으로, R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 정리하였습니다. \n\n\n\n\n\n2019/01/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트: tidyverse\n\n\n\n\n\n\nlecture\n\n\ntidyverse\n\n\ndata.table\n\n\npurrr\n\n\nR\n\n\n\n파일을 읽는 readr, 읽기 쉬운 코드를 만드는 %&gt;% 연산자, 데이터를 다루는 dplyr 그리고 반복문을 다루는 purrr 패키지를 중심으로 tidyverse 생태계에서 데이터를 다루는 방법을 정리하였습니다. \n\n\n\n\n\n2019/01/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n진료실 밖 의사로서의 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2019/01/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학 연구에서의 기술 통계 with R\n\n\n\n\n\n\nlecture\n\n\nstatistics\n\n\nR\n\n\nshiny\n\n\n\n중앙보훈병원 정신건강의학과에서 강의한 내용으로, 의학 연구에 필요한 기술 통계(descriptive statistics)를 정리하고 웹 애플리케이션과 Rstudio Addins을 이용하여 실습하였습니다. \n\n\n\n\n\n2018/11/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nrpackage\n\n\n\n아인슈타인의 일반상대성이론은 태양 근처에서 빛이 휘어지는 현상을 빛이 아닌 시공간이 휘어지는 것으로 해석합니다. 비슷한 아이디어를 통계학에 적용하여 U-shape 관계를 휘어진 다차원 공간에서의 선형모형으로 재해석하였습니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nNew Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\n유전율(heritability)은 어떤 형질의 유전적인 측면을 정량적으로 설명하는 유용한 지표이나 이분형 형질의 경우 해석이 어렵습니다. 이 때는 Sibling recurrence risk가 직관적인 지표이나, 유병률 정보가 필요하고 다른 변수의 보정이 어려운 문제가 있습니다. 본 연구에서는 흔히 쓰는 OR scale을 이용, 이분형 변수에서 직관적이고 다른 변수의 보정도 가능한 유전율 지표를 제안합니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRedefine Null Hypothesis\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\nP value의 가장 큰 문제는 샘플 숫자만 늘리면 아무리 작은 차이라도 유의미한 결과로 만들 수 있다는 것입니다. 이는 대부분의 연구에서 차이가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 생기는 문제인데, 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못합니다. 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였습니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\ndevOps\n\n\nR\n\n\ndocker\n\n\nshiny\n\n\n\nR User Conference in Korea 2018(RUCK 2018)에서 발표했던 내용입니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nradix\n\n\n\nWelcome to Anpanman! \n\n\n\n\n\n2018/09/25\n\n\nJinseob Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-04-02-model_compare_index/index.html",
    "href": "posts/2025-04-02-model_compare_index/index.html",
    "title": "Comparison of Models for Competing Risk Analysis",
    "section": "",
    "text": "status: 1=melanoma 사망, 2=생존, 3=melanoma 외 사망\nstatus_competing: 0=생존, 1=melanoma 사망, 2=melanoma 외 사망\n\nlibrary(data.table);library(survival);library(boot);library(magrittr);library(riskRegression);library(prodlim);library(pec)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\n\nriskRegression version 2023.03.22\n\n\n\nAttaching package: 'pec'\n\n\nThe following objects are masked from 'package:riskRegression':\n\n    ipcw, selectCox\n\nmelanoma_dt &lt;- as.data.table(melanoma) %&gt;% .[,c(\"sex\",\"ulcer\"):=lapply(.SD,as.factor),.SDcols=c(\"sex\",\"ulcer\")] %&gt;% \n  .[,status_competing:=factor(ifelse(status==1,1,ifelse(status==2,0,2)), levels=0:2)] %&gt;% \n  .[,time_Y:=time/365.25] %&gt;% .[,-c(\"year\",\"status\",\"time\")]\n\nmodel_var_list &lt;- list(c(\"sex\",\"age\"), c(\"sex\",\"age\",\"thickness\"), c(\"sex\",\"age\",\"ulcer\"), c(\"sex\",\"age\",\"thickness\",\"ulcer\"))\n\nHist_formula_list &lt;- list( Hist(time_Y, status_competing) ~ sex+age,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness,\n                           Hist(time_Y, status_competing) ~ sex+age+ulcer,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness+ulcer )"
  },
  {
    "objectID": "posts/2025-04-02-model_compare_index/index.html#data-preprocess-and-prepare",
    "href": "posts/2025-04-02-model_compare_index/index.html#data-preprocess-and-prepare",
    "title": "Comparison of Models for Competing Risk Analysis",
    "section": "",
    "text": "status: 1=melanoma 사망, 2=생존, 3=melanoma 외 사망\nstatus_competing: 0=생존, 1=melanoma 사망, 2=melanoma 외 사망\n\nlibrary(data.table);library(survival);library(boot);library(magrittr);library(riskRegression);library(prodlim);library(pec)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\n\nriskRegression version 2023.03.22\n\n\n\nAttaching package: 'pec'\n\n\nThe following objects are masked from 'package:riskRegression':\n\n    ipcw, selectCox\n\nmelanoma_dt &lt;- as.data.table(melanoma) %&gt;% .[,c(\"sex\",\"ulcer\"):=lapply(.SD,as.factor),.SDcols=c(\"sex\",\"ulcer\")] %&gt;% \n  .[,status_competing:=factor(ifelse(status==1,1,ifelse(status==2,0,2)), levels=0:2)] %&gt;% \n  .[,time_Y:=time/365.25] %&gt;% .[,-c(\"year\",\"status\",\"time\")]\n\nmodel_var_list &lt;- list(c(\"sex\",\"age\"), c(\"sex\",\"age\",\"thickness\"), c(\"sex\",\"age\",\"ulcer\"), c(\"sex\",\"age\",\"thickness\",\"ulcer\"))\n\nHist_formula_list &lt;- list( Hist(time_Y, status_competing) ~ sex+age,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness,\n                           Hist(time_Y, status_competing) ~ sex+age+ulcer,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness+ulcer )"
  },
  {
    "objectID": "posts/2025-04-02-model_compare_index/index.html#each-model",
    "href": "posts/2025-04-02-model_compare_index/index.html#each-model",
    "title": "Comparison of Models for Competing Risk Analysis",
    "section": "2. Each model",
    "text": "2. Each model\n\ninvisible(capture.output({\n  table_1 &lt;- lapply (1:length(model_var_list), function(i) { \n  # Harrell_C_index_info\n  fgr_data_for_Harrell_C_index &lt;- survival::finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n  fgr_model_for_Harrell_C_index &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index)\n  Harrell_C_index_info &lt;- survival::concordance(fgr_model_for_Harrell_C_index)\n  \n  # Wolbers_C_index_info\n  fgr_model_for_Wolbers_C_index &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  Wolbers_C_index_info &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index, formula=Hist_formula_list[[i]], data=melanoma_dt, cause=1, confInt=T, verbose=F)\n  \n  cindex_values &lt;- numeric()\n  for(b in 1:20){\n    indices &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n    boot_data &lt;- melanoma_dt[indices]\n    fgr_model_for_Wolbers_C_index_boot &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=boot_data, cause=1)\n    Wolbers_C_index_info_boot &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot, formula=Hist_formula_list[[i]], data=boot_data, cause=1, verbose=F)\n    cindex_values[[b]] &lt;- Wolbers_C_index_info_boot$AppCindex$FGR\n  }\n  Wolbers_C_index_lower &lt;- quantile(cindex_values, probs=0.05/2)\n  Wolbers_C_index_upper &lt;- quantile(cindex_values, probs=(1-(0.05/2)))\n\n  # AUC_and_Brier_info\n  fgr_model_for_AUC_and_Brier &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  AUC_and_Brier_info &lt;- riskRegression::Score(list(fgr_model_for_AUC_and_Brier), formula=Hist_formula_list[[i]] , data=melanoma_dt, times=c(5,10), null.model=F)\n\n  # return\n  data.table( \"Model\"=LETTERS[i],\n              \"Harrell_C_index\"=paste0(sprintf(\"%.04f\",Harrell_C_index_info$concordance),\"(\",\n                                       sprintf(\"%.04f\",Harrell_C_index_info$concordance-qnorm(0.975)*sqrt(Harrell_C_index_info$var)),\"-\",\n                                       sprintf(\"%.04f\",Harrell_C_index_info$concordance+qnorm(0.975)*sqrt(Harrell_C_index_info$var)),\")\"),\n              \"Wolbers_C_index\"= paste0(sprintf(\"%.04f\",Wolbers_C_index_info$AppCindex$FGR),\"(\",\n                                        sprintf(\"%.04f\",Wolbers_C_index_lower),\"-\",\n                                        sprintf(\"%.04f\",Wolbers_C_index_upper),\")\"),\n              \"AUC_t=5\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$AUC),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$upper),\")\"),\n              \"AUC_t=10\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$AUC),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$upper),\")\"),\n              \"Brier_t=5\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$Brier),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$upper),\")\"),\n              \"Brier_t=10\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$Brier),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$upper),\")\")\n  ) }) %&gt;% \n    do.call(rbind,.)\n}))"
  },
  {
    "objectID": "posts/2025-04-02-model_compare_index/index.html#compare-two-model",
    "href": "posts/2025-04-02-model_compare_index/index.html#compare-two-model",
    "title": "Comparison of Models for Competing Risk Analysis",
    "section": "3. Compare two model",
    "text": "3. Compare two model\nCompare Harrell_C_index\n\ntable_2_Harrell_C_index &lt;- lapply(1:length(model_var_list), function(i) {\n  fgr_data_for_Harrell_C_index &lt;- survival::finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n  fgr_model_for_Harrell_C_index &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index)\n \n  lapply(1:length(model_var_list), function(j) {\n    fgr_data_for_Harrell_C_index_new &lt;- finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[j]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n    fgr_model_for_Harrell_C_index_new &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[j]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index_new)\n  \n    ctest &lt;- concordance(fgr_model_for_Harrell_C_index, fgr_model_for_Harrell_C_index_new)\n    contr &lt;- c(-1, 1)\n    dtest &lt;- contr %*% coef(ctest)\n    dvar &lt;- contr %*% vcov(ctest) %*% contr\n    z &lt;- dtest/sqrt(dvar)\n    p_value_temp &lt;- 2 * (1 - pnorm(abs(z))) \n    p_value &lt;- ifelse(p_value_temp&lt;0.001,\"&lt;0.001\",sprintf(\"%.03f\",p_value_temp))\n    return(p_value)\n  }) %&gt;% unlist()\n}) %&gt;% do.call(rbind,.) %&gt;% as.data.table() %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Harrell_C_index=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\nprint(table_2_Harrell_C_index)\n\nCompare Wolbers_C_index\nbootstraping이므로 하나의 pair(model1, model2)에 대해 1번만 코드 실행\n\nset.seed(10)\ninvisible(capture.output({\n   table_2_Wolbers_C_index &lt;- lapply(1:(length(model_var_list)-1), function(i) {\n  cindex_values &lt;- numeric()\n  for(b in 1:20){\n    indices &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n    boot_data &lt;- melanoma_dt[indices]\n    fgr_model_for_Wolbers_C_index_boot &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=boot_data, cause=1)\n    Wolbers_C_index_info_boot &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot, formula=Hist_formula_list[[i]], data=boot_data, cause=1, verbose=F)\n    cindex_values[[b]] &lt;- Wolbers_C_index_info_boot$AppCindex$FGR\n  }\n\n  lapply ((i+1):length(model_var_list), function(j) { # A와 B,C,D 비교, B와 C,D 비교, C와 D 비교 \n    cindex_values_new &lt;- numeric()\n    for(b in 1:20){\n      indices_new &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n      boot_data_new &lt;- melanoma_dt[indices_new]\n      fgr_model_for_Wolbers_C_index_boot_new &lt;- riskRegression::FGR(Hist_formula_list[[j]], data=boot_data_new, cause=1)\n      Wolbers_C_index_info_boot_new &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot_new, formula=Hist_formula_list[[j]], data=boot_data_new, cause=1, verbose=F)\n      cindex_values_new[[b]] &lt;- Wolbers_C_index_info_boot_new$AppCindex$FGR\n    }\n      \n    p_value_temp &lt;- t.test(cindex_values-cindex_values_new, mu=0)$p.value\n    return( ifelse(p_value_temp&lt;0.001,\"&lt;0.001\",sprintf(\"%.03f\",p_value_temp)) )\n  }) %&gt;% unlist() %&gt;% c(rep(\"-\",(i-1)),.)\n}) %&gt;% do.call(rbind,.) %&gt;% as.data.table() %&gt;% setnames(c(\"B\",\"C\",\"D\")) %&gt;% \n  cbind(\"A\"=rep(\"-\",3),.) %&gt;% rbind(\"D\"=data.table(\"-\",\"-\",\"-\",\"-\"), use.names=F) %&gt;% cbind(Wolbers_C_index=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\n}))\nprint(table_2_Wolbers_C_index)\n\nCompare AUC_and_Brier\n\ninvisible(capture.output({\n  table_2_AUC_and_Brier &lt;- lapply(1:length(model_var_list), function(i) {\n  fgr_model_AUC_and_Brier &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  \n  lapply (1:length(model_var_list), function(j) {\n    fgr_model_AUC_and_Brier_new &lt;- riskRegression::FGR(Hist_formula_list[[j]], data=melanoma_dt, cause=1)\n    \n    AUC_and_Brier &lt;- riskRegression::Score(list(fgr_model_AUC_and_Brier,fgr_model_AUC_and_Brier_new),\n                                           formula=Hist(time_Y, status_competing) ~ 1, data=melanoma_dt, times=c(5,10), null.model=F)\n    \n    data.table(\"V1\"=c(sprintf(\"%.03f\",AUC_and_Brier$AUC$contrasts$p), sprintf(\"%.03f\",AUC_and_Brier$Brier$contrasts$p))) %&gt;% setnames(as.character(j))\n  }) %&gt;% do.call(cbind,.)\n  })\n}))\n\n\ntable_2_AUC_t_5 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][1]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(AUC_t_5=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_AUC_t_5[i,(i+1):=\"-\"]\n}\nprint(table_2_AUC_t_5)\n\n\ntable_2_AUC_t_10 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][2]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(AUC_t_10=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_AUC_t_10[i,(i+1):=\"-\"]\n}\nprint(table_2_AUC_t_10)\n\n\ntable_2_Brier_t_5 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][3]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Brier_t_5=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_Brier_t_5[i,(i+1):=\"-\"]\n}\nprint(table_2_Brier_t_5)\n\n\ntable_2_Brier_t_10 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][4]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Brier_t_10=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\nfor (i in seq(length(model_var_list))) {\n  table_2_Brier_t_10[i,(i+1):=\"-\"]\n}\nprint(table_2_Brier_t_10)"
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html",
    "href": "posts/2025-03-27-Sankey/index.html",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "",
    "text": "Sankey Plot은 데이터의 흐름이나 분포를 시각적으로 표현하는 그래프다.\n선(링크)의 굵기로 양 또는 비율을 보여주며, 한 집단에서 다른 집단으로 얼마나 이동했는지를 한눈에 확인할 수 있게 도와 전체적인 구조와 흐름을 직관적으로 이해할 수 있도록 한다.\n\nR에는 sankeyNetwork() 함수로 Sankey Plot을 생성할 수 있는 기능이 존재한다. 이 함수는 networkD3 패키지에서 제공되며, 비교적 간단한 코드로 노드와 링크 데이터를 연결해 Sankey Plot을 만들 수 있다는 장점이 있다 (특히 마우스 오버 시 강조되거나, 노드를 드래그할 수 있는 등의 동작은 시각적으로 유용하다).\n하지만 이 함수만으로는 각 링크 위에 텍스트를 추가하거나, 노드 옆에 퍼센트 또는 샘플 수(n=) 같은 세부 정보를 표시하는 것이 어렵다. 그 이유는 Sankey Plot 내부 요소들의 위치(x, y 좌표)가 JavaScript에서 실시간으로 계산되기 때문이다. 따라서 R에서는 그 위치 정보를 직접 참조하거나 조작할 수가 없어 sankeyNetwork()를 이용한 기본적인 시각화 요소 외에는 커스터마이징 옵션이 제한적이다.\n따라서 Sankey Plot의 시각적 완성도를 높이기 위해서는 결국 R 코드와 JavaScript를 함께 사용하는 접근이 요구된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#텍스트를-추가하려고-했더니",
    "href": "posts/2025-03-27-Sankey/index.html#텍스트를-추가하려고-했더니",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "",
    "text": "R에는 sankeyNetwork() 함수로 Sankey Plot을 생성할 수 있는 기능이 존재한다. 이 함수는 networkD3 패키지에서 제공되며, 비교적 간단한 코드로 노드와 링크 데이터를 연결해 Sankey Plot을 만들 수 있다는 장점이 있다 (특히 마우스 오버 시 강조되거나, 노드를 드래그할 수 있는 등의 동작은 시각적으로 유용하다).\n하지만 이 함수만으로는 각 링크 위에 텍스트를 추가하거나, 노드 옆에 퍼센트 또는 샘플 수(n=) 같은 세부 정보를 표시하는 것이 어렵다. 그 이유는 Sankey Plot 내부 요소들의 위치(x, y 좌표)가 JavaScript에서 실시간으로 계산되기 때문이다. 따라서 R에서는 그 위치 정보를 직접 참조하거나 조작할 수가 없어 sankeyNetwork()를 이용한 기본적인 시각화 요소 외에는 커스터마이징 옵션이 제한적이다.\n따라서 Sankey Plot의 시각적 완성도를 높이기 위해서는 결국 R 코드와 JavaScript를 함께 사용하는 접근이 요구된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#각-node-옆에-비율-및-개수-표시",
    "href": "posts/2025-03-27-Sankey/index.html#각-node-옆에-비율-및-개수-표시",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "각 node 옆에 비율 및 개수 표시",
    "text": "각 node 옆에 비율 및 개수 표시\n\np &lt;- htmlwidgets::onRender(p, '\n  function(el, x) {\n    var sankey = this.sankey;\n\n    d3.select(el).selectAll(\".node text\")\n      .text(function(d) {\n        var perc = (d.value / 120) * 100;  // 총 샘플 수: 120\n        return d.name + \" \" + perc.toFixed(1) + \"% (n=\" + d.value + \")\";\n      })\n      .attr(\"x\", function(d) {\n        return (d.x === 0) ? -10 : x.options.nodeWidth + 10;\n      })\n      .attr(\"text-anchor\", function(d) {\n        return (d.x === 0) ? \"end\" : \"start\";\n      });\n  }\n')\n\nhtmlwidgets::onRender(p, '...')는 p라는 Sankey 플롯 객체에 JavaScript 코드를 추가해주는 함수다. 이 코드는 Sankey plot이 브라우저에 렌더링된 이후 실행된다.\n위 코드는 먼저 var sankey = this.sankey로 Sankey Plot의 내부 구조를 가져온 뒤, .selectALL (\".node text\")로 Sankey의 모든 노드 텍스트를 선택한다.\n이어서 function(el, x) { ... }의 el은 Sankey 그래프가 들어있는 HTML의 요소이고, x는 sankeyNetwork 함수에서 설정된 옵션들(fontSize, nodeWidth 등)을 담고 있다. 텍스트를 추가하기 위해서 이 function 안에 있는 .text 함수를 가장 중심적으로 사용한다:\n\n.text(function(d) {\n        var perc = (d.value / 87) * 100;  // 총 샘플 수: 87\n        return d.name + \" \" + perc.toFixed(1) + \"% (n=\" + d.value + \")\";\n      }\n\n이 부분은 d.name(각 노드의 이름)에 전체 값 대비 해당 노드가 차지하는 비율(%)과 개수(n=)를 텍스트로 함께 붙이는 역할을 한다. 여기서 120이라는 숫자는 전체 합계(sum of N)를 의미하기 때문에, 사용하는 데이터의 전체 값에 따라 이 숫자는 다르게 삽입한다. 혹은, R에서 전체 합을 미리 계산해서 onRender()에 넘겨주는 방식을 사용해도 된다.\n그 밑에 있는 .attr(\"x\", function(d) {...})와 .attr(\"text-anchor\", function(d) {...})는 위치 설정에 관련된 코드다. 노드가 왼쪽에 있을 경우(d.x === 0)에는 텍스트를 왼쪽 바깥에 정렬하고, 그렇지 않으면 오른쪽 바깥에 정렬하도록 설정한다. 텍스트가 노드와 겹치거나 너무 떨어져 보일 경우, -10 또는 x.options.nodeWidth + 10 값을 조정해서 위치를 맞춰주면 된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#각-flow의-확률-텍스트-추가",
    "href": "posts/2025-03-27-Sankey/index.html#각-flow의-확률-텍스트-추가",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "각 flow의 확률 텍스트 추가",
    "text": "각 flow의 확률 텍스트 추가\n아래 코드는 Sankey plot의 각 링크 위에 해당 링크가 출발 노드 전체에서 차지하는 비율(%)을 텍스트로 표시해주는 역할을 한다.\n\np &lt;- htmlwidgets::onRender(p, '\n  function(el) {\n    var sankey = this.sankey;\n    var nodeWidth = sankey.nodeWidth();\n    var links = sankey.links();\n\n    // 각 링크에 대해 비율 계산 및 텍스트 추가\n    links.forEach(function(d) {\n      var outflow = d3.sum(d.source.sourceLinks, function(l) {\n        return l.value;\n      });\n\n      var ratio = (d.value / outflow) * 100;\n\n      // 텍스트 위치 계산 (링크 중간)\n      var startX = d.source.x + nodeWidth;\n      var startY = d.source.y + d.sy + (d.dy / 2);\n\n      // 텍스트 추가\n      d3.select(el).select(\"svg g\")\n        .append(\"text\")\n        .attr(\"text-anchor\", \"middle\")\n        .attr(\"alignment-baseline\", \"middle\")\n        .attr(\"x\", startX + 25)\n        .attr(\"y\", startY)\n        .text(ratio.toFixed(1) + \"%\");\n    });\n  }\n')\n\nvar sankey = this.sankey는 sankey 객체를 가져오는 부분이며, nodeWidth는 노드의 너비, links는 그래프에 존재하는 모든 링크를 가져온다.\n이 코드의 핵심은 links.forEach(function(d) {...}) 반복문이다. 각 링크에 대해 다음과 같은 작업이 이루어진다:\n\nvar outflow = d3.sum(d.source.sourceLinks, function(l) {\n  return l.value;\n})\n\n이 부분은 d.source(해당 링크가 출발하는 노드)에서 outflow(나가는 전체 흐름의 합)을 계산한다.\n이어서 그 링크가 차지하는 비율을 아래와 같이 계산한다.\n\nvar ratio = (d.value / outflow) * 100;\n\n예를 들어 10명이 이 링크를 통해 이동했고, 전체 outflow가 40이면, 비율은 25%가 된다.\n텍스트를 표시할 위치도 직접 계산된다:\n\nvar startX = d.source.x + nodeWidth;\nvar startY = d.source.y + d.sy + (d.dy / 2);\n\nstartX은 출발 노드의 x좌표와 노드 너비의 합으로, 텍스트의 x 좌표 위치는 노드의 오른쪽 바깥이 된다. 비슷하게 startY은 출발 노드의 y좌표, 링크가 시작되는 위치, 그리고 링크 높이의 절반의 합이다. 따라서 텍스트의 위치는 링크 중간의 y좌표가 된다. 이 계산 과정으로 텍스트는 선의 중간쯤 되는 위치에 뜨게 된다.\n마지막으로 텍스트를 실제로 추가한다:\n\nd3.select(el).select(\"svg g\")\n  .append(\"text\")\n  .attr(\"text-anchor\", \"middle\")\n  .attr(\"alignment-baseline\", \"middle\")\n  .attr(\"x\", startX + 25)      \n  .attr(\"y\", startY)\n  .text(ratio.toFixed(1) + \"%\");\n\n이 부분에서 숫자 포맷, 텍스트 위치(x, y), 텍스트 내용을 자유롭게 커스터마이징할 수 있다. d3.select(el).select(\"svg g\").append(\"text\")로 텍스트 요소를 생성하고, .attr(\"text-anchor\", \"middle\")과 .attr(\"alignment-baseline\", \"middle\")을 통해 텍스트가 x와 y좌표의 가운데 정렬로 위치하도록 설정한다. x 좌표는 앞서 계산한 startX에 25를 더해 노드에서 약간 떨어진 위치에 텍스트가 뜨게 하고, y 좌표는 startY로 그대로 둔다. 마지막으로 .text(ratio.toFixed(1) + \"%\")를 통해 소수점 첫째자리까지 계산된 비율을 문자열로 표시한다. 소수점 없이 정수로만 표시하고 싶다면 ratio.toFixed(0)으로 바꿀 수 있고, 텍스트에 단어를 붙이고 싶다면 “비율:” + ratio.toFixed(1) + “%”처럼 바꿀 수도 있다.\n이와 같이 추가적으로 코드를 작성한 것을 실행하면 처음 만들었던 Sankey plot에 원하는 텍스트가 추가된 것을 확인할 수 있다:"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html",
    "href": "posts/2025-03-18-Kappa/index.html",
    "title": "Kappa 분석 이해하기",
    "section": "",
    "text": "Kappa 통계는 두 명 이상의 평가자(rater)가 범주형 데이터를 얼마나 일관되게 평가하는지를 측정하는 방법이다. 단순한 일치율과 달리, Kappa 계수는 무작위로 일치할 가능성을 보정하여 보다 신뢰할 수 있는 평가 일치도를 제공한다.이는 평가자의 주관적인 판단이 개입되는 연구에서 필수적이라고 볼 수 있다.\nKappa 분석에는 여러 가지 변형이 있으며, 상황에 따라 적절한 방법을 선택해야 한다. 대표적인 Kappa 분석 방법은 다음과 같다:\n\nCohen’s Kappa (κ): 두 명의 평가자가 범주형 데이터를 평가할 때 사용\nCohen’s Weighted Kappa: 평가자 간의 불일치 정도를 가중치로 고려할 때 사용 (순위형 변수)\nFleiss’ Kappa: 두 명 이상의 평가자가 있을 때 사용 (범주형 변수)\nGeneralized Fleiss’ Kappa: Fleiss’ Kappa의 확장판으로, 순위형 데이터를 다룰 때 사용\nKrippendorff’s Alpha: 범주형, 순위형, 연속형 데이터 모두 적용 가능\n\nCohen’s와 Cohen’s Weighted는 평가자가 두 명일때 적용되고, Fleiss’와 Generalized Fleiss’는 두 명 이상일 때 사용된다.\n이 글에서는 R을 활용하여 다양한 Kappa 분석 방법을 구현하는 방법을 설명한다.\n\n\n단순한 일치율(Percent Agreement)은 평가자 간의 동일한 판단이 나온 비율을 계산하는 방식이다. 하지만, 이는 무작위로 일치한 경우도 포함하기 때문에 신뢰도가 낮을 수 있다.\n예를 들어, 두 평가자가 100개의 사례를 평가했을 때, 70개에서 동일한 판단을 내렸다면 단순 일치율은 70%이다. 하지만, 무작위로도 70%의 일치가 나올 가능성이 있다면, 실제 평가자의 일치 정도를 과대평가할 수 있다.\n이를 보완하기 위해 Kappa 계수(\\(κ\\))는 무작위 일치율(Expected Agreement)을 고려하여 조정된 값을 제공한다. 즉, Kappa 계수는 실제 일치율과 무작위 일치율 간의 차이를 기반으로 계산되며, 0~1 사이의 값으로 표현된다.\n\nKappa 계수(\\(κ\\)) 해석:\n\n\\(κ\\) = 1: 완벽한 일치\n\\(κ\\) = 0: 무작위 일치 수준\n\\(κ\\) &lt; 0: 평가자가 오히려 무작위보다 더 불일치\n0.6 ≤ \\(κ\\) ≤ 0.8: 상당한 일치\n0.4 ≤ \\(κ\\) &lt; 0.6: 중간 수준의 일치\n\n\n따라서 \\(κ\\)의 값은 크면 클수록 좋은 것이라고 본다.\n이제 Kappa 분석이 왜 중요한지를 이해했으므로, 다음 섹션에서는 각 Kappa 분석 방법을 살펴보고, R을 이용하여 실제 데이터를 분석하는 방법을 설명한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#단순-일치율-vs.-kappa-계수",
    "href": "posts/2025-03-18-Kappa/index.html#단순-일치율-vs.-kappa-계수",
    "title": "Kappa 분석 이해하기",
    "section": "",
    "text": "단순한 일치율(Percent Agreement)은 평가자 간의 동일한 판단이 나온 비율을 계산하는 방식이다. 하지만, 이는 무작위로 일치한 경우도 포함하기 때문에 신뢰도가 낮을 수 있다.\n예를 들어, 두 평가자가 100개의 사례를 평가했을 때, 70개에서 동일한 판단을 내렸다면 단순 일치율은 70%이다. 하지만, 무작위로도 70%의 일치가 나올 가능성이 있다면, 실제 평가자의 일치 정도를 과대평가할 수 있다.\n이를 보완하기 위해 Kappa 계수(\\(κ\\))는 무작위 일치율(Expected Agreement)을 고려하여 조정된 값을 제공한다. 즉, Kappa 계수는 실제 일치율과 무작위 일치율 간의 차이를 기반으로 계산되며, 0~1 사이의 값으로 표현된다.\n\nKappa 계수(\\(κ\\)) 해석:\n\n\\(κ\\) = 1: 완벽한 일치\n\\(κ\\) = 0: 무작위 일치 수준\n\\(κ\\) &lt; 0: 평가자가 오히려 무작위보다 더 불일치\n0.6 ≤ \\(κ\\) ≤ 0.8: 상당한 일치\n0.4 ≤ \\(κ\\) &lt; 0.6: 중간 수준의 일치\n\n\n따라서 \\(κ\\)의 값은 크면 클수록 좋은 것이라고 본다.\n이제 Kappa 분석이 왜 중요한지를 이해했으므로, 다음 섹션에서는 각 Kappa 분석 방법을 살펴보고, R을 이용하여 실제 데이터를 분석하는 방법을 설명한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#cohens-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#cohens-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "1.1 Cohen’s Kappa 공식",
    "text": "1.1 Cohen’s Kappa 공식\nCohen’s Kappa는 다음과 같은 공식으로 계산된다:\n\\[\nκ = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\): 평가자간 일치 확률 (Observed Accuracy)\n\\(P_e\\): 우연히 일치된 평가를 받을 비율 (Expected Accuracy)\n\n여기서 \\(P_e\\)는 ’우연히 일치할 확률’을 나타낸다. 예를 들어, 두 명의 상담사가 환자에게 우울증이 있는지 없는지에 대해 완전히 무작위로 판단했다고 가정한다. 마치 동전을 던지는 것처럼 말이다. 그럼에도 불구하고 두 상담사가 우연히 같은 결론을 내릴 가능성이 어느 정도 존재하게 되는데, 이 우연에 의한 일치 확률을 나타내는 값이 바로 \\(P_e\\)가 된다. 즉, \\(P_e\\)는 평가자들이 실제로 동의한 정도가 아니라, 순전히 우연으로 평가 결과가 같아질 가능성을 나타내는 가상의 확률이라고 이해하면 된다. \\(P_e\\)의 비율이 높을 수록 우연하게 일치한다는 것이고, 이 값이 최소에 가까워질수록 높은 \\(κ\\)의 값을 얻을 수 있게 된다.\n\nObserved Accuracy \\(P_o\\)\n\\[\nP_0 = \\frac{1}{n} \\sum_{i=1}^{g} f_{ii}\n\\]\n\n\\(n\\): 전체 평가 개수\n\\(g\\): 평가 범주의 개수 (예: 3개의 등급, 5개의 점수 등)\n\\(f_{ii}\\): 평가자 두 명이 동일한 범주를 선택한 횟수\n\n즉, \\(P_0\\)는 전체 평가 중에서 평가자들이 동일한 범주를 선택한 비율을 의미한다. 이는 실제 데이터에서 평가자들이 얼마나 일치했는지를 보여주는 값이다.\n\n\nExpected Accuracy \\(P_e\\)\n\\[\nP_e = \\frac{1}{n^2} \\sum_{i=1}^{g} f_{i+} f_{+i}\n\\]\n\n\\(f_{i+}\\): 특정 범주의 행 합 (첫 번째 평가자가 해당 범주를 선택한 횟수)\n\\(f_{+i}\\): 특정 범주의 열 합 (두 번째 평가자가 해당 범주를 선택한 횟수)\n\n\\(P_e\\)는 평가자들이 무작위로 평가했을 때 동일한 범주를 선택할 확률을 의미한다. 이는 두 평가자가 특정 범주를 선택할 확률을 각각 곱하여 계산되며, 모든 범주에 대해 합산하여 전체적인 기대 일치도를 구하는 방식이다.\n\n\nBinary Classifications\nCohen’s Kappa는 이진 분류에서도 모델의 예측 신뢰도를 평가하는 중요한 지표로 활용될 수 있다. 이는 다음과 같은 공식으로 표현된다.\n\\[\nκ = \\frac{2 \\times (TP \\times TN - FN \\times FP)}\n{(TP + FP) \\times (FP + TN) + (TP + FN) \\times (FN + TN)}\n\\]\n여기서 각 항목은 다음과 같은 의미를 가진다:\n\n\\(TP\\) (True Positives): 실제로 긍정(positive)인 경우를 정확하게 예측한 수\n\\(FP\\) (False Positives): 실제로는 부정(negative)이지만, 긍정으로 잘못 예측한 수\n\\(TN\\) (True Negatives): 실제로 부정인 경우를 정확하게 예측한 수\n\\(FN\\) (False Negatives): 실제로는 긍정이지만, 부정으로 잘못 예측한 수\n\n이진 분류에서 Cohen’s Kappa는 단순한 정확도보다는 무작위 예측과 비교하여 모델이 얼마나 신뢰할 만한지를 측정하는 데 유용하다. 예를 들어, 불균형한 데이터에서 단순한 정확도는 높은 값이 나올 수 있지만, Kappa 값이 낮게 나오는 경우가 있을 수 있다. 이는 모델이 특정 클래스를 과도하게 예측하고 있음을 나타낼 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-1",
    "href": "posts/2025-03-18-Kappa/index.html#example-1",
    "title": "Kappa 분석 이해하기",
    "section": "1.2 Example 1",
    "text": "1.2 Example 1\n첫 번째 예시에서는 두 평가자가 다섯 개 항목을 각각 평가한 뒤, Cohen’s Kappa 통계량을 이용해 두 평가자 간의 일치도를 분석했다. R에서 Cohen’s Kappa를 구하기 위해서는 irr 패키지를 사용한다 (Desc Tools, psych 등 다른 패키지도 존재).\nlibrary(irr)\n\n# 두 평가자가 5개의 항목을 평가\nratings &lt;- data.frame(\n  rater1 = c(\"A\", \"A\", \"B\", \"A\", \"C\"),\n  rater2 = c(\"A\", \"B\", \"B\", \"A\", \"C\")\n)\n\n# Cohen's Kappa 계산\nresult &lt;- kappa2(ratings, weight = \"unweighted\")\nprint(result)\n출력:\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 5 \n   Raters = 2 \n    Kappa = 0.688 \n\n        z = 2.28 \n  p-value = 0.0224\n이 예시의 결과는 Kappa 값이 0.688로 나타났고, z 통계량은 2.28, p값은 0.0224로 나타나 통계적으로 유의미한 일치도를 보였다. 이는 두 평가자가 단순히 우연히 일치하는 것을 넘어 실제로 상당히 일치된 평가를 내렸다는 의미로 해석할 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-2",
    "href": "posts/2025-03-18-Kappa/index.html#example-2",
    "title": "Kappa 분석 이해하기",
    "section": "1.3 Example 2",
    "text": "1.3 Example 2\n다음 예시는 실제 의료 환경에서 자주 발생하는 사례를 바탕으로 Cohen’s Kappa를 적용한 것이다. 두 명의 영상의학 전문의가 CT, MRI, PET, X-ray 총 4가지 영상진단 방식으로 환자를 각각 독립적으로 평가했을 때, 두 전문의 간 평가가 얼마나 일치하는지를 분석한다.\nlibrary(ggplot2); library(officer)\n\nimaging.modalities &lt;- c(\"CT\", \"MRI\", \"PET\", \"Xray\")\n\nkappa.results &lt;- sapply(imaging.modalities, function(modality){\n  var1 &lt;- paste0(modality, \"_Rater1\")\n  var2 &lt;- paste0(modality, \"_Rater2\")\n\n  kappa_calc &lt;- irr::kappa2(diagnosis_data[, .SD, .SDcols = c(var1, var2)], weight = \"unweighted\")\n  standard_error &lt;- kappa_calc$value / kappa_calc$statistic\n  conf_interval &lt;- c(kappa_calc$value - qnorm(0.975) * standard_error,\n                     kappa_calc$value + qnorm(0.975) * standard_error)\n\n  return(paste0(round(kappa_calc$value, 3), \" (95% CI: \", round(conf_interval[1], 3),\n                \"-\", round(conf_interval[2], 3), \")\"))\n})\n이 예시에서 계산된 Kappa 값은 각 영상진단 방식별로 제공되며, 각 값에 대한 표준오차와 95% 신뢰구간도 함께 계산된다. irr::kappa2 함수를 통해 두 평가자 간의 Cohen’s Kappa 값을 계산했고, weight = “unweighted” 옵션으로 평가 항목 간의 차이에 동일한 가중치를 부여한다.\n이와 같이 Cohen’s Kappa는 두 평가자가 같은 값을 측정했는지 여부를 고려하지만, 불일치의 정도는 고려하지 않는다. Example 1을 보면 Cohen’s Kappa는 평가자가 A와 C를 선택했을 때와 A와 B를 선택했을 때를 동일한 불일치로 간주하고 있는 것을 확인할 수 있다. 그렇다면 범주형 변수가 아닌 순위형 변수가 있다면 어떨까?"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#weighted-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#weighted-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "2.1 Weighted Kappa 공식",
    "text": "2.1 Weighted Kappa 공식\n\\[\n\\kappa_w = \\frac{P_o - P_e}{1 - P_e}\n\\]\n이 식은 일반적인 Cohen’s Kappa의 공식과 같지만, Weighted Kappa의 경우 Observed Accuracy \\(P_o\\)와 Expected Accuracy \\(P_e\\)을 계산할 때 가중치를 적용한 값을 사용한다는 점에서 차이가 있다. 각 항목은 다음과 같이 계산한다.\n\nObserved Accuracy (\\(P_o\\))\n\\(P_o\\)는 두 평가자의 실제 평가 결과를 바탕으로 각 범주 간에 가중치를 적용하여 계산한 값이다.\n\\[\nP_{o} = \\sum_{i}\\sum_{j} W_{ij}P_{ij}\n\\]\n\n\\(W_{ij}\\) : 각 범주(i,j) 간의 가중치\n\\(P_{ij}\\) : 두 평가자가 범주 (i,j)를 선택한 관측 비율\n\n\n\nExpected Accuracy (\\(P_e\\))\n\\(P_e\\)는 각 평가자의 범주별 평가 확률의 곱에 가중치를 곱하여 계산된다.\n\\[\nP_e = \\sum_{i}\\sum_{j} W_{ij}P_{i+}P_{+j}\n\\]\n\n\\(P_{i+}\\) : 평가자 1이 범주 i를 선택한 전체 비율 (행 방향)\n\\(P_{+j}\\) : 평가자 2가 범주 j를 선택한 전체 비율 (열 방향)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#가중치w의-종류",
    "href": "posts/2025-03-18-Kappa/index.html#가중치w의-종류",
    "title": "Kappa 분석 이해하기",
    "section": "2.2 가중치(\\(W\\))의 종류",
    "text": "2.2 가중치(\\(W\\))의 종류\nWeighted Kappa에서 주로 사용하는 대표적인 가중치 부여 방식은 두 가지다: 선형(linear)과 제곱(quadratic)\n\n선형 가중치 (Linear weights)\n선형 가중치는 Cicchetti-Allison weights라고도 하며, 평가 항목 간의 불일치 정도에 따라 일정한 간격으로 가중치를 부여한다. 즉, 두 평가자 간의 평가가 한 단계씩 멀어질 때마다 일정한 비율로 일치도가 감소한다. 특징은 각 평가 간의 차이에 비례하여(선형적으로) 가중치를 부여한다는 것이다.\n수식 표현:\n\\[\nW_{ij}^{linear} = 1 - \\frac{|i - j|}{k - 1}\n\\]\n\n\\(i\\), \\(j\\): 두 평가자가 선택한 범주(단계)\n\\(k\\): 범주의 전체 개수\n\n예시:\n범주가 1~4단계로 구성된 경우 (\\(k\\)=4):\n\n평가자가 각각 1단계와 2단계를 선택했다면:\n\n\\[\nW_{12}^{linear} = 1 - \\frac{|1 - 2|}{4 - 1} = \\frac{2}{3} \\approx 0.67\n\\]\n이는 평가자들이 약 67% 정도로 일치하고 있다고 볼 수 있으며, 33%는 불일치한다고 해석할 수 있다. 이 결과를 제곱 가중치를 적용했을 때와 비교해 본다.\n\n\n제곱 가중치 (Quadratic weights)\n제곱 가중치는 Fleiss-Cohen weights라고도 하며, 평가 항목 간의 불일치가 클수록 가중치를 제곱에 비례하여(비선형적으로) 부여한다. 특히 평가자 간의 작은 불일치는 가볍게, 큰 불일치는 더 무겁게 여긴다. 즉, 불일치의 정도가 증가할수록 가중치는 비선형적으로(제곱의 비율로) 감소하게 된다.\n수식 표현:\n\\[\nW_{ij}^{quadratic} = 1 - \\frac{(i - j)^2}{(k - 1)^2}\n\\]\n\n\\(i\\), \\(j\\): 두 평가자가 선택한 범주(단계) - (\\(k\\)): 범주의 전체 개수\n\n예시:\nLinear의 예시와 마찬가지로 범주가 1~4단계로 구성된 경우(\\(k\\)=4):\n\n평가자가 각각 1단계와 2단계를 선택했다면:\n\n\\[\nW_{12}^{quadratic} = 1 - \\frac{(1 - 2)^2}{(4 - 1)^2} = 1 - \\frac{1}{9} = \\frac{8}{9} \\approx 0.89\n\\]\n이는 평가자들이 약 89%로 상당히 높은 일치도를 나타내며, 한 단계만 차이가 나기에 작은 불일치 정도에 높은 가중치를 부여한 것이다. 그러나 두 단계 이상의 큰 차이가 발생하면 가중치가 급격히 감소하여, 큰 불일치로 인식을 한다.\n이와 같이 Quadratic 방식에서는 큰 불일치를 더욱 엄격하게 평가한다.\n\n\n가중치 선택 방법\n\n선형 가중치(Linear weights)는 모든 단계 간의 차이를 동일한 중요도로 평가할 때 사용한다.\n\n진단 결과가 1단계에서 2단계로 바뀌는 것과, 2단계에서 3단계로 바뀌는 것의 중요도가 같은 경우\n\n제곱 가중치(Quadratic weights)는 작은 차이보다 큰 차이에 더 큰 중요성을 부여할 때 적합하다.\n\n1단계와 2단계 간의 차이는 그리 크지 않지만, 2단계와 3단계 간의 차이는 매우 큰 의미를 가지는 경우\n\n\n결국, 분석하려는 데이터와 평가 기준의 특성에 따라 적절한 가중치를 선택하면 된다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-1-1",
    "href": "posts/2025-03-18-Kappa/index.html#example-1-1",
    "title": "Kappa 분석 이해하기",
    "section": "2.3 Example 1",
    "text": "2.3 Example 1\nWeighted Kappa는 R코드를 활용해 수월하게 구할 수 있다.\n아래는 두 명의 영상의학 전문의가 MRI 영상을 보고 병변의 심각도를 5단계 척도(1: 정상, 2: 경미, 3: 중등도, 4: 심함, 5: 매우 심함)로 평가한 경우를 예로 든 것이다. 두 평가자 간의 일치도를 Weighted Kappa를 사용하여 분석하며, 선형(linear) 가중치를 적용하여 평가 간의 차이를 부분적으로 반영한다.\nlibrary(irr)\n\n# 평가 데이터 예시 (랜덤하게 생성)\nset.seed(123)\nlesion_assessment &lt;- data.frame(\n  Rater1 = sample(1:5, 100, replace = TRUE),\n  Rater2 = sample(1:5, 100, replace = TRUE)\n)\n\n# 두 평가자 간의 Weighted Cohen's Kappa\nkap &lt;- irr::kappa2(lesion_assessment, weight = \"equal\")\nse &lt;- kap$value / kap$statistic\nci_lower &lt;- kap$value - qnorm(0.975) * se\nci_upper &lt;- kap$value + qnorm(0.975) * se\n\nweighted_kappa_result &lt;- paste0(round(kap$value, 3), \" (95% CI: \", round(ci_lower, 3), \"~\", round(ci_upper, 3), \")\")\nweighted_kappa_result\n이 예시를 통해 두 평가자 간의 의견 일치 수준을 파악할 수 있다.위 코드 실행 결과로 선형 가중치를 사용한 Weighted Kappa 값을 확인할 수 있으며, 범주 간의 불일치 정도를 반영한 평가자 간 일치도를 평가할 수 있다. 여기서 quadratic 가중치를 부여하기 위해서는 weight = “squared”로 수정하면 된다.\n출력:\n[1] \"0.048 (95% CI: -0.089~0.185)\"\n처음에 설명했던 것 처럼, Cohen’s Kappa 값이 0.048이라는 것은 두 평가자가 거의 무작위로 평가를 했거나, 일치도가 매우 낮다는 것을 의미한다. Example의 결과 값으로 0.048로 나타났기 때문에 이런 경우 평가자 간의 의견이 거의 일치하지 않는다고 결론을 내린다.\n신뢰구간을 살펴보면 -0.089에서 0.185 사이에 위치하고 있다. 95% 신뢰구간은 실제 모집단에서의 Kappa 값이 이 범위 안에 있을 확률이 95%라는 뜻이다. 하지만 이 신뢰구간에는 음수 값(-0.089)이 포함되어 있으므로 일반적으로 평가자 간의 일치도가 단순한 우연보다도 낮다는 뜻이다. 이는 평가자들이 무작위로 답변한 것보다도 더 일치도가 낮을 가능성이 있다는 것이며, 신뢰구간이 넓다는 것은 데이터가 불안정하거나, 평가자 간의 일치도가 일정하지 않다는 것을 나타낸다.\n예시에서 이러한 결과가 나오는 이유는 평가 데이터 자체가 랜덤하게 생성되었기 때문이다. 그러므로 당연히 통계적으로 신뢰하기 어려운 결과가 나온다. 실제 상황에서 이러한 Kappa 값이 나온다면, 평가 기준을 명확하게 정리하고, 데이터의 질을 개선하는 것이 필요할 것이다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#fleiss-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#fleiss-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "3.1 Fleiss’ Kappa 공식",
    "text": "3.1 Fleiss’ Kappa 공식\nFleiss’ Kappa(\\(\\kappa\\))를 정의하는 수식은 다음과 같다.\n\\[\n\\kappa = \\frac{\\bar{P_o} - \\bar{P_e}}{1 - \\bar{P_e}}\n\\]\n\n\\(\\bar{P_o}\\): 관찰된 평균 일치율 (Observed agreement)\n\\(\\bar{P_e}\\): 우연에 의한 평균 일치율 (Expected agreement)\n\n이와 같이 Fleiss’ Kappa는 관찰된 평균 일치율(\\(\\bar{P}_0\\))과 우연에 의한 평균 일치율(\\(\\bar{P}_e\\))을 사용하여 계산한다. 각각의 공식은 아래와 같다.\n\nObserved agreement \\(\\bar{P}_o\\):\n\\(\\bar{P}_o\\)는 평가자들이 실제로 얼마나 의견이 일치했는지를 측정하는 값인데, 이는 실제 평가에서 동일한 범주를 선택한 평가자들의 비율을 정량적으로 나타내는 값이며, 다음과 같은 수식으로 계산된다.\n\\[\n\\bar{P}_o = \\frac{1}{N n (n - 1)}\\left(\\sum_{i=1}^{N}\\sum_{j=1}^{k}n_{ij}^{2} - N n\\right)\n\\] - \\(N\\): 평가 항목의 총 개수 - \\(n\\): 각 대상당 평가자의 수 - \\(k\\): 평가 범주의 수 - \\(n_{ij}\\): \\(i\\)번째 대상에서 \\(j\\)번째 카테고리를 선택한 평가자의 수\n이 공식에서 첫 번째 항인 \\(\\sum_{i=1}^{N}\\sum_{j=1}^{k}n_{ij}^{2}\\)는 특정 범주를 선택한 평가자 수를 제곱하고 합산한다. 동일한 범주를 선택한 평가자가 많을수록 그 값이 더 커지며, 평가자들이 일관된 결정을 내릴수록 이 항의 값이 증가하게 된다. 이 식에서 \\(Nn\\)을 빼는 이유는, \\(\\sum n_{ij}^2\\) 항에는 평가자가 한 명만 특정 범주를 선택한 경우도 포함되기 때문이다. 평가자 수가 많을수록 이 값이 증가하므로, 이를 보정하기 위해 전체 평가 대상 개수(\\(N\\))와 평가자 수(\\(n\\))를 곱한 값을 빼준다. 이를 통해, 평가자가 많을수록 발생할 수 있는 단순한 일치 효과를 제거하고, 실제로 의미 있는 평가자 간의 일치도를 측정할 수 있도록 조정한다.\n마지막으로, 이 값을 \\(N n (n - 1)\\)로 나누어 정규화한다. 이는 전체 평가 수에 대해 평균을 구하는 과정이며, 결과적으로 Observed Agreement는 “평균적인 일치도”를 나타내는 값이 된다. 이 값이 클수록 평가자들이 동일한 평가를 내린 비율이 높다는 것을 의미하며, 평가자 간의 의견이 더욱 일관되게 나타난다는 것을 보여준다. 이 값은 Expected Agreement(\\(\\bar{P}_e\\))와 비교하여 Fleiss’ Kappa 값을 계산하는 핵심 요소다.\n\n\nExpected agreement by chance \\(\\bar{P_e}\\)\n\\(\\bar{P_e}\\)은 평가자들이 평가를 무작위로 진행했을 때 결과가 우연히 일치하게 될 확률을 나타낸다.\n\\[\n\\bar{P}_e = \\sum_{j=1}^{k} P_j^{2}\n\\]\n여기서 \\(P_j\\)는 모든 대상과 평가자를 통틀어 \\(j\\)번째 카테고리에 선택된 횟수의 합을 전체 평가 횟수(\\(N \\times n\\))로 나눈 값이다. 이를 식으로 표현할 수 있다.\n\\[\nP_j = \\frac{1}{Nn} \\sum_{i=1}^{N} n_{ij}\n\\] 이 공식에서 \\(\\sum_{i=1}^{N} n_{ij}\\)는 모든 평가 대상에서 특정 범주 \\(j\\)가 선택된 총 횟수를 의미하며, 이를 전체 평가 횟수(\\(N \\times n\\))로 나누어 특정 범주 \\(j\\)가 선택될 확률을 구한다.\n무작위로 평가를 했다고 가정하면, 한 평가자가 특정 범주 \\(j\\)를 선택할 확률은 \\(P_j\\)이고, 또 다른 평가자도 동일한 범주를 선택할 확률 역시 \\(P_j\\)이므로, 이 두 확률을 곱한 \\(P_j^2\\)이 해당 범주에서 평가가 우연히 일치할 확률이 된다. 따라서 모든 범주 \\(j=1\\)부터 \\(k\\)까지에 대해 이러한 우연 일치 확률을 더하면, 전체적으로 평가자들이 무작위로 평가했을 때 기대되는 평균적인 일치 확률 \\(\\bar{P}_e\\)을 계산할 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example",
    "href": "posts/2025-03-18-Kappa/index.html#example",
    "title": "Kappa 분석 이해하기",
    "section": "3.2 Example",
    "text": "3.2 Example\n다음 예시 데이터로 Fleiss’ Kappa의 개념을 살펴본다.\n\n\n\n항목\n범주1\n범주2\n범주3\n\n\n\n\n1\n0\n0\n5\n\n\n2\n0\n1\n4\n\n\n3\n1\n0\n4\n\n\n4\n0\n2\n3\n\n\n5\n0\n1\n4\n\n\n\n\n평가 항목 수: \\(N = 5\\)\n평가자 수: \\(n = 5\\) (각 항목마다 평가자가 5명)\n평가 범주 수: \\(k = 3\\)\n\nR 코드로 구하는 방식:\n# `irr` 패키지의 `kappam.fleiss()` 함수를 사용한다\ninstall.packages(\"irr\")\nlibrary(irr)\n\n# 데이터 행렬 생성\nratings &lt;- matrix(c(\n  0, 0, 5,\n  0, 1, 4,\n  1, 0, 4,\n  0, 2, 3,\n  0, 1, 4), \n  nrow = 5, byrow = TRUE)\n\n# Fleiss' Kappa 계산\nfleiss_kappa &lt;- kappam.fleiss(ratings)\nprint(fleiss_kappa)\n출력:\n Fleiss' Kappa for m Raters\n\n Subjects = 5 \n   Raters = 3 \n    Kappa = -0.25 \n\n        z = -1.83 \n  p-value = 0.067 \nCohen’s Kappa와 같이 Fleiss’ Kappa는 평가 값이 동일한 경우 1, 다르면 0으로 단순 비교한다. 순위형 변수로 Weight를 부여해야 할 경우, Generalized Fleiss’ Kappa를 사용하면 된다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#generalized-fleiss-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#generalized-fleiss-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "4.1 Generalized Fleiss’ Kappa 공식",
    "text": "4.1 Generalized Fleiss’ Kappa 공식\nGeneralized Fleiss’ Kappa (\\(\\kappa_G\\))의 수식은 다음과 같다.\n\\[\n\\kappa_{G} = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\): 관찰된 가중 평균 일치율 (Observed weighted agreement)\n\\(P_e\\): 우연히 기대되는 가중 평균 일치율 (Expected weighted agreement by chance)\n\n각 항목의 계산법은 아래와 같다.\n\nObserved weighted agreement \\(P_o\\)\n\\[\nP_o = \\frac{1}{\\sum_{i=1}^{N} n_i(n_i - 1)} \\sum_{i=1}^{N}\\sum_{j=1}^{k}\\sum_{l=1}^{k} W_{jl} \\cdot n_{ij}(n_{il}-\\delta_{jl})\n\\] - \\(N\\): 평가된 항목(대상)의 총 개수 - \\(k\\): 평가 범주의 수 - \\(n_i\\): \\(i\\)번째 항목을 평가한 평가자의 수 - \\(n_{ij}\\): \\(i\\)번째 항목을 \\(j\\)번째 범주로 평가한 평가자의 수 - \\(W_{jl}\\): 범주 간 가중치 - \\(\\delta_{jl}\\): 범주가 같으면 1, 다르면 0인 값\nGeneralized Fleiss’ Kappa에서 Observed Weighted Agreement \\(P_o\\)는 평가자들이 실제로 얼마나 일치했는지를 측정하는 값이다. 기존 Fleiss’ Kappa가 단순한 평가 일치율을 계산하는 방식이라면, Generalized Fleiss’ Kappa는 평가 값 사이의 차이를 반영하여 가중치를 적용하는 방식으로 평가자 간의 일치도를 보다 정교하게 측정한다.\n\\(P_o\\)는 평가자들이 동일한 평가를 내린 정도를 가중(weighted) 방식으로 계산하며, 전체 평가 대상에서 발생한 모든 평가 쌍에 대한 가중 평균을 구하는 방식으로 정의된다. 먼저, 전체 평가자들이 내린 평가 쌍의 총 개수는 \\(\\sum_{i=1}^{N} n_i(n_i - 1)\\)로 계산된다. 여기서 \\(N\\)은 평가 대상의 개수이며, \\(n_i\\)는 특정 평가 대상에 대해 평가를 수행한 평가자 수이다. 평가 대상마다 평가자 수가 다를 수 있으므로 이를 반영하여 전체적인 합을 계산한다.\n식에서 그 외의 부분은 평가자 간의 일치도를 계산한다. \\(W_{jl}\\)은 범주 \\(j\\)와 범주 \\(l\\) 사이의 가중치로, 두 평가 값이 얼마나 다른지를 수치적으로 반영하는 역할을 하는데, 앞서 설명한 linear 또는 quadratic 방식으로 설정된다. \\(n_{ij}\\)는 평가 대상 \\(i\\)에서 범주 \\(j\\)를 선택한 평가자의 수를 의미하며, \\(n_{il}\\)은 동일한 평가 대상에서 범주 \\(l\\)을 선택한 평가자의 수를 나타낸다. 또한 \\(\\delta_{jl}\\)은 Kronecker Delta로, \\(j\\)와 \\(l\\)이 동일한 경우 1, 다르면 0을 반환하는 지표이다.\n\\(P_o = 1\\)이면 평가자들이 완벽하게 동일한 평가를 내린 경우이며, \\(P_o = 0\\)이면 평가자 간 평가가 완전히 무작위로 이루어진 경우를 의미한다.\n\n\nExpected weighted agreement by chance \\(P_e\\)\n\\[\nP_e = \\frac{1}{\\left(\\sum_{i=1}^{N} n_i\\right)^2 - \\sum_{i=1}^{N} n_i} \\sum_{j=1}^{k}\\sum_{l=1}^{k} W_{jl}\\left(\\sum_{i=1}^{N}n_{ij}\\right)\\left(\\sum_{i=1}^{N}n_{il}-\\delta_{jl}\\right)\n\\] \\(P_e\\)**는 Fleiss’ Kappa에서 Expected agreement by chance (\\(P_e\\))를 확장한 개념으로, 평가 값들 사이의 거리를 고려하여 가중(weighted) 방식으로 측정한다.\n\\(\\left(\\sum_{i=1}^{N} n_i\\right)^2 - \\sum_{i=1}^{N} n_i\\)는 전체 평가 데이터에서 발생할 수 있는 모든 평가 쌍의 개수를 나타낸다. 여기서 \\(\\sum_{i=1}^{N} n_i\\)는 전체 평가자가 수행한 총 평가 개수이며, 이를 제곱한 값에서 자기 자신과의 비교를 제외하기 위해 \\(\\sum_{i=1}^{N} n_i\\)를 빼준다. 이는 평가자들이 임의로 범주를 선택했을 때 가능한 모든 평가 쌍의 개수를 정규화하는 역할을 한다.\n\\(\\sum_{i=1}^{N} n_{ij}\\)와 \\(\\sum_{i=1}^{N} n_{il}\\)은 특정 범주 \\(j\\)와 \\(l\\)이 전체 평가에서 각각 몇 번 선택되었는지를 나타낸다. 이 값에 가중치 행렬 \\(W_{jl}\\)을 곱하는데, \\(W_{jl}\\)은 범주 \\(j\\)와 범주 \\(l\\) 사이의 거리를 반영하는 값으로, linear 또는 quadratic 가중치로 결정된다.마지막으로, \\(\\delta_{jl}\\)는 두 범주가 동일할 경우 1, 다를 경우 0을 갖는 함수다.\n결과적으로 \\(P_e\\)를 \\(P_o\\)와 비교하여 Kappa 값이 계산되며, \\(P_o\\)가 \\(P_e\\)보다 클수록 평가자 간의 신뢰도가 높다는 것을 의미한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-3",
    "href": "posts/2025-03-18-Kappa/index.html#example-3",
    "title": "Kappa 분석 이해하기",
    "section": "4.2 Example",
    "text": "4.2 Example\n다음과 같은 예시 데이터를 통해 계산 방식을 간단히 이해해 본다. 평가 범주는 1~3으로 순서형이며, 일부 평가자는 특정 항목을 평가하지 않았다.\n\n\n\n항목\n평가자1\n평가자2\n평가자3\n평가자4\n\n\n\n\n1\n1\n2\n2\n-\n\n\n2\n2\n2\n3\n2\n\n\n3\n3\n3\n-\n-\n\n\n4\n1\n1\n1\n2\n\n\n\nGeneralized Fleiss’ Kappa는 R의 irrCAC 패키지에 있는 fleiss.kappa.raw 함수를 사용하여 쉽게 계산할 수 있다.\nlibrary(irrCAC)\n\n# 여러 평가자의 병변 평가 데이터\nratings &lt;- data.frame(\n  rater1 = c(1, 2, 3, 1),\n  rater2 = c(2, 2, 3, 1),\n  rater3 = c(2, 3, NA, 1),\n  rater4 = c(NA, 2, NA, 2)\n)\n\n# Linear Weighted Fleiss Generalized Kappa\nkappa_linear &lt;- fleiss.kappa.raw(ratings, weights = \"linear\")\nprint(kappa_linear)\n\n# Quadratic Weighted Fleiss Generalized Kappa\nkappa_quadratic &lt;- fleiss.kappa.raw(ratings, weights = \"quadratic\")\nprint(kappa_quadratic)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#신뢰도-데이터",
    "href": "posts/2025-03-18-Kappa/index.html#신뢰도-데이터",
    "title": "Kappa 분석 이해하기",
    "section": "5.1 신뢰도 데이터",
    "text": "5.1 신뢰도 데이터\n먼저 Krippendorff’s Alpha를 계산하기 위해서는 평가자들이 동일한 단위(unit)에 대해 내린 평가 데이터를 분석해야 한다. 평가자들은 독립적으로 하나 이상의 값을 할당할 수 있으며, 이러한 데이터를 m × N 행렬로 표현할 수 있다. 여기서 m은 평가자의 수, N은 평가된 항목의 개수이다.\n평가자가 특정 단위 \\(u_j\\)에 할당한 값 \\(v_{ij}\\)를 포함하는 행렬을 다음과 같이 정의할 수 있다.\n\\[\n\\begin{array}{c|cccc}\n    & u_1 & u_2 & u_3 & \\cdots & u_N \\\\\\hline\nc_1 & v_{11} & v_{12} & v_{13} & \\cdots & v_{1N} \\\\\nc_2 & v_{21} & v_{22} & v_{23} & \\cdots & v_{2N} \\\\\nc_3 & v_{31} & v_{32} & v_{33} & \\cdots & v_{3N} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_m & v_{m1} & v_{m2} & v_{m3} & \\cdots & v_{mN}\n\\end{array}\n\\]\n\n\\(m\\): 평가자의 수\n\\(N\\): 평가된 항목(unit)의 개수\n\\(v_{ij}\\): 평가자 \\(c_i\\)가 특정 단위 \\(u_j\\)에 대해 부여한 값\n\\(m_j\\): 단위 \\(u_j\\)에 대해 평가된 개수\n\n여기서 \\(m_j\\)는 특정 단위 \\(u_j\\)에 대해 평가된 개수이다. 일부 평가자가 특정 단위에 대한 평가를 하지 않을 수도 있기 때문에, \\(m_j\\)는 평가자가 동일하지 않은 경우 \\(m\\)보다 작을 수도 있다.\nKrippendorff’s Alpha를 계산하려면 평가된 값들이 서로 비교 가능(pairable) 해야 하므로, \\(m_j \\geq 2\\)의 조건이 필요하다. 즉, 특정 단위에서 최소 두 명 이상의 평가자가 값을 할당해야 한다. 전체 데이터에서 가능한 쌍의 개수는 다음과 같다.\n\\[\n\\sum_{j=1}^{N} m_j = n \\leq mN\n\\]\n이러한 행렬 표현은 Krippendorff’s Alpha에서 관찰된 불일치 \\(D_o\\)와 기대되는 불일치 \\(D_e\\)를 계산하는 기본적인 데이터 구조를 나타낸다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#krippendorffs-alpha-공식",
    "href": "posts/2025-03-18-Kappa/index.html#krippendorffs-alpha-공식",
    "title": "Kappa 분석 이해하기",
    "section": "5.1 Krippendorff’s Alpha 공식",
    "text": "5.1 Krippendorff’s Alpha 공식\nKrippendorff’s Alpha는 평가자 간의 일치도를 분석하기 위해 관찰된 불일치(\\(D_o\\))와 우연히 예상된 불일치(\\(D_e\\))를 비교하여 계산된다. 평가자들이 응답할 수 있는 모든 가능한 값들의 집합을 \\(R\\)이라고 하고, 평가자들이 특정 예제에 대해 내린 응답을 하나의 단위(unit)라고 할 때, Krippendorff’s Alpha는 다음과 같이 정의된다.\n\\[\n\\alpha = 1 - \\frac{D_o}{D_e}\n\\]\n\n\\(D_o\\): 실제로 관찰된 불일치 (Observed Disagreement)\n\\(D_e\\): 우연히 예상된 불일치 (Expected Disagreement by chance)\n\n\nObserved Disagreement \\(D_o\\)\n\\(D_o\\)는 다음과 같이 정의된다.\n\\[\nD_o = \\frac{1}{n} \\sum_{c \\in R} \\sum_{k \\in R} \\delta(c, k) \\sum_{u \\in U} m_u \\frac{n_{cku}}{P(m_u, 2)}\n\\]\n\n\\(\\delta(c, k)\\): 두 개의 평가 값 \\(c\\)와 \\(k\\) 사이의 차이\n\\(n\\): 총 가능한 쌍(pair)의 개수 - \\(m_u\\): 특정 단위 \\(u\\)에 포함된 평가 수\n\\(n_{cku}\\): 단위 \\(u\\)에서 평가 값 \\((c, k)\\) 쌍이 나타난 횟수\n\\(P\\): 순열(permutation)\n\n이 식은 평가자들이 특정 단위에서 얼마나 불일치했는지를 개념적으로 가중 평균(weighted average)한 것으로 해석할 수 있다. 여기서 중요한 역할을 하는 \\(\\delta(c, k)\\)는 평가 값 \\(c\\)와 평가 값 \\(k\\) 사이의 차이를 의미하는데, 순위형 데이터의 경우 제곱을 취하여 거리의 크기를 강조하는 방식으로 계산한다. 즉, 두 평가 값이 동일하면 \\(\\delta(c, k) = 0\\)이 되고, 평가 값 간 차이가 크면 불일치도가 더 커지는 방식이다.\n\nKrippendorff’s Alpha는 데이터의 유형에 따라 다른 \\(\\delta(c, k)\\)의 정의를 사용한다. 이 값을 기반으로 평가자 간의 불일치를 측정하므로, 이 함수가 어떻게 정의가 되느냐에 따라 Alpha \\(α\\)의 값이 달라진다.\n\n또한, \\(P(m_u, 2)\\)는 특정 단위 \\(u\\)에서 평가된 값들 중에서 2개의 값을 선택하여 순서를 고려한 쌍의 개수를 의미한다. 이는 순열 함수로 표현되며, Krippendorff’s Alpha에서 관찰된 불일치도를 계산할 때 중요한 역할을 한다. 공식은 다음과 같다.\n\\[\nP(m_u, 2) = \\frac{m_u (m_u - 1)}{2}\n\\]\n이러한 계산이 필요한 이유는, Krippendorff’s Alpha에서 평가자 간의 불일치를 측정할 때 단순한 개별 평가 값을 비교하는 것이 아니라, 각 단위에서 이루어진 모든 평가 값들 간의 관계를 분석해야 하기 때문이다.\n이 식은 평가 값 자체의 범주를 기반으로 전체적인 불일치도를 직접적으로 계산하는 방식이라면, 단위별 불일치도를 먼저 계산한 후 전체 평균을 구하는 방식도 존재한다. \\(D_o\\)에 대한 단위 중심의 접근법은 다음과 같다.\n\\[\nD_o = \\frac{1}{n} \\sum_{j=1}^{N} m_j E(\\delta_j)\n\\]\n여기서 \\(E(\\delta_j)\\)는 모든 가능한 쌍에 대해 평균적인 거리이며, 아래와 같은 식으로 표현할 수 있다.\n\\[\nE(\\delta_j) = \\frac{ \\sum_{i&gt;i'} \\delta(v_{ij}, v_{i'j}) }{\\binom{m_j}{2}}\n\\]\n\n\\(v_{ij}\\): 평가자 \\(i\\)가 단위 \\(j\\)에 대해 부여한 평가 값\n\\(\\delta(v_{ij}, v_{i'j})\\): 두 평가 값 \\(v_{ij}\\)와 \\(v_{i'j}\\) 간의 거리\n\\(\\sum_{i&gt;i'}\\): 단위 \\(j\\)에서 모든 평가자 간의 가능한 쌍을 고려한 합 - \\(\\binom{m_j}{2}\\): 단위 \\(j\\)에서 가능한 모든 평가 쌍의 개수. \\(\\frac{m_j(m_j - 1)}{2}\\)로 계산된다.\n\n이 수식은 단위 \\(j\\)에서 평가자들이 부여한 모든 값들을 비교하여 평균적인 불일치도를 구하는 과정이다. 평가자들이 같은 값을 부여했다면 \\(\\delta(v_{ij}, v_{i'j}) = 0\\)이 되어 \\(E(\\delta_j)\\) 값이 작아지고, 평가 값이 크게 차이 날수록 \\(E(\\delta_j)\\) 값이 증가한다.\n또한, 만약 모든 단위에서 평가자 수가 일정하다면, \\(D_o\\)는 전체 평가 단위에서 가능한 모든 평가 쌍에 대한 평균적인 거리로 해석할 수 있다. 이는 일반적으로 평가 행렬에서 대각선에서의 평균적인 거리로 볼 수 있으며, 평가자들이 특정 경향을 가지고 평가했는지 또는 무작위로 평가했는지를 판단하는 중요한 지표가 된다.\n\n\nExpected Disagreement by chance \\(D_e\\)\n우연히 예상된 불일치 \\(D_e\\)는 평가자들이 무작위로 응답했다고 가정할 때 예상되는 불일치도를 의미하는데, 모든 가능한 평가 값 쌍(c, k)의 발생 확률을 이용해 불일치를 추정한다. 이는 Krippendorff’s Alpha에서 평가자 간의 일치도를 측정할 때, 실제 관찰된 불일치도(\\(D_o\\))와 비교하는 기준이 된다.\n\\[\nD_e = \\frac{1}{P(n,2)} \\sum_{c \\in R} \\sum_{k \\in R} \\delta(c,k) P_{ck}\n\\] - \\(P(n,2)\\): 전체 가능한 평가 쌍(pair)의 개수 - \\(\\delta(c,k)\\): 두 개의 평가 값 \\(c\\)와 \\(k\\) 사이의 차이 - \\(P_{ck}\\): 특정 평가 값 \\((c,k)\\) 쌍이 발생할 확률\n\\[\nP_{ck} = \\begin{cases}\n  n_c n_k & \\text{if } c \\neq k \\\\\n  n_c (n_c - 1) & \\text{if } c = k\n\\end{cases}\n\\]\n이 식에서 \\(n_c\\)와 \\(n_k\\)는 각각 평가 값 \\(c\\)와 \\(k\\)가 전체 데이터에서 나타난 횟수이다. 만약 \\(c \\neq k\\)이면, 서로 다른 두 개의 평가 값이 선택될 확률은 \\(n_c n_k\\)로 표현된다. 반면, \\(c = k\\)이면 동일한 평가 값이 두 번 선택될 확률은 \\(n_c (n_c - 1)\\)로 계산된다. 이러한 확률을 고려하여 평가 값들이 랜덤하게 분포되었을 때 기대되는 불일치도를 구할 수 있다.\n즉, \\(D_e\\)는 평가자들이 일관된 기준 없이 평가한 경우 예상되는 불일치도의 평균적인 크기를 나타낸다.\nKrippendorff’s Alpha는 개념적으로 직관적이지만, 계산적으로는 다소 복잡할 수 있다. 그러나 이 지표는 다양한 데이터 유형에 적용할 수 있으며, 평가자의 수가 일정하지 않거나 결측값이 포함된 경우에도 안정적인 신뢰도 분석을 수행할 수 있는 장점이 있다.\n\n\nKrippendorff’s Alpha 값 해석\n\\(α\\)에 대한 해석은 다음과 같다.\n\nα = 1: 완벽한 평가자 간 일치 (모든 평가자가 동일한 응답)\n0.8 ≤ α ≤ 1: 높은 신뢰도를 의미하며 연구 결과로 활용 가능\n0.67 ≤ α &lt; 0.8: 신뢰할 수 있는 수준이지만 엄격한 연구에서는 보완이 필요함\n0 ≤ α &lt; 0.67: 신뢰도가 낮아 추가적인 평가 기준 수정 또는 평가자 교육이 필요함\nα &lt; 0: 평가자 간 일치도가 우연보다도 낮음 (평가 기준이 모호하거나 데이터에 문제 가능성)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-4",
    "href": "posts/2025-03-18-Kappa/index.html#example-4",
    "title": "Kappa 분석 이해하기",
    "section": "5.2 Example",
    "text": "5.2 Example\n아래는 irrCAC 패키지를 사용하여 Krippendorff’s Alpha를 계산하는 R 코드 예제이다.\nlibrary(irrCAC)\n\n# 여러 평가자의 병변 평가 데이터\nratings &lt;- data.frame(\n  rater1 = c(1, 2, 3, 1, 2, NA, 4, 3, NA, 2),\n  rater2 = c(2, 2, 3, 1, 3, 2, 4, 3, 2, 1),\n  rater3 = c(2, 3, NA, 1, 4, 2, NA, 3, 2, NA),\n  rater4 = c(NA, 2, NA, 2, 3, 1, 4, NA, 3, 2)\n)\n\n# Krippendorff’s Alpha 계산 (순위형 데이터)\nalpha_result &lt;- krippen.alpha.raw(ratings, weights = \"ordinal\")\nprint(alpha_result)\n이 코드는 순위형 데이터를 기반으로 Krippendorff’s Alpha를 계산하는 방법을 보여준다. 평가자 간 일치도를 보다 유연하게 측정할 수 있으며, Fleiss Kappa나 Weighted Cohen’s Kappa보다 데이터 특성에 덜 제한을 받는다는 장점이 있다. 또한, 결측값이 포함된 데이터를 그대로 분석할 수 있다는 점에서 다른 신뢰도 측정법보다 강력한 활용성을 가진다.\nKrippendorff’s Alpha는 평가자가 많을수록, 그리고 평가 기준이 명확할수록 신뢰도가 높게 나타난다. 하지만 평가자 간 의견 차이가 크다면 신뢰도 값이 낮아질 수 있으며, 이런 경우 평가 기준을 조정하거나 추가적인 훈련이 필요할 수 있다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html",
    "href": "posts/2025-02-28-reg3/index.html",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "",
    "text": "3장에서는 2장에서 다룬 Generalized Linear Model (GLM)에서 더 나아가, 데이터 내에 군집(Clustered) 구조가 존재하거나, 반복측정(Repeated measures) 데이터로 인해 독립성 가정이 깨지는 경우를 다루는 방법론인 GEE (Generalized Estimating Equation)와 GLMM (Generalized Linear Mixed Model)을 다루며, 그 전에 중요한 추정 방법론 중 하나인 M-estimation과 Robust(Sandwich) estimation에 대해서 다루겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#들어가며",
    "href": "posts/2025-02-28-reg3/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "",
    "text": "3장에서는 2장에서 다룬 Generalized Linear Model (GLM)에서 더 나아가, 데이터 내에 군집(Clustered) 구조가 존재하거나, 반복측정(Repeated measures) 데이터로 인해 독립성 가정이 깨지는 경우를 다루는 방법론인 GEE (Generalized Estimating Equation)와 GLMM (Generalized Linear Mixed Model)을 다루며, 그 전에 중요한 추정 방법론 중 하나인 M-estimation과 Robust(Sandwich) estimation에 대해서 다루겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#m-estimation",
    "href": "posts/2025-02-28-reg3/index.html#m-estimation",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "1. M-estimation",
    "text": "1. M-estimation\n1.1. M-estimation 정의\n\n통계 분석에서 통계 모델이 비모수(non-parametric)가 아니라 모수(parametric)인 경우, 우리는 model의 모수, 즉 parameter (\\(\\boldsymbol{\\theta}\\), \\(\\boldsymbol{\\beta}\\) 등)를 추정해야 합니다. 이는 생각보다 어려운 일이 될 수 있으며, 이전에 언급한 MLE, OLS, Method of Moments (MOM) 등 다양한 model의 estimation 방법이 제안되어 왔습니다. 그런데, 이러한 추정 방법들은 사실상 하나의 추정방정식(estimating equation, 통계 모델의 parameter 추정 방향을 제시하는 모든 식)을 세운 뒤, 그 방정식을 만족하는 \\(\\hat{\\boldsymbol{\\theta}}\\)를 찾는 과정으로 해석할 수 있습니다. 예를 들어, MLE에서는 log likelihood를 parameter로 미분한 함수(score function)가 0이 되는 parameter point를 추정하는 과정이었고, OLS에서는 cost function (SSR, 오차 제곱합)을 parameter로 미분한 함수가 0이 되는 parameter point을 찾는 과정이었습니다. M-estimation은 이러한 공통된 개념을 일반화하여 공통된 parameter의 성질을 제시해줍니다.\n즉, M-estimation은 다음 과 같은 형태를 지닌 추정 방정식을 세우고, 이를 만족하는 모수의 값을 해로 삼습니다:\n\\[\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0},\n\\]\n여기서 \\(\\psi_i(\\boldsymbol{\\theta})\\)는 (i)번째 관측치에 대해 정의된 estimating function (e.g. score function), \\(\\boldsymbol{\\theta}\\)는 추정하고자 하는 parameter(위 식에서는 우항이 scalar 0이 아닌 벡터 0이므로 \\(\\boldsymbol{\\theta}\\) 또한 벡터.)입니다. 위에서 언급한 것처럼 MLS와 OLS, OLS의 일반화 버전인 Non-linear Least Squares 모두 M-estimation입니다. 1, 2장에 걸쳐 이미 익숙하시겠지만 다시 확인해보면, MLE를 M-estimation 형태로 작성해보면 다음과 같고,\n\\[\n\\psi_i(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\psi_i(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) = \\mathbf{0}.\n\\]\nOLS는 다음과 같습니다. \\[\n\\psi_i(\\boldsymbol{\\beta}) = (Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\mathbf{x}_i, \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\mathbf{x}_i(Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) = \\mathbf{0}.\n\\]\n여기서 OLS의 estimation equation이 다음과 같은 이유는,\\[\n\\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big)\n\\]\n\\[\n= \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big)\n\\]\n\\[\n= X^\\top X \\beta - X^\\top y\n\\]\n\\[\n=\\sum_{i=1}^n \\mathbf{x}_i(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} - Y_i) = \\mathbf{0}\n\\] 에서 유도된 식입니다.\n1.2. M-estimation 특징\n\nM-estimation의 가장 중요한 특징은 일반성과 확장성입니다. 즉, parameter estimation 문제를\n\\[\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0}\n\\]\n의 해로서 바라보면, 여러 기존 추정법들을 하나의 큰 이론적 틀에서 이해할 수 있고, 이로부터 발생하는 성질들은 해당되는 방법론 모두에 적용됩니다. 이렇게 M-estimation을 강조하여 설명하는 이유는, M-estimation은 아래 두 가지 수렴 이론(Asymptotic theory)을 제공하기 때문입니다.\n(1) 적절한 정규성 조건(regularity conditions) 하에서(종속변수의 정규 분포 가정이 아니며, 언급드린 적이 없지만 아주 general한 조건이라고 생각해주시면 됩니다.), 위 M-estimation의 estimating equation의 추정해 \\(\\hat{\\boldsymbol{\\theta}}\\)가 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 일치성(Consistency)과 점근정규성(Asymptotic Normality)을 가집니다.\n\n(2) 또한, 정규성을 갖는 모수의 점근분포가 중심극한정리(CLT)의 연장선상에 있다고 볼 수 있으며, 그 결과 위 \\(\\hat{\\boldsymbol{\\theta}}\\)의 asymptotic Normality에서 parameter의 분산은 Asymptotically&robust하게 추정 가능하고, 이 Robust Estimator의 형태가 샌드위치(sandwich) 형태로 생겼기 때문에 Sandwich Estimation(or)이라고도 부릅니다.\n즉, M-estimation으로부터 얻는 의의를 살펴보자면, 우리가 Regression Model의 parameters를 추정하는 과정에서 estimating equation이 위 M-estimation의 형태를 만족한다면, 어떠한 methods를 사용하든 이를 통해 추정한 parameter \\(\\hat{\\boldsymbol{\\theta}}\\)는 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 consistent함과, robust한 parameter의 분산을 얻을 수 있다는 것입니다. 이제 (1)과 (2)에 대한 수학적 증명을 걸친 뒤, 이들의 의미를 살펴보겠습니다.\n1.3. M-estimation의 Asymptotic Normality 증명\n\nM-estimation 추정량 \\(\\hat{\\theta}\\)의 점근적 성질을 유도하기 위해 1차 Taylor 전개를 사용합니다. 아래와 같은 M-estimation 추정 방정식 (증명의 편리를 위해 양변에 \\(\\frac{1}{n}\\)을 나누었으며, 나누지 않아도 똑같이 증명 가능하고, 등식에서 상수 term을 곱하고 나누는 것은 당연히 문제되지 않습니다. ) \\[\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) = 0\n\\] 을 참 모수 \\(\\theta_0\\) Taylor 식으로 전개하면,\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0) + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} (\\hat{\\theta} - \\theta_0) = 0.\n\\]\n가 됩니다. 이때 좌항 \\(\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta})\\)은 우리가 위 estimating equation에서 보았듯이, 이 항이 0이 되도록 하는 parameter를 추정한 결과가 \\(\\hat{\\theta}\\)였기 때문에 당연히 0일 것이고, 따라서 중앙항 (\\(\\theta_0\\)에 대한 Taylor 1차 식 전개) 또한 0이 되는 것입니다. 이제 \\(\\theta\\)에 대한 식을 도출하기 위해 \\(\\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0)\\) term을 넘기고 양변에 \\(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}\\)을 inverse하여(Matrix 이므로) 곱해주고 \\(\\sqrt{n}\\)을 곱하면,\n\\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\]\n가 됩니다. 여기서 다음 두 Matrix들을 정의하겠습니다:\n\n\n\\[ \\mathbf{A} = \\mathbb{E}\n\\left[ -\\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n\\] (2차 도함수 또는 score function의 미분의 기댓값)\n\n\\[ \\mathbf{B} = \\mathbb{E}\n\\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] \\] (score function의 분산의 기댓값)\n\n이제 대수의 법칙(LLN)과 중심극한정리(CLT)를 각각 적용하면 다음 두 식을 얻을 수 있습니다.\n\\[\n-\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} \\mathbf{A}\n\\] \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n\\]\n각 정리를 간단하게 설명드리자면, 대수의 법칙(LLN, Law of Large Numbers)은 표본 크기 \\(n\\)이 충분히 크면, 표본 평균이 모평균에 점근적으로 수렴한다는 법칙으로, 확률 변수 \\(X_i\\)가 동일 분포이고 기대값 \\(\\mathbb{E}[X_i] = \\mu\\)를 가지면, 수학적으로 표현하면 아래 식과 같습니다.\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_i \\xrightarrow{p} \\mu.\n\\]\n즉, 위 식에서는 \\[\n- \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}\n\\approx - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n= \\mathbf{A}\n\\] 가 되는 것입니다. 중심극한정리(CLT, Central Limit Theorem)는 독립이고 동일한 분포를 따르는 확률변수들의 표본 평균이 정규 분포를 따른다는 정리로, 분산이 \\(\\sigma^2\\)인 확률변수 \\(X_i\\)들에 대해,\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} (X_i - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2).\n\\] 입니다. (양변에 \\(\\sqrt{n}\\)이 나눠진 식이 더 친숙하실 겁니다.) 즉, 위 식에서는 \\(\\psi_i(\\theta_0)=0\\)이고, 따라서 \\[\n\\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] = Var(\\psi_i(\\theta_0))\n\\] 이므로,\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n\\] 임을 확인할 수 있습니다. 정리하자면, 대수의 법칙이 평균값으로의 수렴을 보장한다면, 중심극한정리는 표본 평균이 정규성을 띤다는 것을 보장하고, 이를 통해 우리가 고려하던 아래 식 \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\] 의 우항에 대한 두 정보를 얻을 수 있었습니다. 최종적으로 점근정규성을 보이기 위해선 이 두 수렴하는 분포의 곱을 나타낼 수 있는 Slutsky 정리를 보고, 최종적으로 식을 도출하겠습니다. Slutsky 정리는 두 개의 점근적 확률 분포를 결합하는 방법으로, 만약 \\(X_n \\xrightarrow{d} X\\) (약한 수렴)과, \\(Y_n \\xrightarrow{p} c\\) (확률적 수렴)이면,\n\\[\nX_i Y_i \\xrightarrow{d} Xc.\n\\]\n입니다. 즉, 확률적으로 수렴하는 변수와 분포적으로 수렴하는 변수를 곱하면, 여전히 위 식과 같이 분포적으로 수렴한다는 것이 증명된 정리이고, 위 식에서는\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} A\n\\]\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, B)\n\\] 이므로, Slutsky 정리를 사용하면 최종적으로\n\\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\]\n\\[\n\\xrightarrow{d} A^{-1} \\mathcal{N}(0, B) = \\mathcal{N}(0, A^{-1} B A^{-1})\n\\]\n입니다.(deteminant한 값은 분산 term에서 제곱된다는 것은 몇 번 보았었습니다.) 결국 M-estimation의 추정을 통해 얻은 \\(\\hat{\\boldsymbol{\\theta}}\\)가 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 일치성(Consistency)을 갖고, \\(\\hat{\\boldsymbol{\\theta}}\\)는 점근정규성(Asymptotic Normality)을 갖으며 그 식은 \\(\\mathcal{N}(0, A^{-1} B A^{-1})\\)입니다. 또한, 이 샌드위치(sandwich) 형태(\\(A^{-1}\\) 빵 사이에 껴있는 고기 \\(B\\))처럼 생긴 점근적 분산 식이 바로 Sandwich estimator의 general version입니다. 이제 이 sandwich estimator에 대해 좀더 설명드리겠습니다.\n1.4. Sandwich(Robust) Estimator\n\nSandwich(Robust) Estimator의 식은 써보면 다음과 같습니다: \\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\theta}}) = A^{-1} B A^{-1}\n\\] \\[\nwhere, \\; \\mathbf{A} = - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right],\n\\; \\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right]\n\\]\n2장에서 GLM case에 대해 log likelihood의 1차 도함수를 score function, 이의 negative 2차 도함수를 Fisher Information matrix라고 언급한 적이 있습니다. 이의 general한 버전이 위와 같으며, 여기에서는 이 score function \\(\\psi_i(\\theta_0)\\)의 분산을 \\(\\mathbf{B}\\), 2차 도함수를 \\(\\mathbf{A}\\)로 표기하고 있습니다.\n\\(\\mathbf{A}\\)는 모형의 곡률(curvature)을, \\(\\mathbf{B}\\)은 모형의 분산을 반영합니다. 또한, Estimating equation이 log likelihood로부터 MLE 철학으로 나온 parameter라면, \\(\\mathbf{A} = \\mathbf{B}\\)입니다. 이 이유는, 2장에서 2차 도함수가 정의되는 임의의 distribution을 따르는 종속변수 \\(Y\\)와 그의 parameter \\(\\theta\\)에 대해서 \\[ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} \\] 가 참임을 보였고, \\(\\ell''\\)은 \\(\\mathbf{A}\\), \\(\\frac{d^2\\ell}{d\\theta^2}\\)는 \\(\\mathbf{B}\\)와 같기 때문입니다. (\\(\\ell' = \\psi\\) 이므로.)\n즉 철학적으로 해석해보면, Regression Model의 selection이 정확한 경우 Fisher Information 행렬 동일성에 의해 \\(\\mathbf{A} = \\mathbf{B}\\)가 성립하게 되고, 이에 따라 parameter의 분산은 \\(A^{-1}\\) 만으로 추정될 수 있습니다. 그러나 Regression Model이 정확하지 않은 경우, consistent한 parameter estimation을 한다고 하더라도 이 모델의 추정 분산 \\(A^{-1}\\)은 더이상 신뢰할 수 없으며, 이때 Sandwich estimtor는 경험적 분산(empirical variance) \\(\\mathbf{B}\\)를 통해 robust하게 이를 추정할 수 있습니다. 즉, Regression Model의 몇 가지 가정이 의심될 때, 심지어는 의심되지 않더라도 Sandwich estimtor는 robust하게 parameter의 분산을 추정할 수 있는 것입니다.\n또한 이전에 스포한대로, 이전 장들에서 다루어 왔던 robust한 parameter variance estimator인 Heteroskedasticity-Consistent SE, Cluster-robust SE는 모두 이 Sandwich estimator의 special한 case입니다.(생김새부터 짐작할 수 있으셨을 겁니다.) LM version에서만 이를 증명한 뒤(GLM 버전도 같습니다.), GLM을 복습하고 GEE, GLMM에 대해서 설명드리겠습니다.\n\nProve HC0 is Sandwich estimator. (LM version)\n\nOLS의 score function은은 위에서 보았듯 다음과 같습니다:\n\\[\n\\psi_i(\\beta) = x_i (Y_i - x_i^T \\beta).\n\\] 그렇다면, \\(A\\)는 베타로 미분 후 -를 씌워주면 다음과 같이 계산되며,\n\\[\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_i(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ x_i x_i^T \\right].\n\\]\n\\(A\\)의 추정치는 결국\n\\[\n\\hat{A} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T = \\frac{1}{n} X^T X.\n\\] 가 됩니다. 마찬가지로 \\(B\\)를 계산하면,\n\\[\nB = \\mathbb{E} \\left[ \\psi_i(\\beta) \\psi_i(\\beta)^T \\right] = \\mathbb{E} \\left[ x_i x_i^T (Y_i - x_i^T \\beta)^2 \\right].\n\\]\n이며, 추정치는\n\\[\n\\hat{B} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T e_i^2 = \\frac{1}{n} X^T \\text{diag}(e_i^2) X.\n\\] 입니다. 결국\n\\[\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} X^T \\text{diag}(e_i^2) X \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n\\]\n\\[\n= (X^T X)^{-1} X^T \\text{diag}(e_i^2) X (X^T X)^{-1}.\n\\]\n가 되고, 이 식은 1장에서 보았던 HC0의 식과 동일함을 확인할 수 있습니다.\n\nProve Clustered-Robust SE is Sandwich estimator. (LM version)\n\n이 또한 OLS와 같은 환경이므로(LM, cluster가 \\(g\\)개로 구성되어 있다고 할 때, score function은 다음과 같습니다:\n\\[\n\\psi_g(\\beta) = \\sum_{i \\in g} x_i (Y_i - x_i^T \\beta).\n\\]\n\\(A\\)의 식과 추정치 또한 비슷하게 구해지고,\n\\[\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_g(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ \\sum_{i \\in g} x_i x_i^T \\right].\n\\] \\[\n\\hat{A} = \\frac{1}{n} \\sum_{g=1}^{G} \\sum_{i \\in g} x_i x_i^T = \\frac{1}{n} X^T X.\n\\]\n\\(B\\)도 비슷하게 계산되며, cluster간의 independent는 여전히 가정됩니다.\n\\[\nB = \\mathbb{E} \\left[ \\psi_g(\\beta) \\psi_g(\\beta)^T \\right].\n\\]\n\\[\n\\hat{B} = \\frac{1}{n} \\sum_{g=1}^{G} \\left( \\sum_{i \\in g} x_i e_i \\right) \\left( \\sum_{i \\in g} x_i e_i \\right)^T = \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g.\n\\]\n이에 따라 분산의 Sandwich estimator를 구하면\n\\[\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n\\]\n\\[\n= (X^T X)^{-1} \\left( \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) (X^T X)^{-1}.\n\\]\n이고, 이는 Cluster-robust SE의 식과 동일합니다.\n1.5. GLM 복습\n\nGeneralized Linear Model (GLM)의 모델 식은 다음과 같이 표현됩니다: \\[\ng(\\mathbb{E}[Y_i | X_i]) = g(\\mu_i) =\\eta_i = X_i^T \\beta \\\\\nwhere, Y_i \\sim \\text{Exponential Family}(\\mu_i, \\phi).\n\\] 이때 \\(g(\\cdot)\\)은 링크 함수(link function)로 logit, log의 예시를 보았고, \\(\\mu_i = \\mathbb{E}[Y_i | X_i]\\)는 반응 변수의 기대값으로 모델의 mapping의 목적이 되는 값(예측하고자 하는 값), \\(\\phi\\) 분산과 관련된 parameter(dispersion parameter) 로 정규 분포의 경우 \\(\\sigma^2\\)였습니다.\n간략하게 복습하면 링크 함수(link function) \\(g(\\cdot)\\)를 통해 \\(E(Y_i) = \\mu_i\\)와 \\(\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)를 연결하고, 분산 함수(variance function) \\(V(\\mu_i)\\)를 이용해 \\(\\operatorname{Var}(Y_i)\\)를 표현하며, 추정방정식(estimating equation)을 세워\n\\[\n\\sum_{i=1}^n \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)} = \\mathbf{0}\n\\]\n을 푸는 방식으로 \\(\\hat{\\boldsymbol{\\beta}}\\)를 구합니다.\n이 해석 또한 M-estimation의 한 사례로 볼 수 있습니다. GLM에서 score 함수(추정방정식)는 \\(\\psi_i(\\boldsymbol{\\beta}) = \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)}\\) 꼴로 정의되며, 이를 0으로 만드는 \\(\\hat{\\boldsymbol{\\beta}}\\)가 우리가 구하고자 하는 파라미터 추정치가 됩니다. 2장에서는 GLM의 parameter \\(\\hat{\\boldsymbol{\\beta}}\\)를 추정하는 방법으로 IRLS(Iteratively Reweighted Least Squares)이나 Newton-Raphson/Fisher Scoring을 소개하였으며, 이는 결국 M-estimation에서 구체적으로 어떻게 “estimating equation을 수치적으로 풀어낼지” 알고리즘으로 구현한 예시 중에 하나였다고 이해할 수 있습니다. 또한, 2장에서 예고한대로, 왜 parameter \\(\\hat{\\boldsymbol{\\beta}}\\)의 분산이\n\\[\nVar(\\hat{\\boldsymbol{\\beta}}) = -fisher\n\\] 라고 했었는지 이제 살펴보면, GLM의 추정 또한 M-estimation에 해당하므로 GLM의 estimating equation을 만족하는 estimator에 대해서 위에서 확인한 점근정규성이 만족함을 알 수 있고, 때문에 consistent한 parameter estimator에 대해서 다음과 같은 robust한 Sandwich 분산을 갖습니다.\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1} \\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1}\n\\]\n그리고, 계속 보아왔던 것처럼 여기서 \\(\\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) = \\mathcal{I}(\\boldsymbol{\\beta}_0)\\)가 만족(스코어 함수의 분산의 기댓값과 Fisher information matrix가 같습니다.) 하기 때문에 이를 대입하면 위 식이 아래 처럼 소거되고,\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\mathcal{I}(\\boldsymbol{\\beta}_0)^{-1}\n\\]\n가 됩니다. 결국 GLM의 모형 기반 분산은 다음과 같습니다:\n\\[ \\mathbb{V}ar_{\\text{모형}}(\\hat{\\beta}) = \\mathbf{A}^{-1} = \\left( \\sum_{i=1}^{n} \\frac{\\partial^2 \\log f(Y_i; \\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}. \\]\n또한, 이때 경험적 분포를 고려하여 Sandwich로 추정한 분산은,\n\\[ \\mathbb{V}ar_{\\text{robust}}(\\hat{\\beta}) = \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{A}^{-1}, \\]\n\\[ where, \\mathbf{B} = \\sum_{i=1}^{n} \\psi_i(\\hat{\\beta}) \\psi_i(\\hat{\\beta})^T, \\; \\psi_i(\\beta) = (Y_i - \\mu_i) x_i / V(\\mu_i). \\]\n로, 이는 이전에 확인한 HC0의 형태와도 이어집니다.\n다시 돌아와서.. (for clustered data)\n일반적으로 선형 모델(Linear Model)과 일반화 선형 모델(Generalized Linear Model, GLM)은 독립 동일 분포(i.i.d.)를 가정합니다. 즉, 기존의 GLM은 관측치(observations, data points)들이 서로 독립이며(Independent)일 때 동일한 분산 구조에서 잘 작동합니다. 그러나 학교나 병원 등 군집(클러스터) 단위로 샘플이 묶여 있는, 비슷한 특성을 지닌 대상들을 클러스터(cluster)로 묶은 패널 데이터(panel data)나 동일한 실험 대상(피험자)에게서 반복 측정된 데이터(longitudinal data)의 경우, 같은 cluster(또는 group: 같은 피험자, 같은 단위 등)에 속한 data간에는 correlation이 존재합니다. 때문에 더이상 data들이 독립이 아니게 되며, GLM만으로는 이 상관구조를 모델 자체에서 고려할 수 없기에, GEE와 GLMM 와 같은, 더욱 general한 Regression Model이 개발되었습니다. 이제 아래에서 위 두 model에 대해서 살펴보겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#generalized-estimating-equation-gee",
    "href": "posts/2025-02-28-reg3/index.html#generalized-estimating-equation-gee",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "2. Generalized Estimating Equation (GEE)",
    "text": "2. Generalized Estimating Equation (GEE)\n2.1. GEE 정의\n\nGEE (Generalized Estimating Equation)는 GLM이 독립성 가정을 전제로 하는 한계마저 뛰어넘어, 군집(Clustered) 자료나 반복측정 자료 등 상관구조가 존재하는 데이터에 적용될 수 있도록 확장한 방법론입니다. 가장 critical하게 다른 점을 보면, GLM은 \\(\\operatorname{Var}(Y_i) = \\phi V(\\mu_i)\\)로 종속변수의 분산을 표현할 때 diagonal matrix로 두어 data points 간에는 correlation이 없음을 표현하였다면, GEE는 \\(\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}\\)와 같은 working correlation 행렬 \\(\\mathbf{R}_i(\\alpha)\\)를 사용하여, 관측치들 간의 상관관계를(반복 측정, 클러스터 내 상관) 모델에 반영합니다. 또한, 이러한 가정을 위해 종속변수의 확률 모델(공동 확률 분포)을 완전히 명시하지 않아도, Quasi-Likelihood(준 우도) 접근법을 통해 점근적(score) 방정식을 확장하여 모델을 적합합니다. 간단히 말하면, GEE는 “평균 모형은 GLM처럼 유지하되, 상관 구조를 적절히 지정하여 군집성이나 반복측정을 고려하자”라는 접근입니다.\nLM이나 GLM은 서로 독립적인(i.i.d.) 표본을 가정하여 이를 기반으로 추정하는 반면, GEE에서는 상관 구조(correlation structure) \\(R(\\alpha)\\)를 추가하여 이러한 독립 가정을 완화하고, 평균 모형과 분산-공분산 구조에 대한 가정을 분리해서 Quasi 형태로 추정합니다. 이 때, Quasi-likelihood에 대한 적용을 짧게 설명하자면, GEE는 확률 모델을 직접 설정(완전한 공동 확률 밀도 함수 명시)하지 않고, GLM의 log likelihood function에 상관구조를 추가하는 형태로 접근합니다. 즉 GLM에서 종속 변수의 Exponential family 가정 -&gt; 독립 가정 후 모든 data point의 확률(likelihood)를 곱해서 얻은 likelihood finction -&gt; 로그 씌워서 log likelihood -&gt; model parameter로 미분한 결과인 score function 순으로 추정 과정을 설명했었다면, GEE는 처음부터 직접적인 종속변수의 가정으로 시작하는 대신 log likelihood에서 시작하고, 이 때 독립이 아님을 고려하기 위해 variance function에 상관 행렬 \\(\\mathbf{R}_i(\\alpha)\\)을 추가하여 \\(\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}\\)로 둔 후 추정하는 것입니다. 이러한 접근은, 실제로 종속변수의 완전한 확률적 기반(joint PDF)이 존재하지 않아도, 점근적 성질을 활용하여 일관된 추정량을 얻을 수 있고, 오류 항이 독립적이지 않은 경우에도 GLM과 유사한 방식으로 추정할 수 있는 장점이 있습니다. 마지막으로, GEE는 상관행렬과 Quasi의 개념을 통해 GLM과 같이 data points들을 marginal하게 고려하여 fit하기 때문에 Population-Average GEE(or 모델) 이고, 이는 무작위 효과(Random Effect)를 통해 각 실험 단위(피험자)에 특화된 효과를 추정하는 GLMM(Generalized Linear Mixed Model)과 철학이 다르며,이 GLMM은 Subject-Specific GEE(or 모델)이라고도 부릅니다.\n2.2. GEE 수학적 표현 및 추정\n\n위에서 언급하였듯, GLM과 동일하게 GEE는 아래와 같은 marginal 모델입니다:\\[\ng\\bigl(\\boldsymbol{\\mu}_i\\bigr) = \\mathbf{X}_i \\boldsymbol{\\beta},\n\\] 여기서 \\(\\mathbf{Y}_i\\)는 (i)번째 클러스터(또는 피험자)에서 나온 \\(n_i\\)개의 관측치 벡터, \\(\\boldsymbol{\\mu}_i = E(\\mathbf{Y}_i) \\; or \\; E(\\mathbf{Y}_i|X_i)\\)입니다. 또한, 언급한 대로 Working correlation을 아래와 같이 설정하며,\n\\[\n\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}.\n\\] 이때 \\(\\mathbf{A}_i\\)가 기존 \\(V(\\mu_{ij})\\)의 역할 이었다면 이에 루트를 씌워 A라고 두고 (행렬에서의 square root, 또는 1/2 승은 기존 \\(V\\)가 diagonal 이었으므로 이때는 단순히 각 대각 성분을 루트 씌운 값입니다.) 그 사이에 클러스터 당 상관관계를 \\(\\mathbf{R}_i(\\alpha)\\)로 표현합니다. 이때 상관행렬 \\(\\mathbf{R}(\\alpha)\\)의 종류로는 크게 아래와 같은 예시들이 있습니다.\n(1) Independent (기존 GLM)\n\\[\nR(\\alpha) = I, \\quad V_k = V_k'\n\\]\n(2) Exchangeable Correlation (동일 상관 구조)\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha \\\\\n\\alpha & \\ddots & \\alpha \\\\\n\\alpha & \\alpha & 1\n\\end{pmatrix}\n\\]\n(3) Autoregressive (AR-1)\n\\[\n\\text{Corr}(y_{ki}, y_{kj}) = \\alpha^{|i-j|}\n\\]\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha^2 & \\dots & \\alpha^{n_k} \\\\\n\\alpha & 1 & \\alpha & \\dots & \\alpha^{n_k-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha^{n_k} & \\alpha^{n_k-1} & \\alpha^{n_k-2} & \\dots & 1\n\\end{pmatrix}\n\\]\n(4) Unstructured Form\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha_1 & \\alpha_2 & \\alpha_3 \\\\\n\\alpha_1 & 1 & \\alpha_4 & \\alpha_5 \\\\\n\\alpha_2 & \\alpha_4 & 1 & \\alpha_6 \\\\\n\\alpha_3 & \\alpha_5 & \\alpha_6 & 1\n\\end{pmatrix}\n\\]\n이러한 상관행렬 \\(\\mathbf{R}(\\alpha)\\)는 사전에 정의되어야 하므로 분석 대상인 data의 성질에 따라 선정해야 하며, 이러한 관계의 구조를 어떻게 선택하는 지에 따라 분산이 다르게 나오므로, 위에서 다룬 Sandwich를 통한 robust한 추정이 GEE에서 대게 사용됩니다.\nGEE’s Estimating Equation\n이전에 GLM에서는 다음과 같이 score functions로 부터 estimating equation을 세웠습니다:\n\\[\n\\Psi = \\sum \\psi_i = \\sum_{i=1}^{N} \\frac{y_i - \\mu_i}{a(\\phi)V(\\mu_i)} \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i = 0\n\\]\n이제 이를 GLM 때와 다르게 각 cluster \\(k\\)에 대해 \\(D_k = diag(\\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i)\\), \\(V_k = \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}\\)라고 하면,\n\\[\n\\sum_{k=1}^{K} \\frac{1}{a(\\phi)}  D_k V_k^{-1} (y_k - \\mu_k) = 0\n\\] 으로 식을 GLM의 score function 으로부터 변형해서 얻을 수 있고, 최종적으로 벡터와 행렬 연산으로 모든 클러스터 \\(k\\)에 대해 block diagonal로 한 번에 표현하면(\\(V\\)), \\[\n\\Psi = \\frac{1}{a(\\phi)} D V^{-1} (y - \\mu) = 0\n\\] 가 되며, \\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr) = \\mathbf{0},\n\\] 로 표현할 수 있습니다. 이는 GLM의 score 함수와 같은 시작이지만, GEE는 \\(\\mathbf{V}_i\\)가 군집/반복측정 상관을 반영하도록 조작합니다. 결국 이 추정방정식을 품으로써 GEE의 추정이 가능할 것입니다.\n가장 중요한 GEE에서 분산 term의 변형을 다시 한 번 강조하자면, \\(V_k\\) 는 cluster 별 (Co)variance 행렬로, data가 independent하다면 \\(V_k\\)와 이를 모두 합친 \\(V\\)가 diagonal matrix가 되지만, 그룹 내 상관을 고려할 경우 \\(V_k\\)가 더 이상 diagonal하지 않고, 이에 따라 \\(V\\)는 block diagonal matrix 형태를 갖습니다. (block diagonal한 이유는 cluster끼리 독립이고 cluster안은 상관관계가 있는 1차 clustered data에서 다룬 2장의 cluster-robust를 떠올리면 좋을 것 같습니다.)\n\\[\nV = \\begin{pmatrix}\nV_1 & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & V_K\n\\end{pmatrix}\n\\]\nGEE parameter 추정(IRLS)\nGEE의 parameter 추정 또한 GLM에서 비롯된 만큼, 이전에 다루었던 방식과 유사한 반복 알고리즘으로 \\(\\hat{\\boldsymbol{\\beta}}\\)를 추정할 수 있습니다. 하나의 스텝을 예시로 들어보면,\n\n현재 추정치 \\(\\hat{\\boldsymbol{\\beta}}^{(t)}\\)에서, 각 클러스터 \\(i\\)에 대해 \\(\\mathbf{D}_i\\) (편미분 행렬), \\(\\boldsymbol{\\mu}_i = \\mu(\\hat{\\boldsymbol{\\beta}}^{(t)})\\), \\(\\mathbf{V}_i\\) (working correlation \\(\\mathbf{R}_i(\\alpha)\\), dispersion parameter \\(\\phi\\) 포함)을 계산합니다. 이때, working correlation \\(\\mathbf{R}_i(\\alpha)\\)와 \\(\\phi\\)도 반복적으로 업데이트됩니다. 예컨대, gee나 geepack 패키지에서는 각 반복 단계에서 잔차(residual)를 기반으로 \\(\\alpha\\)와 \\(\\phi\\)를 재추정하여 새로운 \\(\\mathbf{V}_i\\)를 구합니다.\n\n아래 식을 만족하도록 \\(\\hat{\\boldsymbol{\\beta}}^{(t+1)}\\)를 업데이트합니다:\n\\[\n\\hat{\\boldsymbol{\\beta}}^{(t+1)}\n= \\hat{\\boldsymbol{\\beta}}^{(t)}\n- \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1}\n\\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr).\n\\]\n\n이전처럼 parameter의 변화량(distance between \\(\\hat{\\boldsymbol{\\beta}}^{(t+1)}\\)and \\(\\hat{\\boldsymbol{\\beta}}^{(t)}\\))가 특정 threshold 아래로 수렴할 때까지 이 과정을 반복합니다.\n\nR에서는 geepack이나 gee 라이브러리에서 내부적으로 이러한 절차를 수행합니다. 1에서 어떻게 잔차로부터 \\(\\alpha\\)를 추정할 수 있는지 간단하게 예시를 보면 아래와 같습니다. 이는 이전과 원리는 같으며, 상관행렬에서 추정해야 하는 parameter 개수에 따른 자유도를 고려하기 때문에 식이 좀더 복잡해진 것입니다.\n잔차는 아래와 같이 계산됩니다(Pearson): \\[\n\\hat{r}_{ki} = \\frac{y_{ki} - \\hat{\\mu}_{ki}}{\\sqrt{V(\\hat{\\mu}_{ki})}},\n\\] where \\(y_{ki}\\)는 observed response for cluster \\(k\\) and observation \\(i\\), \\(\\hat{\\mu}_{ki}\\)는 predicted mean for cluster \\(k\\) and observation \\(i\\), \\(V(\\hat{\\mu}_{ki})\\)는 variance function evaluated at \\(\\hat{\\mu}_{ki}\\). 이제 \\(n_k = n\\)라고 가정하면 다음과 같습니다. (이는 클러스터 당 data point 개수가 같다는 가정이며, 이를 만족하지 않아도 식이 복잡해질 뿐 똑같이 계산됩니다.)\n(2) Exchangeable Correlation: \\[\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i &gt; j} \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K \\cdot \\frac{1}{2}n(n-1) - p},\n\\]\n(3) Autoregressive (AR-1): \\[\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i=1}^{n_k - 1} \\frac{\\hat{r}_{ki} \\hat{r}_{k(i+1)}}{K(n-1) - p},\n\\]\n(4) Unstructured Form: \\[\n\\hat{a}_{ij} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K - p},\n\\]\n위 식들은 그저 잔차로부터 (co)variance를 추정하는 것일 뿐이고, 분산 term은 degree of freedom을 고려하기 때문에 그저 각각의 상관 행렬 속 미지수(parameter)의 개수에 따른 반영입니다.\n2.3. GEE parameter’s Variance\n\nGEE의 모수 추정치 (\\(\\hat{\\boldsymbol\\beta}\\))도 M-estimation의 범주에 속하므로, 점근 분산은 Sandwich 형태를 갖습니다.\n\\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol\\beta})_{\\text{robust}}\n= \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1}\n\\mathbf{D}_i \\right)^{-1} \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} (\\mathbf{Y}_i -\n\\boldsymbol\\mu_i) (\\mathbf{Y}_i - \\boldsymbol\\mu_i)^\\top\n\\mathbf{V}_i^{-1} \\mathbf{D}_i \\right) \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1}.\n\\]\n이를 robust 또는 empirical 표준오차라고 하며, 실질적으로 상관구조 (\\(\\mathbf{R}_i(\\alpha)\\))가 부정확하게 지정되었을지라도 일관성을 보장해 줍니다. 즉, 위 M-estimation으로 GLM을 해석할 때와 일치하게, Model-based SE 는 \\(\\mathbf{A}^{-1}\\)만을 사용해서 계산하는 것이고, 이는 설정한 working correlation(상관행렬) 가정이 정확하다고 믿을 때이기 때문에, 이를 신뢰할 수 없는 경우 거의 무조건 Robust SE\\(\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{A}^{-1}\\) 를 사용합니다. 이는 R에서 GEE를 계산할 때 기본 SE(model-based)와 robust SE(empirical) 두 가지가 함께 리포팅되는 이유이기도 합니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#generalized-linear-mixed-model-glmm",
    "href": "posts/2025-02-28-reg3/index.html#generalized-linear-mixed-model-glmm",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "3. Generalized Linear Mixed Model (GLMM)",
    "text": "3. Generalized Linear Mixed Model (GLMM)\n3.1. GLMM 정의\n\nGLMM(Generalized Linear Mixed Model)은, 우리가 이미 익숙하게 다뤄온 GLM(Generalized Linear Model)을 GEE와는 다른 방식으로 (Mixed model) “군집(cluster) 또는 계층적 구조를 가지는 자료”에까지 확장하기 위한 방법론입니다. 즉, GLMM은 이러한 내재된 상관(또는 군집성)을 모델화하기 위해서 고정 효과 + 무작위 효과의 결합으로 모형을 설정합니다. 즉, GLMM은\n\n\n고정 효과(fixed effects): 전체 모집단에 공통적으로 적용되는 회귀계수(예: 전체 평균 경향)에 해당,\n\n무작위 효과(random effects): 피험자(또는 군집, 클러스터)별로 달라지는 편차(“개인별 random intercept” 혹은 “개인별 random slope” 등)를 도입\n\n을 둘다 고려하는 모델이며, 즉 “Generalized Linear Model + Linear Mixed Model(Random Effects)”의 결합이라고 요약할 수 있습니다. GEE와 비교하여 이 GLMM은 각 cluster(또는 group)마다 직접적인 고려를 모델에 넣기 때문에(random effect) Subject-Specific 모델(또는 GEE)라고도 불리며, 이는 Population-Average GEE와 대비되는 특징입니다. 무작위 효과는 정규분포로 가정하는 것이 일반적이며, 경우에 따라서는 다른 분포(예: Gamma)로 가정하기도 하고, GLMM에서은 이러한 LMM을 GLM으로 확정한 것이기 때문에 종속변수의 분포를, Exponential Family로 확장합니다.\n3.2. LMM 수학적 표현 및 추정\n\nGLMM을 이해하기 위해서는 먼저 선형혼합모형(LMM; Linear Mixed Model)을 확실하게 이해할 필요가 있습니다. (이 LMM과 GLMM을 완벽하게 이전처럼 분석하려면 내용이 산만해지기 때문에 여기선 중요한 점을 위주로 짚고, 추가적인 공부가 필요하신 분들은 위키피디아에서 비롯되는 교재 및 논문 내용들을 집중적으로 살펴보시면 좋을 것 같습니다.) LMM은 종속변수 \\(Y\\)의 분포가 정규분포라는 전제하에서, 고정 효과(fixed effects)와 무작위 효과(random effects)가 동시에 존재한다고 보는 모형입니다.\nLMM 수학적 표현\n\n가장 단순한 형태의 LMM(임의절편 모형, random intercept model)을 생각해 보겠습니다. (이때 LMM에서 모형을 나누는 기준은 random effect, 즉 group을 어느 정도로 복잡하게 고려하는 지에 따른 설계의 차이입니다. random effect의 분포, 차원 등을 다양하게 고려할 수 있겠지요.) 예를 들어, \\(i\\)번째 클러스터(또는 피험자) 내에서 \\(j\\)번째 관측값을 나타내는 \\(Y_{ij}\\)를 다음과 같이 모델링합니다:\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + b_i + \\varepsilon_{ij},\n\\quad i=1,\\dots,K,\\quad j=1,\\dots,n_i.\n\\]\n이때 \\(\\beta_0, \\beta_1\\)은 고정 효과(fixed effects) parameter, 즉 모든 클러스터에 공통 적용되는 평균적인 효과)이고, \\(b_i\\)는 무작위 효과(random effect) parameter로, 클러스터 \\(i\\)마다 서로 다른 절편(intercept) 편차를 갖는 것을 모델링하고 있습니다. \\(\\varepsilon_{ij}\\)는 흔히 오차항(residual)으로 간주하고, 대게 \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\)로 가정합니다.\n추가적으로, 무작위 효과 \\(b_i\\)는 다음과 같은 분포로 가정합니다:\n\\[\nb_i \\sim N(0,\\;\\tau^2).\n\\]\n이는 “(피험자마다) 임의로 달라지는 절편(intercept)”이 정규분포를 따른다는 것을 의미합니다. 모든 \\(b_i\\)를 독립으로 가정하면,\n\\[\n\\operatorname{Var}(b_i) = \\tau^2,\\quad \\operatorname{Var}(\\varepsilon_{ij}) = \\sigma^2.\n\\]\n결국, 어떤 \\(Y_{ij}\\)에 대해서는\n\\[\nY_{ij} = \\beta_0 + b_i + \\beta_1 X_{ij} + \\varepsilon_{ij},\n\\]\n이고,\n\\[\n\\operatorname{Var}(Y_{ij}) = \\tau^2 + \\sigma^2.\n\\]\n이먀, 더 일반화 된 모델로 무작위 절편 + 무작위 기울기(random intercept + random slope)를 도입하여 독립변수 \\(X\\)에 대해서도 개인별로 기울기가 달라지도록 만들 수 있습니다. 이 경우,\n\\[\nY_{ij}\n= (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) X_{ij} + \\varepsilon_{ij},\n\\quad\nb_{0i} \\sim N(0,\\;\\tau_{00}),\\;\nb_{1i} \\sim N(0,\\;\\tau_{11}),\\;\n\\operatorname{Cov}(b_{0i}, b_{1i}) = \\tau_{01}.\n\\]\n가 될 것입니다. 이처럼 무작위 효과를 하나 혹은 여러 개 갖는다는 것은, “클러스터마다 고유하게 발생하는 변동”을 모델에 포함하는 방식으로, LMM은 이러한 방식로 상관구조를 모델링 해낸다고 생각할 수 있습니다. 이를 벡터와 행렬 형태로 표현해보면, 각 클러스터(또는 피험자) \\(i\\)에 대해\n\\[\n\\mathbf{Y}_i\n= \\mathbf{X}_i\\,\\boldsymbol{\\beta}\n\\;+\\; \\mathbf{Z}_i\\,\\mathbf{b}_i\n\\;+\\; \\boldsymbol{\\varepsilon}_i,\n\\]\n\n\n\\(\\mathbf{Y}_i\\): \\(i\\)번째 클러스터에서의 \\(n_i\\)차원 응답벡터\n\n\\(\\mathbf{X}_i\\): \\(n_i \\times p\\) 차원의 고정 효과 설계 행렬(fixed effect parameter \\(\\boldsymbol{\\beta}\\)와 매칭)\n\n\\(\\mathbf{Z}_i\\): \\(n_i \\times q\\) 차원의 무작위 효과 설계 행렬(random effect parameter \\(\\mathbf{b}_i\\)와 매칭)\n\n\\(\\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G})\\) 이며 \\(\\boldsymbol{G}\\)는 \\(q \\times q\\) 공분산 행렬\n\n\\(\\boldsymbol{\\varepsilon}_i \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I}_{n_i})\\)로 일반적으로 가정(독립 동일 분포)\n\n여기서 설계 행렬이란, 1장의 LM에서부터 사용하였지만, data point(observation)당 미리 input으로 지정되는 행렬로, 정확한 의미는 “일련의 개체에 대한 설명 변수 값을 나열한 행렬로 각 행은 개별 개체를 나타내며, 열은 해당 개체에 대한 변수 및 특정 값에 해당한다”입니다. X는 계속 봐왔지만 Z는 이번에 처음 나온 설계 행렬인데, 이는 각 data point마다 해당되는 cluster에는 1, 해당되지 않는 나머지 cluster는 0의 값을 갖는, cluster를 선택하는 스위치 느낌으로, input으로 정해지는 행렬이라고 생각하시면 됩니다.\n이 LMM의 (Co) variance matrix는 단순하게 분산 term을 씌우면 random한 (determinant하지 않은) 항만 남아 다음과 같이 계산 될 것입니다:\n\\[\n\\operatorname{Var}(\\mathbf{Y}_i)\n= \\mathbf{Z}_i\\,\\boldsymbol{G}\\,\\mathbf{Z}_i^\\mathsf{T}\n+ \\sigma^2\\,\\mathbf{I}_{n_i}.\n\\]\nLMM’s parameter 추정(Maximum Likelihood, REML)\n\n이 LMM에서 \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{G}\\) (또는 \\(\\tau^2\\) 등), \\(\\sigma^2\\) 모두 미지수입니다. 이를 추정하기 위해 주로 최대우도추정법(ML; Maximum Likelihood) 또는 제한최대우도추정법(REML; Restricted Maximum Likelihood)을 사용합니다. 각각이 어떻게 계산될지 설명드리면 다음과 같습니다.\n(1) With ML(MLE).\\(b_i\\)가 정규분포라는 가정 하에, \\(\\mathbf{Y}_i\\)의 joint distribution도 다변량 정규분포로 표현할 수 있습니다. 모든 \\(i\\)에 대해 \\(b_i\\) 또는 \\(\\mathbf{Y}_i\\)가 독립이라 가정하면(cluster간은 독립), 전체 자료의 joint density를 곱해서 likelihood 함수를 정의할 수 있고, 이를 최대화하는 \\(\\hat{\\boldsymbol{\\beta}}\\)와 \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\sigma}^2\\)를 찾으면 됩니다. 단점으로는, ML은 \\(\\boldsymbol{\\beta}\\) 추정에서 편향(bias)이 발생할 수 있어, 표본 크기가 작거나 모형 구조가 복잡해질 때 문제될 수 있습니다. 수식을 중요 부분만 전개해보면, LMM에서 모든 \\(b_i\\)를 각각 적분하여(즉 클러스터 마다 적분) 얻은 \\(\\mathbf{Y}_i\\)의 분포는\n\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, V_i) \\quad \\text{with} \\quad V_i = \\mathbf{Z}_i \\mathbf{G} \\mathbf{Z}_i^T + \\sigma^2 I_{n_i}.\n\\]\n입니다. 이제 \\(i\\)번째 클러스터 자료 \\(\\mathbf{Y}_i\\)에 대한 log-likelihood를 계산해보면, 다차원 정규분포이므로 다음과 같이 나옵니다:\n\\[\n\\ell_i(\\beta, \\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ n_i \\log(2\\pi) + \\log |V_i| + (\\mathbf{Y}_i - \\mathbf{X}_i \\beta)^T V_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\beta) \\right].\n\\]\n이를 전체 \\(K\\)개의 cluster에 대해 모두 합하면, 전체 자료에 대한 log-likelihood 함수 \\(\\ell(\\beta, \\mathbf{G}, \\sigma^2)\\)가 도출될 것입니다.\n\\[\n\\ell(\\beta, \\mathbf{G}, \\sigma^2) = \\sum_{i=1}^{K} \\ell_i(\\beta, \\mathbf{G}, \\sigma^2).\n\\]\nMLE(\\(\\hat{\\beta}_{ML}, \\hat{\\mathbf{G}}_{ML}, \\hat{\\sigma}^2_{ML}\\))를 구하기 위해서는 위의 log-likelihood를 \\(\\beta, \\mathbf{G}, \\sigma^2\\)에 대해 미분하여 0이 되게 하는 해를 찾으면 됩니다. 그러나 일반적으로 \\(\\mathbf{G}\\)와 \\(\\sigma^2\\)에 대한 미분은 해석적으로 단순화하기 어렵고, 또한 \\(\\mathbf{G}\\)가 양의 정부호(positive definite)가 되어야 하는 제약이 있으므로, 수치적 최적화(EM 알고리즘, Newton-Raphson, Fisher scoring 등)를 사용해야 합니다.\n(2) With REML.\n이는 ML를 직접 바로 계산하는 대신, 고정 효과 \\(\\boldsymbol{\\beta}\\)와 관련이 없는 term을 이용해 \\(\\boldsymbol{G}\\)와 \\(\\sigma^2\\)를 먼저 추정한 후 모델을 추정하는 방식입니다. 일반적으로 LMM 을 추정할 때는 REML이 고정 효과 추정에 대한 편의를 줄여주고, 분산 요소에 대한 추정이 좀 더 안정적이기 때문에 ML보다 선호됩니다. 이는 모델에서 무작위 효과를 적분(marginal likelihood)하는 접근을 통해 \\(\\boldsymbol{\\beta}, \\boldsymbol{G}, \\sigma^2\\)에 대한 우도 함수를 세우고(restricted likeli hood), 이 함수를 최대화하는 방식으로 진행됩니다. 실제 계산은 Iterative 알고리즘(EM 알고리즘, 또는 Fisher scoring, Newton-Raphson 등)을 사용합니다. 식을 보면,\n\\[\n\\ell_{REML} (\\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ \\sum_{i=1}^{K} \\log |V_i| + \\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}| + (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta})^T \\mathbf{V}^{-1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}) \\right] + \\text{const}.\n\\]\n이고, 이때 \\(\\mathbf{V} = \\text{blockdiag}(V_1, \\dots, V_K)\\), \\(\\hat{\\beta}\\)는 \\(\\mathbf{G}, \\sigma^2\\)가 주어졌을 때의 일반화최소제곱(GLS) 해입니다. 여기서 \\(\\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}|\\)가 REML에서 추가로 나타나는 항으로, 이것이 \\(\\beta\\)를 제거(또는 \\(\\beta\\)에 무관한 부분만 모아놓음)하여 우선적으로 식을 전개한 효과라고 이해할 수 있습니다. REML에서는 이 \\(\\ell_{REML} (\\mathbf{G}, \\sigma^2)\\)를 \\(\\mathbf{G}, \\sigma^2\\)에 대해 최대화한 뒤, 그 해 \\(\\hat{\\mathbf{G}}_{REML}, \\hat{\\sigma}^2_{REML}\\)를 이용해 최종적으로 \\(\\hat{\\beta}_{REML}\\) 을 구합니다. REML은 ML보다 fixed effect estimator의 편향 문제가 덜하며, 분산 성분 \\(\\mathbf{G}, \\sigma^2\\)에 대해 좀 더 안정적인 추정을 제공합니다. 특히 샘플이 작거나 무작위효과 구조가 복잡할 때 일반적으로 더욱 안정적인 REML을 권장하는 편입니다.\n이를 통해 얻은 \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\sigma}^2\\)는 LMM의 MLE(or REML) 추정치이며, R에서는 lme4 패키지 등에서 이 과정을 내부적으로 수행합니다.\n3.3. GLMM의 수학적 표현 및 추정\n\n이제 LMM에서 정규 오차항을 일반화하여, 종속변수가 이항, 포아송, 혹은 다른 지수분포족을 따를 수 있도록 확장하면, GLMM으로 이어집니다. GLMM은\n\\[\ng\\bigl(\\mu_{ij}\\bigr)\n= \\mathbf{x}_{ij}^\\mathsf{T}\\,\\boldsymbol{\\beta}\n+ \\mathbf{z}_{ij}^\\mathsf{T}\\,\\mathbf{b}_i\n\\]\n\\[\nwhere, \\; \\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G}),\n\\]\n\\[\nY_{ij} \\mid \\mathbf{b}_i \\sim \\text{Exponential Family}(\\mu_{ij}, \\phi),\n\\]\n의 구조입니다. 직관적으로도 GLMM은 LMM+GLM임을 볼 수 있고, 당연히 \\(g(\\cdot)\\)는 link function으로, GLM과 마찬가지로 \\(\\mu_{ij} = E(Y_{ij} \\mid \\mathbf{b}_i)\\)를 적절한 링크 함수 \\(g\\)로 mapping하는 함수이며, 예시로 binomial case에서 로짓 링크(logit)를 사용하면 \\(Y_{ij}\\)는 0 또는 1 값을 가지는 이항분포가 될 수 있고, 이는 \\(\\log\\bigl(\\mu_{ij}/(1-\\mu_{ij})\\bigr)\\)를 회귀식을 표현하는 것이었습니다.\n이때, 위 식의 likelihood는\n\\[\n\\mathbf{Y}_i \\mid \\mathbf{b}_i\n\\sim \\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr),\n\\]\n로 쓸 수 있으며, \\(\\mathbf{b}_i\\)를 적분(marginalizing over \\(\\mathbf{b}_i\\))하면,\n\\[\np(\\mathbf{Y}_i)\n= \\int\n\\prod_{j=1}^{n_i}\nf\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr)\\,\n\\varphi\\bigl(\\mathbf{b}_i\\bigr)\\,\nd\\mathbf{b}_i,\n\\]\n가 최종적으로 cluster \\(i\\)에 대한 (marginal) 분포를 만들어냅니다.\n\\[\np(\\mathbf{Y}_i) = \\int \\left[ \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\right] \\varphi (b_i) db_i, \\quad \\varphi (b_i) = \\frac{1}{\\sqrt{(2\\pi)^q |\\mathbf{G}|}} \\exp \\left(-\\frac{1}{2} b_i^T \\mathbf{G}^{-1} b_i \\right).\n\\]\n모든 \\(\\mathbf{Y}_i\\)가 (조건부) 독립이라면, 전체 자료에 대한 marginal likelihood는\n\\[\nL(\\beta, \\mathbf{G}, \\phi) = \\prod_{i=1}^{K} \\int \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\varphi (b_i) db_i.\n\\]\n입니다. 문제는 \\(\\mu_{ij} (b_i)\\)가 비선형이기 때문에 적분이 closed-form으로 풀리지 않는 경우가 대부분이라는 것이고, 따라서 실제로는 이 적분을 수치적(또는 근사적)으로 계산한 뒤, 그 결과(근사치)를 최대화하여 \\(\\hat{\\beta}, \\hat{\\mathbf{G}}, \\hat{\\phi}\\)를 구하게 됩니다.\nGLMM’s parameter 추정(Marginal Likelihood & Approximation)\n\n다시 한 번 언급하자면 문제는 \\(\\mathbf{b}_i\\)가 랜덤효과이므로 이를 적분해야 한다는 것인데, \\(\\mu_{ij}(\\mathbf{b}_i)\\)가 비선형이기 때문에 이 적분이 closed-form으로 표현되지 않는 것이고, 다음과 같은 근사화 기법을 사용하여 계산합니다.\n\nLaplace Approximation\\(\\mathbf{b}_i\\) 주변에서 2차 근사를 수행하여 적분을 근사화하는 방법입니다. 한 번(1차) 또는 고차(AGQ, Adaptive Gauss-Hermite Quadrature) 버전으로 더 정확하게 시도할 수 있습니다.\nGauss-Hermite Quadrature\n적분을 수치적(Numerical)으로 가까운 근사값으로 계산합니다. 무작위 효과 차원이 높아질수록 계산량이 기하급수적으로 늘어날 수 있으므로, 실무에서는 차원이 작은 랜덤 효과 구조(예: 랜덤 인터셉트만)에서 자주 사용합니다.\nPenalized Quasi-Likelihood (PQL)\n고전적으로 제안된 근사 기법으로, GLM의 IRLS 절차를 변형하여 무작위효과를 순차적으로 추정합니다. 데이터가 크거나, 근사 정밀도가 크게 중요하지 않은 상황에서 가볍게 쓰일 수 있습니다.\n\n최종적으로, (1) 적분으로 정의된 marginal likelihood를 (2) 수치적 근사화를 통해 (3) 최적화(예: Newton-Raphson, EM 등)하여, \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\phi}\\) 등을 찾습니다.\n\\[\n\\hat{\\boldsymbol{\\beta}}, \\;\\hat{\\boldsymbol{G}}, \\;\\hat{\\phi}\n= \\underset{\\boldsymbol{\\beta}, \\boldsymbol{G}, \\phi}{\\mathrm{argmax}}\n\\;\\;\\Bigl\\{\n\\prod_{i=1}^{K}\n\\int\n\\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i), \\phi\\bigr)\n\\, \\varphi(\\mathbf{b}_i)\\, d\\mathbf{b}_i\n\\Bigr\\}.\n\\]\nGLMM vs. GEE\n\n이 data간 상관관계를 고려하기 위해 개발된 두 모델을 짧게 정리해보면, GEE는 “Population-Average” 접근으로 군집 내 상관을 working correlation 방식으로 모델링하며, 완전한 joint PDF를 명시하지 않고 Quasi-likelihood처럼 추정하는 기법이었고, GLMM은 “Subject-Specific” 접근으로 군집/클러스터 효과를 무작위 효과로 모델링하여 종속변수를 (조건부) Exponential Family distribution으로 가정하고, 이 likelihood를 marginal하게 적분함으로써 추정합니다.\n3.4. GLMM parameter’s Variance\n\n마지막으로, GLMM에서의 추정된 파라미터(고정 효과 \\(\\boldsymbol{\\beta}\\), 무작위 효과 분산-공분산 행렬 \\(\\boldsymbol{G}\\) )의 분산 추정(표준 오차, 신뢰구간 등) 방법을 보겠습니다. GLMM의 경우, 근사화하여 최대화한 marginal log-likelihood에서의 헤시안 행렬(Hessian)을 기반으로 고정효과 \\(\\hat{\\boldsymbol{\\beta}}\\) 의 분산을 추정할 수 있습니다. 구체적으로, 아래와 같은 일반적 형식을 취합니다:\n\\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}})\n=\n\\bigl[\n  -\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}\n   \\,\\ell(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{G}}, \\hat{\\phi})\n\\bigr]^{-1},\n\\]\n여기서 \\(\\ell\\)은 GLMM의 (근사) marginal log-likelihood, \\(\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}\\)는 고정효과 파라미터 \\(\\boldsymbol{\\beta}\\)에 대한 2차 미분(Hessian)으로, 이 헤시안을 (적절한 수치 방법으로) 근사화하여 얻고, 그 역행렬이 분산 추정의 결과에 해당합니다. 이는 여전히 log likelihood을 통한 추정이기 때문에 Fisher information matrix로 분산을 추정한다고 생각하면 될 것 같습니다. GEE(2장에서)와 마찬가지로, GLMM에서도 모델의 설계에서의 작은 misspecification이 있을 가능성을 고려하여 안정적으로 Sandwich estimator를 통해 추정할 수 있는지 고민할 수 있습니다. GLMM의 경우, 군집 간 독립 이나 군집 내 random effect의 정규성 가정과 같은 가정이 크게 벗어나지 않는다고 믿으면 위 모델 기반(model-based) 추정 분산을 사용하면 되고, 그렇지 않은 “무작위 효과 분포가 정규가 아닐 가능성” 혹은 “link/variance function 형태가 부정확할 가능성” 등을 고려하기 위해 적절한 샌드위치 추정(sandwich-type variance) 기법을 시도할 수도 있습니다. 다만, GEE와 달리 GLMM에서의 robust variance estimation은 쉽게 구현되지 않으며, 근사기법, 부트스트랩(bootstrap) 등을 통해 대안적으로 접근하는 사례도 많습니다.\nrandom effect의 분산-공분산 행렬 \\(\\boldsymbol{G}\\) 또한 우도(또는 제한 우도)에서 편미분이 0 조건을 이용하여 추정하지만, 그 표준 오차(불확실성)를 추정하는 과정 역시 (1) 2차 미분, (2) 프로파일(profile) likelihood, (3) 수치적 근사화 등을 거쳐야 합니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#r-코드-예제-gee-glmm",
    "href": "posts/2025-02-28-reg3/index.html#r-코드-예제-gee-glmm",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "4. R 코드 예제: GEE, GLMM",
    "text": "4. R 코드 예제: GEE, GLMM\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요.\n\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환\n#\n## GEE 모델 적합 (Exchangeable 상관 구조)\n#library(geepack)\n#gee_fit &lt;- geeglm(binary ~ age + Sex,\n#                 id = Subject,          # 클러스터 변수\n#                 data = Orthodont,\n#                 family = binomial,\n#                 corstr = \"exchangeable\")  # 동일 상관 가정\n#summary(gee_fit)  # 결과 출력\n#\n## GLMM 모델 적합 (랜덤 절편 모델)\n#library(lme4)\n#glmm_fit &lt;- glmer(binary ~ age + Sex + (1|Subject),  # 랜덤 절편\n#                 data = Orthodont,\n#                 family = binomial)\n#summary(glmm_fit)  # 결과 출력"
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#마무리하며",
    "href": "posts/2025-02-28-reg3/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "마무리하며",
    "text": "마무리하며\n이번 장에서는 M-estimation 개념부터 시작하여, GLM이 어떻게 “estimating equation”의 한 사례로 해석되는지, GEE가 GLM을 확장하여 상관구조를 모델링하고, robust 분산을 제공함으로써 군집/반복측정 데이터를 다루는 과정을, GLMM이 임의효과를 통해 계층적 구조를 명시적으로 모델링하는 방식을 자세히 살펴보았습니다. 그리고 샌드위치 추정량(robust variance) 형태가 M-estimation의 일반 이론에서 비롯된다는 점도 수식과 함께 설명했습니다.\n정리하자면, M-estimation은 MLE, OLS, GEE, GLMM 모두를 포괄하는 추정 이론적 틀로서, 샌드위치 분산은 그 점근 정규성(Asymptotic Normality)의 결과물이며, GEE는 marginal mean에 주목하고 robust한 표준오차를 산출해주는 반면, GLMM은 임의효과를 통해 개체별(군집별) 차이를 직접 모델링합니다. 실제 데이터 분석에서는 연구 목적(개체별 효과 추정 vs 전체 평균 효과 추정), 데이터 특성(정확한 상관 구조 가정 vs 모형 가정의 유연성) 등을 종합하여 GEE와 GLMM 중 적절한 접근을 택하거나 비교하는 것이 중요합니다. 사실 Regression Model에는 이번 블로그 “Exploring Regression Models for Regression Analysis”에서 다룬 모델들을 제외하고도 아주 다양한 철학과 수식을 가진 모델들이 있습니다. 다만 여기서는 의학 분석에서 자주 사용되는 모델을 다루었으며, 이를 어느 정도 이해하셨다면 이외의 모델을 이해하는 데에 부족함이 없을 것이라고 생각합니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html",
    "href": "posts/2025-02-28-reg1/index.html",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "",
    "text": "차라투 블로그 “Exploring Regression Models for Regression Analysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한 여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는 Regression Analysis의 개념, (simple, multiple or general) linear regression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의 특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로 추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는 방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent standard errors (heteroskedasticity-robust standard errors)와 Wild Bootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의 clustered data 버전인 Cluster-robust standard error를 다룹니다. 3장에서는 GLM에서 여전히 남아있는 observations의 independent 조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를 고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust (sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다. {4장은 계획 중에 있습니다.}\n결국 “Exploring Regression Models for Regression Analysis”는 Regression Analysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며, 이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들 중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게 수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를 공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로 제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련 분석이 필요하시다면 해당 연구를 차라투로 문의해주시길 바랍니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#들어가며",
    "href": "posts/2025-02-28-reg1/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "",
    "text": "차라투 블로그 “Exploring Regression Models for Regression Analysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한 여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는 Regression Analysis의 개념, (simple, multiple or general) linear regression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의 특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로 추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는 방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent standard errors (heteroskedasticity-robust standard errors)와 Wild Bootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의 clustered data 버전인 Cluster-robust standard error를 다룹니다. 3장에서는 GLM에서 여전히 남아있는 observations의 independent 조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를 고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust (sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다. {4장은 계획 중에 있습니다.}\n결국 “Exploring Regression Models for Regression Analysis”는 Regression Analysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며, 이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들 중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게 수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를 공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로 제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련 분석이 필요하시다면 해당 연구를 차라투로 문의해주시길 바랍니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#regression-analysis",
    "href": "posts/2025-02-28-reg1/index.html#regression-analysis",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "1. Regression Analysis",
    "text": "1. Regression Analysis\nRegression Analysis는 통계학에서 가장 널리 사용되는 방법론 중 하나로, 어떤 종속변수(dependent variable)와 하나 이상의 독립변수(independent variables) 간의 관계를 추정하고 해석하는 분석 기법입니다. (종속변수 또한 벡터, 즉 여러 개일 수 있습니다.)\n의학 분야에서는 예후를 예측하는 모델을 만들거나(예: 특정 약물 투여 후 혈압, outcome 등의 변화를 예측), 특정 위험인자(risk factor)가 결과에 유의미한 영향을 미치는지(예: 어떤 치료법이 환자의 생존율에 유의미한 차이를 주는지) 등을 살펴보기 위해 Regression Analysis가 필수적으로 활용됩니다. 이러한 Regression Analysis을 완벽하게 수행하기 위해서는 본인의 data가 어떤 특성을 갖고 있는지 파악하고, 이에 대해 어떠한 Regression Model을 고려해야 하는지 알고, 이 모델이 어떻게 data에 적합(fit)하는지에 대한 대략적인 수학 또는 알고리즘의 원리와, 적합된 모델이 통계적으로 유의미한지 검정(test)하고 예측 성능이나 해석력을 평가하는 방법까지 전 과정을 이해해야 합니다. 본 1장에서는 가장 간단한, 대신 많은 가정이 필요한 Regression Model인 (General) Linear Regression에 대해서 위 과정과 함께 살펴볼 것이며, 이후의 장들에서는 data의 가정이 조금씩 깨질 때 (data의 특성이 변할 때) 어떤 Regression Model을 고려해야 하는지와 이 Model들의 parameters는 어떻게 수학적으로 추정하는지 살펴볼 것입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#linear-regression-lm",
    "href": "posts/2025-02-28-reg1/index.html#linear-regression-lm",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "2. Linear Regression (LM)",
    "text": "2. Linear Regression (LM)\n2.1. (Simple, Multiple) Linear Regression 정의\n\n(General) Linear Regression은 의학뿐만 아니라 다양한 분야에서 가장 basic한 Regression Model입니다. Linear Regression은 독립변수 \\(X\\) 와 종속변수 \\(Y\\) 간의 선형(linear) 관계를 가정하고, 이를 통해 종속변수를 설명하거나 예측합니다. 독립변수가 1개일 때는 Simple Linear Regression, 2개 이상일 때는 Multiple Linear Regression라고 부릅니다. 의학 연구에서 예를 들면, 혈압(종속변수)을 나이, 체중, 성별(독립변수) 등으로 설명하거나 예측하는 과정을 생각할 수 있습니다. 이러한 Linear regression은 아래 네 가지 가정을 기반으로 추정하며, 이 가정들에 대해서 잘 이해하는 것은 매우 중요하고, 이들 중 특정 가정들이 위배될 경우 이후에 다룰 Regression Model들을 고려해야 함을 명심해주시면 좋을 것 같습니다.\n\n\n선형성(Linearity) 가정\n독립변수와 종속변수가 선형 관계에 있다고 가정합니다. 산점도(scatter plot)를 통해 대략적인 선형 관계 여부를 확인할 수 있습니다.\n\n\n오차항의 정규성(Normality) 가정\n주어진 독립변수의 값에서 종속변수의 확률분포는 정규분포를 따른다고 가정합니다. 이는 오차항 ε이 정규분포를 따른다고도 표현할 수 있습니다.(종속변수가 정규분포를 따른다는 뜻은 Linear Model이 종속변수의 mean을 예측하므로, 이 둘의 차인 오차항은 평균이 0이고 분산은 종속변수와 같은 정규분포를 따르게 되기 때문입니다.) 이는 정규 P-P plot 혹은 Q-Q plot 등을 통해 가정 위배 여부를 대략적으로 확인할 수 있습니다.\n\n\n오차항의 독립성(Independence) 가정\n각 관측치(Observations, Data(set)) 또는 잔차(residual)가 서로 독립이라고 가정합니다. 즉, data간의 상관관계가 없다고 가정하는 것이고, 잔차산점도(residual plot), Durbin-Watson 통계량 등을 통해 자기상관(autocorrelation)이 있는지 살펴볼 수 있습니다.\n\n\n오차항의 등분산성(Homoscedasticity) 가정\n모든 독립변수의 값에서 종속변수의 분산이 동일하다고 가정합니다. 잔차산점도 등을 통해 잔차가 일정한 분산을 가지는지 대략적으로 확인할 수 있습니다.\n\n\n이후 Linear Regression Model은 다음과 같은 과정으로 분석을 수행하게 됩니다; (1) 최소제곱법(Least Squares)를 통해 model parameters를 추정(estimate), (2) 결정계수(\\(R^2\\))를 통해 모델이 종속변수를 얼마나 잘 설명하는지 확인, (3) F-검정(F-test)으로 전체 회귀식의 유의성을 검정, (4) 검정(t-test)으로 각 회귀계수(regression coefficient)가 유의미한지 확인. 특히, 독립변수 각각의 Model coefficient(parameter)를 검정 함으로써 종속변수와의 상관성을 분석하는 (4) 과정은 유의성을 판단하는 가장 중요한 검정 과정으로, 고전적으로 Wald Test, Likelihood Ratio Test, Score Test가 자주 사용됩니다.\n2.2. Linear Regression 수학적 표현 및 추정\n\nLinear Regression은 다음과 같은 형태를 가집니다:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_k X_{ik} + \\varepsilon_i\n\\]\n혹은 Matrix 형태로 간단히 쓰면,\n\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\n\\[\nwhere, \\quad \\mathbf{y} \\in \\mathbb{R}^n, \\mathbf{X}\n\\in \\mathbb{R}^{n \\times p}, \\boldsymbol{\\beta} \\in \\mathbb{R}^p,\n\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\n\\]\n\n\\(\\mathbf{y} \\in \\mathbb{R}^n\\): 관측된 종속변수들의 벡터\n\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\): 설계 행렬, 각 행은 하나의 관측치(Observation)이며 (\\(n\\)), 각 열은 하나의 독립변수에 (\\(p\\)) 해당\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\): 추정하고자 하는 회귀 계수(or 모수 or parameters) 벡터 (여기선 intercept \\(\\beta_0\\) 제외)\n\\(\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\\): 랜덤 오차 항 벡터, 위 네 가정에 따라 랜덤 오차항은 평균이 \\(E(\\boldsymbol{\\varepsilon}) = 0\\)이고, 분산이 모든 관측치에서 같으며 독립인 정규분포\n\n입니다. Multiple Linear Regression의 모델 \\(\\boldsymbol{\\beta}\\)의 분산은 scalar variance가 아니라 (co)variance matrix 형태가 될 것입니다. (simple도 intercept를 고려하면 matrix) 앞으로 세 장에 걸쳐 이를 분산 행렬이라고 부르겠지만, 정확히는 분산-공분산 행렬이라고도 부릅니다. 또한, 위 matrix form 수식처럼 intercept를 제외하는 경우도 많은데, 항상 있다고 가정하며 가독성을 위해 제외한 것이니 혼동할 필요 없이 자연스럽게 읽으시면 됩니다. Covariance matrix에서는 회귀계수 서로 간의 공분산이 들어있으므로, 각 회귀계수(독립변수의 유의성)에 대한 검정은 Covariance Matrix에 있는 각 diagonal 원소들을 사용하여 수행하게 됩니다. (회귀계수 각각에서 스스로의 대한 공분산 = 회귀계수의 분산) 예를 들어 \\(\\beta_2\\)의 분산은 \\(p\\)가 5 (독립변수가 4개, intercept 포함)일 때 Covariance Matrix의 3행 3열 값이 될 것입니다. (intercept가 없다면 2행 2열) 여기서 확인할 수 있듯 Regression Model의 parameter 추정 역시 중요하지만, parameter의 Covariance Matrix를 추정하는 것 또한 유의성 판단에서 매우 중요하며, 이후의 장들 또한 자연스럽게 Model의 소개, paramater 추정법, parameter의 (Co)variance Matrix를 추정법으로 이루어지게 될 것입니다.\n또한, Linear Model 식은 선형대수학에서 Hyper Plane으로 정의하는 형태가 되는데, 쉽게 설명하자면 독립변수가 하나일 경우 종속변수와의 2차원 평면에서 직선(2차원의 Hyper Plane)을 띄고, 독립변수가 두 개일 경우 종속변수와의 3차원 공간에서 평면을 띄는(3차원의 Hyper Plane), 선형적으로 공간을 두 부분으로 가르는 공간이라고 생각하시면 될 것 같습니다. (마찬가지로 쉽게 생각하면, 비선형적일 경우 직선이 아니라 곡선, 평면이 아니라 곡면일 것입니다.)\n이제 어떻게 Linear Model을 regression, 즉 fit(parameter를 추정)할 것인지 설명하겠습니다. 가장 기본적으로 알려진 Model의 통계학적 parameter 추정법으로는 Ordinary Least Squares(OLS), Maximum Likelihood Estimation(MLE), Method of Moment(MOM)가 있습니다. 이후의 모델들은 대부분 MLE로 모델을 추정하지만, Linear Model은 특별하게도 MLE와 OLS의 추정 결과가 같고, OLS의 추정 결과는 BLUE(Best Linear Unbiased Estimator) 입니다. 최소제곱법(OLS, Ordinary Least Squares) 추정량(estimator)은 오차 제곱합(SSR, Sum of Squared Residuals), 즉 모든 데이터에서 실제 종속변수 값과 모델의 예측 값의 차이를 제곱한 수들의 합을 최소화하는 \\(\\boldsymbol{\\beta}\\)를 찾는 것이며, 다음의 closed-form solution을 갖습니다: \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]\n그리고, 고전적 가정(특히 등분산성, 독립성, 정상성)이 모두 충족된다고 할 때, 이 \\(\\hat{\\boldsymbol{\\beta}}\\)의 추정오차의 공분산행렬(covariance matrix)은 \\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}\n\\] 이며, 실제 계산할 때는 데이터에서 추정한 \\(\\hat{\\sigma}^2\\)를 통해 \\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}, \\quad where \\; \\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}, \\; e_i = y_i - x_i \\hat{\\boldsymbol{\\beta}}\n\\]\n로 추정합니다. (분모에서 K를 뺀 이유는 degree of freedom에 의한 것이며, 에러 term이 정확히는 차원에 맞춰 \\(x_i^\\top \\hat{\\boldsymbol{\\beta}}\\) 지만, 앞으로 이정도 표기는 가독성을 위해 넘어가겠습니다.) 이렇게 구한 모델 추정량과 추정오차의 공분산행렬을 통해 각 회귀계수에 대한 검정을 수행하거나 신뢰구간(CI)을 계산할 수 있게 됩니다.\n다음으로 넘어가기 전에, Linear Regression에서 (1) \\(\\hat{\\beta}\\)이 closed-form solution \\((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\)을 갖게 되는 과정과, (2) 위에서 언급하였듯 model의 parameter (\\(\\hat{\\beta}\\)) OLS 추정량이 MLE(Maximum likelihood estimation) 추정량과 같음을 증명하겠습니다.\n(1) Prove \\(\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\).\n우선, OLS 추정량은 오차 제곱합(SSR)을 최소화하는 식이므로 오차 SSR을 표현하는 식 \\(J(\\beta)\\)를 적으면 다음과 같습니다.\n\\[\nJ(\\beta) = \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) = \\frac{1}{2} \\sum_{i=1}^{n} \\big( x_i \\beta - y^{(i)} \\big)^2\n\\]\n이제 위 \\(J(\\beta)\\)를 \\(\\beta\\)에 대해 미분하였을 때 0이 나오는 \\(\\hat{\\beta}\\)이 SSR이 최소가 되는 OLS 추정량입니다.(이에 대한 증명은 안하겠지만, 간단하게 볼록한 2차 함수에서는 미분값이 0인 점이 최소점인 것과 같은 원리라고 생각하시면 되겠습니다.) 식을 풀어보면,\n\\[ \\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big) \\] 입니다. \\(\\nabla\\)는 미분하는 변수가 scalar가 아니라 vector이기 때문에 사용되는 기호이며(gradient), 그냥 \\(\\beta\\)로 미분한다고만 생각하시면 됩니다. 이후 미분하는 과정 또한 크게 다르지 않습니다. 식을 계속 진행해보면, \\(y^\\top y\\)는 식에 \\(\\beta\\)가 없고 \\((X\\beta)^\\top y\\)와 \\(y^\\top (X\\beta)\\) 둘다 단순히 두 벡터의 내적인 똑같은 식이므로 \\[ = \\frac{1}{2} \\nabla_\\beta \\big( \\beta^\\top (X^\\top X) \\beta - 2 (X^\\top y)^\\top \\beta \\big) \\]이고, 이제 \\(\\beta\\)로 미분하면, 우항은 그냥 미분, 좌항은 두 번 나오므로 곱미분을 수행하여\\[ = \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big) \\]\\[ = X^\\top X \\beta - X^\\top y = 0 \\]\n입니다. 결국 OLS 추정량 \\(\\hat{\\beta}\\)는\n\\[ X^\\top X \\hat{\\beta} = X^\\top y \\] 이고, 양변 앞에 역행렬을 곱해주면\\[ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y \\]가 됩니다.\n(2) Prove OLS estimator is same as MLE estimator in Linear Model (Regression).\n\n위 Linear Regression Model 식을 다음과 같이 작성할 수 있습니다. \\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n식이 어색할 수 있지만 정확히 이전의 수학적 표현과 동일하며, \\(\\boldsymbol{\\varepsilon}\\) term이 복잡해 보이는 이유는, 이전의 네 가지 조건들을 수식으로 반영하여 모델을 확률 기반으로 해석하였기 때문입니다. 식을 설명드리자면, 선형성(Linearity) 가정에 의해 error의 mean이 (기댓값이) 0이 되고, 오차항의 정규성 가정에 의해 정규 분포를 따르며, 오차항의 독립성 및 등분산성 가정에 의해 분산(Covariance Matrix)이 값이 모두 \\(\\sigma^2\\)인 대각행렬, 즉 다른 observations간의 correlation은 0이고(없고), 각 observations의 분산은 \\(\\sigma^2\\)로 일정함을 표현한 것입니다.\n최대가능도방법 또는 최대우도법(Maximum Likelihood Method)은 어떤 모수와 표집한 값들이 주어졌을 때, 표집한 값들이 나올 가능도(확률)을 최대로 만드는 모수를 선택하는 아주 general한 모델의 점 추정 방법이며, 처음 들어보셨다면 대표님께서 더욱 자세하게 설명하신 블로그 글을 읽으시면 좋을 것 같습니다. 철학적으로는 종속변수의 확률 분포를 데이터에 맞게 가정한 후, 모든 observations(data) 각각이 나올 확률(분포)을 모두 곱한 식이 최대가 되도록 하는 모수값이 바로 MLE를 통해 추정한 parameter라고 생각하시면 됩니다. 간단한 예시를 들어보자면, 동전을 던져 앞면이 7번, 뒷면이 3번 나온 데이터에서 동전이 앞면이 나올 확률을 모수로 두고 종속변수의 분포를 베르누이 분포로 가정하면, 각각의 동전을 던져 얻은 관측치들 10개의 data의 Likelihood(확률)를 모두 곱하면 모수가 하나였으니 변수가 하나인 하나의 식(함수)가 나오고, 이 식(함수)을 최대화 하는 모수를 찾는 method가 MLE라고 간단하게 정리할 수 있을 것 같습니다. (이때 추정된 결과는 0.7일 것이고, 모수로 식을 미분한 함수(score function)이 0인 값을 찾음으로써 모수를 추정하게 되며, MLE 이외에도 MAP, 베이지안 등 다른 추정 기법들도 있지만 여기에서는 MLE 정도를 이해하면 충분할 것 같습니다.)\n이제 돌아와서 LM에서는 OLS estimator와 MLE estimator가 상응함을 보이겠습니다. 이후의 식 전개는 dim이 1일 때로 증명하겠습니다. Multi-dimension에서는 분포식과 term이 더 복잡하지만 meaning은 1차원과 정확히 동일하기 때문입니다. 오차 항 \\(\\boldsymbol{\\varepsilon}\\)는 정규분포를 따르므로, n개의 관측치에 대한 Likelihood는 \\[\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n\\]\n이며, 로그를 씌운 log Likelihood는 \\[\n\\ell(\\boldsymbol{\\beta}) = \\log \\mathcal{L}(\\boldsymbol{\\beta})\n\\]\n입니다. 식이 복잡해 보이지만 데이터 n개에 대하여 정규(가우시안)분포를 따르는 종속변수가 모델의 예측값을 가질 확률을 단순히 모두 곱한 식입니다. 결국 얻어진 표본 data 자체가 나올 확률에 대한 식을 세운 후, 이를 최대화 하는 parameter를 찾는 과정이 MLE estimation이라고 직관적으로 해석할 수 있습니다.\n돌아와서, log를 씌운 이유를 생각해보겠습니다. log의 그래프를 생각해보시면 log함수는 직관적으로도 monotonically increase하기 때문에 likelihood를 최대화 하는 parameter와 log likelihood를 최대화 하는 parameter는 같으며, log는 곱셈을 덧셈으로, 지수 term을 아래로 만들어주어 likelihood 식이 매우 간단해지기 때문에 (아래에서 실제로 그런지 확인해보세요.) MLE에서는 항상 log likelihood를 최대화하는 parameter를 찾는 방향으로 parameter를 추정합니다. 이어서 전개하면,\\[\n= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n\\]\\[\n= \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2}{2 \\sigma^2}\n\\]\\[\n= n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2\n\\]\n입니다. Log likelihood를 최대화 하는 것은 negative Log likelihood를 최소화 하는 것과 같으므로 양변에 -를 곱하면\\[\n- \\ell(\\boldsymbol{\\beta}) = -n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n이고, 상수 \\(-n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\)는 \\(\\boldsymbol{\\beta}\\)에 영향을 주지 않으므로 제거하면, \\[\n- \\log \\mathcal{L}(\\boldsymbol{\\beta}) \\approx \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n가 됩니다. 아까 보았듯 OLS estimator는 SSR을 최소화하는 아래 문제로 정의되므로, \\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n결국 위 두 식, OLS estimator와 MLE estimator의 목적식이 같음을 알 수 있고(종속변수와 모델 예측의 차이의 제곱을 모든 data point에서 더한 값을 최소화 하는 beta), 따라서 추정량 또한 같습니다. \\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}\n\\]\n위 증명은 MLE로 LM의 parameter를 추정한 것이 아니라 식 전개를 통해 OLS와 목적식이 같음을 보인 것이고, 실제 MLE로 parameter를 추정하는 과정은 2장부터 자세히 보게 될 것입니다. 한 가지 첨언하자면, Linear model에서는 이 두 추정량이 같지만, 정규성 가정이 깨진 경우 두 추정량은 같지 않을 수 있습니다. 이유를 생각해보면, 정규분포는 확률이 가장 큰, 즉 mode가 mean이며 mean을 기준으로 양 옆이 symmetric한 분포이기 때문에 두 추정량이 같으며, 정규분포가 아닐 경우 mean을 추정하려고 하는 MLE는 절대적인 거리를 좁히려는 OLS와 다른 값을 추정하게 된다고 직관적으로 생각해볼 수 있습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#linear-regression-in-homo-vs.-heteroskedasticity",
    "href": "posts/2025-02-28-reg1/index.html#linear-regression-in-homo-vs.-heteroskedasticity",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "3. Linear Regression in Homo vs. Heteroskedasticity",
    "text": "3. Linear Regression in Homo vs. Heteroskedasticity\n앞서 언급한 Linear Regression 모형은 “오차항의 등분산성(homoscedasticity)”을 가정합니다. 그러나 실제 연구 상황에서는 이 가정이 위배되는 경우가 흔합니다. Heteroskedasticity는 이러한 오차항은 독립 변수에 따라 같지 않을 수 있다는 뜻으로, 등분산성 가정이 깨진 경우를 의미합니다. 즉,\n\n\nHomoscedasticity: 모든 관측값에 대해 오차항(또는 종속변수)의 분산이 동일\n\nHeteroskedasticity: 오차항(또는 종속변수)의 분산이 관측값에 따라 다름\n\n입니다. 즉 이전 수식 중 오차 항을 \\(\\boldsymbol{\\varepsilon}\\)로 두었다면, observations의 분산은 더이상 \\(\\sigma^2\\)로 일정하지 않고 observations마다 다른 값을 갖을 수 있다는 뜻입니다. 예를 들어, 임상 연구에서 나이(age)에 따라 혈압(blood pressure)의 분산이 달라질 수 있습니다. 이런 상황에선 등분산성을 가정하는 것이 부적절해집니다. 등분산성 가정이 깨진 상황에서 Linear regression을 수행하면, \\(\\hat{\\boldsymbol{\\beta}}\\) 자체가 편향되진 않더라도(즉, 일관성(consistency)은 여전히 유지), \\(\\hat{\\boldsymbol{\\beta}}\\) 의 분산 추정치 \\(\\hat{\\sigma}^2 ( \\mathbf{X}^\\top \\mathbf{X} )^{-1}\\) 가 bias를 갖게 되어 잘못된 표준오차, 잘못된 유의성 검정 결과로 이어질 수 있습니다. 따라서, Heteroskedasticity가 의심되는 상황에서는 모델의 분산을 안정적으로 추정해야 하며, 이때 1번으로 고려되는 방법 중 하나가 Heteroskedasticity-consistent(heteroskedasticity-robust) standard errors, 줄여서 HC standard errors를 통한 (Co)variance Matrix estimation입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#heteroskedasticity-consistent-standard-errors-hc-standard-errors",
    "href": "posts/2025-02-28-reg1/index.html#heteroskedasticity-consistent-standard-errors-hc-standard-errors",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)",
    "text": "4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)\n4.1. HC Standard Errors 정의\n\nHC(Heteroskedasticity-Consistent) standard errors는, 고전적 선형회귀 가정 중 등분산성만 깨졌을 때(나머지 선형성, 정규성, 독립성 가정 in LM 은 그대로 유지) 회귀계수 추정치의 분산을 “강건(robust)”하게 추정하기 위한 method입니다. 이는 heteroskedasticity-robust standard errors 또는 HCCME(Heteroskedasticity-Consistent Covariance Matrix Estimation) 라고도 부르며, 이전에로 구한 \\(\\hat{\\beta}\\)를 그대로 사용하되, 그 공분산행렬 추정만 새롭게(robust하게) 구하는 방식입니다. 다만, 독립성(오차항들이 서로 독립), 선형성, 정규성 등의 다른 가정이 또 깨져 있다면 HC SE만으로는 대응할 수 없음을 명심하시길 바랍니다. 예를 들어, 오차항이 서로 상관되어 있는 clustered data에서는 더 이상 추정량이 consist하지 않기 때문에, 이보다 한 단계 더 나아간 cluster-robust standard errors, 혹은 GEE, GLMM 등의 모델을 고려해야 합니다. 즉, “등분산성 가정만 깨졌을 때” 쓰는 표준오차(or covariance matrix) 추정량이라고 생각하면 됩니다. 또한, 모델이 실제로 크게 잘못 설정되어 있다면(선형성조차 안 맞는 경우, 모델의 estimator가 크게 편향, HC se가 모델에서 구한 se와 크게 차이가 나는 경우 등), 그때는 variance(standard error)만 “robust standard errors”로 바꾼다고 해서 문제가 해결되지 않고, 모델을 수정하거나 data를 다시 확인하고 분석을 다른 방식으로 수행해야 합니다.\n즉 Greene의 말처럼,\n\n“Simply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.”\n\n이라는 점을 늘 유의해야 합니다. 정리하자면, 모델을 잘 구성하고, 모델의 분산과 큰 차이가 없을 경우에는 모델의 parameter의 분산 효율성이 최대가 아니거나 MLE 추정을 통해 얻은 모델의 분산이 더 이상 불편향이 아닐 때도 heteroskedasticity-robust standard errors를 통해서 모델의 분산을 robust하게 estimate할 수 있으며, 분석 결과 편향이 너무 크지 않다면 이후의 검정에서 이 HC se를 사용함으로써 이후의 통계 분석에서 설득력을 높일 수 있습니다. 추가로 스포하자면, 아래에서 다룰 Heteroskedasticity-robust Standard Errors 식은 Linear Model에서 robust하게 분산을 추정하는 Robust(or Sandwich) Estimation이며, 이후의 모델에서도 해당 모델에 대한 Sandwich Estimator를 고려함으로써 안정적인 분산 추정 방법에 대해 다룰 것입니다.\n4.2. HC(CME)0 수학적 표현\n\n이제 위에서 설명한 Heteroskedasticity한 data에서 robust하게 standard errors, 혹은 Covariance Matrix를 estimate 하는 heteroskedasticity-consistent standard errors의 수식을 유도하겠습니다.\n이전에 선형 회귀 모델을 다음과 같이 정의한 적이 있습니다.\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\] 이때, 위에서는 등분산성을 가정하였기 때문에 \\(\\boldsymbol{\\varepsilon}\\)를 원소값이 모두 \\(\\sigma^2\\) 인 대각행렬로 가정하여 각 observations의 분산은 \\(\\sigma^2\\)로 일정함을 가정했다면, 이번에 정의한 \\(\\boldsymbol{\\varepsilon}\\)은 \\(E(\\boldsymbol{\\varepsilon}) = 0\\), \\(E(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^\\top) = \\Phi\\) 입니다. 이때 \\(\\Phi\\)는 \\(\\operatorname{diag}(\\varepsilon_i^2)\\)로 정의되는 대각행렬이며(대각 성분을 제외하면 모두 0인 행렬), 식을 통해서 여전히 error term의 mean이 0이고 observations들은 independent하지만(대각행렬이므로), 더 이상 observations의 분산이 서로 일치하지 않음을 표현한 것입니다. 즉, 이제 Homoscedasticity에서 Heteroskedasticity을 고려하기 시작했다는 것을 수식을 해석함으로써 알 수 있습니다.\nVariance estimatation\n위 식의 OLS 추정량은 여전히 \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]\n입니다. (still consistent) 이제 모델의 variance 식을 전개해보면, \\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\operatorname{Var} \\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\right )\\]\n이며, \\((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top\\)는 determinant한 값으로, variance는 식 내의 determinant한 salar는 Var 밖으로 제곱과 함께 나오는 것처럼 (\\(\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X)\\)), 행렬 또는 벡터에 대해도 비슷하게 \\[\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} ) \\left ((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\right ) ^ \\top \\\\\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} )  \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\] 가 됩니다. (벡터와 행렬을 다룰 때에는 위처럼 제곱한다고 생각하시면 될 것 같습니다.) 참고로, \\[\\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\right )^ \\top = \\left ( (\\mathbf{X}^\\top \\mathbf{X})^\\top \\right )^ {-1} =  (\\mathbf{X}^\\top \\mathbf{X})^ {-1}\\] 입니다.\n따라서, 처음 설정한대로 \\(\\operatorname{Var} ( \\mathbf{y} ) = \\Phi\\)를 넣으면, 최종적으로 추정된 consistent model \\(\\hat{\\boldsymbol{\\beta}}\\)의 (co)variance matrix \\(\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}})\\)는 다음과 같습니다. \\[\n(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n하나 확인할 수 있는 사실은, 등분산성 가정(\\(\\Phi = \\sigma^2 \\mathbf{I}\\)) 하에선 위 식이 아래처럼 단순화되어 이전 (co)variance matrix가 나왔던, 가정하의 특수한 결과라는 것입니다. \\[ \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X} ^ \\top \\mathbf{X})^ {-1} \\mathbf{X} ^ \\top \\Phi \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\sigma ^ 2 \\mathbf{I} \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\] 또한, 이 때의 (OLS 추정량)의 분산 \\(\\sigma^2\\)의 식도 그냥 보여드리고 넘어갔었는데, 이는 대각 성분이 모두 같기 때문에 단순히 잔차 제곱합 \\(\\sum e_i^2\\)을 자유도(\\(N-K\\))로 나눈 값(\\(s^2\\))으로 추정하여 \\(\\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}\\) 로 한번에 표현한 식이었다는 것도 확인할 수 있습니다.\nHC0’s Robustness\n이분산성이 존재할 때 \\(\\Phi\\)는 대각행렬이지만 대각원소가 서로 다릅니다. White는 1980년에 이 heteroskedasticity에 robust하게 분산을 추정하기 위해 위 식에서 \\(\\Phi = \\sigma^2 \\mathbf{I}\\)로 term이 소거되게 하는 대신, \\(\\Phi_{ii}\\)를 잔차 제곱 \\(e_i^2\\)로 추정하는 HC0을 제안했습니다: \\[\n\\hat{\\Phi}_{ii} = e_i^2 \\quad \\Rightarrow \\quad \\hat{\\Phi} = \\operatorname{diag}(e_i^2)\n\\]\n이를 위 (co)variance 식에 대입하면 HC0 분산 추정량이 얻어집니다: \\[\n\\operatorname{HC0} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n이는 variance 식으로부터 시작해서 얻은 식이고, 데이터로부터 얻은 matrix \\(\\hat{\\Phi} = \\operatorname{diag}(e_i^2)\\)를 사용하여 이분산성의 구체적 형태를 가정하지 않았기 때문에 안정적으로 heteroskedasticity를 고려합니다. 이처럼 앞 뒤가 같은 모양 사이에 데이터로부터 얻은 분산 추정 식이 들어간 형태가 샌드위치 같다고 해서 Sandwich Estimator라고도 부르며, 여전히 가운데 matrix 항이 diagonal 함을 통해 오차(또는 관측치)간의 상관관계는 없다는 것도 기억해야 합니다.\n4.3. HC(CME) 1~3 수학적 표현\n\n이 HC Standard Error가 나온 이후, 위 식으로부터 구한 분산은 소표본에서는 모델의 overfitting(과적합, 딥러닝과 머신러닝에서도 자주 대두되는 문제로 데이터가 적어 학습한 데이터에서는 오차가 지만 보지 못한 데이터에 대해서는 오차가 크게 나오는 상황, 통계 분석은 test data를 따로 두지 않기 때문에 overfitting 대처가 더욱 중요.)에 의해 residual이 작게 나오고, 이에 따라 모델의 covariance가 과소평가될 가능성이 크다는 문제가 제시되었습니다. 이에, 소표본에서도 안정적으로 추정하기 위해 White(1980)의 HC Standard Errors를 HC0로 두고, 여러 버전의 HC Standard Errors가 최근까지 다양한 학자들에 의해 연구되고 있습니다. 보통 HC0~HC5까지를 HC Standard Errors로 부르며, HC 오른쪽 숫자가 버전을 뜻하여 이 수가 커질수록 최근에 나온 것이고, 각각은 simulation 실험을 통해 소표본에서 더욱 강건함을 보이는 식으로 논문이 나옵니다. 중요한 점은, 첫 번째로 이 HC Standard Errors들은 소표본에서는 값이 다르지만 표본의 크기가 무수히 커질수록 asymptotically하게 같으며 소표본에서의 고려를 위해 값을 더 크게 만드는 추가 term을 넣어준다는 점(물론 수식적으로 필요성을 증명하지만), 둘 째로 R의 sandwich 패키지에서는 4까지 지원하고, 3을 넘는 HC 시리즈가 쓰이는 경우는 거의 볼 수 없기 때문에 3까지의 식 정도만 다루어도 충분하다는 점, 마지막으로 위 직관적인 이유로나 수식적으로나 식을 해석해 보면 거의 모든 표본 cases에 대해 버전이 클수록 분산을 더욱 크게 추정한다는 점입니다. 따라서 유의성을 보이기 위해서는 버전이 작은게 유리할 것입니다.^^ 아래에서는 HC1부터 3까지의 수식을 살펴보겠습니다. 각각의 철학에 대한 자세한 증명 과정 또한 다룰까 했지만 중요하지 않기 때문에 간단히 소개한 후 로컬에서 돌려 보실 R 예시 코드를 보여드릴 것입니다.\nHC1: 소표본 편향 보정\nHC1은 소표본에서 자유도 조정 인자 \\(\\frac{n}{n-k}\\)를 도입하여 편향을 줄인다는 철학에서 출발하여, 잔차 \\(e_i^2\\)의 기대값이 \\(\\sigma_i^2(1-h_{ii})\\)이므로, \\(E(e_i^2) \\approx \\sigma_i^2\\)이 되도록 HC0에 \\(\\frac{N}{N-K}\\)을 나누어서(1보다 크며, n이 커짐에 따라 1로 수렴하는 term 추가) 스케일링합니다. 즉, 식은 다음과 같습니다: \\[\n\\operatorname{HC1} = \\frac{N}{N-K} \\cdot \\operatorname{HC0} \\\\\n= \\frac{N}{N-K} \\cdot (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nHC2: Leverage 보정\n위 HC1에서 \\(E(e_i^2) = \\sigma_i^2(1-h_{ii})\\) 라고 이야기 하였습니다. 일단 그렇다고 하면, 기존에는 \\(\\sigma_i^2\\)를 \\(e_i^2\\)으로 추정하였다면, 사실 \\(\\frac{e_i^2}{1-h_{ii}}\\)의 기대값이 \\(\\sigma_i^2\\)이기 때문에 HC1처럼 앞에 scalar term을 추가하는 대신, HC2는 이를 직접적으로 반영하여 \\(\\hat{\\Phi}\\)를 \\(\\operatorname{diag}(e_i^2)\\) 대신 \\(\\operatorname{diag}( \\frac{e_i^2}{1-h_{ii}})\\)로 추정합니다. 이 때 \\(h_{ii}\\)은 Leverage(래버리지), \\(H(or \\;h)\\)는 Hat Matrix라고 불리고, 식은 \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\)이며, 위 \\(h_{ii}\\)은 이 matrix의 대각원소를 뜻합니다. Leverage에 대한 이야기 또한 길지만 간단하게 설명하면, \\(\\mathbf{H}\\)가 Hat Matrix라고 부르는 이유는 이 \\(\\mathbf{H}\\) term에 \\(\\mathbf{y}\\)를 곱하면, \\[\n\\mathbf{Hy} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y} \\\\\n= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n=\\hat{\\mathbf{y}}\n\\] 이 되어 \\(\\mathbf{y}\\)에 hat을(추정치) 씌운다는 의미에서 비롯되었으며, 이는 기하학적으로 더욱 복잡하게 설명될 수 있지만 간단한 의미는 각 관측치가 얼마나 모델의 estimation에 영향을 주었는지를 보여주는 matrix입니다. 이 matrix의 diagonal값인 레버리지를 보며 관측치 하나하나의 영향력을 볼 수 있고, 이 값이 큰 관측치에 대해서는 아래 식에서처럼 분모를 작게 함으로써 분산을 크게 추정한다고 직관적으로 생각할 수 있습니다. \\[\n\\operatorname{HC2} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{1-h_{ii}} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n넘어가기 전에, 증명 없이 사용해왔던 수식 \\(E(e_i^2) = \\sigma_i^2(1-h_{ii})\\) 를 간단하게만 증명해보겠습니다. 예측 오차는 실제값 \\(y_i\\)와 예측값 \\(\\hat{y}_i\\)의 차이입니다: \\[\ne_i = y_i - \\hat{y}_i.\n\\] 잔차 벡터 \\(\\mathbf{e}\\)는 다음과 같이 표현됩니다: \\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{y}.\n\\]\n에러 제곱의 기댓값은 잔차의 분산을 의미합니다. 잔차 벡터 \\(\\mathbf{e}\\)의 공분산 행렬은 다음과 같이 계산됩니다: \\[\n\\text{Cov}(\\mathbf{e}) = \\text{Cov}((\\mathbf{I} - \\mathbf{H}) \\mathbf{y}) = (\\mathbf{I} - \\mathbf{H}) \\text{Cov}(\\mathbf{y}) (\\mathbf{I} - \\mathbf{H})^T.\n\\] \\(\\text{Cov}(\\mathbf{y}) = \\sigma_i^2 \\mathbf{I}\\)이므로 (당연히 \\(\\sigma\\)는 관측치 \\(i\\)마다 다를 수 있습니다.): \\[\n\\text{Cov}(\\mathbf{e}) = (\\mathbf{I} - \\mathbf{H}) \\sigma_i^2 \\mathbf{I} (\\mathbf{I} - \\mathbf{H})^T = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n\\] 여기서 \\(\\mathbf{H}\\)는 대칭 행렬(\\(\\mathbf{H} = \\mathbf{H}^T\\))이고 멱등 행렬(\\(\\mathbf{H}^2 = \\mathbf{H}\\))이므로 (이 부분은 기하학적인 설명이 많이 필요해서 증명하지 않겠다만, 이들은 단지 특정 행렬들의 성질이며 이를 만족한다고 하고 넘기겠습니다.): \\[\n\\text{Cov}(\\mathbf{e}) = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n\\]\n\\(i\\)번째 잔차 \\(e_i\\)의 분산은 공분산 행렬의 \\(i\\)번째 대각 성분이고, \\[\n\\text{Var}(e_i) = [\\text{Cov}(\\mathbf{e})]_{ii} = \\sigma_i^2 (1 - h_{ii}).\n\\] 에러 제곱의 기댓값은 잔차의 분산과 동일하므로 (에러의 mean이 0이므로), \\[\nE[e_i^2] = \\text{Var}(e_i) = \\sigma_i^2 (1 - h_{ii}).\n\\] 가 증명됩니다.\nHC3: 강화된 Leverage 보정\nHC3은 Jackknife 접근법에서 영감을 받아 HC2에서 \\(\\Phi\\)의 분모 term을 더 강하게 \\((1-h_{ii})^2\\)로 둡니다. 직관적으로, 같은 \\(h_{ii}\\)에 대해 더 크게 분산을 추정 (\\((1-h_{ii})^2 &lt; 1-h_{ii}\\))하여, 고레버리지, 즉 outlier에 대해 더 안정적으로 소표본의 분산을 추정할 수 있습니다. \\[\n\\operatorname{HC3} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{(1-h_{ii})^2} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nHC(CME)s 비교\n위 내용을 간단하게 표로 정리하면 다음과 같습니다.\n\n\n\n\n\n\n\n\n유형\n수식\n보정 요소\n사용 시나리오\n\n\n\nHC0\n\\(\\operatorname{diag}(e_i^2)\\)\n없음\n대표본\n\n\nHC1\n\\(\\frac{n}{n-k}\\operatorname{HC0}\\)\n자유도\n소표본 기본\n\n\nHC2\n\\(\\frac{e_i^2}{1-h_{ii}}\\)\n레버리지 1차\n고레버리지 존재 시\n\n\nHC3\n\\(\\frac{e_i^2}{(1-h_{ii})^2}\\)\n레버리지 2차\n소표본 + 고레버리지\n\n\n\n정리하자면, 동분산성 검정을 검정하여 이분산성이 확인되면 HC se 시리즈의 적용을 고려해야 하고, 표본 크기가 \\(n &lt; 50\\)인 경우는 HC3 또는 아래에서 다룰 bootstrap기반의 (Clustered) Bootstrap Covariance Matrix Estimation를, \\(n &gt; 500\\)인 경우는 HC0/HC1로 충분하다고 간단하게 생각해볼 수 있습니다. (물론 이는 정해진 규칙이 아니고, 시작은 항상 HC0부터 검정해보는 것이 맞습니다.)"
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#wild-bootstrap",
    "href": "posts/2025-02-28-reg1/index.html#wild-bootstrap",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "5. (Wild) Bootstrap",
    "text": "5. (Wild) Bootstrap\n마지막으로, sandwich estimator는 아니지만, 모든 모델에서 data로부터 안정적으로 분산을 추정하여 의학 연구에서 인정하는 기법 중 하나인 Bootstraping을 통한 분산 term 추정 기법을 간단하게 소개드리겠습니다. 이 method는 특정 가정으로부터 수학적인 추정식을 세워 분산을 추정하는 대신, Bootstraping 통계 기법을 사용하여 data 만으로 경험적인 분산을 추정합니다. 때문에 이번에 소개하는 (Wild) Bootstrap 은 어떤 가정이나 모델이든 사용할 수 있는, 심지어 2장에서 다룰 clustered(grouped) data 에서도 사용할 수 있는 방법입니다.(사용 대상인 모델이 정해지지 않은, 데이터 만으로 구하는 분산이기 때문에 가정이 전혀 들어가지 않아서입니다. 예를 들어 LM에서 HC se를 사용하는 경우 나머지 세 가정이 만족해야 했지만, Bootstrap은 그렇지 않습니다.) 이후에 다룰 샌드위치 추정량(Sandwich Estimator)과, 그의 OLS 버전인 HC(Heteroskedasticity-Consistent) 시리즈가 개발되었지만, 이들은 결국 대표본에서의 점근적 성질로부터 얻은 수식이고, 극단적인 쇼포본 같은 작은 클러스터 수에서는 이러한 데이터 만을 고려하여 추정하는 부트스트랩이 더 강력한 대안이 될 수도 있습니다. 이를 소개하기 전에 간단히 중요한 통계학적 resampling 기법인 잭나이프(Jackknife)와 부트스트랩(Bootstrap)을 얘기해보겠습니다. 이 둘은 모두 표본을 계속해서 얻기 힘든 상황 (현실에서는 대부분 그럴 것입니다.)에서 모수를 추정하기 위해, 이미 갖고 있는 표본의 data로부터 resampling을 하여 추정하는 통계학적 방법론입니다.\n잭나이프(Jackknife)\nJackknife method는 한 번에 하나의 클러스터를 제외하고 모델을 추정 하는 것을 모든 클러스터에 대해 반복하여 분산을 계산합니다:\n\\[\n\\mathrm{Var}(\\hat{\\beta}_c) = \\frac{n-1}{n} \\sum_{c=1}^C \\left(\\hat{\\beta}_c - \\bar{\\hat{\\beta}}\\right)^2,\n\\]\n여기서 \\(\\hat{\\beta}_c\\)는 c번째 클러스터를 제외한 추정치, \\(\\bar{\\hat{\\beta}}\\)는 추정치의 평균이고, 이는 resampling 기법이지만 위에서 설명드렸던 HC3 추정량이 이 식으로부터 도출된 결과입니다. 물론, 이는 이론적인 결과이므로 각각이 서로 다른 결과를 도출할 수도 있을 것입니다. 단, resampling 기법들은 그 특성상 클러스터나 그 data point의 수가 많을 경우 계산 부하가 큽니다.\n부트스트랩(Bootstrap)\nBootstrap method는 데이터를 무작위로 resampling하여 여러 버전의 데이터셋을 생성하고, 각각에서 모델을 추정한 후 추정값의 변동성을 분산으로 사용합니다. 대표적인 예시로 와일드 부트스트랩 (Wild Bootstrap)은 잔차(residual)에 랜덤 가중치를 곱해 인위적 으로 데이터의 분산을 크게 보정하고, 케이스 기반 (XY/Pairs 부트스트랩)은 기본적인 방법으로 클러스터 자체를 복원 추출하여 새 data를 생성한 뒤 구하는 과정을 반복하며, 잔차 기반 (Residual 부트스트랩)은 잔차 만을 리샘플링하고 재추정하는 방법입니다다.\nWild Bootstrap\n위에서 설명드렸듯, 잔차 \\(e_i\\)에 “랜덤 가중치” \\(w_i\\)를 곱해 새로운 반응 변수를 생성하는 방법으로 다음과 같을 것입니다:\n\\[\ny^* = \\hat{y} + w_i \\hat{e}.\n\\]\n이때 가중치 분포 로 사용되는 대표적인 예시는 Rademacher: \\(w_i \\in \\{-1, 1\\}\\), Mammen: \\(w_i = \\frac{\\sqrt{5} \\pm 1}{2}\\), Webb: 6점 분포(\\(w_i \\in \\left\\{ -\\sqrt{\\frac{3}{2}}, -\\sqrt{\\frac{2}{2}}, -\\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{2}{2}}, \\sqrt{\\frac{3}{2}} \\right\\}\\))로 클러스터별 정규분포 등 다른 적용도 가능합니다. 위에서 언급하였듯, 이외에도 케이스 기반 (XY) 부트스트랩, 잔차 부트스트랩, 프랙셔널 부트스트랩(Fractional Bootstrap)등의 방법론이 있지만, data를 크게 왜곡하지 않는 Wild가 이러한 분산 추정에서 디폴트하게 고려됩니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#r-예시-hc03-및-부트스트랩",
    "href": "posts/2025-02-28-reg1/index.html#r-예시-hc03-및-부트스트랩",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "6. R 예시: HC0~3 및 부트스트랩",
    "text": "6. R 예시: HC0~3 및 부트스트랩\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. 위 내용 중 “시리즈가 클 수록 강건하게 추정한다”는 점을 기억하시고 해석하면 됩니다.\n\n## mtcars 데이터에서 mpg(연비)를 wt(차량 무게), hp(마력)으로 설명하는 회귀모형\n#data(mtcars)\n#model &lt;- lm(mpg ~ wt + hp, data = mtcars)\n#\n## 기본 OLS 표준오차 (등분산 가정)\n#summary(model)  \n#\n## HC 표준오차 계산\n#library(sandwich)\n#library(lmtest)\n#\n## HC 유형별 분산-공분산 행렬\n#cov_hc0 &lt;- vcovHC(model, type = \"HC0\")  # 기본 White 추정량\n#cov_hc1 &lt;- vcovHC(model, type = \"HC1\")  # 자유도 보정\n#cov_hc2 &lt;- vcovHC(model, type = \"HC2\")  # 잔차 스케일링\n#cov_hc3 &lt;- vcovHC(model, type = \"HC3\")  # 작은 표본 보정\n#\n## 계수 테스트 결과 비교\n#coeftest(model, vcov = cov_hc0)  \n#coeftest(model, vcov = cov_hc1)  \n#\n## 부트스트랩 분산-공분산 행렬 (기본 100회 복제)\n#cov_wild &lt;- vcovBS(fit, cluster = ~cluster_id, type = \"wild\", R = 1000)\n#cov_xy &lt;- vcovBS(model, R = 200, type = \"xy\")  # xy-쌍 부트스트랩\n#\n## 결과 비교\n#coeftest(model, vcov = cov_wild)  \n#coeftest(model, vcov = cov_xy)  \n#\n## 주의: 부트스트랩 결과는 실행 횟수 R, 실행 시마다 다를 수 있음"
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#마무리하며",
    "href": "posts/2025-02-28-reg1/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "마무리하며",
    "text": "마무리하며\n이번 1장에서는 Regression Analysis의 기본 개념과 (simple, multiple or General) Linear Regression의 이론적 배경, 그리고 등분산성 가정이 깨진(Heteroskedasticity) 상황에서 유용하게 쓰이는 Heteroskedasticity-Consistent Standard Errors(HC standard errors)에 대해 살펴보았습니다.\nHC standard errors를 사용하면, Linear Regression 모델에서 등분산성 가정이 위배되더라도 Standard Errors(or Covariance Matrix)를 좀 더 타당하게 추정할 수 있습니다. 하지만, “오차항의 독립성, 선형성, 정규성” 등 나머지 가정이 크게 어긋난다면, 단순히 HC SE로 분산을 보정하는 것만으로는 해결되지 않습니다. HC SE가 기본 OLS 분산추정치와 크게 다르다면, “정말 모델이 맞는지”를 다시 고민하고, 필요하다면 모델을 재설정하거나 다른 방법을 모색해야 합니다.\n어쨌든, Heteroskedasticity가 의심되는 상황에서 가장 먼저 고려할 만한 접근인 HC(Heteroskedasticity-Consistent) standard errors는 모델의 유의성 검정을 위해 안정적으로 분산을 추정할 때 널리 쓰이며, 의학 연구에서도 일상적으로 활용되고 있습니다. 다음 2장에서는 이런 Linear Regression을 더 확장한 Generalized Linear Model(GLM)의 개념을 본격적으로 다루고, HC standard errors의 clustered data 버전인 Cluster-robust standard error에 대해서도 다룰 예정입니다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Randomized controlled trial(RCT)를 시행할 때, control/treatment group의 baseline characteristics이 비슷하게 맞춰주기 위해 subject를 random하게 배정한다. Observational study에서도 RCT와 유사한 causality를 확보하기 위한 방법 중 propensity score analysis가 있다. propensity score가 비슷한 subject끼리는 baseline characteristics도 비슷하다는 특성이 있다. 이 때, 두 군 baseline covariates의 유사성을 어떻게 평가할 수 있을까? 이 때 Standardized Mean Difference(SMD)를 사용하면 유사성을 평가할 수 있다.\nSMD는 두 군의 mean차이를 measure하는 Effect size의 일종이다. 보통 의학 논문 연구에서는시 귀무/대립 가설 및 p-value를 표시하여 결과를 보고한다. 통계적 유의성을 검정하는 방식으로 진행되는데, 통계적으로 유의하다 할 지라도 effect가 얼마나 큰 지에 대해서는 말할 수가 없다는 한계가 있다. 이를 해결하기 위해 도입된 개념이 Effect size이다.\nEffect size는 세 가지 특징을 살펴보자. 첫째, Effect size는 연구 결과의 해석을 이분법이 아닌 연속 선상에서 할 수 있게 해준다. 기존에 사용되던 p-value는 귀무가설 기각 여부밖에 알려주지 못하지만, effect size는 실제로 얼마나 큰 차이가 있는 지를 구체적으로 보여준다. 둘째, effect size는 p-value와는 달리 표본 수에 의한 영향을 받지 않는다. p-value를 이용한 검정의 경우 표본 수가 커질 때, power가 증가한다. 그 말은 통계적으로 유의하지 않음에도 불구하고, 단지 표본 수가 많다는 이유 만으로 유의하다는 결과를 얻을 수 있다는 뜻이다. 이에 비해 effect size는 표본 수의 영향을 받지 않는다. 셋째, 효과크기는 다양한 형태의 결과들을 비교 가능한 공통의 단위로 변화 시켜줌으로써 다른 통계적 방법에 의해 시행된 연구 결과들을 비교할 수 있게 해준다. 이 특성은 메타분석 시 자주 이용된다.\n이제 SMD의 수식에 대해 살펴보자. 참고로 통상적으로 SMD는 0.2 - 0.5일 경우 small, 0.5 - 0.8일 경우 medium, 0.8을 넘을 때 large라고 한다.\n\nSMD는 크게 SMD for continuous or categorical baseline variables 두 가지가 있고, formula는 다음과 같다.\n\n\nContinuous baseline variable\n\\[\nd = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}\n\\]\n\n\n\\(\\overline{X}_1\\), \\(\\overline{X}_2\\)은 각 group의 baseline variable의 sample mean, \\(s_1, s_2\\)은 각 group의 sample variance를 의미한다. variable이 치우쳐 있다면(skewed), d를 rank statistics을 이용하여 modify하여 사용하면 된다.\n\n\nCategorical baseline variable\n1) Binary categorical variable\n\\[\nd = \\frac{\\hat{P}_1 - \\hat{P}_2}{\\sqrt{\\frac{\\hat{P}_1(1 - \\hat{P}_1) + \\hat{P}_2(1 - \\hat{P}_2)}{2}}}\n\\]\n\\(\\hat{P_1}, \\hat{P_2}\\)는 control/treatment group의 binary baseline variable의 proportion(mean)을 의미한다.\n2) Categorical variable with K levels\n\nTable 1처럼 baseline variable이 2개 초과의 level을 가지는 categorical variable의 SMD에 대해 살펴보자.\n\n\n\\[ T = (\\hat{P}_{12}, \\hat{P}_{13}, ..., \\hat{P}_{1K})'\\] \\[ C = (\\hat{P}_{22}, \\hat{P}_{23}, ..., \\hat{P}_{2K})'\\]\n이 때, \\(\\hat{P}_{JK} =\\) Pr(category k|treatment group j), $ j $ and \\(k \\in \\{2, 3, ..., K\\}\\)\nStandardized difference는 다음과 같이 정의된다.\n\\[\nd = \\sqrt{(T - C)'S^{-1}(T-C)}\n\\]\nS는 (k - 1)X(k - 1) covariance이며, 다음과 같다.\n\\[\nS = [S_{kl}] = \\begin{cases}\\dfrac{[\\hat{P}_{1k}(1-\\hat{P}_{1k})+\\hat{P}_{2k}(1-\\hat{P}_{2k})]}{2}, & k = l \\\\\\dfrac{[\\hat{P}_{1k}\\hat{P}_{1l}+\\hat{P}_{2k}\\hat{P}_{2l}]}{2}, & k \\neq l\\end{cases}\n\\]\n지금까지 SMD를 구하는 과정을 살펴보았다. RCT시 subject의 group 배정 후 혹은 propensity score analysis시 baseline covariates의 balance를 확인할 때, SMD를 이용한다. SMD가 0.1미만이면 작은 차이로 간주하고, 대부분의 변수에서 SMD 0.1미만이면서, 모두 0.2 미만이면 balance가 잘 맞는 것으로 평가한다.\n\nHedges and Olkin (1985)에서 SMD에 대한 confidence interval 공식을 내놓았는데 식은 다음과 같다.\n\\[\nd\\;\\pm 1.96\\times\\sigma[d]\n\\]\n\\[\n\\sigma[d] = \\sqrt{\\frac{n_1 + n_2}{n_1 \\times n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\\]\n\n예제 데이터와 tableone package를 이용해 SMD를 구하는 과정을 살펴보자. tableone package에 대한 자세한 사용법이 궁금한다면 이 포스트를 참고하길 바란다.\n데이터를 불러오는 과정이다.\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 8.3.0 with Schannel\n\nlibrary(magrittr)\nlibrary(tableone)\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url,header=T)\n\nhead(dt)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n4:         2009  183321     200908           NA            NA           NA\n5:         2009  942671     200909           NA            NA           NA\n6:         2009  979358     200912           NA            NA           NA\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N HGHT\n1:           0            0           NA        3        1              0  144\n2:           0            0           NA        2        1              0  162\n3:           0            0           NA        3        1              0  163\n4:          NA           NA           NA        3        1              0  152\n5:          NA           NA           NA        3        1              0  159\n6:          NA           NA           NA        2        1              0  157\n   WGHT WSTC  BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT  HGB FBS TOT_CHOL  TG HDL\n1:   61   90 29.4   0.7   0.8    120     80        1 12.6 117      264 128  60\n2:   51   63 19.4   0.8   1.0    120     80        1 13.8  96      169  92  70\n3:   65   82 24.5   0.7   0.6    130     80        1 15.0 118      216 132  55\n4:   51   70 22.1   0.8   0.9    101     62        1 13.1  90      199 100  65\n5:   50   73 19.8   0.7   0.8    132     78        1 13.0  92      162  58  40\n6:   55   73 22.3   1.5   1.5    110     70        1 11.9 100      192 109  53\n   LDL CRTN SGOT SGPT GGT GFR\n1: 179  0.9   25   20  25  59\n2:  80  0.9   18   15  28  74\n3: 134  0.8   26   30  30  79\n4: 114  0.9   18   14  11  61\n5: 111  0.9   24   23  15  49\n6: 117  0.7   15   12  14  83\n\n\ntableone object를 생성한 뒤 print function을 이용해 결과물을 출력할 수 있다. 이때 smd = TRUE로 처리해주면 smd값이 같이 출력된다.\n\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n                       \n                        Overall        \n  n                       1644         \n  HGHT (mean (SD))      164.55 (9.19)  \n  WGHT (mean (SD))       65.10 (12.53) \n  BMI (mean (SD))        23.92 (3.38)  \n  HDL (mean (SD))        55.90 (19.47) \n  LDL (mean (SD))       118.69 (201.99)\n  TG (mean (SD))        134.90 (104.75)\n  SGPT (mean (SD))       25.98 (27.18) \n  Q_PHX_DX_STK = 1 (%)      12 ( 1.1)  \n  Q_PHX_DX_HTDZ = 1 (%)     26 ( 2.4)  \n  Q_HBV_AG (%)                         \n     1                      77 ( 4.7)  \n     2                    1102 (67.1)  \n     3                     463 (28.2)  \n  Q_SMK_YN (%)                         \n     1                     995 (60.6)  \n     2                     256 (15.6)  \n     3                     391 (23.8)  \n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nprint(t2, smd = T)\n\n                       Stratified by Q_SMK_YN\n                        1               2               3               p     \n  n                        995             256             391                \n  HGHT (mean (SD))      160.67 (8.34)   168.83 (6.45)   171.61 (7.09)   &lt;0.001\n  WGHT (mean (SD))       61.17 (11.08)   70.09 (10.72)   71.76 (13.07)  &lt;0.001\n  BMI (mean (SD))        23.63 (3.39)    24.52 (2.93)    24.27 (3.54)   &lt;0.001\n  HDL (mean (SD))        57.83 (14.08)   53.91 (36.73)   52.37 (13.54)  &lt;0.001\n  LDL (mean (SD))       112.26 (32.81)  147.52 (505.27) 116.34 (56.89)   0.046\n  TG (mean (SD))        114.05 (76.97)  162.89 (126.51) 169.24 (133.28) &lt;0.001\n  SGPT (mean (SD))       23.33 (28.42)   28.61 (20.62)   31.00 (26.96)  &lt;0.001\n  Q_PHX_DX_STK = 1 (%)      11 (  1.8)       1 (  0.5)       0 (  0.0)   0.051\n  Q_PHX_DX_HTDZ = 1 (%)     18 (  2.9)       5 (  2.6)       3 (  1.1)   0.287\n  Q_HBV_AG (%)                                                           0.193\n     1                      40 (  4.0)      19 (  7.5)      17 (  4.3)        \n     2                     679 ( 68.3)     164 ( 64.3)     259 ( 66.2)        \n     3                     275 ( 27.7)      72 ( 28.2)     115 ( 29.4)        \n  Q_SMK_YN (%)                                                          &lt;0.001\n     1                     995 (100.0)       0 (  0.0)       0 (  0.0)        \n     2                       0 (  0.0)     256 (100.0)       0 (  0.0)        \n     3                       0 (  0.0)       0 (  0.0)     391 (100.0)        \n                       Stratified by Q_SMK_YN\n                        test SMD   \n  n                                \n  HGHT (mean (SD))            0.972\n  WGHT (mean (SD))            0.611\n  BMI (mean (SD))             0.181\n  HDL (mean (SD))             0.197\n  LDL (mean (SD))             0.091\n  TG (mean (SD))              0.341\n  SGPT (mean (SD))            0.196\n  Q_PHX_DX_STK = 1 (%)        0.137\n  Q_PHX_DX_HTDZ = 1 (%)       0.084\n  Q_HBV_AG (%)                0.109\n     1                             \n     2                             \n     3                             \n  Q_SMK_YN (%)                  NaN\n     1                             \n     2                             \n     3                             \n\n\nQ_HBV_AG, Q_SMK_YN등의 3개의 level을 가지는 categorical variable도 하나의 SMD를 출력해줌을 알 수 있다. 참고로 tableone package에서는 strata가 3개 이상일 때도 하나의 SMD를 출력해준다. 이는 n개의 strata에 대해 pairwise로 \\(\\binom{n}{2}\\)개의의 SMD를 구한 뒤, 절댓값을 취하고, 평균을 낸 값이다.\n\n\nYang, D. and Dalton, JE. (2012). A unified approach to measuring the effect size between two groups using SAS. SAS Global Forum 2012, Paper 335-2012.\nHedges LV, Olkin I. (1985). Statistical Methods for Meta-Analysis. Academic Press: San Diego, CA .\n남상건.(2015). 효과크기의 이해. Hanyang Medical Reviews. Paper 40-43\n이유진(2022). tableone 패키지 소개. 차라투 블로그"
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#background",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#background",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Randomized controlled trial(RCT)를 시행할 때, control/treatment group의 baseline characteristics이 비슷하게 맞춰주기 위해 subject를 random하게 배정한다. Observational study에서도 RCT와 유사한 causality를 확보하기 위한 방법 중 propensity score analysis가 있다. propensity score가 비슷한 subject끼리는 baseline characteristics도 비슷하다는 특성이 있다. 이 때, 두 군 baseline covariates의 유사성을 어떻게 평가할 수 있을까? 이 때 Standardized Mean Difference(SMD)를 사용하면 유사성을 평가할 수 있다.\nSMD는 두 군의 mean차이를 measure하는 Effect size의 일종이다. 보통 의학 논문 연구에서는시 귀무/대립 가설 및 p-value를 표시하여 결과를 보고한다. 통계적 유의성을 검정하는 방식으로 진행되는데, 통계적으로 유의하다 할 지라도 effect가 얼마나 큰 지에 대해서는 말할 수가 없다는 한계가 있다. 이를 해결하기 위해 도입된 개념이 Effect size이다.\nEffect size는 세 가지 특징을 살펴보자. 첫째, Effect size는 연구 결과의 해석을 이분법이 아닌 연속 선상에서 할 수 있게 해준다. 기존에 사용되던 p-value는 귀무가설 기각 여부밖에 알려주지 못하지만, effect size는 실제로 얼마나 큰 차이가 있는 지를 구체적으로 보여준다. 둘째, effect size는 p-value와는 달리 표본 수에 의한 영향을 받지 않는다. p-value를 이용한 검정의 경우 표본 수가 커질 때, power가 증가한다. 그 말은 통계적으로 유의하지 않음에도 불구하고, 단지 표본 수가 많다는 이유 만으로 유의하다는 결과를 얻을 수 있다는 뜻이다. 이에 비해 effect size는 표본 수의 영향을 받지 않는다. 셋째, 효과크기는 다양한 형태의 결과들을 비교 가능한 공통의 단위로 변화 시켜줌으로써 다른 통계적 방법에 의해 시행된 연구 결과들을 비교할 수 있게 해준다. 이 특성은 메타분석 시 자주 이용된다.\n이제 SMD의 수식에 대해 살펴보자. 참고로 통상적으로 SMD는 0.2 - 0.5일 경우 small, 0.5 - 0.8일 경우 medium, 0.8을 넘을 때 large라고 한다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#standardized-mean-difference-1",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#standardized-mean-difference-1",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "SMD는 크게 SMD for continuous or categorical baseline variables 두 가지가 있고, formula는 다음과 같다.\n\n\nContinuous baseline variable\n\\[\nd = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}\n\\]\n\n\n\\(\\overline{X}_1\\), \\(\\overline{X}_2\\)은 각 group의 baseline variable의 sample mean, \\(s_1, s_2\\)은 각 group의 sample variance를 의미한다. variable이 치우쳐 있다면(skewed), d를 rank statistics을 이용하여 modify하여 사용하면 된다.\n\n\nCategorical baseline variable\n1) Binary categorical variable\n\\[\nd = \\frac{\\hat{P}_1 - \\hat{P}_2}{\\sqrt{\\frac{\\hat{P}_1(1 - \\hat{P}_1) + \\hat{P}_2(1 - \\hat{P}_2)}{2}}}\n\\]\n\\(\\hat{P_1}, \\hat{P_2}\\)는 control/treatment group의 binary baseline variable의 proportion(mean)을 의미한다.\n2) Categorical variable with K levels\n\nTable 1처럼 baseline variable이 2개 초과의 level을 가지는 categorical variable의 SMD에 대해 살펴보자.\n\n\n\\[ T = (\\hat{P}_{12}, \\hat{P}_{13}, ..., \\hat{P}_{1K})'\\] \\[ C = (\\hat{P}_{22}, \\hat{P}_{23}, ..., \\hat{P}_{2K})'\\]\n이 때, \\(\\hat{P}_{JK} =\\) Pr(category k|treatment group j), $ j $ and \\(k \\in \\{2, 3, ..., K\\}\\)\nStandardized difference는 다음과 같이 정의된다.\n\\[\nd = \\sqrt{(T - C)'S^{-1}(T-C)}\n\\]\nS는 (k - 1)X(k - 1) covariance이며, 다음과 같다.\n\\[\nS = [S_{kl}] = \\begin{cases}\\dfrac{[\\hat{P}_{1k}(1-\\hat{P}_{1k})+\\hat{P}_{2k}(1-\\hat{P}_{2k})]}{2}, & k = l \\\\\\dfrac{[\\hat{P}_{1k}\\hat{P}_{1l}+\\hat{P}_{2k}\\hat{P}_{2l}]}{2}, & k \\neq l\\end{cases}\n\\]\n지금까지 SMD를 구하는 과정을 살펴보았다. RCT시 subject의 group 배정 후 혹은 propensity score analysis시 baseline covariates의 balance를 확인할 때, SMD를 이용한다. SMD가 0.1미만이면 작은 차이로 간주하고, 대부분의 변수에서 SMD 0.1미만이면서, 모두 0.2 미만이면 balance가 잘 맞는 것으로 평가한다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#confidence-interval-for-standardized-difference",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#confidence-interval-for-standardized-difference",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Hedges and Olkin (1985)에서 SMD에 대한 confidence interval 공식을 내놓았는데 식은 다음과 같다.\n\\[\nd\\;\\pm 1.96\\times\\sigma[d]\n\\]\n\\[\n\\sigma[d] = \\sqrt{\\frac{n_1 + n_2}{n_1 \\times n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\\]"
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#example",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#example",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "예제 데이터와 tableone package를 이용해 SMD를 구하는 과정을 살펴보자. tableone package에 대한 자세한 사용법이 궁금한다면 이 포스트를 참고하길 바란다.\n데이터를 불러오는 과정이다.\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 8.3.0 with Schannel\n\nlibrary(magrittr)\nlibrary(tableone)\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url,header=T)\n\nhead(dt)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n4:         2009  183321     200908           NA            NA           NA\n5:         2009  942671     200909           NA            NA           NA\n6:         2009  979358     200912           NA            NA           NA\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N HGHT\n1:           0            0           NA        3        1              0  144\n2:           0            0           NA        2        1              0  162\n3:           0            0           NA        3        1              0  163\n4:          NA           NA           NA        3        1              0  152\n5:          NA           NA           NA        3        1              0  159\n6:          NA           NA           NA        2        1              0  157\n   WGHT WSTC  BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT  HGB FBS TOT_CHOL  TG HDL\n1:   61   90 29.4   0.7   0.8    120     80        1 12.6 117      264 128  60\n2:   51   63 19.4   0.8   1.0    120     80        1 13.8  96      169  92  70\n3:   65   82 24.5   0.7   0.6    130     80        1 15.0 118      216 132  55\n4:   51   70 22.1   0.8   0.9    101     62        1 13.1  90      199 100  65\n5:   50   73 19.8   0.7   0.8    132     78        1 13.0  92      162  58  40\n6:   55   73 22.3   1.5   1.5    110     70        1 11.9 100      192 109  53\n   LDL CRTN SGOT SGPT GGT GFR\n1: 179  0.9   25   20  25  59\n2:  80  0.9   18   15  28  74\n3: 134  0.8   26   30  30  79\n4: 114  0.9   18   14  11  61\n5: 111  0.9   24   23  15  49\n6: 117  0.7   15   12  14  83\n\n\ntableone object를 생성한 뒤 print function을 이용해 결과물을 출력할 수 있다. 이때 smd = TRUE로 처리해주면 smd값이 같이 출력된다.\n\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n                       \n                        Overall        \n  n                       1644         \n  HGHT (mean (SD))      164.55 (9.19)  \n  WGHT (mean (SD))       65.10 (12.53) \n  BMI (mean (SD))        23.92 (3.38)  \n  HDL (mean (SD))        55.90 (19.47) \n  LDL (mean (SD))       118.69 (201.99)\n  TG (mean (SD))        134.90 (104.75)\n  SGPT (mean (SD))       25.98 (27.18) \n  Q_PHX_DX_STK = 1 (%)      12 ( 1.1)  \n  Q_PHX_DX_HTDZ = 1 (%)     26 ( 2.4)  \n  Q_HBV_AG (%)                         \n     1                      77 ( 4.7)  \n     2                    1102 (67.1)  \n     3                     463 (28.2)  \n  Q_SMK_YN (%)                         \n     1                     995 (60.6)  \n     2                     256 (15.6)  \n     3                     391 (23.8)  \n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nprint(t2, smd = T)\n\n                       Stratified by Q_SMK_YN\n                        1               2               3               p     \n  n                        995             256             391                \n  HGHT (mean (SD))      160.67 (8.34)   168.83 (6.45)   171.61 (7.09)   &lt;0.001\n  WGHT (mean (SD))       61.17 (11.08)   70.09 (10.72)   71.76 (13.07)  &lt;0.001\n  BMI (mean (SD))        23.63 (3.39)    24.52 (2.93)    24.27 (3.54)   &lt;0.001\n  HDL (mean (SD))        57.83 (14.08)   53.91 (36.73)   52.37 (13.54)  &lt;0.001\n  LDL (mean (SD))       112.26 (32.81)  147.52 (505.27) 116.34 (56.89)   0.046\n  TG (mean (SD))        114.05 (76.97)  162.89 (126.51) 169.24 (133.28) &lt;0.001\n  SGPT (mean (SD))       23.33 (28.42)   28.61 (20.62)   31.00 (26.96)  &lt;0.001\n  Q_PHX_DX_STK = 1 (%)      11 (  1.8)       1 (  0.5)       0 (  0.0)   0.051\n  Q_PHX_DX_HTDZ = 1 (%)     18 (  2.9)       5 (  2.6)       3 (  1.1)   0.287\n  Q_HBV_AG (%)                                                           0.193\n     1                      40 (  4.0)      19 (  7.5)      17 (  4.3)        \n     2                     679 ( 68.3)     164 ( 64.3)     259 ( 66.2)        \n     3                     275 ( 27.7)      72 ( 28.2)     115 ( 29.4)        \n  Q_SMK_YN (%)                                                          &lt;0.001\n     1                     995 (100.0)       0 (  0.0)       0 (  0.0)        \n     2                       0 (  0.0)     256 (100.0)       0 (  0.0)        \n     3                       0 (  0.0)       0 (  0.0)     391 (100.0)        \n                       Stratified by Q_SMK_YN\n                        test SMD   \n  n                                \n  HGHT (mean (SD))            0.972\n  WGHT (mean (SD))            0.611\n  BMI (mean (SD))             0.181\n  HDL (mean (SD))             0.197\n  LDL (mean (SD))             0.091\n  TG (mean (SD))              0.341\n  SGPT (mean (SD))            0.196\n  Q_PHX_DX_STK = 1 (%)        0.137\n  Q_PHX_DX_HTDZ = 1 (%)       0.084\n  Q_HBV_AG (%)                0.109\n     1                             \n     2                             \n     3                             \n  Q_SMK_YN (%)                  NaN\n     1                             \n     2                             \n     3                             \n\n\nQ_HBV_AG, Q_SMK_YN등의 3개의 level을 가지는 categorical variable도 하나의 SMD를 출력해줌을 알 수 있다. 참고로 tableone package에서는 strata가 3개 이상일 때도 하나의 SMD를 출력해준다. 이는 n개의 strata에 대해 pairwise로 \\(\\binom{n}{2}\\)개의의 SMD를 구한 뒤, 절댓값을 취하고, 평균을 낸 값이다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#reference",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#reference",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Yang, D. and Dalton, JE. (2012). A unified approach to measuring the effect size between two groups using SAS. SAS Global Forum 2012, Paper 335-2012.\nHedges LV, Olkin I. (1985). Statistical Methods for Meta-Analysis. Academic Press: San Diego, CA .\n남상건.(2015). 효과크기의 이해. Hanyang Medical Reviews. Paper 40-43\n이유진(2022). tableone 패키지 소개. 차라투 블로그"
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "두 그룹의 분포가 같은 지 다른 지 검정하기 위한 방법 중 Mann-Whitney U test라는 비모수검정법이 있다. 비모수적검정법이기에 population의 분포에 대한 가정을 하지 않는다. 첫 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(X_1, X_2, …, X_n\\)이고, 두 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(Y_1, Y_2, …, Y_m\\)이라고 하자. Mann-Whitney U test에서 Null hypothesis, Alternative hypothesis는 다음과 같다.\nNull hypothesis\n\\(H_0\\): 두 그룹의 분포가 동일하다.\nAlternative hypothesis 대립가설은 다음 중 한 가지이다.\n\\(H_1\\): 두 그룹의 분포가 다르다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 오른쪽에 있다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 왼쪽에 있다.\nMann-Whitney statistic은 다음과 같다.\n\\[\nU = \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} S(X_i, Y_j) =  R_1 - \\frac{n_1 (n_1 + 1)}{2}\n\\]\n\\[\n\\text{단, } S(X, Y) =\n\\begin{cases}\n1 & \\text{if } X &gt; Y \\\\\n\\frac{1}{2} & \\text{if } X = Y \\\\\n0 & \\text{if } X &lt; Y\n\\end{cases}\n\\]\n\\(R_1\\)은 첫 번째 그룹의 순위합을 나타낸다.\n이 때 U를 n1, n2로 나누어 정규화를 하면 그 값은 X &gt; Y일 확률을 추정하는 통계량이 되며 이는 AUC의 값과 동일하다.\n\\[AUC = \\frac{U}{n_1 n_2} = P(X &gt; Y) + \\frac{1}{2} P(X = Y)\n\\]\n시각적으로 왜 AUC와 정규화한 U statistic이 같은 지를 시각적으로 보여주는 그림이다. positive case와 negative case를 rank순으로 배치한 뒤, positive case만 모아 다시 그림을 그린다.\n\n왼쪽 직사각형에 속하는 부분의 넓이가 U statistic이 되며 이를 n1과 n2로 나누어주면 AUC가 됨을 확인할 수 있다.\n\n\n새로운 영상 의학의 판독법의 효과를 종래의 판독법과 성능을 비교해야 하는 상황을 생각해보자. 영상 의학 검사는 reader마다 양성임을 판단하는 threshold가 다르기 때문에, sensitivity와 specificity의 variability가 다를 수 밖에 없다. 그렇기에 그런 효과를 보정하기 위하여 Multi-Reader Multi-Case analysis가 도입되었다. Each case는 multiple readers에 review되며, each reader는 multiple cases를 review한다. 통계적으로는 모형에서 case와 reader의 효과를 random factor로 처리하고, 그에 따라 각각의 variability를 설명할 수 있다.\ndifferent modality의 성능평가 뿐만 아니라, machine learning algorithm result도 비교할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#auc와-mann-whitney-statistic-관계",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#auc와-mann-whitney-statistic-관계",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "두 그룹의 분포가 같은 지 다른 지 검정하기 위한 방법 중 Mann-Whitney U test라는 비모수검정법이 있다. 비모수적검정법이기에 population의 분포에 대한 가정을 하지 않는다. 첫 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(X_1, X_2, …, X_n\\)이고, 두 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(Y_1, Y_2, …, Y_m\\)이라고 하자. Mann-Whitney U test에서 Null hypothesis, Alternative hypothesis는 다음과 같다.\nNull hypothesis\n\\(H_0\\): 두 그룹의 분포가 동일하다.\nAlternative hypothesis 대립가설은 다음 중 한 가지이다.\n\\(H_1\\): 두 그룹의 분포가 다르다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 오른쪽에 있다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 왼쪽에 있다.\nMann-Whitney statistic은 다음과 같다.\n\\[\nU = \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} S(X_i, Y_j) =  R_1 - \\frac{n_1 (n_1 + 1)}{2}\n\\]\n\\[\n\\text{단, } S(X, Y) =\n\\begin{cases}\n1 & \\text{if } X &gt; Y \\\\\n\\frac{1}{2} & \\text{if } X = Y \\\\\n0 & \\text{if } X &lt; Y\n\\end{cases}\n\\]\n\\(R_1\\)은 첫 번째 그룹의 순위합을 나타낸다.\n이 때 U를 n1, n2로 나누어 정규화를 하면 그 값은 X &gt; Y일 확률을 추정하는 통계량이 되며 이는 AUC의 값과 동일하다.\n\\[AUC = \\frac{U}{n_1 n_2} = P(X &gt; Y) + \\frac{1}{2} P(X = Y)\n\\]\n시각적으로 왜 AUC와 정규화한 U statistic이 같은 지를 시각적으로 보여주는 그림이다. positive case와 negative case를 rank순으로 배치한 뒤, positive case만 모아 다시 그림을 그린다.\n\n왼쪽 직사각형에 속하는 부분의 넓이가 U statistic이 되며 이를 n1과 n2로 나누어주면 AUC가 됨을 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#motivation",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#motivation",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "새로운 영상 의학의 판독법의 효과를 종래의 판독법과 성능을 비교해야 하는 상황을 생각해보자. 영상 의학 검사는 reader마다 양성임을 판단하는 threshold가 다르기 때문에, sensitivity와 specificity의 variability가 다를 수 밖에 없다. 그렇기에 그런 효과를 보정하기 위하여 Multi-Reader Multi-Case analysis가 도입되었다. Each case는 multiple readers에 review되며, each reader는 multiple cases를 review한다. 통계적으로는 모형에서 case와 reader의 효과를 random factor로 처리하고, 그에 따라 각각의 variability를 설명할 수 있다.\ndifferent modality의 성능평가 뿐만 아니라, machine learning algorithm result도 비교할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#roe-and-metz-model",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#roe-and-metz-model",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "Roe and Metz model",
    "text": "Roe and Metz model\n\\[X_{ijk}^{R\\&M} = \\mu_t + \\tau_{it} + R_{jt} + C_{kt} + [RC]_{jkt} + [\\tau R]_{ijt} + [\\tau C]_{ikt} + [\\tau RC]_{ijkt} + E_{ijkt}\\]\n이때, \\(X_{ijk}\\)에서 i는 modality, j는 reader, case는 k, t는 truth state를 의미한다. modality와 truth state는 fixed factor이고, reader와 case는 random factor이다. 이외 나머지 term들은 고유한 variance를 가진independent zero-mean Gaussian random variables이다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#imrmc-package",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#imrmc-package",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "iMRMC package",
    "text": "iMRMC package\niMRMC는 MRMC analysis/simulation을 하기 위한 package로 다음의 주요한 2개의 function을 포함하고 있다.\n\ndoIMRMC: ROC data를 받아서 MRMC analysis를 수행하는 function\nsim.gRoeMetz: MRMC data를 simulation하여 two modalities를 비교할 수 있는 function\n\ncardiac CT study 예시 자료를 통해 MRMC를 수행해보자. colorScaleStudyData dataset은 다음 링크에서 다운로드 받을 수 있다.\n이 study는 Grayscale, Rainbow and Hotiron등 세 가지의 modalities를 포함한다. 경험이 적은 판독자를 Low, 경험이 많은 판독자를 High로 표시하였으며, Low는 1~8까지 High는 1~4까지 총 12명의 판독자가 포함되어 있다. 총 210 cases를 포함하며, 그중 양성은 105개이다. 실습에 필요한 data는 cardioCTReaderData(판독자의 판독 결과)와 cardioCTGroundTruth data(정답)이다. 그리고 이 study는fully crossed가 아닌 split-plot study이다.\n\nlibrary(data.table); library(magrittr)\nlibrary(ggplot2); library(knitr)\n\nload(\"data/cardioCTGroundTruth.rda\")\nload(\"data/cardioCTReaderData.rda\")\n\n\nsetDT(cardioCTGroundTruth)\nsetDT(cardioCTReaderData)\n\ncardioCTGroundTruth와 cardioCTReaderData는 다음과 같다.\n\nnames(cardioCTReaderData)[4] &lt;- \"score\"\nkable(cardioCTGroundTruth[1:30,], caption = \"cardioCTGroundTruth\", align = 'c' )\n\n\ncardioCTGroundTruth\n\ncaseID\ntruth\n\n\n\n1\npositive\n\n\n2\npositive\n\n\n3\npositive\n\n\n4\npositive\n\n\n5\npositive\n\n\n6\npositive\n\n\n7\npositive\n\n\n8\npositive\n\n\n9\npositive\n\n\n10\npositive\n\n\n11\npositive\n\n\n12\npositive\n\n\n13\npositive\n\n\n14\npositive\n\n\n15\npositive\n\n\n16\npositive\n\n\n17\npositive\n\n\n18\npositive\n\n\n19\npositive\n\n\n20\npositive\n\n\n21\npositive\n\n\n22\npositive\n\n\n23\npositive\n\n\n24\npositive\n\n\n25\npositive\n\n\n26\npositive\n\n\n27\npositive\n\n\n28\npositive\n\n\n29\npositive\n\n\n30\npositive\n\n\n\n\nkable(cardioCTReaderData[1:30,], caption = \"cardioCTReaderData\", align = 'c' )\n\n\ncardioCTReaderData\n\nreaderID\ncaseID\nmodalityID\nscore\n\n\n\nLow1\n141\nGrayscale\n31\n\n\nLow1\n142\nGrayscale\n54\n\n\nLow1\n143\nGrayscale\n88\n\n\nLow1\n144\nGrayscale\n88\n\n\nLow1\n145\nGrayscale\n54\n\n\nLow1\n146\nGrayscale\n32\n\n\nLow1\n147\nGrayscale\n55\n\n\nLow1\n148\nGrayscale\n33\n\n\nLow1\n149\nGrayscale\n66\n\n\nLow1\n150\nGrayscale\n99\n\n\nLow1\n151\nGrayscale\n75\n\n\nLow1\n152\nGrayscale\n54\n\n\nLow1\n153\nGrayscale\n21\n\n\nLow1\n154\nGrayscale\n66\n\n\nLow1\n155\nGrayscale\n88\n\n\nLow1\n156\nGrayscale\n22\n\n\nLow1\n157\nGrayscale\n42\n\n\nLow1\n158\nGrayscale\n76\n\n\nLow1\n159\nGrayscale\n32\n\n\nLow1\n160\nGrayscale\n75\n\n\nLow1\n161\nGrayscale\n65\n\n\nLow1\n162\nGrayscale\n65\n\n\nLow1\n163\nGrayscale\n55\n\n\nLow1\n164\nGrayscale\n32\n\n\nLow1\n165\nGrayscale\n52\n\n\nLow1\n166\nGrayscale\n77\n\n\nLow1\n167\nGrayscale\n20\n\n\nLow1\n168\nGrayscale\n87\n\n\nLow1\n169\nGrayscale\n65\n\n\nLow1\n170\nGrayscale\n42\n\n\n\n\n\ncardioCTGroundTruth에 readerID와 modalityID columns를 추가하고 truth를 positive = 1, negative = 0이 되게끔 변형해준다.\n\ncardioCTGroundTruth$readerID &lt;- factor(\"truth\")\ncardioCTGroundTruth$modalityID &lt;- factor(\"truth\")\ncardioCTGroundTruth$truth &lt;- as.numeric(cardioCTGroundTruth$truth) -1 #truth는 factor형 pos: 2 neg: 1\nnames(cardioCTGroundTruth)[names(cardioCTGroundTruth) == \"truth\"] &lt;- \"score\"\n\nkable(cardioCTGroundTruth[1:30,], caption = \"cardioCTGroundTruth\", align = 'c' )\n\n\ncardioCTGroundTruth\n\ncaseID\nscore\nreaderID\nmodalityID\n\n\n\n1\n1\ntruth\ntruth\n\n\n2\n1\ntruth\ntruth\n\n\n3\n1\ntruth\ntruth\n\n\n4\n1\ntruth\ntruth\n\n\n5\n1\ntruth\ntruth\n\n\n6\n1\ntruth\ntruth\n\n\n7\n1\ntruth\ntruth\n\n\n8\n1\ntruth\ntruth\n\n\n9\n1\ntruth\ntruth\n\n\n10\n1\ntruth\ntruth\n\n\n11\n1\ntruth\ntruth\n\n\n12\n1\ntruth\ntruth\n\n\n13\n1\ntruth\ntruth\n\n\n14\n1\ntruth\ntruth\n\n\n15\n1\ntruth\ntruth\n\n\n16\n1\ntruth\ntruth\n\n\n17\n1\ntruth\ntruth\n\n\n18\n1\ntruth\ntruth\n\n\n19\n1\ntruth\ntruth\n\n\n20\n1\ntruth\ntruth\n\n\n21\n1\ntruth\ntruth\n\n\n22\n1\ntruth\ntruth\n\n\n23\n1\ntruth\ntruth\n\n\n24\n1\ntruth\ntruth\n\n\n25\n1\ntruth\ntruth\n\n\n26\n1\ntruth\ntruth\n\n\n27\n1\ntruth\ntruth\n\n\n28\n1\ntruth\ntruth\n\n\n29\n1\ntruth\ntruth\n\n\n30\n1\ntruth\ntruth\n\n\n\n\n\n이제 cardioCTReaderData와 cardioCTGroundTruth dataset 두 개를 합쳐준다.\n\ncomb_data &lt;- merge.data.frame(cardioCTGroundTruth, cardioCTReaderData, by = names(cardioCTReaderData), all = TRUE, sort = FALSE)\nkable(comb_data[1:30,], caption = \"combined dataset\", align = 'c' )\n\n\ncombined dataset\n\nreaderID\ncaseID\nmodalityID\nscore\n\n\n\ntruth\n1\ntruth\n1\n\n\ntruth\n2\ntruth\n1\n\n\ntruth\n3\ntruth\n1\n\n\ntruth\n4\ntruth\n1\n\n\ntruth\n5\ntruth\n1\n\n\ntruth\n6\ntruth\n1\n\n\ntruth\n7\ntruth\n1\n\n\ntruth\n8\ntruth\n1\n\n\ntruth\n9\ntruth\n1\n\n\ntruth\n10\ntruth\n1\n\n\ntruth\n11\ntruth\n1\n\n\ntruth\n12\ntruth\n1\n\n\ntruth\n13\ntruth\n1\n\n\ntruth\n14\ntruth\n1\n\n\ntruth\n15\ntruth\n1\n\n\ntruth\n16\ntruth\n1\n\n\ntruth\n17\ntruth\n1\n\n\ntruth\n18\ntruth\n1\n\n\ntruth\n19\ntruth\n1\n\n\ntruth\n20\ntruth\n1\n\n\ntruth\n21\ntruth\n1\n\n\ntruth\n22\ntruth\n1\n\n\ntruth\n23\ntruth\n1\n\n\ntruth\n24\ntruth\n1\n\n\ntruth\n25\ntruth\n1\n\n\ntruth\n26\ntruth\n1\n\n\ntruth\n27\ntruth\n1\n\n\ntruth\n28\ntruth\n1\n\n\ntruth\n29\ntruth\n1\n\n\ntruth\n30\ntruth\n1\n\n\n\n\n\n이제 iMRMC 패키지를 불러오고 doIMRMC함수를 이용하여 MRMC analysis를 수행한다. iMRMC에선 Ustat은 non-parametric estimation에 의한 result를 나타내며, MLEstat은 각각 parameter의 maximum likelihood estimation을 나타낸다.\n\nlibrary(iMRMC)\n\nWarning: package 'iMRMC' was built under R version 4.4.2\n\nresult &lt;- doIMRMC(data = comb_data)\n\nAUCDf &lt;- data.frame(rbind(result$MLEstat$AUCA[1:3],result$MLEstat$varAUCA[1:3], sqrt(result$MLEstat$varAUCA[1:3])),\nrow.names = c(\"AUC\", \"variance of AUC\", \"SE of AUC\"))\nnames(AUCDf) &lt;- result$MLEstat$modalityA[1:3]\nkable(AUCDf, caption = \"AUC for different modalities : MLEstat\", align = 'c' )\n\n\nAUC for different modalities : MLEstat\n\n\nGrayscale\nHot\nRainbow\n\n\n\nAUC\n0.5902954\n0.5671724\n0.5176793\n\n\nvariance of AUC\n0.0015535\n0.0008297\n0.0009073\n\n\nSE of AUC\n0.0394148\n0.0288037\n0.0301211\n\n\n\n\n\n\nAUCDf &lt;- data.frame(rbind(result$Ustat$AUCA[1:3],result$Ustat$varAUCA[1:3], sqrt(result$Ustat$varAUCA[1:3])),\nrow.names = c(\"AUC\", \"variance of AUC\", \"SE of AUC\"))\n\nWarning in sqrt(result$Ustat$varAUCA[1:3]): NaNs produced\n\nnames(AUCDf) &lt;- result$Ustat$modalityA[1:3] \nkable(AUCDf, caption = \"AUC for different modalities : Ustat\", align = 'c' )\n\n\nAUC for different modalities : Ustat\n\n\nGrayscale\nHot\nRainbow\n\n\n\nAUC\n0.5902954\n0.5671724\n0.5176793\n\n\nvariance of AUC\n0.0010402\n-0.0000249\n0.0000338\n\n\nSE of AUC\n0.0322528\nNaN\n0.0058162\n\n\n\n\n\ndifferent modalities에 대해 AUC의 difference도 Ustat, MLEstat 두 가지 방법으로 estimation할 수 있다.\n\nAUCDf &lt;- data.frame(rbind(result$MLEstat$AUCAminusAUCB[4:6],result$MLEstat$varAUCAminusAUCB[4:6], sqrt(result$MLEstat$varAUCAminusAUCB[4:6]),\nresult$MLEstat$AUCAminusAUCB[4:6] - 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[4:6]), result$MLEstat$AUCAminusAUCB[4:6] + 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[4:6])), row.names = c(\"difference of AUC\", \"variance of difference of AUC\", \"SE of different of AUC\", \"95% CI lower bound\", \"95% CI upper bound\"))\n\nnames(AUCDf) &lt;- paste(result$MLEstat$modalityA[4:6], result$MLEstat$modalityB[4:6], sep = \" vs. \") \nkable(AUCDf, caption = \"Difference of AUC among different modalities : MLEstat\", align = 'c' )\n\n\nDifference of AUC among different modalities : MLEstat\n\n\n\n\n\n\n\n\nGrayscale vs. Hot\nGrayscale vs. Rainbow\nHot vs. Rainbow\n\n\n\ndifference of AUC\n0.0231230\n0.0726161\n0.0494932\n\n\nvariance of difference of AUC\n0.0021459\n0.0024036\n0.0019731\n\n\nSE of different of AUC\n0.0463239\n0.0490261\n0.0444191\n\n\n95% CI lower bound\n-0.0676719\n-0.0234750\n-0.0375683\n\n\n95% CI upper bound\n0.1139178\n0.1687072\n0.1365547\n\n\n\n\n\n\nAUCDf &lt;- data.frame(rbind(result$Ustat$AUCAminusAUCB[4:6],result$Ustat$varAUCAminusAUCB[4:6], sqrt(result$Ustat$varAUCAminusAUCB[4:6]),\n                          result$Ustat$AUCAminusAUCB[4:6] - 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[4:6]), \n                          result$Ustat$AUCAminusAUCB[4:6] + 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[4:6])), \n                    row.names = c(\"difference of AUC\", \"variance of difference of AUC\", \"SE of different of AUC\", \"95% CI lower bound\", \"95% CI upper bound\"))\n\nnames(AUCDf) &lt;- paste(result$Ustat$modalityA[4:6], result$Ustat$modalityB[4:6], sep = \" vs. \") \nkable(AUCDf, caption = \"Difference of AUC among different modalities : Ustat\", align = 'c' )\n\n\nDifference of AUC among different modalities : Ustat\n\n\n\n\n\n\n\n\nGrayscale vs. Hot\nGrayscale vs. Rainbow\nHot vs. Rainbow\n\n\n\ndifference of AUC\n0.0231230\n0.0726161\n0.0494932\n\n\nvariance of difference of AUC\n0.0012101\n0.0014347\n0.0010837\n\n\nSE of different of AUC\n0.0347870\n0.0378770\n0.0329203\n\n\n95% CI lower bound\n-0.0450596\n-0.0016227\n-0.0150307\n\n\n95% CI upper bound\n0.0913055\n0.1468550\n0.1140170\n\n\n\n\n\n이제 Roe and Metz model을 사용하여, MRMC analysis simulation을 위한 dataset을 만들어보자. 이때 sim.gRoeMetz.config, sim.gRoemetz 두 함수를 이용하여, simulated dataset을 만들 수 있다.\nsim.gRoeMetz.config 함수의 arguments는 다음과 같으며, 이를 통해 Roe and Metz model의 fixed/random effect를 조절할 수 있다.\nsim.gRoeMetz.config의 argument를 통해 Roe and Metz model을 조절할 수 있다. sim.gRoemetz(config)함수는 sim.gRoeMetz.config를 통해 생성된 configuration object를 이용하여, simulated data를 생성한다.\nconfig에 저장된 simulation parameter는 에 대한 자세한 설명은 iMRMC package확인할 수 있다.\n이제, 이 두 함수를 이용하여 MRMC simulation을 수행한다.\n\nlibrary(ggplot2)\n\nconfig &lt;- sim.gRoeMetz.config() # Create a sample configuration file\ndf.MRMC &lt;- sim.gRoeMetz(config) # Simulate an MRMC data set\n\n이 두 함수를 통해 simulated dataset을 visualization 시켜보았다.\n\nggplot(subset(df.MRMC, modalityID %in% c(\"testA\", \"testB\")),\n       aes(x = score,\n           color = factor(unlist(lapply(as.character(caseID),\n                                        function(x) {strsplit(x, 'Case')[[1]][1]}))))) +\n  geom_density(position = \"identity\", alpha = 0.2) +\n  facet_grid(rows = \"modalityID\") +\n  labs(x = \"MRMC Reading Score\", y = \"Density\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\nMRMC analysis를 수행해본 결과는 다음과 같다.\n\nresult &lt;- doIMRMC(df.MRMC)\n\nAUCDf &lt;- rbind(result$MLEstat$AUCA[1:2],\n      result$MLEstat$varAUCA[1:2],\n      sqrt(result$MLEstat$varAUCA[1:2])) %&gt;% as.data.frame()\nrownames(AUCDf) &lt;- c(\"AUC\", \"variance of AUC\", \"SE of AUC\")\nnames(AUCDf) &lt;- paste(\"test\", c(\"A\",\"B\"))\nAUCDf$`test A vs test B` &lt;- c(result$MLEstat$AUCAminusAUCB[3], result$MLEstat$varAUCAminusAUCB[3], sqrt(result$MLEstat$varAUCAminusAUCB[3]))\n\nkable(AUCDf, caption = \"MRMC analysis result with simulated data : MLEstat\", align = 'c')\n\n\nMRMC analysis result with simulated data : MLEstat\n\n\ntest A\ntest B\ntest A vs test B\n\n\n\nAUC\n0.8411250\n0.8797500\n-0.0386250\n\n\nvariance of AUC\n0.0016160\n0.0011408\n0.0024655\n\n\nSE of AUC\n0.0401991\n0.0337760\n0.0496538\n\n\n\n\n\nReference\n\n박서영, & 김화영. (2023). 바이오통계학. 한국방송통신대학교출판문화원.\nBischl et al. Chapter 04.13: AUC & Mann-Whitney-U Test Introduction to Machine Learning\n\nWen et al. R Data Packages of Multi-Reader Multi-Case Studies and Simulation Tools to Support the Development of Reader Performance Evaluation Methods Wen\n\nClarkson et al. (2006). A Probabilistic Model for the MRMC Method. Part 1. Theoretical Development. Acad Radiol.\ncolorScaleStudyData"
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#footnotes",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#footnotes",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nClarkson et al. (2006). A Probabilistic Model for the MRMC Method. Part 1. Theoretical Development. Acad Radiol.↩︎"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html",
    "href": "posts/2024-12-06-tidyplots/index.html",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots는 ggplot2과 비슷하게 R에서 데이터를 이용하여 그래프를 그릴 수 있게 해 주는 라이브러리이다. tidyplots는 Rstudio에서 다음과 같이 설치 후 실행할 수 있다.\n\ninstall.packages('tidyplots')\nlibrary(tidyplots)\n\n\nlibrary(tidyplots)\nlibrary(DT)\nlibrary(magrittr)\nlibrary(ggplot2)\n\n\ntidyplots와 ggplot2는 그래프를 그릴 수 있다는 점은 동일하고 실제로 tidyplots object는 ggplot object와 동일하게 다른 작업이 모두 가능하다. 하지만 코드를 작성함에 있어서 차이점이 있다. study dataset의 treatment, score 컬럼으로 그래프를 그리는 코드를 통해 비교해 보자.\n\n데이터 확인\n\n\ndt &lt;- study\nhead(dt)\n\n# A tibble: 6 × 7\n  treatment group   dose  participant   age sex    score\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 A         placebo high  p01            23 female     2\n2 A         placebo high  p02            45 male       4\n3 A         placebo high  p03            32 female     5\n4 A         placebo high  p04            37 male       4\n5 A         placebo high  p05            24 female     6\n6 B         placebo low   p06            23 female     9\n\n\n\nggplot2로 작성한 코드\n\n\nggplot(dt, aes(x = treatment, y = score)) +\n  geom_boxplot() +   \n  geom_jitter(width = 0.2, color = \"blue\", alpha = 0.5) +  \n  labs(title = \"Treatment vs Score\",\n       x = \"Treatment\",\n       y = \"Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntidyplots로 작성한 코드\n\n\ndt %&gt;% \n  tidyplot(x = treatment, y = score) %&gt;% \n  add_data_points_jitter() %&gt;% \n  adjust_title(\"Treatment vs Score\") %&gt;% \n  adjust_x_axis_title(\"Treatment\") %&gt;% \n  adjust_y_axis_title(\"Score\") %&gt;%\n  add_boxplot(alpha = 0) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n코드로 확인할 수 있듯이 “+” 기호로 코드를 잇는 ggplot2와는 다르게, tidyplots는 공통적으로 ’tidyplot’이라는 코드로 그래프를 선언한 다음, 그 뒤에 파이프 연산자 %&gt;%를 이어서 그래프를 구체화시킬 수 있다. 각각의 함수의 이름에 기능이 직관적으로 잘 드러나 있기에 쉽게 그래프 코드를 작성할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#tidyplots",
    "href": "posts/2024-12-06-tidyplots/index.html#tidyplots",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots는 ggplot2과 비슷하게 R에서 데이터를 이용하여 그래프를 그릴 수 있게 해 주는 라이브러리이다. tidyplots는 Rstudio에서 다음과 같이 설치 후 실행할 수 있다.\n\ninstall.packages('tidyplots')\nlibrary(tidyplots)\n\n\nlibrary(tidyplots)\nlibrary(DT)\nlibrary(magrittr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#ggplot2와의-차이점",
    "href": "posts/2024-12-06-tidyplots/index.html#ggplot2와의-차이점",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots와 ggplot2는 그래프를 그릴 수 있다는 점은 동일하고 실제로 tidyplots object는 ggplot object와 동일하게 다른 작업이 모두 가능하다. 하지만 코드를 작성함에 있어서 차이점이 있다. study dataset의 treatment, score 컬럼으로 그래프를 그리는 코드를 통해 비교해 보자.\n\n데이터 확인\n\n\ndt &lt;- study\nhead(dt)\n\n# A tibble: 6 × 7\n  treatment group   dose  participant   age sex    score\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 A         placebo high  p01            23 female     2\n2 A         placebo high  p02            45 male       4\n3 A         placebo high  p03            32 female     5\n4 A         placebo high  p04            37 male       4\n5 A         placebo high  p05            24 female     6\n6 B         placebo low   p06            23 female     9\n\n\n\nggplot2로 작성한 코드\n\n\nggplot(dt, aes(x = treatment, y = score)) +\n  geom_boxplot() +   \n  geom_jitter(width = 0.2, color = \"blue\", alpha = 0.5) +  \n  labs(title = \"Treatment vs Score\",\n       x = \"Treatment\",\n       y = \"Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntidyplots로 작성한 코드\n\n\ndt %&gt;% \n  tidyplot(x = treatment, y = score) %&gt;% \n  add_data_points_jitter() %&gt;% \n  adjust_title(\"Treatment vs Score\") %&gt;% \n  adjust_x_axis_title(\"Treatment\") %&gt;% \n  adjust_y_axis_title(\"Score\") %&gt;%\n  add_boxplot(alpha = 0) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n코드로 확인할 수 있듯이 “+” 기호로 코드를 잇는 ggplot2와는 다르게, tidyplots는 공통적으로 ’tidyplot’이라는 코드로 그래프를 선언한 다음, 그 뒤에 파이프 연산자 %&gt;%를 이어서 그래프를 구체화시킬 수 있다. 각각의 함수의 이름에 기능이 직관적으로 잘 드러나 있기에 쉽게 그래프 코드를 작성할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#plotting",
    "href": "posts/2024-12-06-tidyplots/index.html#plotting",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "1. Plotting",
    "text": "1. Plotting\ntidyplots는 기본적으로 항상 데이터와 함께 tidyplot()으로 그래프를 선언한 후 코드를 작성해야 한다. 아래의 예시는 모두 같은 결과를 수행한다. 예시에는 tidyplots 패키지에서 기본으로 제공하는 study dataset을 이용하였다.\n\n# example 1\nplot1 &lt;- tidyplot(study, x = treatment, y = score)\n\n# example 2\nplot2 &lt;- study %&gt;% \n  tidyplot(x = treatment, y = score)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-add",
    "href": "posts/2024-12-06-tidyplots/index.html#method-add",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "2. Method : Add",
    "text": "2. Method : Add\ntidyplots에서 그래프를 구체화하는 작업은 이후 %&gt;% 연산자를 이용하여 연결한다. tidyplots의 대부분의 함수는 add, adjust, remove로 구분할 수 있으며, 이 섹션에서는 그래프의 요소를 더하는 add 에 해당하는 대표적인 기능에 대해 알아본다.\nadd_data_points\n각각의 데이터들을 그래프 상에 표시하고 싶다면 add_data_points()를 사용하면 된다. 목적에 따라 add_data_points_jitter(), add_data_points_beeswarm() 옵션도 지원한다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points_jitter() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points_beeswarm() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n이외에도 함수 내부에서 다양한 argument를 조정하여 커스터마이징할 수 있다.\nadd_count, add_sum, add_mean, add_median\ntidyplots에서는 그래프를 그리고 난 이후 다양한 지표요소를 그래프에 추가할 수 있는 기능을 제공한다. 종류는 mean, median, sum, count가 있고, value를 통해 직접 값을 표시할 수도 있고 bar, dash, line, area 등 다양한 형식으로 표현할 수 있다. 가령 데이터의 평균값을 bar의 형태로 표시하고 싶다면 add_mean_bar() 함수를 쓰면 된다. 예시를 통해 알아보자.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_dash() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_dot() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_value() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_area() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_bar(alpha = 0.4) %&gt;%\n  add_mean_dash() %&gt;%\n  add_mean_dot() %&gt;%\n  add_mean_value() %&gt;%\n  add_mean_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n기타\n이외에도 add_test_pvalue()를 통해 \"wilcox_test\", \"t_test\", \"sign_test\", \"dunn_test\", \"emmeans_test\", \"tukey_hsd\", \"games_howell_test\"등의 통계 검정 결과를 추가하거나 add_data_labels(), add_title() 등으로 차트 요소를 추가할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-adjust",
    "href": "posts/2024-12-06-tidyplots/index.html#method-adjust",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "3. Method : Adjust",
    "text": "3. Method : Adjust\n이 섹션에서는 그래프의 요소를 수정하는 adjust 에 해당하는 대표적인 기능에 대해 알아본다.\n1. adjust_title, adjust_axis_title, adjust_caption\n그래프에서 제목이나, 축 제목을 수정해야 할 때 위의 함수들을 이용할 수 있다. 아래와 같은 방식으로 그래프의 요소를 추가한 후, 바로 파이프를 이어서 수정하고자 하는 제목, 축에 대해 값을 입력하면 된다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points() %&gt;%\n  add_mean_bar(alpha = 0.4) %&gt;%\n  add_sem_errorbar() %&gt;%\n  adjust_title(\"This is my fantastic plot title\") %&gt;%\n  adjust_x_axis_title(\"Treatment group\") %&gt;%\n  adjust_y_axis_title(\"Disease score\") %&gt;%\n  adjust_legend_title(\"Legend title\") %&gt;%\n  adjust_caption(\"Here goes the caption\") %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n2. adjust_padding, adjust_axis\n그래프에서 여백을 조정해야 할 때, adjust_padding()함수를 사용하면 도움이 된다. 또한 adjust_axis()를 사용하면 각각의 축에 대하여 세세한 수정이 가능하며, parameter와 사용 예시는 아래에 첨부한다.\n\nadjust_x_axis(\n  plot,\n  title = ggplot2::waiver(),\n  breaks = ggplot2::waiver(),\n  labels = ggplot2::waiver(),\n  limits = NULL,\n  padding = c(NA, NA),\n  rotate_labels = FALSE,\n  transform = \"identity\",\n  cut_short_scale = FALSE,\n  force_continuous = FALSE,\n  ...\n)\n# y axis도 동일한 argument를 가짐\n\nbreaks, labels, limits는 ggplot2와 동일한 방식으로 수정할 수 있다. transform의 경우 필요하다면 log, date, exp 등 여러 가지 옵션으로 수정할 수 있다.\n\n# Axes limits\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(limits = c(-1000, 4000)) %&gt;%\n  adjust_y_axis(limits = c(-200, 600)) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Rotate labels\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(rotate_labels = 90) %&gt;%\n  adjust_y_axis(rotate_labels = 90) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Increase plot area padding\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(padding = c(0.2, 0.2)) %&gt;%\n  adjust_y_axis(padding = c(0.2, 0.2)) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Scale transformation\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(transform = \"log10\") %&gt;%\n  adjust_y_axis(transform = \"log2\") %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-remove",
    "href": "posts/2024-12-06-tidyplots/index.html#method-remove",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "4. Method : Remove",
    "text": "4. Method : Remove\n이 섹션에서는 그래프의 요소를 수정하는 remove에 해당하는 대표적인 기능에 대해 알아본다.\n1. remove_padding\n그래프를 그리다 보면 x축, y축 여백이 필요하지 않은 경우가 있는데, 이 경우 remove_padding()을 넣으면 바로 해결할 수 있다.\n\n# Before removing\nanimals %&gt;%\n  tidyplot(x = weight, y = speed, color = family) %&gt;%\n  add_data_points() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# After removing\nanimals %&gt;%\n  tidyplot(x = weight, y = speed, color = family) %&gt;%\n  add_data_points() %&gt;%\n  remove_padding() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n2. remove_title, remove_caption, remove_axis_\n제목, 캡션, 범례 뿐만 아니라 축과 관련된 요소 또한 선택적으로 제거할 수 있다.\n\n# Before removing\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# After removing\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_ticks() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_labels() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_title() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis() %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#ggplot-to-tidyplot-add",
    "href": "posts/2024-12-06-tidyplots/index.html#ggplot-to-tidyplot-add",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "1. ggplot to tidyplot, add",
    "text": "1. ggplot to tidyplot, add\nggplot2 라이브러리에서는 그래프의 세부 요소를 매우 다양하게 조정할 수 있는데, as_tidyplot()을 통해 미리 만들어 놓은 ggplot object를 tidyplots object로 변환하여 tidyplots 문법으로 나머지 작업을 시행할 수 있다.\n\ngg &lt;- ggplot(study, aes(x = treatment, y = score, color = treatment)) +\n  geom_point()\n\ngg\n\n\n\n\n\n\ngg %&gt;% as_tidyplot() %&gt;% \n  remove_legend() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n또한 tidyplot 코드 내부에서 ggplot2의 기능을 사용하고 싶을때는 add::ggplot2()함수를 사용하여 파이프로 연결하여 적용할 수 있다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add(ggplot2::geom_point()) %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#split",
    "href": "posts/2024-12-06-tidyplots/index.html#split",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "2. split",
    "text": "2. split\ntidyplots에서 제공하는 기능 중 하나는 그래프를 그린 후. 특정 컬럼을 기준으로 그룹핑 후 그래프를 분리할 수 있는 기능이다. 이는 split_plot()으로 시행 가능하다. argument는 다음과 같다.\n\nsplit_plot(\n  plot,\n  by,\n  ncol = NULL,\n  nrow = NULL,\n  byrow = NULL,\n  widths = 30,\n  heights = 25,\n  guides = \"collect\",\n  tag_level = NULL,\n  design = NULL,\n  unit = \"mm\"\n)\n\n특정 컬럼을 지정하면 그 컬럼을 기준으로 그래프가 분리되며, nrow, ncol을 지정하면 페이지가 분리가 된다. 주의할 점은 split_plot()은 코드의 맨 뒤에 위치해야 한다는 점이다.\n\n# Before splitting\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Split by year\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year)\n\n✔ split_plot: split into 4 plots across 1 page\n\n\n\n\n\n\n\n# Spread plots across multiple pages\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year, ncol = 2, nrow = 1) \n\n✔ split_plot: split into 4 plots across 2 pages\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n[[2]]"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#vector-graphics",
    "href": "posts/2024-12-06-tidyplots/index.html#vector-graphics",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "3. vector graphics",
    "text": "3. vector graphics\ntidyplots object도 ggplot2 object와. rvg 패키지로 동일하게 그래프를 벡터 그래픽으로 변환 후 officer 패키지와 연계하여 수정 가능한 pptx 파일로 저장할 수 있다. 그 예시를 소개한다.\n\nlibrary(officer)\nlibrary(rvg)\n\nplot3 &lt;- energy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year) \n\nppt &lt;- read_pptx() %&gt;% \n  add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;% \n  ph_with(dml(ggobj = plot3), location = ph_location_type(type = \"body\")) %&gt;% \n  print(target = \"plot.pptx\")"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#setup",
    "href": "posts/2024-10-28-Collapse/index.html#setup",
    "title": "collapse 패키지 소개 v2",
    "section": "Setup",
    "text": "Setup\n\n##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr);library(dplyr);library(data.table) \n\nlibrary(collapse);library(microbenchmark);library(lmtest)"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#basic",
    "href": "posts/2024-10-28-Collapse/index.html#basic",
    "title": "collapse 패키지 소개 v2",
    "section": "Basic",
    "text": "Basic\ndata.table처럼 fread & fwrite를 이용하여 csv파일을 처리한다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\nfwrite(dt, \"aa.csv\")\n\nColumns: ‘fselect’로 원하는 열을 불러올 수 있다.\n\nfselect(dt, 1:3, 13:16) |&gt; head()\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  334536     200911   162    51    63  19.4\n3:         2009  911867     200903   163    65    82  24.5\n4:         2009  183321     200908   152    51    70  22.1\n5:         2009  942671     200909   159    50    73  19.8\n6:         2009  979358     200912   157    55    73  22.3\n\nfselect(dt, EXMD_BZ_YYYY,RN_INDI,HME_YYYYMM )|&gt; head() # fselect(dt, \"EXMD_BZ_YYYY\",\"RN_INDI\",\"HME_YYYYMM\" )\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt;\n1:         2009  562083     200909\n2:         2009  334536     200911\n3:         2009  911867     200903\n4:         2009  183321     200908\n5:         2009  942671     200909\n6:         2009  979358     200912\n\n\nRows: ‘fsubset()’로 원하는 행/열을 불러올 수 있다.\n\nfsubset(dt, 1:3)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt;        &lt;int&gt;         &lt;int&gt;        &lt;int&gt;\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N  HGHT\n         &lt;int&gt;        &lt;int&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;          &lt;int&gt; &lt;int&gt;\n1:           0            0           NA        3        1              0   144\n2:           0            0           NA        2        1              0   162\n3:           0            0           NA        3        1              0   163\n    WGHT  WSTC   BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT   HGB   FBS TOT_CHOL\n   &lt;int&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt; &lt;num&gt; &lt;int&gt;    &lt;int&gt;\n1:    61    90  29.4   0.7   0.8    120     80        1  12.6   117      264\n2:    51    63  19.4   0.8   1.0    120     80        1  13.8    96      169\n3:    65    82  24.5   0.7   0.6    130     80        1  15.0   118      216\n      TG   HDL   LDL  CRTN  SGOT  SGPT   GGT   GFR\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:   128    60   179   0.9    25    20    25    59\n2:    92    70    80   0.9    18    15    28    74\n3:   132    55   134   0.8    26    30    30    79\n\n#fsubset(dt, c(1:3, 13:16)) #rows\nfsubset(dt, 1:3, 13:16)  #(dt, row, col)\n\n    HGHT  WGHT  WSTC   BMI\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:   144    61    90  29.4\n2:   162    51    63  19.4\n3:   163    65    82  24.5\n\nfsubset(dt, c(1:nrow(dt)),c(1:3, 13:16)) |&gt; head() #cols\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  334536     200911   162    51    63  19.4\n3:         2009  911867     200903   163    65    82  24.5\n4:         2009  183321     200908   152    51    70  22.1\n5:         2009  942671     200909   159    50    73  19.8\n6:         2009  979358     200912   157    55    73  22.3\n\n# fsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) %&gt;%  fsubset(c(1:3),c(1:3,13:16))\nfsubset(dt, c(1:nrow(dt)),c(1:3, 13:16)) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) |&gt; head() # same\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  318669     200904   155    66    78  27.5\n3:         2009  668438     200904   160    71    94  27.7\n4:         2009  560878     200903   144    58    93  28.0\n5:         2009  375694     200906   151    70    94  30.7\n6:         2009  446652     200909   158    64    80  25.6\n\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) %&gt;%\n  fsubset(c(1:nrow(dt)),c(1:3,13:16)) |&gt; head()\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  560878     200903   144    58    93  28.0\n3:         2011  562083     201111   144    59    88  28.5\n4:         2011  519824     201109   145    58    79  27.6\n5:         2011  914987     201103   145    70    95  33.3\n6:         2012  560878     201208   145    59    85  28.1"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#fast-statistical-function",
    "href": "posts/2024-10-28-Collapse/index.html#fast-statistical-function",
    "title": "collapse 패키지 소개 v2",
    "section": "Fast Statistical Function",
    "text": "Fast Statistical Function\n\n.FAST_STAT_FUN\n # [1]  \"fmean\"      \"fmedian\"    \"fmode\"      \"fsum\"       \"fprod\"      \n # [6]  \"fsd\"        \"fvar\"       \"fmin\"       \"fmax\"       \"fnth\"       \n # [11] \"ffirst\"     \"flast\"      \"fnobs\"      \"fndistinct\"\n\n# 데이터 구조에 구애받지않음.\nv1 &lt;- c(1,2,3,4)\nm1 &lt;- matrix(1:50, nrow = 10, ncol = 5)\n \nfmean(v1); fmean(m1); fmean(dt)\nfmode(v1); fmode(m1); fmode(dt)\n# fmean(m1): by columns\n\n\n# collapse; baseR과 비교했을 때 보다 빠른 속도를 보인다.\nx &lt;- rnorm(1e7)\nmicrobenchmark(mean(x), fmean(x), fmean(x, nthreads = 4)) \n\nUnit: milliseconds\n                   expr       min        lq      mean    median        uq\n                mean(x) 23.761096 23.786943 23.802908 23.799427 23.815928\n               fmean(x) 15.332085 15.367978 15.388554 15.387914 15.404170\n fmean(x, nthreads = 4)  4.217606  6.684896  7.634676  7.741456  8.499509\n      max neval cld\n 23.94914   100 a  \n 15.57999   100  b \n 11.20740   100   c\n\nmicrobenchmark(colMeans(dt), sapply(dt, mean), fmean(dt))\n\nUnit: microseconds\n             expr      min       lq       mean   median        uq      max\n     colMeans(dt) 3154.750 3302.700 3300.31781 3307.968 3312.7130 3641.641\n sapply(dt, mean)  190.076  199.417  208.52219  206.010  215.9145  318.417\n        fmean(dt)   52.889   53.803   56.23603   55.644   56.8805   90.947\n neval cld\n   100 a  \n   100  b \n   100   c\n\n\n\nData size가 더 클 경우, 보다 유용하다. (GGDC10S: 5000rows, 11cols, ~10% missing values)\n\n\n\nmicrobenchmark(base = sapply(GGDC10S[6:16], mean, na.rm = TRUE), fmean(GGDC10S[6:16]))\n\nUnit: microseconds\n                 expr     min      lq     mean   median       uq      max neval\n                 base 412.369 429.161 773.8810 807.4445 818.8705 7949.178   100\n fmean(GGDC10S[6:16])  94.481  95.856 102.7777 103.9790 108.0860  142.060   100\n cld\n  a \n   b\n\n\n\n이처럼, Collapse는 data 형식에 구애받지 않고, 보다 빠른 속도를 특징으로 하는 package이다.\n\n이들의 문법을 알아보자.\n-   Fast Statistical Functions\n\n  Syntax:\n\nFUN(x, g = NULL, \\[w = NULL,\\] TRA = NULL, \\[na.rm = TRUE\\], use.g.names = TRUE, \\[drop = TRUE,\\] \\[nthreads = 1,\\] ...)\n\n       \nArgument            Description\n      g             grouping vectors / lists of vectors or ’GRP’ object\n      w             a vector of (frequency) weights\n    TRA             a quoted operation to transform x using the statistics\n  na.rm             efficiently skips missing values in x\n  use.g.names       generate names/row-names from g\n  drop              drop dimensions if g = TRA = NULL\n  nthreads          number of threads for OpenMP multithreading\n사용예시 : fmean\n\n# Weighted Mean\nw &lt;- abs(rnorm(nrow(iris)))\nall.equal(fmean(num_vars(iris), w = w), sapply(num_vars(iris), weighted.mean, w = w))\n\n[1] TRUE\n\nwNA &lt;- na_insert(w, prop = 0.05)\nsapply(num_vars(iris), weighted.mean, w = wNA) # weighted.mean(): 결측치를 처리하지 못한다.\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n          NA           NA           NA           NA \n\nfmean(num_vars(iris), w = wNA) #결측치를 자동으로 무시한다.\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.797389     3.048473     3.683776     1.151507 \n\n# Grouped Mean\nfmean(iris$Sepal.Length, g = iris$Species)\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\nfmean(num_vars(iris), iris$Species)  \n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\n# Weighted Group Mean\nfmean(num_vars(iris), iris$Species, w)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa         5.015518    3.460443     1.479887   0.2495797\nversicolor     5.918636    2.698947     4.259102   1.2888099\nvirginica      6.568402    2.959146     5.577613   2.0433786\n\n# 속도 상의 이점. \nmicrobenchmark(fmean = fmean(iris$Sepal.Length, iris$Species),\n               tapply = tapply(iris$Sepal.Length, iris$Species, mean))\n\nUnit: microseconds\n   expr    min      lq     mean  median      uq     max neval cld\n  fmean  7.488  7.8230  8.49842  8.2905  8.5785  32.276   100  a \n tapply 46.609 47.8695 49.83706 48.5615 48.9820 152.046   100   b"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#consideration-w-missing-data-결측치-처리",
    "href": "posts/2024-10-28-Collapse/index.html#consideration-w-missing-data-결측치-처리",
    "title": "collapse 패키지 소개 v2",
    "section": "Consideration w/ missing data: 결측치 처리",
    "text": "Consideration w/ missing data: 결측치 처리\n\n#wlddev$GINI, g: country, function: mean, median, min, max, sum, prod\ncollap(wlddev, GINI ~ country, list(mean, median, min, max, sum, prod),\n       na.rm = TRUE, give.names = FALSE) |&gt; head()\n\n         country     mean median  min  max   sum         prod\n1    Afghanistan      NaN     NA  Inf -Inf   0.0 1.000000e+00\n2        Albania 31.41111   31.7 27.0 34.6 282.7 2.902042e+13\n3        Algeria 34.36667   35.3 27.6 40.2 103.1 3.916606e+04\n4 American Samoa      NaN     NA  Inf -Inf   0.0 1.000000e+00\n5        Andorra      NaN     NA  Inf -Inf   0.0 1.000000e+00\n6         Angola 48.66667   51.3 42.7 52.0 146.0 1.139065e+05\n\n# na.rm=T가 기본값이며, NA를 연산한 값은 모두 NA를 결과값으로 반영함. \ncollap(wlddev, GINI ~ country, list(fmean, fmedian, fmin, fmax, fsum, fprod),\n       give.names = FALSE) |&gt; head()\n\n         country    fmean fmedian fmin fmax  fsum        fprod\n1    Afghanistan       NA      NA   NA   NA    NA           NA\n2        Albania 31.41111    31.7 27.0 34.6 282.7 2.902042e+13\n3        Algeria 34.36667    35.3 27.6 40.2 103.1 3.916606e+04\n4 American Samoa       NA      NA   NA   NA    NA           NA\n5        Andorra       NA      NA   NA   NA    NA           NA\n6         Angola 48.66667    51.3 42.7 52.0 146.0 1.139065e+05\n\nmicrobenchmark(a = collap(wlddev, GINI ~ country, list(mean, median, min, max, sum, prod),\n                          na.rm = TRUE, give.names = FALSE) |&gt; head(),\n               b=collap(wlddev, GINI ~ country, list(fmean, fmedian, fmin, fmax, fsum, fprod),\n                        give.names = FALSE) |&gt; head())\n\nUnit: microseconds\n expr      min       lq       mean   median         uq       max neval cld\n    a 9783.454 9857.552 10483.5492 9942.606 10258.8435 15291.497   100  a \n    b  534.478  577.196   604.1118  615.318   626.4175   855.808   100   b\n\n# 속도 상 이점을 다시 한 번 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#tra-function",
    "href": "posts/2024-10-28-Collapse/index.html#tra-function",
    "title": "collapse 패키지 소개 v2",
    "section": "TRA function",
    "text": "TRA function\n\nTRA function을 이용, 여러 행/열의 연산을 간편하게 처리할 수 있다.\n\nSyntax:\n  TRA(x, STATS, FUN = \"-\", g = NULL, set = FALSE, ...)\n\n\n  setTRA(x, STATS, FUN = \"-\", g = NULL, ...)\n\n  STATS = vector/matrix/list of statistics\n\n0        \"replace_NA\"     replace missing values in x\n1        \"replace_fill\"   replace data and missing values in x\n2        \"replace\"        replace data but preserve missing values in x\n3        \"-\"              subtract (i.e. center)\n4        \"-+\"             center on overall average statistic\n5        \"/\"              divide (i.e. scale)\n6        \"%\"              compute percentages (i.e. divide and multiply by 100)   \n7        \"+\"              add\n8        \"*\"              multiply\n9        \"%%\"             modulus (i.e. remainder from division by STATS)\n10       \"-%%\"            subtract modulus (i.e. make data divisible by STATS)\n\ndt2 &lt;- as.data.table(iris)\n\nattach(iris)    #data.table에서처럼 변수명을 직접 호출하기 위해 attach를 사용할 수 있다.\n\n# 평균값과의 차: g= Species\nall_obj_equal(Sepal.Length - ave(Sepal.Length, g = Species),\n              fmean(Sepal.Length, g = Species, TRA= \"-\"),\n              TRA(Sepal.Length, fmean(Sepal.Length, g = Species), \"-\", g = Species))\n\n[1] TRUE\n\nmicrobenchmark(baseR= Sepal.Length - ave(Sepal.Length, g = Species),\n               fmean = mean(Sepal.Length, g = Species, TRA= \"-\"),\n               TRA_fmean = TRA(Sepal.Length, fmean(Sepal.Length, g = Species), \"-\", g = Species));detach(iris)\n\nUnit: microseconds\n      expr    min      lq     mean  median      uq     max neval cld\n     baseR 57.640 58.9120 61.35788 59.9555 61.2070 156.905   100 a  \n     fmean  3.754  3.9635  4.29474  4.1200  4.2485  19.150   100  b \n TRA_fmean 11.907 12.4290 13.49149 13.0765 13.4220  55.347   100   c\n\n\n\n\nTRA()를 사용하기보다 Fast Statistical Function에서 TRA 기능을 호출하자!\n\n\n#예시\nnum_vars(dt2) %&lt;&gt;%  na_insert(prop = 0.05)\n\n# NA 값을 median값으로 대체.\nnum_vars(dt2) |&gt; fmedian(iris$Species, TRA = \"replace_NA\", set = TRUE)\n# num_vars(dt2) |&gt; fmean(iris$Species, TRA = \"replace_NA\", set = TRUE) --&gt; mean으로 대체.\n\n\n# 다양한 연산 및 작업을 한 번에 다룰 수 있다.\nmtcars |&gt; ftransform(A = fsum(mpg, TRA = \"%\"),\n                     B = mpg &gt; fmedian(mpg, cyl, TRA = \"replace_fill\"),\n                     C = fmedian(mpg, list(vs, am), wt, \"-\"),\n                     D = fmean(mpg, vs,, 1L) &gt; fmean(mpg, am,, 1L)) |&gt; head(3)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb        A     B\nMazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.266449  TRUE\nMazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.266449  TRUE\nDatsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 3.546430 FALSE\n                 C     D\nMazda RX4      1.3 FALSE\nMazda RX4 Wag  1.3 FALSE\nDatsun 710    -7.6  TRUE"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#grouping-object",
    "href": "posts/2024-10-28-Collapse/index.html#grouping-object",
    "title": "collapse 패키지 소개 v2",
    "section": "Grouping Object",
    "text": "Grouping Object\n\n\nGRP function을 이용, group을 쉽게 연산할 수 있다.\nSyntax:\n\n    GRP(X, by = NULL, sort == TRUE, decreasing = FALSE, na.last = TRUE, \n    return.groups = TRUE, return.order = sort, method = \"auto\", ...)\n\n\n\ng &lt;- GRP(iris, by = ~ Species)\nprint(g)\n\ncollapse grouping object of length 150 with 3 ordered groups\n\nCall: GRP.default(X = iris, by = ~Species), X is sorted\n\nDistribution of group sizes: \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     50      50      50      50      50      50 \n\nGroups with sizes: \n    setosa versicolor  virginica \n        50         50         50 \n\nstr(g)\n\nClass 'GRP'  hidden list of 9\n $ N.groups    : int 3\n $ group.id    : int [1:150] 1 1 1 1 1 1 1 1 1 1 ...\n $ group.sizes : int [1:3] 50 50 50\n $ groups      :'data.frame':   3 obs. of  1 variable:\n  ..$ Species: Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 2 3\n $ group.vars  : chr \"Species\"\n $ ordered     : Named logi [1:2] TRUE TRUE\n  ..- attr(*, \"names\")= chr [1:2] \"ordered\" \"sorted\"\n $ order       : int [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n  ..- attr(*, \"starts\")= int [1:3] 1 51 101\n  ..- attr(*, \"maxgrpn\")= int 50\n  ..- attr(*, \"sorted\")= logi TRUE\n $ group.starts: int [1:3] 1 51 101\n $ call        : language GRP.default(X = iris, by = ~Species)\n\n# GRP 기능- 호출하여 사용하자!\nfmean(num_vars(iris), g)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nfmean(num_vars(iris), iris$Species)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#factors-in-operation",
    "href": "posts/2024-10-28-Collapse/index.html#factors-in-operation",
    "title": "collapse 패키지 소개 v2",
    "section": "Factors in operation",
    "text": "Factors in operation\nCollaspe는 형식에 구애받지 않는다; factor를 바로 연산할 수 있으며, qF로 빠르게 factor를 생성할 수 있다.\n\nx &lt;- na_insert(rnorm(1e7), prop = 0.01) \ng &lt;- sample.int(1e6, 1e7, TRUE)         \n# grp와 비교\nsystem.time(gg &lt;- GRP(g))\n\n   user  system elapsed \n  0.649   0.027   0.677 \n\nsystem.time(f &lt;- qF(g, na.exclude = FALSE))\n\n   user  system elapsed \n  0.254   0.044   0.298 \n\nclass(f)\n\n[1] \"factor\"      \"na.included\"\n\n\n\nmicrobenchmark(fmean(x, g), \n               fmean(x, gg), \n               fmean(x, gg, na.rm = FALSE), \n               fmean(x, f))\n ## Unit: milliseconds\n ##       expr                    min         lq          mean        median\n ## fmean(x, g)                   146.060983  150.493309  155.02585   152.197822\n ## fmean(x, gg)                  25.354564   27.709625   29.48497    29.022157\n ## fmean(x, gg, na.rm = FALSE)   13.184534   13.783585   15.61769    14.128067\n ## fmean(x, f)                   24.847271   27.503661   29.47271    29.248580\n\n# qF를 통해 grp와 유사한 성능 향상을 기대할 수 있다."
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#summary-fast-grouping-and-ordering",
    "href": "posts/2024-10-28-Collapse/index.html#summary-fast-grouping-and-ordering",
    "title": "collapse 패키지 소개 v2",
    "section": "Summary: FAST grouping and Ordering",
    "text": "Summary: FAST grouping and Ordering\n다양한 기능이 있다. \nGRP()           Fast sorted or unsorted grouping of multivariate data, returns detailed object of class ’GRP’ \nqF()/qG()       Fast generation of factors and quick-group (’qG’) objects from atomic vectors \nfinteraction()  Fast interactions: returns factor or ’qG’ objects \nfdroplevels()   Efficiently remove unused factor levels\n\nradixorder()    Fast ordering and ordered grouping \ngroup()         Fast first-appearance-order grouping: returns ’qG’ object \ngsplit()        Split vector based on ’GRP’ object \ngreorder()      Reorder the results\n\n- that also return ’qG’ objects \ngroupid()       Generalized run-length-type grouping seqid() Grouping of integer sequences \ntimeid()        Grouping of time sequences (based on GCD)\n\ndapply()        Apply a function to rows or columns of data.frame or matrix based objects. \nBY()            Apply a function to vectors or matrix/data frame columns by groups.\n\n-   Specialized Data Transformation Functions \nfbetween()      Fast averaging and (quasi-)centering. \nfwithin()\nfhdbetween()    Higher-Dimensional averaging/centering and linear prediction/partialling out \nfhdwithin()     (powered by fixest’s algorithm for multiple factors).\nfscale()        (advanced) scaling and centering.\n\n-   Time / Panel Series Functions \nfcumsum()       Cumulative sums \nflag()          Lags and leads \nfdiff()         (Quasi-, Log-, Iterated-) differences \nfgrowth()       (Compounded-) growth rates\n\n-    Data manipulation functions\nfselect(),      fsubset(),      fgroup_by(),    [f/set]transform[v](),          \nfmutate(),      fsummarise(),   across(),       roworder[v](),            \ncolorder[v](),  [f/set]rename(),                [set]relabel()"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#collapse는-빠르다",
    "href": "posts/2024-10-28-Collapse/index.html#collapse는-빠르다",
    "title": "collapse 패키지 소개 v2",
    "section": "Collapse는 빠르다!",
    "text": "Collapse는 빠르다!\n\nfdim(wlddev)    ##faster dim for dt. col/row: 13176 13\n\n# 1990년 이후를 기준으로, ODA/POP의 값 (g: region, income, OECD)\nmicrobenchmark( \n  \ndplyr = qDT(wlddev) |&gt;\n        filter(year &gt;= 1990) |&gt;\n        mutate(ODA_POP = ODA / POP) |&gt;\n        group_by(region, income, OECD) |&gt;\n        summarise(across(PCGDP:POP, sum, na.rm = TRUE), .groups = \"drop\") |&gt;\n        arrange(income, desc(PCGDP)),\n\ndata.table = qDT(wlddev)[, ODA_POP := ODA / POP][\n             year &gt;= 1990, lapply(.SD, sum, na.rm = TRUE),\n             by = .(region, income, OECD), .SDcols = PCGDP:ODA_POP][\n             order(income, -PCGDP)],\n\ncollapse_base = qDT(wlddev) |&gt;\n                fsubset(year &gt;= 1990) |&gt;\n                fmutate(ODA_POP = ODA / POP) |&gt;\n                fgroup_by(region, income, OECD) |&gt;\n                fsummarise(across(PCGDP:ODA_POP, sum, na.rm = TRUE)) |&gt;\n                roworder(income, -PCGDP),\n\ncollapse_optimized = qDT(wlddev) |&gt;\n                    fsubset(year &gt;= 1990, region, income, OECD, PCGDP:POP) |&gt;\n                    fmutate(ODA_POP = ODA / POP) |&gt;\n                    fgroup_by(1:3, sort = FALSE) |&gt; fsum() |&gt;\n                    roworder(income, -PCGDP)\n)\n\n\n## Unit: microseconds\n##        expr            min         lq            mean            median          uq            max         neval\n## dplyr                  71955.523   72291.9715    80009.2208      72453.1165      76902.671   393947.262  100 \n## data.table             5960.503    6310.7045     7116.6673       6721.3450       7046.837    18615.736     100   \n## collapse_base          859.505     948.2200      1041.1137       990.1375        1061.864     3148.804       100 \n## collapse_optimized     442.040     482.9705      542.6927        523.6950        574.921     1036.817      100"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#collapse-w-fast-statistical-function-다양한-활용",
    "href": "posts/2024-10-28-Collapse/index.html#collapse-w-fast-statistical-function-다양한-활용",
    "title": "collapse 패키지 소개 v2",
    "section": "Collapse w/ Fast Statistical Function: 다양한 활용",
    "text": "Collapse w/ Fast Statistical Function: 다양한 활용\n\n# 아래 셋은 동일한 결과를 보인다.\n# cyl별 mpg sum\n mtcars %&gt;% ftransform(mpg_sum = fsum(mpg, g = cyl, TRA = \"replace_fill\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(mpg_sum = fsum(mpg)) %&gt;% head(10)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4   138.2\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4   138.2\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   293.3\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1   138.2\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2   211.4\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1   138.2\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4   211.4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2   293.3\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2   293.3\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4   138.2\n\n\n\nad-hoc grouping, often fastest!\n\n\n\nmicrobenchmark(a=mtcars %&gt;% ftransform(mpg_sum = fsum(mpg, g = cyl, TRA = \"replace_fill\")),\n               b=mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")),\n               c=mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(mpg_sum = fsum(mpg)))\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval cld\n    a 27.362 28.9525 30.22495 29.9020 31.5925  39.924   100 a  \n    b 64.001 66.0010 68.26108 67.2155 68.9120 114.379   100  b \n    c 78.678 80.4105 84.21481 81.2445 82.4050 264.876   100   c\n\n\n\n\nftransform()은 앞의 fgroupby를 무시한다. 아래 둘은 값이 다르다. (fmutate, fsummarise만 이전 group을 반영한다.)\n\n\nmtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")) %&gt;% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   138.2\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   138.2\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   293.3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   138.2\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   211.4\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   138.2\n\nmtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, TRA = \"replace_fill\")) %&gt;% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   642.9\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   642.9\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   642.9\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   642.9\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   642.9\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   642.9\n\n\n\n위 언급과 같이 baseR 의 “/”보다 collapse의 TRA function을 이용하는 것이 더 빠르다.\n\n\nmicrobenchmark(\n\"/\"=      mtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop = mpg / fsum(mpg))      |&gt; head(),     \n\"TRA=/\" = mtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop = fsum(mpg, TRA = \"/\")) |&gt; head()\n)\n\nUnit: microseconds\n  expr     min       lq     mean   median       uq     max neval cld\n     / 207.643 210.3915 214.5014 212.7320 215.4790 281.881   100  a \n TRA=/ 196.455 200.0970 205.9386 202.4655 205.0695 471.228   100   b\n\n\n\n\nfsum은 grp 별로 연산을 처리하나, sum은 전체를 반영한다.\n\n\nmtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop2 = fsum(mpg) / sum(mpg))|&gt; head() #\"!=1\" \n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_prop2\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 0.2149634\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 0.2149634\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 0.4562140\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 0.2149634\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 0.3288225\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 0.2149634\n\n\n\n자유로운 %&gt;% 의 사용\n\n\n# 아래 둘은 동일하다.\n mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(fselect(., hp:qsec) %&gt;% fsum(TRA = \"/\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(across(hp:qsec, fsum, TRA = \"/\")) %&gt;% head()\n\n                   mpg cyl disp         hp       drat         wt       qsec vs\nMazda RX4         21.0   6  160 0.12850467 0.15537849 0.12007333 0.13080102  0\nMazda RX4 Wag     21.0   6  160 0.12850467 0.15537849 0.13175985 0.13525111  0\nDatsun 710        22.8   4  108 0.10231023 0.08597588 0.09227220 0.08840435  1\nHornet 4 Drive    21.4   6  258 0.12850467 0.12270916 0.14734189 0.15448188  1\nHornet Sportabout 18.7   8  360 0.05974735 0.06967485 0.06144064 0.07248414  0\nValiant           18.1   6  225 0.12266355 0.10996016 0.15857012 0.16068023  1\n                  am gear carb\nMazda RX4          1    4    4\nMazda RX4 Wag      1    4    4\nDatsun 710         1    4    1\nHornet 4 Drive     0    3    1\nHornet Sportabout  0    3    2\nValiant            0    3    1\n\n\n\n\nset = TRUE를 통해 원본 데이터에 반영할 수 있다.\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# mtcars의 열 hp:qsec의 값과 해당하는 g:cyl별 합의 비율.\nmtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(across(hp:qsec, fsum, TRA = \"/\", set = TRUE)) %&gt;% invisible()\nhead(mtcars)\n\n                   mpg cyl disp         hp       drat         wt       qsec vs\nMazda RX4         21.0   6  160 0.12850467 0.15537849 0.12007333 0.13080102  0\nMazda RX4 Wag     21.0   6  160 0.12850467 0.15537849 0.13175985 0.13525111  0\nDatsun 710        22.8   4  108 0.10231023 0.08597588 0.09227220 0.08840435  1\nHornet 4 Drive    21.4   6  258 0.12850467 0.12270916 0.14734189 0.15448188  1\nHornet Sportabout 18.7   8  360 0.05974735 0.06967485 0.06144064 0.07248414  0\nValiant           18.1   6  225 0.12266355 0.10996016 0.15857012 0.16068023  1\n                  am gear carb\nMazda RX4          1    4    4\nMazda RX4 Wag      1    4    4\nDatsun 710         1    4    1\nHornet 4 Drive     0    3    1\nHornet Sportabout  0    3    2\nValiant            0    3    1\n\n\n\n\n.apply = FALSE를 통해 subset group에만 적용할 수 있다.\n\n\n# 각 g:cyl의 hp:qsec까지의 변수에 대한 부분 상관관계\nmtcars %&gt;% fgroup_by(cyl) %&gt;% fsummarise(across(hp:qsec, \\(x) qDF(pwcor(x), \"var\"), .apply = FALSE)) %&gt;% head()\n\n  cyl  var         hp       drat         wt       qsec\n1   4   hp  1.0000000 -0.4702200  0.1598761 -0.1783611\n2   4 drat -0.4702200  1.0000000 -0.4788681 -0.2833656\n3   4   wt  0.1598761 -0.4788681  1.0000000  0.6380214\n4   4 qsec -0.1783611 -0.2833656  0.6380214  1.0000000\n5   6   hp  1.0000000  0.2171636 -0.3062284 -0.6280148\n6   6 drat  0.2171636  1.0000000 -0.3546583 -0.6231083"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#이름순번vectors정규표현식으로-행열을-지칭할-수-있다.",
    "href": "posts/2024-10-28-Collapse/index.html#이름순번vectors정규표현식으로-행열을-지칭할-수-있다.",
    "title": "collapse 패키지 소개 v2",
    "section": "이름/순번/vectors/정규표현식으로 행/열을 지칭할 수 있다.",
    "text": "이름/순번/vectors/정규표현식으로 행/열을 지칭할 수 있다.\nget_vars(x, vars, return = \"names\", regex = FALSE, ...) \nget_vars(x, vars, regex = FALSE, ...) &lt;- value \n\n- 위치도 선택가능하다.\nadd_vars(x, ..., pos = \"end\") \nadd_vars(x, pos = \"end\") &lt;- value \n\n- data type을 지정할 수 있다. \nnum_vars(x, return = \"data\");   cat_vars(x, return = \"data\");   char_vars(x, return = \"data\"); \nfact_vars(x, return = \"data\");  logi_vars(x, return = \"data\");  date_vars(x, return = \"data\") \n\n- replace 또한 가능하다.\nnum_vars(x) &lt;- value;   cat_vars(x) &lt;- value;   char_vars(x) &lt;- value; \nfact_vars(x) &lt;- value;  logi_vars(x) &lt;- value;  date_vars(x) &lt;- value"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#efficient-programming",
    "href": "posts/2024-10-28-Collapse/index.html#efficient-programming",
    "title": "collapse 패키지 소개 v2",
    "section": "Efficient programming",
    "text": "Efficient programming\n    &gt; quick data conversion\n-   qDF(),  qDT(),  qTBL(),   qM(),   mrtl(),   mctl()\n    \n-   anyv(x, value) / allv(x, value)     # Faster than any/all(x == value)\n-   allNA(x)                            # Faster than all(is.na(x))\n-   whichv(x, value, invert = F)        # Faster than which(x (!/=)= value)\n-   whichNA(x, invert = FALSE)          # Faster than which((!)is.na(x))\n-   x %(!/=)=% value                    # Infix for whichv(v, value, TRUE/FALSE)\n-   setv(X, v, R, ...)                  # x\\[x(!/=)=v\\]\\&lt;-r / x\\[v\\]\\&lt;-r\\[v\\] (by reference)\n-   setop(X, op, V, rowwise = F)        # Faster than X \\&lt;- X +/-/\\*// V (by reference)\n-   X %(+,-,\\*,/)=% V                   # Infix for setop,()\n-   na_rm(x)                            # Fast: if(anyNA(x)) x\\[!is.na(x)\\] else x,\n-   na_omit(X, cols = NULL, ...)        # Faster na.omit for matrices and data frames\n-   vlengths(X, use.names=TRUE)         # Faster version of lengths()\n-   frange(x, na.rm = TRUE)             # Much faster base::range\n-   fdim(X)                             # Faster dim for data frames"
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html",
    "href": "posts/2024-10-14-reportGeneration/index.html",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "",
    "text": "Quarto는 오픈소스 기반의 과학 및 기술 출판 시스템입니다.\n코드를 기반으로 문서를 생성하기 때문에 동일한 형식의 데이터으로부터 일정한 결과물을 얻을 수 있습니다.\n이는 문서의 일관성과 재사용성을 증대시키고 사람에 의한 실수를 줄이는 데 큰 도움이 됩니다.\n그러나 문서의 모든 요소를 코드의 형태로 만들 수 있는것은 아닙니다.\n이를테면 문서에 포함된 표 또는 그래프에 대한 설명은 작성자가 직접 작성해야합니다.\n이 과정을 효율적으로 해결하기 위해서 차라투에서는 LLM을 활용했습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#libraries",
    "href": "posts/2024-10-14-reportGeneration/index.html#libraries",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Libraries",
    "text": "Libraries\nhttr2\nOpenAI endpoint에 GET 요청을 실행하기 위한 라이브러리\nggplot2\n그래프를 그리기 위한 라이브러리\nbase64enc\n이미지를 base64로 인코딩 하기 위한 라이브러리\nflextable\n복잡한 table을 만들기 위한 라이브러리\nofficer\nflextable을 html format으로 변환할 때 사용되는 라이브러리\nmagrittr\n파이프 연산자를 사용하기 위한 라이브러리\nDBI (Optional)\nDB와 통신하기 위한 라이브러리\nRSQLite (Optional)\nSqlite3 인터페이스 라이브러리\ndigest (Optional)\n해쉬 알고리즘을 위한 라이브러리"
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#openai-api-key",
    "href": "posts/2024-10-14-reportGeneration/index.html#openai-api-key",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "OpenAI api key",
    "text": "OpenAI api key\n본 과정에서는 OpenAI 사의 GPT4o-mini 모델을 활용했습니다.\n본인의 OpenAI api 키를 발급받을 경우 동일한 방식으로 실습을 진행하실 수 있습니다.\nhttps://openai.com/index/openai-api/\n\nopenai_key &lt;- \"your openAI key\"\nendpoint &lt;- \"https://api.openai.com/v1/\""
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#instruction",
    "href": "posts/2024-10-14-reportGeneration/index.html#instruction",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Instruction",
    "text": "Instruction\nOpenAI endpoint로 요청 시 Text parameter가 포함 됩니다.\n이 Text에는 작업 지시 사항인 Instruction과 분석할 데이터가 포함됩니다.\nInstruction의 예시는 다음과 같습니다.\n\ninstruction &lt;- \"지침:\n- 한글로 4문장 이상 답변하세요.\n- 표에 있는 범주별로 내용을 설명하세요.\n- 각 범주에 포함된 모든 항목에 대해 설명하세요.\""
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#request",
    "href": "posts/2024-10-14-reportGeneration/index.html#request",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Request",
    "text": "Request\nEndpoint(https://api.openai.com/v1/chat/completions)로 요청 시 다음의 parameter가 사용되었습니다.\n\nHeader\n\nContent-Type: application/json\nAuthorization: Bearer $OPENAI_API_KEY\n\n\nBody\n\nmodel(*): gpt-4o-mini [openAI의 다른 모델도 사용할 수 있습니다]\ntemperature: 0.2 [0과 2 사이의 값, 높을수록 무작위적이며 창의적인 대답을 내놓음]\nmessages(*):\n\nrole: user\ncontent:\n\ntype: text\ntext: Introduction과 Table data\n\n\n\n\n\n\n\nTable\nLLM이 table을 인식하기 위해서는 table이 적절한 형식으로 변환되어야 합니다. LLM이 인식 할 수 있는 형식은 크게 두가지입니다.\nMarkdown\n마크다운은 일반 텍스트 기반의 경량 마크업 언어입니다. 누구나 쉽게 작성할 수 있다는 장점이 있으며 표 또한 텍스트를 통해 생성 할 수 있습니다. 다만 복잡한 형태의 표 (예: 계층 구조)의 경우 표현하기 어렵다는 단점이 있습니다.\n\nconvert_to_markdown &lt;- function(table) {\n  df_from_ft &lt;- as.data.frame(table)\n  markdown_table &lt;- capture.output(cat(knitr::kable(table, format = \"markdown\"), sep = \"\\n\"))\n  return(paste(markdown_table, collapse = \"\\n\"))\n}\n\n\nhead(mtcars[1:5])\n\n                   mpg cyl disp  hp drat\nMazda RX4         21.0   6  160 110 3.90\nMazda RX4 Wag     21.0   6  160 110 3.90\nDatsun 710        22.8   4  108  93 3.85\nHornet 4 Drive    21.4   6  258 110 3.08\nHornet Sportabout 18.7   8  360 175 3.15\nValiant           18.1   6  225 105 2.76\n\ncat(convert_to_markdown(head(mtcars[1:5])))\n\n|                  |  mpg| cyl| disp|  hp| drat|\n|:-----------------|----:|---:|----:|---:|----:|\n|Mazda RX4         | 21.0|   6|  160| 110| 3.90|\n|Mazda RX4 Wag     | 21.0|   6|  160| 110| 3.90|\n|Datsun 710        | 22.8|   4|  108|  93| 3.85|\n|Hornet 4 Drive    | 21.4|   6|  258| 110| 3.08|\n|Hornet Sportabout | 18.7|   8|  360| 175| 3.15|\n|Valiant           | 18.1|   6|  225| 105| 2.76|\n\n\nHTML\n일반적인 data.frame 객체는 마크다운으로 변환할 수 있습니다.\n그러나 flextable처럼 Cell이 합쳐진 형태나 계층적 구조일 경우 markdown으로의 변환이 어렵습니다.\n이 경우 html table 형태로 변환할 수 있습니다.\n\nproc_freq(mtcars, \"gear\", \"vs\")\n\n\n\n\n\n\ngear\n\nvs\n\n\n0\n1\nTotal\n\n\n\n\n3\nCount\n12 (37.5%)\n3 (9.4%)\n15 (46.9%)\n\n\nMar. pct (1)\n66.7% ; 80.0%\n21.4% ; 20.0%\n\n\n\n4\nCount\n2 (6.2%)\n10 (31.2%)\n12 (37.5%)\n\n\nMar. pct\n11.1% ; 16.7%\n71.4% ; 83.3%\n\n\n\n5\nCount\n4 (12.5%)\n1 (3.1%)\n5 (15.6%)\n\n\nMar. pct\n22.2% ; 80.0%\n7.1% ; 20.0%\n\n\n\nTotal\nCount\n18 (56.2%)\n14 (43.8%)\n32 (100.0%)\n\n\n (1) Columns and rows percentages\n\n\n\n\n\ncat(proc_freq(mtcars, \"gear\", \"vs\") %&gt;% to_html)\n\n&lt;div class=\"tabwid\"&gt;&lt;style&gt;.cl-d42a3a38{}.cl-d4230326{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d4230330{font-family:'DejaVu Sans';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-d425c64c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c660{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c661{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c662{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c66a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425e550{width:0.625in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e551{width:0.952in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e55a{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e55b{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e564{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e565{width:0.625in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e566{width:0.952in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e567{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e56e{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e56f{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e570{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e571{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e572{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e578{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e579{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e57a{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e57b{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e582{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e583{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e584{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e585{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58c{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58d{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58e{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58f{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e596{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e597{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e598{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e599{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e59a{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e59b{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a0{width:0.952in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a1{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a2{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5aa{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.tabwid {\n  font-size: initial;\n  padding-bottom: 1em;\n}\n\n.tabwid table{\n  border-spacing:0px !important;\n  border-collapse:collapse;\n  line-height:1;\n  margin-left:auto;\n  margin-right:auto;\n  border-width: 0;\n  border-color: transparent;\n  caption-side: top;\n}\n.tabwid-caption-bottom table{\n  caption-side: bottom;\n}\n.tabwid_left table{\n  margin-left:0;\n}\n.tabwid_right table{\n  margin-right:0;\n}\n.tabwid td, .tabwid th {\n    padding: 0;\n}\n.tabwid a {\n  text-decoration: none;\n}\n.tabwid thead {\n    background-color: transparent;\n}\n.tabwid tfoot {\n    background-color: transparent;\n}\n.tabwid table tr {\nbackground-color: transparent;\n}\n.katex-display {\n    margin: 0 0 !important;\n}&lt;/style&gt;&lt;table data-quarto-disable-processing='true' class='cl-d42a3a38'&gt;&lt;thead&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;th  rowspan=\"2\"class=\"cl-d425e550\"&gt;&lt;p class=\"cl-d425c64c\"&gt;&lt;span class=\"cl-d4230326\"&gt;gear&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th  rowspan=\"2\"class=\"cl-d425e551\"&gt;&lt;p class=\"cl-d425c64c\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th  colspan=\"3\"class=\"cl-d425e55a\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;vs&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;th class=\"cl-d425e567\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;0&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class=\"cl-d425e56e\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;1&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class=\"cl-d425e56f\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;Total&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;12 (37.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;3 (9.4%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;15 (46.9%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e57b\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;span class=\"cl-d4230330\"&gt; (1)&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e582\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;66.7% ; 80.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e583\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;21.4% ; 20.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e584\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;2 (6.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;10 (31.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;12 (37.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e57b\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e582\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;11.1% ; 16.7%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e583\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;71.4% ; 83.3%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e584\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;4 (12.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;1 (3.1%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;5 (15.6%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e58c\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58d\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;22.2% ; 80.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58e\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;7.1% ; 20.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58f\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e596\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Total&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e597\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e598\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;18 (56.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e599\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;14 (43.8%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e59a\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;32 (100.0%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;tfoot&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  colspan=\"5\"class=\"cl-d425e59b\"&gt;&lt;p class=\"cl-d425c66a\"&gt;&lt;span class=\"cl-d4230330\"&gt; (1)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt; Columns and rows percentages&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tfoot&gt;&lt;/table&gt;&lt;/div&gt;\n\n\nFigure\n만약 이미지에 대한 설명을 원한다면 이미지에 대한 파라미터를 추가해야합니다.\n\nHeader\n\nContent-Type: application/json\nAuthorization: Bearer $OPENAI_API_KEY\n\n\nBody\n\nmodel(*): gpt-4o-mini [openAI의 다른 모델도 사용할 수 있습니다]\ntemperature: 0.2 [0과 2 사이의 값, 높을수록 무작위적이며 창의적인 대답을 내놓음]\nmessages(*):\n\nrole: user\ncontent:\n\ntype: text\ntext: Introduction과 Table data\ntype: image_url\nimage_url:\n\nurl: base64로 인코딩된 이미지\n\n\n\n\n\n\n\n\n\n이미지는 base64로 인코딩되어 전달됩니다.\n\nLLM_description &lt;- function(inst = \"다음에 대해 설명하시오\", ## 지침\n                         add_inst = NULL, ## 추가 지침\n                         table = \"\", ## 데이터가 포함된 표\n                         image_path = NULL, ## 이미지 경로 (Local 또는 웹)\n                         model = \"gpt-4o-mini\", ## 사용 할 LLM 모델\n                         temperature = 0.2, ## 무작위성 (낮을수록 일관된 답변)\n                         endpoint = \"https://api.openai.com/v1/\" ## 엔드포인트\n                         ){\n  \n  ## LLM에게 전달 할 내용\n  ### 주요 지침\n  ### 추가 지침\n  ### 표\n  text &lt;- paste(\n    \"주요 지침: \",\n    inst,\n    ifelse(is.null(add_inst), ## 추가 지침이 존재 할 경우 추가\n           \" \",\n           \"추가 지침: \"),\n    add_inst,\n    \"도표: \",\n    table, ## Markdown 또는 html 형태의 table\n    sep = \"\\n\\n\"\n\n  )\n  \n  \n  url &lt;- NULL\n  if(!is.null(image_path)){ ## image path가 전달\n    if(file.exists(image_path)){\n      ## 로컬파일인 경우\n      url &lt;- paste0(\"data:image/png;base64,\",base64encode(image_path))\n    }else{\n      ## 로컬파일이 아닌 경우(Web 이미지 가정)\n      \n      ## 이미지 다운로드\n      resp &lt;- request(image_path) %&gt;% \n        req_perform()\n      \n      \n      if(resp$status_code == 200){\n        ## 정상적인 응답 시\n        image_raw &lt;- resp_body_raw(resp)\n        encoded_image &lt;- base64encode(image_raw)\n        url &lt;- paste0(\"data:image/png;base64,\",encoded_image )\n        \n      } else{\n        ## 비정상적인 응답 시\n        warning(\"Failed to download the image. Status code:\", response$status_code)\n      }\n      \n      \n    }\n  }\n  \n  ## 이미지 컴포넌트\n  image_cmp &lt;- NULL\n  \n  \n  if(!is.null(url)){\n    ## 이미지가 존재할 경우\n    \n    ## 이미지\n    image_cmp &lt;- list(\n            type = \"image_url\",\n            image_url = list(\n              url = url\n            )\n          )\n    \n    ## 이미지가 포함된 content\n    content &lt;- \n      list(\n          list(\n            type = \"text\",\n            text =  text\n          ),\n          image_cmp\n\n        )\n  }else{\n    ## 이미지가 존재하지 않을 경우\n    ## text만 content에 포함\n    content &lt;- list(\n          list(\n            type = \"text\",\n            text =  text\n          )\n\n        )\n  }\n  \n  \n\nreq &lt;- request(endpoint) %&gt;% ## 엔드포인트\n  req_url_path_append(\"chat\") %&gt;% ## 주소 1\n  req_url_path_append(\"completions\") %&gt;% ##주소 2\n  req_headers(\n    `Content-Type` = \"application/json\",\n    Authorization = paste(\"Bearer\", openai_key) ## API키 전달\n  ) %&gt;% \n  req_body_json(\n    list(\n      model = model, ## LLM 모델\n      temperature = temperature, \n      messages = list(list(\n        role = \"user\",\n        content = content\n      )\n    )),\n    auto_unbox = T\n  )\n\n\n\nresp &lt;- req %&gt;% \n  req_perform() ## 요청 실행\ntmp &lt;- resp %&gt;% \n  resp_body_json() ## 응답을 json 형태로 전환\n\n\n\nreturn(tmp$choices[[1]]$message$content) ## 응답 부분만 반환\n}"
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#database-optional",
    "href": "posts/2024-10-14-reportGeneration/index.html#database-optional",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Database (Optional)",
    "text": "Database (Optional)\nQuarto 문서를 랜더링 할 때마다 OpenAI endpoint에 결과를 요청할 경우 상당한 비용을 초래할 수 있습니다. 따라서 동일한 parameter로 request 할 경우 DB에 저장된 결과를 대신 반환하도록 sqlite3를 이용하여 캐시 DB를 구성했습니다. 본 포스팅에서는 생략합니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#data.frame-형태의-표에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#data.frame-형태의-표에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "data.frame 형태의 표에 대한 설명",
    "text": "data.frame 형태의 표에 대한 설명\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\ncat(LLM_description(\n  inst = \"다음 표에 대해 설명하시오\",\n  add_inst = \"이 표는 자동차에 관한 표이다.\",\n  table = convert_to_markdown(mtcars)\n))\n\n이 표는 다양한 자동차 모델에 대한 여러 가지 성능 및 사양 데이터를 나열하고 있습니다. 각 열은 자동차의 특정 특성을 나타내며, 각 행은 특정 모델에 대한 정보를 제공합니다. 아래는 각 열의 설명입니다:\n\n1. **mpg (Miles Per Gallon)**: 연비를 나타내며, 차량이 1갤런의 연료로 주행할 수 있는 마일 수를 의미합니다. 이 값이 높을수록 연료 효율성이 좋습니다.\n\n2. **cyl (Cylinders)**: 엔진의 실린더 수를 나타냅니다. 일반적으로 실린더 수가 많을수록 엔진의 출력이 높아지지만, 연비는 낮아질 수 있습니다.\n\n3. **disp (Displacement)**: 엔진의 배기량을 나타내며, 보통 리터 또는 세제곱 인치로 측정됩니다. 배기량이 클수록 엔진의 힘이 강해지는 경향이 있습니다.\n\n4. **hp (Horsepower)**: 엔진의 출력, 즉 마력을 나타냅니다. 마력이 높을수록 차량의 성능이 좋습니다.\n\n5. **drat (Rear Axle Ratio)**: 후륜 구동차의 후축 비율을 나타내며, 엔진의 회전 속도와 바퀴의 회전 속도 간의 비율을 의미합니다. 이 값이 높을수록 가속력이 좋지만 연비는 떨어질 수 있습니다.\n\n6. **wt (Weight)**: 차량의 무게를 나타내며, 일반적으로 파운드로 측정됩니다. 무게가 가벼울수록 연비가 좋고, 주행 성능이 향상될 수 있습니다.\n\n7. **qsec (Quarter Mile Time)**: 1/4 마일(약 400미터) 주행에 소요되는 시간을 나타냅니다. 이 값이 낮을수록 차량의 가속력이 좋습니다.\n\n8. **vs (V/S)**: 엔진의 배치 방식으로, 0은 V형 엔진, 1은 직렬 엔진을 의미합니다.\n\n9. **am (Transmission Type)**: 변속기의 종류를 나타내며, 0은 자동 변속기, 1은 수동 변속기를 의미합니다.\n\n10. **gear (Number of Gears)**: 변속기의 기어 수를 나타냅니다. 기어 수가 많을수록 다양한 주행 조건에 적합할 수 있습니다.\n\n11. **carb (Carburetors)**: 카뷰레터의 수를 나타내며, 연료 혼합 비율을 조절하는 장치입니다. 카뷰레터 수가 많을수록 연료 공급이 원활해질 수 있습니다.\n\n이 표를 통해 다양한 자동차 모델의 성능과 사양을 비교할 수 있으며, 소비자들이 자신의 필요에 맞는 차량을 선택하는 데 유용한 정보를 제공합니다. 예를 들어, 연비가 중요한 소비자는 mpg가 높은 차량을 선호할 것이고, 성능이 중요한 소비자는 hp와 qsec 값이 높은 차량을 선택할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#flextable-형태의-표에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#flextable-형태의-표에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "flextable 형태의 표에 대한 설명",
    "text": "flextable 형태의 표에 대한 설명\n\nproc_freq(mtcars, \"gear\", \"vs\")\n\n\n\n\n\n\ngear\n\nvs\n\n\n0\n1\nTotal\n\n\n\n\n3\nCount\n12 (37.5%)\n3 (9.4%)\n15 (46.9%)\n\n\nMar. pct (1)\n66.7% ; 80.0%\n21.4% ; 20.0%\n\n\n\n4\nCount\n2 (6.2%)\n10 (31.2%)\n12 (37.5%)\n\n\nMar. pct\n11.1% ; 16.7%\n71.4% ; 83.3%\n\n\n\n5\nCount\n4 (12.5%)\n1 (3.1%)\n5 (15.6%)\n\n\nMar. pct\n22.2% ; 80.0%\n7.1% ; 20.0%\n\n\n\nTotal\nCount\n18 (56.2%)\n14 (43.8%)\n32 (100.0%)\n\n\n (1) Columns and rows percentages\n\n\n\n\n\ncat(LLM_description(\n  inst = \"다음 표에 대해 설명하시오.\",\n  add_inst = \"그래프는 gear 와 vs를 변수로 갖는 frequency table 이다.\",\n  table = proc_freq(mtcars, \"gear\", \"vs\") %&gt;% to_html\n  ))\n\n제공된 표는 \"gear\"와 \"vs\"라는 두 변수 간의 빈도 수를 나타내는 빈도표입니다. 이 표는 각 gear 값에 대해 vs 값이 0 또는 1인 경우의 수와 비율을 보여줍니다. \n\n### 표의 구성 요소 설명:\n\n1. **열 제목**:\n   - 첫 번째 열은 \"gear\" 값을 나타내며, 3, 4, 5의 세 가지 값이 있습니다.\n   - 두 번째 열은 비어 있으며, 주로 \"Count\"와 \"Mar. pct\"를 구분하는 역할을 합니다.\n   - 세 번째 열부터 다섯 번째 열은 \"vs\" 값에 대한 정보를 제공합니다. 세 번째 열은 vs가 0인 경우의 수, 네 번째 열은 vs가 1인 경우의 수, 다섯 번째 열은 두 경우의 합계입니다.\n\n2. **행 제목**:\n   - 각 gear 값에 대해 두 개의 행이 있습니다. 첫 번째 행은 각 경우의 수와 비율을 나타내고, 두 번째 행은 각 경우의 비율을 나타냅니다.\n\n3. **데이터**:\n   - 예를 들어, gear가 3일 때, vs가 0인 경우는 12건(37.5%), vs가 1인 경우는 3건(9.4%)이며, 총 15건(46.9%)입니다.\n   - gear가 4일 때, vs가 0인 경우는 2건(6.2%), vs가 1인 경우는 10건(31.2%)이며, 총 12건(37.5%)입니다.\n   - gear가 5일 때, vs가 0인 경우는 4건(12.5%), vs가 1인 경우는 1건(3.1%)이며, 총 5건(15.6%)입니다.\n\n4. **총계**:\n   - 마지막 행은 각 gear 값에 대한 총계를 보여줍니다. 예를 들어, vs가 0인 경우 총 18건(56.2%), vs가 1인 경우 총 14건(43.8%)이며, 전체 총계는 32건(100%)입니다.\n\n5. **비율**:\n   - \"Mar. pct\"는 각 gear 값에 대한 vs의 비율을 나타내며, 각 gear에 대해 두 개의 비율이 제공됩니다.\n\n### 요약:\n이 빈도표는 gear와 vs 간의 관계를 명확하게 보여주며, 각 gear 값에 따른 vs의 발생 빈도와 비율을 분석하는 데 유용합니다. 이를 통해 특정 gear 값이 vs에 미치는 영향을 이해할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#그래프-등의-이미지에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#그래프-등의-이미지에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "그래프 등의 이미지에 대한 설명",
    "text": "그래프 등의 이미지에 대한 설명\n\nplot_am_bar &lt;- ggplot(mtcars) +\n  geom_bar(aes(factor(am)))\n\nplot_am_bar\n\n\n\n\n\n\nggsave(\"img/am.png\")\n\nSaving 7 x 5 in image\n\n\n\nplot_am_bar_build &lt;- ggplot_build(plot_am_bar)\ncat(LLM_description(\n  inst = \"다음 그래프에 대해 설명하시오.\",\n  add_inst = \"그래프는 자동차의 변속기 유형과 관련되어있다.\",\n  table = convert_to_markdown( plot_am_bar_build$data),\n  image_path = \"img/am.png\"\n  ))\n\n그래프는 자동차의 변속기 유형에 따른 분포를 나타내고 있습니다. x축은 변속기 유형을 나타내며, 0은 자동 변속기(automatic), 1은 수동 변속기(manual)를 의미합니다. y축은 각 변속기 유형에 해당하는 자동차의 수(count)를 나타냅니다.\n\n그래프를 보면, 자동 변속기를 가진 자동차의 수가 약 19대인 반면, 수동 변속기를 가진 자동차는 약 13대입니다. 이는 자동 변속기를 가진 자동차가 수동 변속기보다 더 많다는 것을 보여줍니다. \n\n전체적으로, 이 그래프는 자동차의 변속기 유형에 대한 분포를 시각적으로 표현하며, 자동 변속기가 더 일반적이라는 점을 강조하고 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html",
    "href": "posts/2024-09-13 jstable/index.html",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "",
    "text": "이 문서에서는 GLM, GEE, GLMM, Cox 모델에 대한 개요와 이를 jstable 패키지와 더불어 활용하는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm의-개요",
    "href": "posts/2024-09-13 jstable/index.html#glm의-개요",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM의 개요",
    "text": "GLM의 개요\nGLM은 Generalized Linear Model의 약자로 일반화 선형 모델을 의미합니다. 기존의 선형 회귀 모델에서는 반응 변수가 정규 분포를 따라야한다는 가정이 필요하지만, 현실에서는 그렇지 않은 경우가 많습니다. GLM은 이러한 경우에도 사용하기 위해 만들어진 모델입니다. 정규 분포가 아닌 흔한 예시로는 이항분포, 포아송 분포 등이 있습니다. 나이에 따른 생존 상태(생존은 1, 사망은 0)인 이항분포 반응변수를 간단한 예시로 들어보겠습니다. 일반적 선형회귀 모델과 GLM을 통해 로지스틱 회귀 분석을 한 결과를 비교해보았습니다.\n\n\n\n\n\n\n\n\n일반적인 선형 회귀 함수를 적용했을 때 나이가 어린 경우에는 사망 확률이 음수가 나오며, 고령인 경우 사망 확률이 1이 넘는 등의 문제가 발생합니다. 반면, GLM(로지스틱 회귀모형)을 적용하면 모든 나이에서 사망 확률이 0에서 1사이가 됨을 알 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm을-적용하기-위한-데이터의-조건",
    "href": "posts/2024-09-13 jstable/index.html#glm을-적용하기-위한-데이터의-조건",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM을 적용하기 위한 데이터의 조건",
    "text": "GLM을 적용하기 위한 데이터의 조건\n이항 분포에 대해 GLM을 적용하였지만, 모든 데이터에 GLM을 적용할 수 있는 것은 아닙니다. GLM을 적용하기 위한 조건에 대해 알아보겠습니다.\n반응변수의 분포\n반응 변수의 분포 형태가 Exponential family(지수 분포군)를 따를 때 GLM을 적용할 수 있습니다. Exponential family는 아래와 같은 수식으로 나타낼 수 있습니다. \\[\nf(y | \\theta, \\phi) = \\exp\\left( \\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\right)\n\\] 여기서\\(y\\)는 반응 변수 \\(\\theta\\)는 정규화 매게 변수, \\(\\phi\\)는 분산을 나타냅니다. 이항분포를 예시로 생각해보겠습니다. 이항분포의 경우 성공 확률 \\(p\\)로 정의되며 아래 함수의 경우 다음과 같이 지수 분포군 함수로 변환이 가능합니다.\\[\nP(Y = y) = \\binom{n}{y} p^y (1 - p)^{n - y}\n\\] \\[\nP(Y = y) = \\binom{n}{y} \\exp\\left( y \\log\\left( \\frac{p}{1 - p} \\right) + n \\log(1 - p) \\right)\n\\] 위와 같이 Exponential family로 변형이 가능한 다른 흔한 분포로는 포아송 분포, 감마 분포 등이 있습니다. Exponential Family로 변형이 가능한 분포의 경우, 지수함수의 특성으로 최대우도 추정(Maximum Likelihood Estimation)을 효율적 계산을 통해 할 수 있으며, 링크함수를 통해 선형 예측을 쉽게 할 수 있다는 장점이 있습니다.\n링크 함수\nExponential family 함수의 경우 지수형태로 존재하기 때문에 곧바로 선형 회귀 분석을 할 수는 없습니다. 선형 회귀 분석과 Exponential family를 연결하기 위한 함수가 필요한데, 이를 링크함수라 부릅니다. 링크 함수를 통해 반응 변수의 평균과 예측 변수의 선형 관계를 나타낼 수 있습니다. 반응변수의 분포가 지수 분포군에 있을 때 링크함수를 사용할 수 있으며 이항 분포의 경우 예시는 이와 같습니다. 아래 함수를 적용하면 선형 관계의 그래프를 얻을 수 있습니다. 함수를 적용하니 평소에 알던 선형 회귀의 형태로 식이 나타나는 것을 알 수 있습니다. \\[\n\\log\\left( \\frac{1 - \\mu}{\\mu} \\right) = X\\beta\n\\]\n관측치 독립\nGLM을 적용하기 위해서는 관측치들이 독립되어 합니다. 예를 들어, 한 사람의 혈압을 반복측정하는 경우 관측치들이 독립되어 있지 않기 때문에 다른 모델을 사용하여야 합니다. 이 경우에 대해서는 문서의 아래 내용에서 다루도록 하겠습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm-table-해석하기",
    "href": "posts/2024-09-13 jstable/index.html#glm-table-해석하기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM table 해석하기",
    "text": "GLM table 해석하기\njstable 패키지를 사용하여 이항 분포와 가우스 분포의 GLM 모델의 결과를 직관적인 표로 만들 수 있습니다. 아래 코드는 R에서 기본으로 제공되는 mtcars 데이터를 사용하여과 이항 회귀 모델을 만들고 결과를 표로 나타낸 것입니다. 이항분포를 가지는 am(자동미션 =0, 수동미션 =1)을 wt(차량무게), hp(마력)을 통해 예측하는 GLM에 대해 알아보겠습니다.\n\nbinomial_model &lt;- glm(am ~ wt + hp, family = binomial, data = mtcars)\nglmshow.display(binomial_model, decimal = 2)\n\n$first.line\n[1] \"Logistic regression predicting am\\n\"\n\n$table\n   crude OR.(95%CI) crude P value adj. OR.(95%CI) adj. P value\nwt \"0.02 (0,0.3)\"   \"0.005\"       \"0 (0,0.13)\"    \"0.008\"     \nhp \"0.99 (0.98,1)\"  \"0.181\"       \"1.04 (1,1.07)\" \"0.041\"     \n\n$last.lines\n[1] \"No. of observations = 32\\nAIC value = 16.0591\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n이항 분포의 glm에 대하여 첫줄에서는 로지스틱 회귀 분석을 사용했음을 알 수 있습니다. 테이블 출력 결과를 통해 wt 변수는 p-value가 0.0001로 유의미하게 미션의 종류에 영향을 미치는 것을 알 수 있습니다. Crude OR 변수를 통해 차량 무게 변수가 1이 증가할때마다 수동미션을 가질 odds 비율이 98프로 씩 감소한다는 것을 알 수 있습니다. Adjusted OR 변수가 0이라는 것을 통해 마력 변수를 보정했을 경우에 Odds 비율이 더 강하게 연관되어 있다는 것을 알 수 있으며, adjusted p value또한 유의미하다는 것을 확인 할 수 있습니다. hp의 경우 OR과 p value를 통해 관련이 적음을 유추할 수 있습니다. 마지막 줄에서는 총 몇 개의 관측치가 있었는지, 그리고 AIC 수치(낮을 수록 더 좋은 모델)를 알 수 있습니다.\n\ngaussian_model &lt;- glm(mpg~cyl + disp, data = mtcars)\nglmshow.display(gaussian_model, decimal = 2)\n\n$first.line\n[1] \"Linear regression predicting mpg\\n\"\n\n$table\n     crude coeff.(95%CI)   crude P value adj. coeff.(95%CI)    adj. P value\ncyl  \"-2.88 (-3.51,-2.24)\" \"&lt; 0.001\"     \"-1.59 (-2.98,-0.19)\" \"0.034\"     \ndisp \"-0.04 (-0.05,-0.03)\" \"&lt; 0.001\"     \"-0.02 (-0.04,0)\"     \"0.054\"     \n\n$last.lines\n[1] \"No. of observations = 32\\nR-squared = 0.7596\\nAIC value = 167.1456\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n정규분포를 나타내는 데이터에 대해서도 적용해보겠습니다.첫줄에서는 정규분포에 대하여 선형 회귀 분석을 사용했음을 알 수 있고, 해당하는 p value 들 그리고 선형 회귀의 경우 OR 대신 Coeffecient를 제공함을 알 수 있습니다. 해당 테이블을 통해 cyl(실린더의 갯수)변수는 mpg(연비)에 유의미한 영향을 미치는 것을 알 수 있습니다. disp(배기량)변수는 보정 전에는 영향이 있어 보였으나 보정 이후 0.05보다 크기 때문에 통계적으로 유의미한 영향을 미치지 않는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm과-gee의-공분산-차이",
    "href": "posts/2024-09-13 jstable/index.html#glm과-gee의-공분산-차이",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM과 GEE의 공분산 차이",
    "text": "GLM과 GEE의 공분산 차이\n데이터간의 상관관계가 반영된다는 점에서 결과적으로 GEE는 복잡한 공분산을 반영하고 있다는 것을 알 수 있습니다.\nGLM의 경우 관측치들이 독립이라는 가정이 있기 때문에 공분산 행렬이 대각행렬입니다. 반면 GEE의 경우 관측치들이 상관관계가 있다는 가정을 따르기 때문에 공분산 행렬이 대각행렬이 아닌 다른 형태를 가질 수 있습니다. 따라서 GLM에서의 공분산 행렬은 \\(V_i = \\phi \\cdot W_i\\)와 같은 구조에서 \\(W_i\\)는 대각행렬이 되며, GEE에서의 공분산 행렬은 \\(V_i = \\phi \\cdot A_i^{1/2} R(\\alpha) A_i^{1/2}\\)와 같은 구조에서 \\(A_i\\)는 대각 가중치 행렬의 형태를 따르나, 상관관계를 반영하는 \\(R(\\alpha)\\)라는 상관행렬이 있게 됩니다. 공분산과 앞선 회귀식을 모두 반영하면 GLM 과 GEE는 모두 아래의 수식과 같은 형태를 띄게 되며, \\(V_i\\)에서 그 차이가 나타납니다. \\[\nY_i = g^{-1}(X_i^T \\beta) + \\epsilon_i \\quad \\text{with} \\quad \\epsilon_i \\sim N(0, V_i)\n\\]"
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#gee-table-해석하기",
    "href": "posts/2024-09-13 jstable/index.html#gee-table-해석하기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GEE table 해석하기",
    "text": "GEE table 해석하기\njstable 패키지를 사용하여 군집 데이터의 GEE적용 후 결과 해석에 대해 알아보겠습니다. 아래 코드는 R에서 제공되는 geepack의 dietox 데이터를 이용하여 돼지의 체중을 예상하는 모델을 만드는 예시입니다. 시간과 구리 보충제 상태가 돼지의 체중에 미치는 영향을 분석해보겠습니다. 군집 데이터의 특성을 반영하여, 돼지별 데이터를 분석하였음을 코드에서도 알 수 있습니다.\n\nlibrary(geepack)  ## for dietox data\ndata(dietox)\ndietox$Cu &lt;- as.factor(dietox$Cu)\ndietox$ddn &lt;- as.numeric(rnorm(nrow(dietox)) &gt; 0)\ngee01 &lt;- geeglm (Weight ~ Time + Cu , id = Pig, data = dietox, family = gaussian, corstr = \"ex\")\ngeeglm.display(gee01)\n\n$caption\n[1] \"GEE(gaussian) predicting Weight by Time, Cu - Group Pig\"\n\n$table\n               crude coeff(95%CI)   crude P value adj. coeff(95%CI)  \nTime           \"6.94 (6.79,7.1)\"    \"&lt; 0.001\"     \"6.94 (6.79,7.1)\"  \nCu: ref.=Cu000 NA                   NA            NA                 \n      035      \"-0.59 (-3.73,2.54)\" \"0.711\"       \"-0.84 (-3.9,2.23)\"\n      175      \"1.9 (-1.87,5.66)\"   \"0.324\"       \"1.77 (-1.9,5.45)\" \n               adj. P value\nTime           \"&lt; 0.001\"   \nCu: ref.=Cu000 NA          \n      035      \"0.593\"     \n      175      \"0.345\"     \n\n$metric\n                                 crude coeff(95%CI) crude P value\n                                 NA                 NA           \nEstimated correlation parameters \"0.775\"            NA           \nNo. of clusters                  \"72\"               NA           \nNo. of observations              \"861\"              NA           \n                                 adj. coeff(95%CI) adj. P value\n                                 NA                NA          \nEstimated correlation parameters NA                NA          \nNo. of clusters                  NA                NA          \nNo. of observations              NA                NA          \n\n\n해석을 위해 jstable 패키지를 사용하여 결과를 표로 나타내었습니다. 시간의 p-value를 확인하였을 때, 시간은 돼지 체중 증가에 유의미한 영향을 미치고 있다고 할 수 있으며, 계수를 보았을 때 단위 시간당 체중이 6.94만큼 증가한다고 추정할 수 있습니다. 구리 보충제의 경우 두가지 종류의 보충제 모두 통계적으로 유의미한 영향을 미치지 않는다고 해석할 수 있습니다. Metric 부분에서는 모델의 상관구조와 관련된 정보를 알 수 있습니다. Estimated correlation parameters의 경우 같은 군집의 시간에 따른 관측값들 사이의 상관관계를 나타내며, 0.775는 높은 상관관계를 의미합니다. 아래의 cluster 와 observation에서는 총 72마리의 돼지에 대하여 861개의 관측치로 분석을 했음을 알 수 있습니다.\n\ngee02 &lt;- geeglm (ddn ~ Time + Cu , id = Pig, data = dietox, family = binomial, corstr = \"ex\")\ngeeglm.display(gee02)\n\n$caption\n[1] \"GEE(binomial) predicting ddn by Time, Cu - Group Pig\"\n\n$table\n               crude OR(95%CI)    crude P value adj. OR(95%CI)     adj. P value\nTime           \"1.01 (0.97,1.05)\" \"0.609\"       \"1.01 (0.97,1.05)\" \"0.615\"     \nCu: ref.=Cu000 NA                 NA            NA                 NA          \n      035      \"1.2 (0.91,1.58)\"  \"0.198\"       \"1.2 (0.91,1.58)\"  \"0.199\"     \n      175      \"1.21 (0.91,1.61)\" \"0.189\"       \"1.21 (0.91,1.61)\" \"0.19\"      \n\n$metric\n                                 crude OR(95%CI) crude P value adj. OR(95%CI)\n                                 NA              NA            NA            \nEstimated correlation parameters \"-0.022\"        NA            NA            \nNo. of clusters                  \"72\"            NA            NA            \nNo. of observations              \"861\"           NA            NA            \n                                 adj. P value\n                                 NA          \nEstimated correlation parameters NA          \nNo. of clusters                  NA          \nNo. of observations              NA          \n\n\n이번에는 이항분포를 사용하여 돼지의 체중 증가에 대한 모델을 만들어보았습니다. 이전 코드와는 다르게 family에 binomial을 입력하였으며, ddn은 임의로 0,1이 있는 이항분포 변수이며 이를 시간과 구리 보충제 상태로 예측하는 모델입니다. 이항분포이기 때문에 회귀계수 대신 OR이 있는 것을 알 수 있습니다. 변수들 모두 p-value가 통계적으로 의미 있는 수준은 아니라는 것을 알 수 있습니다. ddn이 임의 변수이기 때문에 시간이나 구리 보충제 여부에 따른 오즈비가 1에 가까울 것일 거라고 추측할 수 있고, 실제로 그러한 결과를 보여줍니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#jstable-패키지를-사용한-lmm-table만들기",
    "href": "posts/2024-09-13 jstable/index.html#jstable-패키지를-사용한-lmm-table만들기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "jstable 패키지를 사용한 LMM table만들기",
    "text": "jstable 패키지를 사용한 LMM table만들기\n아래 코드는 R에서 제공되는 geepack의 dietox 데이터를 이용하여 돼지의 체중을 예상하는 모델을 만드는 예시입니다. GEE에서와 다르게 각 돼지별 체중의 차이가 다르다는 점을 반영한 모델을 만들어볼 것입니다.\n\nlibrary(lme4)\nl1 &lt;- lmer(Weight ~ Time + Cu + (1|Pig), data = dietox) \nlmer.display(l1, ci.ranef = T)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nComputing profile confidence intervals ...\n\n\n$table\n                     crude coeff(95%CI) crude P value adj. coeff(95%CI)\nTime                   6.94 (6.88,7.01)     0.0000000  6.94 (6.88,7.01)\nCu: ref.=Cu000                     &lt;NA&gt;            NA              &lt;NA&gt;\n      035            -0.58 (-4.67,3.51)     0.7811327 -0.84 (-4.47,2.8)\n      175              1.9 (-2.23,6.04)     0.3670740  1.77 (-1.9,5.45)\nRandom effects                     &lt;NA&gt;            NA              &lt;NA&gt;\nPig                 40.34 (28.08,54.95)            NA              &lt;NA&gt;\nResidual             11.37 (10.3,12.55)            NA              &lt;NA&gt;\nMetrics                            &lt;NA&gt;            NA              &lt;NA&gt;\nNo. of groups (Pig)                  72            NA              &lt;NA&gt;\nNo. of observations                 861            NA              &lt;NA&gt;\nLog-likelihood                  -2400.8            NA              &lt;NA&gt;\nAIC value                        4801.6            NA              &lt;NA&gt;\n                    adj. P value\nTime                   0.0000000\nCu: ref.=Cu000                NA\n      035              0.6527264\n      175              0.3442309\nRandom effects                NA\nPig                           NA\nResidual                      NA\nMetrics                       NA\nNo. of groups (Pig)           NA\nNo. of observations           NA\nLog-likelihood                NA\nAIC value                     NA\n\n$caption\n[1] \"Linear mixed model fit by REML : Weight ~ Time + Cu + (1 | Pig)\"\n\n\n고정효과인 Time과 Cu 에 대해서는 이번에도 시간만 통계적으로 유의미한 변수로 나왔으며, 체중의 증가에 통계적으로 유의미한 영향을 미치는 것을 알 수 있습니다. 이번에는 임의 효과에 대해서 조금 더 자세히 알아보겠습니다. 돼지 데이터에서 임의 효과의 분산은 40.34로 나왔습니다. 이는 돼지마다 체중의 차이가 크다는 것을 알 수 있으며, 11.37이라는 잔차는 돼지별 임의 효과를 반영한 후에도 남는 체중의 변동이 있음을 설명합니다.\n\nl2 &lt;- glmer(ddn ~ Weight + Time + (1|Pig), data= dietox, family= \"binomial\")\n\nboundary (singular) fit: see help('isSingular')\n\nlmer.display(l2)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n$table\n                     crude OR(95%CI) crude P value  adj. OR(95%CI) adj. P value\nWeight                    1 (1,1.01)     0.7020746   1 (0.98,1.02)    0.8041261\nTime                1.01 (0.97,1.05)     0.6358656 1.03 (0.9,1.17)    0.7088793\nRandom effects                  &lt;NA&gt;            NA            &lt;NA&gt;           NA\nPig                                0            NA            &lt;NA&gt;           NA\nMetrics                         &lt;NA&gt;            NA            &lt;NA&gt;           NA\nNo. of groups (Pig)               72            NA            &lt;NA&gt;           NA\nNo. of observations              861            NA            &lt;NA&gt;           NA\nLog-likelihood               -596.49            NA            &lt;NA&gt;           NA\nAIC value                    1200.98            NA            &lt;NA&gt;           NA\n\n$caption\n[1] \"Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) : ddn ~ Weight + Time + (1 | Pig)\"\n\n\n이항 분포를 가지는 변수에는 GLM과 비슷한 방식으로 일반화 된 방식인 GLMM이 적용이 가능합니다. ddn의 경우 임의로 형성된 변수이기 때문에 통계적으로 유의미하지 않은 고정 변수들을 확인할 수 있으며 OR모두 1에 가깝다는 것을 확인할 수 있습니다. 임의효과인 돼지의 분산의 경우 앞선 LMM보다 낮게 나왔습니다. 이는 0과 1만 가지는 이항데이터를 다루기 때문에, 돼지 간의 기초 수준 차이가 연속형 데이터처럼 크게 반영되지 않았기 떄문일 것입니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#cluster-옵션을-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#cluster-옵션을-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "cluster 옵션을 사용한 Cox model",
    "text": "cluster 옵션을 사용한 Cox model\nlung의 데이터는 228명의 환자에 대한 생존 데이터를 포함하고 있습니다. ECOG와 나이에 따른 폐암 환자의 생존을 예측하는 모델을 만들고 싶은데, 같은 병원에서 치료를 받게 되면 병원의 비슷한 프로토콜 등에 영향을 받아 비슷한 치료 결과를 보일 수 있습니다. 따라서, 같은 병원 내의 환자들의 상관관계를 고려하여 만들고 싶을 때 cluster 옵션을 사용할 수 있습니다.\n\nfit1 &lt;- coxph(Surv(time, status) ~ ph.ecog + age, cluster = inst, lung, model = T)\ncox2.display(fit1)\n\n$table\n        crude HR(95%CI)       crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"38.34 (38.32,38.36)\" \"&lt; 0.001\"     \"33.03 (33,33.06)\" \"&lt; 0.001\"   \nage     \"14.67 (14.46,14.88)\" \"0.007\"       \"5.59 (4.73,6.61)\" \"0.085\"     \n\n$ranef\n        [,1] [,2] [,3] [,4]\ncluster   NA   NA   NA   NA\ninst      NA   NA   NA   NA\n\n$metric\n                        [,1] [,2] [,3] [,4]\n&lt;NA&gt;                      NA   NA   NA   NA\nNo. of observations  226.000   NA   NA   NA\nNo. of events        163.000   NA   NA   NA\nAIC                 1463.797   NA   NA   NA\n\n$caption\n[1] \"Marginal Cox model on time ('time') to event ('status') - Group inst\"\n\n\ntable의 결과를 보았을 때 ecog는 환자의 생존에 강한 영향을 미치며 통계적으로도 유의미하다는 것을 알 수 있습니다. age 변수의 경우 조정되지 않은 모델에서는 유의미한 위험 요인으로 나타났지만, ecog를 조정한 이후에는 통계적으로 유의미하지 않았다는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#frailty-옵션을-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#frailty-옵션을-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "frailty 옵션을 사용한 Cox model",
    "text": "frailty 옵션을 사용한 Cox model\n같은 병원 내의 환자들간의 상관관계 외에 병원간 차이를 고려하고 싶을 수도 있습니다. 일부 병원은 다른 병원보다 더 좋은 생존 결과를 가지고 있을 수 있기 때문에, 이에 따른 변수를 frailty 옵션을 사용해서 고려합니다.\n\nfit2 &lt;- coxph(Surv(time, status) ~ ph.ecog + age + frailty(inst), lung, model = T)\ncox2.display(fit1)\n\n$table\n        crude HR(95%CI)       crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"38.34 (38.32,38.36)\" \"&lt; 0.001\"     \"33.03 (33,33.06)\" \"&lt; 0.001\"   \nage     \"14.67 (14.46,14.88)\" \"0.007\"       \"5.59 (4.73,6.61)\" \"0.085\"     \n\n$ranef\n        [,1] [,2] [,3] [,4]\ncluster   NA   NA   NA   NA\ninst      NA   NA   NA   NA\n\n$metric\n                        [,1] [,2] [,3] [,4]\n&lt;NA&gt;                      NA   NA   NA   NA\nNo. of observations  226.000   NA   NA   NA\nNo. of events        163.000   NA   NA   NA\nAIC                 1463.797   NA   NA   NA\n\n$caption\n[1] \"Marginal Cox model on time ('time') to event ('status') - Group inst\"\n\n\ntable의 결과를 보았을 때 ecog와 age가 생존에 미치는 영향은 앞선 cluster 옵션을 이용했을 때와 비슷한 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#mixed-effect를-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#mixed-effect를-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "mixed effect를 사용한 Cox model",
    "text": "mixed effect를 사용한 Cox model\n다층 구조나 복잡한 데이터 구조를 처리하고 싶은 경우 frailty 옵션만으로 부족할 수 있습니다. 예를 들어, 병원 간의 기초 수준 차이와 성별에 따른 기초 위험이 차이가 나는 것을 반영한 모델을 만들고 싶을 수 있습니다. 해당 경우 coxme함수를 이용할 수 있습니다.\n\nlibrary(coxme)\nfit &lt;- coxme(Surv(time, status) ~ ph.ecog + age + (1|inst)+(1|sex), lung)\ncoxme.display(fit) \n\nWarning in formula.character(object, env = baseenv()): Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n  Consider formula(paste(x, collapse = \" \")) instead.\nWarning in formula.character(object, env = baseenv()): Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n  Consider formula(paste(x, collapse = \" \")) instead.\n\n\n$table\n        crude HR(95%CI)    crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"1.66 (1.32,2.09)\" \"&lt; 0.001\"     \"1.63 (1.3,2.05)\"  \"&lt; 0.001\"   \nage     \"1.02 (1,1.04)\"    \"0.043\"       \"1.01 (0.99,1.03)\" \"0.234\"     \n\n$ranef\n                [,1] [,2] [,3] [,4]\nRandom effect     NA   NA   NA   NA\ninst(Intercept) 0.02   NA   NA   NA\nsex(Intercept)  0.14   NA   NA   NA\n\n$metric\n                    [,1] [,2] [,3] [,4]\n&lt;NA&gt;                  NA   NA   NA   NA\nNo. of groups(inst)   18   NA   NA   NA\nNo. of groups(sex)     2   NA   NA   NA\nNo. of observations  226   NA   NA   NA\nNo. of events        163   NA   NA   NA\n\n$caption\n[1] \"Mixed effects Cox model on time ('time') to event ('status') - Group inst, sex\""
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html",
    "href": "posts/2024-08-22-competingrisk/index.html",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "",
    "text": "관심이 있는 어떤 event가 outcome에 미치는 영향에 대한 생존분석을 진행할 때 다른 사건에 의해 follow up이 중단되는 상황이 발생할 수 있습니다. 이렇게 관측을 끝내게 만드는 의도치 않은 event를 ’competing risk’라 부릅니다. competing risk로 인한 데이터의 손실과, 결과의 왜곡을 줄이기 위한 2가지 방법으로 fine and gray method와 multi state model을 소개합니다."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#데이터-전처리",
    "href": "posts/2024-08-22-competingrisk/index.html#데이터-전처리",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "1.1 데이터 전처리",
    "text": "1.1 데이터 전처리\n실습에서는 mgus2 데이터를 사용하겠습니다. mgus 데이터는 다발성 골수종 환자에 대한 데이터로, pstat이 1인 경우 다발성 골수종에 걸린 것으로 표시됩니다.\n\ndata &lt;- mgus2\nhead(data)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1\n2  2  78   F 1968 11.5   1.2    2.0    25     0     25     1\n3  3  94   M 1980 10.5   1.5    2.6    46     0     46     1\n4  4  68   M 1977 15.2   1.2    1.2    92     0     92     1\n5  5  90   F 1973 10.7   0.8    1.0     8     0      8     1\n6  6  90   M 1990 12.9   1.0    0.5     4     0      4     1\n\n\n이후 data의 event를 다발성 골수종에 걸렸을 경우(pcm) 1, 다발성 골수종에 걸리지 않았지만 사망한 경우(death) 2, 다발성 골수종에 걸리지도 않았고 생존한 경우(censor) 0으로 코딩합니다. competing risk가 고려하고자 하는 데이터는 다벌성 골수종이 아닌 다른 이유로 사망한 2번 케이스 일것입니다.\n\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))"
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#competing-risk-제거",
    "href": "posts/2024-08-22-competingrisk/index.html#competing-risk-제거",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "1.2 competing risk 제거",
    "text": "1.2 competing risk 제거\nfinegray 함수를 사용하면 해당 데이터가 death로 끝났을 경우, 환자가 사망하지 않았다고 가정한 새로운 데이터들을 여러 시간대 별로 제작합니다. 각 시간대별로 환자가 생존했을 확률을 Kaplan-meier estimate로 계산함으로서 이를 weight로 사용합니다.\n\npdata &lt;- finegray(Surv(etime, event) ~ ., data=data)\nhead(pdata)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death fgstart fgstop\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1       0     35\n2  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      35     44\n3  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      44     47\n4  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      48     52\n5  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      53     56\n6  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      56     57\n  fgstatus      fgwt\n1        0 1.0000000\n2        0 0.9990449\n3        0 0.9980368\n4        0 0.9959629\n5        0 0.9905896\n6        0 0.9873022\n\n\npdata에서 볼 수 있다 싶이 id 1번 환자는 실제로 30일 follow up 이후 사망했지만, pdata에서는 환자가 30일 이후 생존했을 경우를 가정하고, fgwt로 인위적인 데이터의 weight를 보여주고 있습니다. 실제로 시간이 길어질 수록 환자가 생존할 가능성은 떨어지기에 weight 역시 감소하는 것을 볼 수 있습니다. pdata의 event로 사용되는 fgstatus는 다발성 골수종의 발생에만 집중하여 다발성 골수종 발생 경우 1,(data의 pcm과 동일) 미발생시 0(censor와 동일)으로 표시하고 있습니다.\n이제 competing risk로 인한 데이터를 제거했기 때문에 기존의 cox 분석으로 결과를 얻을 수 있습니다.\n\nfgfit &lt;- coxph(Surv(fgstart, fgstop, fgstatus) ~ age+sex,\n               weight=fgwt, data=pdata, model = T)\nsummary(fgfit)\n\nCall:\ncoxph(formula = Surv(fgstart, fgstop, fgstatus) ~ age + sex, \n    data = pdata, weights = fgwt, model = T)\n\n  n= 41775, number of events= 115 \n\n          coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)   \nage  -0.017302  0.982847  0.007022  0.005528 -3.13  0.00175 **\nsexM -0.259757  0.771239  0.187049  0.181707 -1.43  0.15285   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     0.9828      1.017    0.9723    0.9936\nsexM    0.7712      1.297    0.5402    1.1012\n\nConcordance= 0.548  (se = 0.026 )\nLikelihood ratio test= 7.28  on 2 df,   p=0.03\nWald test            = 11.19  on 2 df,   p=0.004\nScore (logrank) test = 7.58  on 2 df,   p=0.02,   Robust = 9.28  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#데이터-전처리-1",
    "href": "posts/2024-08-22-competingrisk/index.html#데이터-전처리-1",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "2.1 데이터 전처리",
    "text": "2.1 데이터 전처리\n이번 실습에서도 mgus2 데이터를 사용합니다. 다만 해당 MSM 모델에서는 data의 event 역시 censor, pcm, death를 위 fine and gray method와 동일하게 설정했습니다. 이 경우 역시, pcm을 거치지 않고 바로 death로 가는 상황이 competing risk라 할 수 있습니다.\n\n\ndata &lt;- mgus2\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))"
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#msm-모델-제작",
    "href": "posts/2024-08-22-competingrisk/index.html#msm-모델-제작",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "2.2 MSM 모델 제작",
    "text": "2.2 MSM 모델 제작\n이후 data를 기존에 0과 1로 outcome이 이진 분류된 데이터로 cox 모델을 돌리듯 모델을 제작해주면 각 상태(pcm, death by pcm)로의 진행에 대한 회귀분석이 진행됩니다. MSM를 통해 ’다발성 골수종으로의 진행’에 대한 생존 분석과, ’그 이외 영향으로 인한 사망’에 대한 생존 분석을 한 번에 진행할 수 있습니다.\n\nMSMfit &lt;- coxph(Surv(etime, event) ~ sex+ age, data = data, id = id, model = T)\nsummary(MSMfit)\n\nCall:\ncoxph(formula = Surv(etime, event) ~ sex + age, data = data, \n    model = T, id = id)\n\n  n= 1384, number of events= 975 \n\n              coef exp(coef)  se(coef) robust se      z Pr(&gt;|z|)    \nsexM_1:2 -0.025138  0.975176  0.188456  0.189391 -0.133   0.8944    \nage_1:2   0.013039  1.013124  0.008259  0.006678  1.953   0.0509 .  \nsexM_1:3  0.393226  1.481753  0.069698  0.066156  5.944 2.78e-09 ***\nage_1:3   0.064824  1.066971  0.003620  0.003691 17.562  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nsexM_1:2    0.9752     1.0255    0.6728     1.413\nage_1:2     1.0131     0.9870    1.0000     1.026\nsexM_1:3    1.4818     0.6749    1.3016     1.687\nage_1:3     1.0670     0.9372    1.0593     1.075\n\nConcordance= 0.66  (se = 0.01 )\nLikelihood ratio test= 389.5  on 4 df,   p=&lt;2e-16\nWald test            = 326.7  on 4 df,   p=&lt;2e-16\nScore (logrank) test = 335.4  on 4 df,   p=&lt;2e-16,   Robust = 285.9  p=&lt;2e-16\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#fine-and-gray의-장단점-적용",
    "href": "posts/2024-08-22-competingrisk/index.html#fine-and-gray의-장단점-적용",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "3.1 fine and gray의 장단점, 적용",
    "text": "3.1 fine and gray의 장단점, 적용\nFine and Gray의 가장 큰 장점은 단순성입니다. competing risks를 다룰 때 문제를 단순화하여, multi state한 상황을 binomial한 문제로 축소시켜 데이터 해석이 쉬워집니다. 이런 장점은 lifetime risk를 추정하는 것을 용이하게 해주며, 특정 사건이 발생할 종합적인 위험을 평가하는데 강점을 가집니다(ex, 여성의 경우 PCM에 대한 평생 리스크가 나이와 혈청 m-스파이크를 조정한 후에도 1.2배 더 높다).\n다만 모델이 새로운 데이터를 인위적으로 만들고, 비례위험 가정을 통해 weight를 부여하기에 그만큼 잘못된 결과가 나올 위험성이 있습니다. 더불어 간단한 competing risk 상황에만 적용 가능하며, risk들간의 관계(ex. pcm으로 인한 death가 일어날 수 있는 케이스)를 반영할 수 없다는 한계가 있습니다."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#msm의-장단점-적용",
    "href": "posts/2024-08-22-competingrisk/index.html#msm의-장단점-적용",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "3.2 MSM의 장단점, 적용",
    "text": "3.2 MSM의 장단점, 적용\nMSM은 다양한 competing risk 상황을 구현할 수 있다는 장점이 있습니다. pstate, death 등의 변수로 event를 더 세부화할시 Fine and Gray에서는 구현하지 못하는 pcm으로 인한 death 등의 관계를 적용시킬 수 있습니다. 위에서 살펴본 것 같이 competing risk를 제거하는 것이 아닌, competing risk에 의한 효과도 확인할 수 있다는 점에서 더욱 더 세심한 분석이 가능합니다.\n다만 더 자세한 모델을 구현하기 위해 state간 전이시간 등 model의 제작을 위해 더 많은 정보를 필요로 한다는 것이 단점입니다. 따라서 MSM은 환자에 질병 진행 과정에 대한 세세한 데이터가 존재할 때 각 상태의 전이의 확인이 필요할 때 적용할 수 있습니다.\n따라서 ’관찰하려는 event의 발생에만 관심이 있고, 그 이외 영향은 모두 제거하고 싶다’는 목적으로 competing risk를 처리하고자 할 때는 fine and gray 방법을,’관측 중 발생한 모든 event에 대한 세세한 정보를 얻고 싶다’는 목적일 때는 MSM을 쓰면 되겠습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html",
    "href": "posts/2024-05-24-app-design/index.html",
    "title": "Shiny app 꾸미기",
    "section": "",
    "text": "기본 shiny app은 스타일이 아쉽습니다. 디자인은 신경 쓰지 않고 기능에만 집중하시는 분들에게는 큰 문제가 되지 않지만 디자인도 서비스에서 매우 중요한 역할을 한다는 점은 부정할 수 없습니다. 이번 포스트에서는 간단하게 shiny app을 꾸밀 수 있는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#page_navbar-nav_panel",
    "href": "posts/2024-05-24-app-design/index.html#page_navbar-nav_panel",
    "title": "Shiny app 꾸미기",
    "section": "page_navbar, nav_panel",
    "text": "page_navbar, nav_panel\n전반적인 테마를 바꾸기 위해 bslib을 불러옵니다. bslib 패키지가 설치되어있지 않다면 install.packages(\"bslib\")을 입력하여 패키지를 설치합니다. 그리고 navbarPage를 page_navbar로, tabPanel을 nav_panel로 바꿔줍니다. 두 함수 모두 shiny 패키지의 함수와 기능은 동일하지만 스타일을 커스텀하기에 더 적합합니다.\n\nlibrary(shiny); library(bslib)\n\nui &lt;- page_navbar(title = \"Obesity classification\",\n                 nav_panel(title = \"Data\"),\n                 ...\n                 )\n\nserver &lt;- function(input, ouput) {\n  ...\n}\n\nshinyApp(ui = ui, server = server)\n\n\n전반적으로 버튼의 디자인이 바뀌었고 선택된 탭을 표시하는 방법도 바뀌었습니다.\npage_navbar함수에는 theme = bs_theme(version = 5)가 기본값으로 적용됩니다. 필요에 따라 해당 인자를 조정하여 부트스트랩 버전을 변경할 수 있습니다.\n차라투 홈페이지와 shiny app의 내비게이션 바의 배경색을 동일하게 하여 통일감을 주고 글자크기도 변경해보겠습니다.\n웹에서 마우스 오른쪽 버튼을 눌러 ’페이지 소스 보기’를 클릭하면 차라투 홈페이지 내비게이션 바 색상의 hex color code는 “#051F20”이고 shiny app의 내비게이션 바 타이틀의 클래스는 navbar-brand, 각 탭의 클래스는 nav-link인 것을 확인할 수 있습니다. 여기서 알아낸 것들을 아래와 같이 적용해보겠습니다.\n\nlibrary(shiny); library(bslib)\n\nui &lt;- page_navbar(title = \"Obesity classification\",\n                  bg = \"#051F20\",\n                  header = tags$head(\n                    tags$style(HTML(\"\n                        .navbar-brand {\n                            font-size: 30px;\n                        }\n                        .nav-link {\n                            font-size: 18px;\n                        }\")\n                        )\n                    ),\n                  nav_panel(title = \"Data\"),\n                  ...\n                  )\n\nserver &lt;- function(input, ouput) {\n  ...\n}\n\nshinyApp(ui = ui, server = server)\n\n\n설정했던 대로 내비게이션 바의 배경색과 글자크기가 바뀐 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#nav_spacer-nav_menu",
    "href": "posts/2024-05-24-app-design/index.html#nav_spacer-nav_menu",
    "title": "Shiny app 꾸미기",
    "section": "nav_spacer, nav_menu",
    "text": "nav_spacer, nav_menu\n여기를 참고해서 내비게이션 바의 우측에 드롭다운 메뉴로 회사 홈페이지와 커뮤니티 페이지 바로가기를 만들겠습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nlink_zarathu &lt;- tags$a(\n  tags$div(\n    style = \"display: inline-block; background-color: #white;\",\n    tags$img(src = \"https://raw.githubusercontent.com/zarathucorp/blog/master/img/logo_favicon.png\",\n             height = \"35px\", width = \"35px\")\n  ),\n  \"차라투 홈페이지\",\n  href = \"https://www.zarathu.com/\",\n  target = \"_blank\"\n)\n\nlink_community &lt;- tags$a(\n  tags$div(\n    style = \"display: inline-block; background-color: #white;\",\n    tags$img(src = \"https://raw.githubusercontent.com/zarathucorp/zarathu/master/public/img/about/logo.jpg\",\n             height = \"40px\", width = \"40px\")\n  ),\n  \"연구지원 신청\",\n  href = \"https://community.zarathu.com/\",\n  target = \"_blank\"\n)\n\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Logistic regression\",\n                            ...),\n                  nav_spacer(),\n                  nav_menu(\n                    title = \"Links\",\n                    align = \"right\",\n                    nav_item(link_zarathu),\n                    nav_item(link_community)\n                    )\n                  )\n\n\ntags$div()의 인자들을 가지고 로고, 글자크기, 배경색 등을 필요에 맞게 조절할 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#radiogroupbuttons",
    "href": "posts/2024-05-24-app-design/index.html#radiogroupbuttons",
    "title": "Shiny app 꾸미기",
    "section": "radioGroupButtons",
    "text": "radioGroupButtons\nshiny의 radioButtons를 shinyWidgets의 radioGroupButtons로 다음과 같이 박스로 바꾸어 선택된 박스에 색이 채워지도록 할 수 있습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Plot\",\n                            sidebarLayout(\n                              sidebarPanel(\n                                # radioButtons(inputId = \"plot_type\", label = \"Plot type\",\n                                #              choices = c(\"violin\", \"box\"), inline = TRUE),\n                                radioGroupButtons(inputId = \"plot_type\", label = \"Plot type\",\n                                                  choices = c(\"violin\", \"box\"), direction = \"horizontal\",\n                                                  individual = TRUE),\n                                ...\n                              ),\n                              mainPanel(...)\n                              )\n                            ),\n                  ...\n                  )\n\n\n\n\nshiny::radioButtons\n\n\n\nshinyWidgets::radioGroupButtons\n\n\n\n당연히 세 개 이상의 버튼도 만들 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#virtualselectinput",
    "href": "posts/2024-05-24-app-design/index.html#virtualselectinput",
    "title": "Shiny app 꾸미기",
    "section": "virtualSelectInput",
    "text": "virtualSelectInput\nshiny의 selectInput을 shinyWidgets의 virtualSelectInput으로 바꿔보겠습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nvarlist &lt;- list(\n  base = c(\"Gender\", \"Age\", \"Height\", \"Weight\", \"family_history_with_overweight\"),\n  condition = c(\"FAVC\", \"FCVC\", \"NCP\", \"CAEC\", \"SMOKE\", \"CH2O\", \"SCC\", \"FAF\", \"TUE\", \"CALC\", \"MTRANS\"),\n  outcome = \"NObeyesdad\"\n)\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Linear regression\",\n                            sidebarLayout(\n                              sidebarPanel(\n                                ...,\n                                # selectInput(inputId = \"ind_var\", label = \"Independent variable\",\n                                #             choices = list(\"base\" = varlist$base, \"condition\" = varlist$condition),\n                                #             multiple = TRUE),\n                                virtualSelectInput(inputId = \"ind_var\", label = \"Independent variable\",\n                                                   choices = list(\"base\" = varlist$base, \"condition\" = varlist$condition),\n                                                   multiple = TRUE, search = TRUE, showValueAsTags = TRUE)\n                              ),\n                              mainPanel(...)\n                              )\n                            ),\n                  ...\n                  )\n\n\n\n\nshiny::selectInput\n\n\n\nshinyWidgets::virtualSelectInput\n\n\n\n기존의 seletInput과는 다르게 전체선택/해제가 가능하고, choices인자에 리스트를 넣었을 때 카테고리별로 선택/해제가 가능하다는 점이 큰 장점으로 다가왔습니다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html",
    "href": "posts/2024-05-13-rhub/index.html",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECK란 R 패키지를 개발한 이후 “정상적으로 개발 되었는지” 검증하는 약 50개 이상의 체크리스트를 실행하는 과정으로, 함수의 사용법이 제대로 작성되었는지, 함수의 parameter가 제대로 작성되었는지 등을 포함한다.\n물론 R CMD CHECK를 엄격하게 수행하지 않아도 github 를 통해 패키지를 배포하고 실행하는 것에 문제는 없지만, 이러한 과정을 통해 패키지의 오류를 최소화하고 사용자에게 안정적인 패키지를 제공할 수 있다는 것이 검증 되어야만 CRAN과 같은 공식 리포지토리를 통해 패키지를 공유할 수 있다.\n이 포스팅에서는 구체적인 내용을 다루진 않지만, 관심이 있다면 Hadley Wickham의 R Packages를 참고하는 것도 좋다.\n아무튼 R CMD CHECK 는 devtools패키지를 사용하여 R 패키지를 만들었다면, devtools::check()함수 또는 Rstudio에서 Check 버튼으로 할 수 있고, Warnings, Errors, Notes 등을 통해 수정되길 권고하는 문제들을 확인할 수 있다.\n\n그러나 이 R CMD CHECK의 특징 중 한가지는 패키지를 개발하는 PC의 환경을 기준으로 체크를 진행한다는 것이다. 즉, 아래의 이미지 같은 경우 macOS (Apple clang) 환경에서는 패키지가 테스트 되었고 실행이 보장되지만 만약 사용자의 OS가 mac이 아닌 window, linux 같은 경우에는 패키지가 정상적으로 작동하지 않을 수도 있다.\nCRAN은 기본 OS를 정하지 않고 있지만 Windows, macOS, linux 중 최소 2개 이상의 OS에서의 테스트에서 문제가 없길 요구하는 만큼 R 패키지를 개발하는 것은 다양한 OS에서의 테스트를 포함하기도 한다.\n이를 위해 다양한 OS 하드웨어, 즉 windows PC, mac, linux 서버가 있다면 베스트겠지만 이러한 경우는 많지 않고 대부분 Github action, AppVeyor, Travis CI 등의 CI/CD 서비스를 활용해 다양한 OS에서의 테스트를 수행하게 된다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "href": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECK란 R 패키지를 개발한 이후 “정상적으로 개발 되었는지” 검증하는 약 50개 이상의 체크리스트를 실행하는 과정으로, 함수의 사용법이 제대로 작성되었는지, 함수의 parameter가 제대로 작성되었는지 등을 포함한다.\n물론 R CMD CHECK를 엄격하게 수행하지 않아도 github 를 통해 패키지를 배포하고 실행하는 것에 문제는 없지만, 이러한 과정을 통해 패키지의 오류를 최소화하고 사용자에게 안정적인 패키지를 제공할 수 있다는 것이 검증 되어야만 CRAN과 같은 공식 리포지토리를 통해 패키지를 공유할 수 있다.\n이 포스팅에서는 구체적인 내용을 다루진 않지만, 관심이 있다면 Hadley Wickham의 R Packages를 참고하는 것도 좋다.\n아무튼 R CMD CHECK 는 devtools패키지를 사용하여 R 패키지를 만들었다면, devtools::check()함수 또는 Rstudio에서 Check 버튼으로 할 수 있고, Warnings, Errors, Notes 등을 통해 수정되길 권고하는 문제들을 확인할 수 있다.\n\n그러나 이 R CMD CHECK의 특징 중 한가지는 패키지를 개발하는 PC의 환경을 기준으로 체크를 진행한다는 것이다. 즉, 아래의 이미지 같은 경우 macOS (Apple clang) 환경에서는 패키지가 테스트 되었고 실행이 보장되지만 만약 사용자의 OS가 mac이 아닌 window, linux 같은 경우에는 패키지가 정상적으로 작동하지 않을 수도 있다.\nCRAN은 기본 OS를 정하지 않고 있지만 Windows, macOS, linux 중 최소 2개 이상의 OS에서의 테스트에서 문제가 없길 요구하는 만큼 R 패키지를 개발하는 것은 다양한 OS에서의 테스트를 포함하기도 한다.\n이를 위해 다양한 OS 하드웨어, 즉 windows PC, mac, linux 서버가 있다면 베스트겠지만 이러한 경우는 많지 않고 대부분 Github action, AppVeyor, Travis CI 등의 CI/CD 서비스를 활용해 다양한 OS에서의 테스트를 수행하게 된다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#github-action",
    "href": "posts/2024-05-13-rhub/index.html#github-action",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Github action",
    "text": "Github action\n이 포스팅에서는 Github action을 기준으로 소개하지만 다른 서비스도 과정과 프로세스는 대동소이하다.\nGithub action은 Github에서 제공하는 CI/CD 서비스로, Github에서 제공하는 다양한 Action을 사용하여 (github 이 제공하는 서버에서의 명령어 실행을 통해) 자동화된 테스트, 빌드, 배포 등을 수행할 수 있다.\n\n여기서 Action이란 yml 파일로 구성된 명령어들의 모음 정도로 생각해도 충분하며, 예시로는 서버에 R을 설치하기, R 패키지를 설치하기, R CMD CHECK를 실행하기 등이 있다. R-hub action 모음, R-lib action 모음 참고.\n즉, 이 R CMD CHECK를 포함한 Action들을 github action을 통해 server에서 실행하고 그 결과를 확인하여 CRAN에 올리는 과정을 거치게 되는데 github action의 문제점 중 하나는 아래의 예시와 같이 action을 사용하기 위한 yml 문법이 상당히 이질적이라는 것이다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-hub2",
    "href": "posts/2024-05-13-rhub/index.html#r-hub2",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "r-hub2",
    "text": "r-hub2\nr-hub 프로젝트는 R 컨소시엄의 프로젝트 중 하나로, R 개발자들이 R 패키지를 더 잘 개발할 수 있게 하는 목적을 가지고 있다. 다양한 OS에서의 테스트를 수행하는 것을 돕는 것 또한 그 중 하나로 위에서 언급한 Github Action을 개발한다거나 인프라를 제공하거나, 커뮤니티를 통해 문제 해결을 돕는 등의 역할이 있다.\n그런데 이 R-hub 프로젝트에서 최근 이 GHA 설정을 돕기 위한 R 패키지, rhub를 개발하여 공개했다.\n공식 블로그에 설명 되어 있는 것처럼, 사용 방법은 기존의 Github Action을 사용하지 않았더라도 다양한 OS에서 R CMD CHECK를 손쉽게 수행할 수 있는 Github Action을 설정할 수 있다.\n제일 먼저 해야하는 것은 당연하게도 rhub 패키지를 설치하는 것이다. 여기서 pak은 기존의 install.packages와 remotes::install_github 같이 다양한 소스에서의 R 패키지 설치를 통합하여 지원하는 함수로 기존의 패키지 설치 방법들을 대체하기를 권장하고 있다.\npak::pkg_install(\"rhub\")\n주의할 점으로 rhub 패키지는 공식적인 버전은 2이지만 rhub2가 아닌 rhub로 설치, 실행해야 하며 포스팅에서는 rhub로 표기하겠다.\n이 rhub 패키지를 실행하기 위해서는 아래 3가지가 필요한데, R 패키지를 개발하여 github에 공유한 경험이 있다면 별도로 새롭게 준비해야 할 것은 없다.\n\ngithub 계정\nR package를 올린 repository. 이때 CRAN을 목적으로 한다면 당연히 Public이어야 한다.\nGithub PAT (Personal Access Token), github 에서 발급 받을 수 있고, gitcreds라는 또 다른 R 패키지를 사용해도 좋다.\n\n\nSetup\nrhub 패키지를 마친 이후 제일 먼저 해야하는 것은 R 패키지 디렉토리에서 rhub_setup 함수를 실행하는 것이다. 이 함수의 역할은 디렉토리의 git repository를 인식하고, Github Action을 위한 yml 파일을 생성한다.\n이전에 만들었던 gemini.R 패키지를 아래 이미지 예시로 사용했다.\n\n큰 문제가 없다면, rhub 패키지에서는 이후 진행해야 할 단계도 안내해준다. 즉, 추가된 yml 파일이 추가된 내용을 github에 커밋하여 업데이트를 반영하고 난 뒤 rhub_doctor 함수를 실행한다.\n\n\nDoctor\nrhub_doctor 함수에서는 Github PAT가 제대로 설정되어 있는지를 확인한다. 이후 소개할 rhub의 rhub_check함수는 Rstudio의 콘솔에서 언급한 Github PAT를 사용하여 수동으로 Github Action을 실행하는 역할을 하기 때문에 PAT의 설정 확인이 필요하다.\nGithub PAT는 “https://github.com/settings/tokens” 링크에서 생성하되 repo와 workflow 권한을 반드시 부여하여 생성해야하만 한다.\n\nRstudio에 Github PAT를 설정 하는 방법은 credential 패키지의 set_github_pat 함수를 이용한다. 링크 참고\nrhub_doctor 함수가 정상적으로 작동했다면 이제 남은 것은 rhub_check 함수를 실행하는 것이다.\n\n\n\nCheck\n이전 단계는 이 함수를 위한 준비 작업이었다 라고 해도 과언이 아니다.\n\nrhub_check 함수는 github repository와 PAT를 인식한 다음 어떤 OS에서 R CMD CHECK를 수행할 것인지 입력값으로 받는다.\n이 때 단순히 Windows, macOS, Linux 외에도 이미지처럼 (rhub 프로젝트에서 제공하는) 다양한 OS를 숫자와 쉼표를 통해 구분하여 선택할 수 있다.\n함수를 실행한 후에는 GHA 페이지로 연결할 수 있는 링크를 제공하는데 이를 통해 진행 상황을 확인할 수 있다.\n\n최종적으로 rhub와 GHA를 사용한 테스트 패스를 repository에 뱃지로 추가하면 아래와 같이 나타난다.\n\n이 때 readme에 뱃지 아이콘을 추가하기 위해서는 다음과 같이 작성해야한다.\n![example workflow](https://github.com/&lt;OWNER&gt;/&lt;REPOSITORY&gt;/actions/workflows/&lt;WORKFLOW_FILE&gt;/badge.svg)\n예시로 사용한 gemini.R은  대신 jhk0530,  대신 gemini.R,  대신 rhub.yaml로 대체한다.\n주의할 점으로 이 Github Action을 통한 R CMD CHECK에는 다소 시간이 소요되기 때문에 우선 개발중인 PC에서 R CMD CHECK를 완료한 이후에 실행하는 것을 권장한다.\n물론 rhub에는 github나 public repository가 아닌 경우를 위한 안내도 있지만, 이는 대부분의 R 패키지, 특히 CRAN과는 크게 연관이 없기 때문에 별도로 서술하지 않는다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#summary",
    "href": "posts/2024-05-13-rhub/index.html#summary",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Summary",
    "text": "Summary\n이번 포스팅에서는 R 패키지 개발자를 위한 Github Action을 활용하기 위한 rhub 패키지를 소개했다. 이를 통해 R 패키지 개발자는 로컬 환경에서 뿐 아니라 다양한 OS에서의 R CMD CHECK를 통해 패키지의 오류를 최소화하고 더 좋은 패키지를 만들 수 있을 것이다.\n원문을 포함한 더 자세한 정보는 rhub의 블로그 에서도 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html",
    "href": "posts/2024-03-30-input-task-button/index.html",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslib이란 bootstrap의 css를 R에서 사용할 수 있도록 만든 패키지입니다. 정확한 원문 설명은 Tools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. 으로, Shiny와 Rmarkdown (당연히 Quarto를 포함)에서 여러 테마를 활용할 수 있게 합니다.\n\n이 글에서는 bslib의 활용 방법들중 Shiny에 집중하여 설명합니다.\n사실 Shiny는 기본적으로 디자인을 위해 bootstrap을 사용합니다. 그런데 Shiny는 패키지를 이루고 있는 구성 요소들과, 관계가 너무 복잡하게 얽혀 있는 상당히 무거운 패키지가 되어버렸고 이로 인해 업데이트에 영향을 받는 부분이 많아, 기능 위주의 업데이트를 하는 것으로 알려져 있습니다.\n즉, UI를 주로 다루는 bootstrap 부분은 5년 전의 버전인 3.4.1 버전을 사용하고 있고, 별도의 테마 설정을 하지 않는다면 특유의 파랑 / 회색 테마를 기본적으로 사용하게 됩니다.(최근 버전은 5.3.3)\n\n그래서 shiny에서는 정체된 UI를 업데이트 하기 위해 UI를 다루는 별도의 R 패키지를 만들어 덮어 씌우듯 최근 bootstrap의 기능들을 제공하게 됩니다.\n\n\n\n\n\n\n이 글에서는 bslib의 주요 사용법은 다루지 않습니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#bslib",
    "href": "posts/2024-03-30-input-task-button/index.html#bslib",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslib이란 bootstrap의 css를 R에서 사용할 수 있도록 만든 패키지입니다. 정확한 원문 설명은 Tools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. 으로, Shiny와 Rmarkdown (당연히 Quarto를 포함)에서 여러 테마를 활용할 수 있게 합니다.\n\n이 글에서는 bslib의 활용 방법들중 Shiny에 집중하여 설명합니다.\n사실 Shiny는 기본적으로 디자인을 위해 bootstrap을 사용합니다. 그런데 Shiny는 패키지를 이루고 있는 구성 요소들과, 관계가 너무 복잡하게 얽혀 있는 상당히 무거운 패키지가 되어버렸고 이로 인해 업데이트에 영향을 받는 부분이 많아, 기능 위주의 업데이트를 하는 것으로 알려져 있습니다.\n즉, UI를 주로 다루는 bootstrap 부분은 5년 전의 버전인 3.4.1 버전을 사용하고 있고, 별도의 테마 설정을 하지 않는다면 특유의 파랑 / 회색 테마를 기본적으로 사용하게 됩니다.(최근 버전은 5.3.3)\n\n그래서 shiny에서는 정체된 UI를 업데이트 하기 위해 UI를 다루는 별도의 R 패키지를 만들어 덮어 씌우듯 최근 bootstrap의 기능들을 제공하게 됩니다.\n\n\n\n\n\n\n이 글에서는 bslib의 주요 사용법은 다루지 않습니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "actionButton",
    "text": "actionButton\nShiny에서 제공하는 기능들은 정말 다양하지만, 핵심 기능을 꼽으라면 actionButton을 꼽을 수 있습니다.\nactionButton이란 사용자가 버튼을 누르면 server에서 미리 선언한 특정 동작을 수행하도록 하는 기능으로 보통은 사용자가 데이터를 업로드 하고 나면, 이 데이터를 활용해 계산 결과를 만들어내게 하는 것에 쓰입니다.\nactionButton의 사용 예시로는 아래의 코드와 같이 (?shiny::actionButton으로 확인할 수 있습니다) 사용자의 선택한 관측수에 맞는 히스토그램을 그릴 수 있게 합니다.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "long actionButton",
    "text": "long actionButton\n그런데 이 actionButton의 문제점 중 하나는 만약 연산에 시간이 오래걸린다면 사용자는 버튼을 누르고 결과를 기다리는 동안 아무것도 할 수 없다는 것입니다.\n심지어 단순히 아무것도 할 수 없는 것을 넘어, 버튼이 클릭되지 않은 것으로 오해하고 버튼을 여러번 클릭하기도 합니다.\n만약 대용량 유전체 데이터를 활용한 계산을 위한 Shiny라면 연산 한번에 분 단위 시간이 필요할 수도 있는데 이는 여러가지 문제점을 초래할 수 있습니다.\n특히 여러번 클릭을 했다면 오랜 시간을 거쳐 연산을 마친 직후 다시 동일한 연산을 또 하고, 또 기다리고, … 의 악순환에 빠지기도 합니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "href": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "title": "bslib: input_task_button 소개",
    "section": "shiny with loading",
    "text": "shiny with loading\nactionButton의 이 문제를 해결하기 위해 여러 방법들이 존재했습니다.\n\n\nprogress indicator를 사용\n\n이 방법은 shiny에서 기본적으로 제공하는 Progress Indicator UI를 활용하는 방법으로 연산의 과정 / 단계가 진행됨에 따라 진행 정도를 사용자에게 보여줄 수 있습니다.\n\nserver &lt;- function(input, output) {\n  output$plot &lt;- renderPlot({\n    input$goPlot \n\n    dat &lt;- data.frame(x = numeric(0), y = numeric(0))\n\n    withProgress(message = 'Making plot', value = 0, {\n      n &lt;- 10\n\n      for (i in 1:n) {\n        dat &lt;- rbind(dat, data.frame(x = rnorm(1), y = rnorm(1)))\n        incProgress(1/n, detail = paste(\"Doing part\", i))\n        Sys.sleep(0.1)\n      }\n    })\n\n    plot(dat$x, dat$y)\n  })\n}\n\nui &lt;- shinyUI(basicPage(\n  plotOutput('plot', width = \"300px\", height = \"300px\"),\n  actionButton('goPlot', 'Go plot')\n))\n\nshinyApp(ui = ui, server = server)\n\n\n그러나 이는 withProgress, incProgress 또는 Progress등의 함수와 오브젝트를 시간이 오래걸리는 연산에 추가로 코드를 작성해야한다는 단점이 있습니다.\n\n별도의 R 패키지 사용\n\nR의 생태계에는 해결하려는 여러 문제가 있고, 그 문제마다의 R 패키지가 있다고 생각해도 과언이 아닌데요. actionButton과 연산 결과 사이의 긴 공백을 UI에 표기하기 위한 기능 또한 마찬가지입니다.\n이전의 progress indicator와 유사하게 추가 코드를 작성하여 해결해야하며 조금 더 디자인이나 세부 설정을 할 수 있는 커스텀 기능이 있다고 생각하면 좋습니다.\n아래는 몇가지 예시 패키지와 사례입니다 (알파벳순).\n\nshinybusy\n\n\n\nshinycssloaders\n\n\n\nshinycustomloader\n\n\n\nwaiter"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "href": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "title": "bslib: input_task_button 소개",
    "section": "input_task_button",
    "text": "input_task_button\ninput_task_button은 위의 방법들과는 다르게 actionButton을 확장한 기능으로, actionButton을 누르면 연산이 진행중임을 알리며 버튼이 비활성화 되며, 연산이 끝나면 다시 버튼이 활성화되는 기능을 제공합니다.\n무엇보다도 가장 큰 차이점은 actionButton을 대체할 수 있기에 추가 코드를 사용할 필요가 없다는 것입니다.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  # actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  input_task_button(\"goButton\", \"Go!\", type = \"success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    Sys.sleep(5)\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)\n\n\ninput_task_button의 사용을 위해 5번째 줄의 actionButton을 6번째 줄의 input_task_button으로 대체하였고, 추가로 의도적으로 오래 걸리는 연산을 만들기 위해 13번째 줄의 Sys.sleep() 코드를 활용하여 5초를 지연시켰습니다. input_task_button 사용 방법은 다음과 같습니다.\n\n\n\n\n\n\ninput_task_button은 actionButton을 무리없이 대체할 수 있지만 약간의 parameter 수정이 필요합니다.\n\n\n\n\n\nactionButton\ninput_task_button\n역할\n\n\n\ninputId\nid\n버튼 id\n\n\nlabel\nlabel\n버튼 라벨\n\n\nicon\nicon\n버튼 아이콘\n\n\n\nlabel_busy\n버튼 비활성화시 라벨\n\n\n\nicon_busy\n버튼 비활성화시 아이콘\n\n\nclass\ntype\n버튼 테마 / 색상\n\n\n\n\n릴리스 노트 원본\ninput_task_button 매뉴얼"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#정리",
    "href": "posts/2024-03-30-input-task-button/index.html#정리",
    "title": "bslib: input_task_button 소개",
    "section": "정리",
    "text": "정리\n이번 글에서는 bslib의 최신 기능인 input_task_button과 간단한 사용 사례를 소개했습니다.\n이는 잠재적으로 사용자 경험을 향상시키는 기능이며, 기존의 shiny에서는 추가 코드를 작성해야하는 번거로움을 해결해주는 기능이라고 생각합니다.\n특히 다른 웹 어플리케이션과 다르게 Shiny에서는 대용량 데이터 연산으로 인해 시간이 오래 걸리는 경우가 많지만, 상대적으로 빈약한 UI/UX를 가지고 있어 기존의 actionButton을 input_task_button으로 대체할 경우 이를 보완하는데 큰 도움이 될 것입니다.\n이번 글이 도움이 되었길 바라며, 다음 글에서 또 만나요!"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html",
    "href": "posts/2024-03-04-shinylive/index.html",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "href": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive",
    "text": "shinylive\nshinylive는 python 버전과 r 버전이 있으며, 이 글에서는 r 버전을 기준으로 소개한다.\nshinylive는 웹 페이지 생성에 필요한 HTML, Javscript, CSS 등의 요소와 shiny 를 사용하기 위한 wasm 관련 파일들을 생성하는 일을 한다.\nshinylive로 만든 예시는 이 링크에서 확인할 수 있다.\n\nshinylive 설치\nshinylive는 CRAN에 올라가 있기도 하지만 최근 릴리즈 된 버전이 0.1.1인만큼 수시로 업데이트 될 수 있어 github의 최신 버전을 사용하는 것을 권장한다. 추가로 pak는 최근 posit에서 R 패키지를 설치하기 위해 권장하는 R 패키지로, 기존의 install.packages(), remotes::install_github() 등의 함수를 대체할 수 있다.\n\n# install.packages(\"pak\")\npak::pak(\"posit-dev/r-shinylive\")\n\n\n\n\n\n\n\nVersion\n\n\n\n통상적으로 1.0 이전의 버전은 아직 개발 중인 버전이라고 생각해도 좋다.\n\n\nshinylive 사용방법\nshinylive는 기존에 만든 shiny application에 wasm을 추가하는 것으로 생각할 수 있다. 즉, 먼저 shiny application을 만들어야 한다.\n예시 실습을 위해 shiny에서 기본으로 제공하는 코드를 사용한다.(이는 Rstudio 콘솔에서 shiny::runExample(\"01_hello\")를 입력해서 확인할 수도 있다.)\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n      breaks = bins, col = \"#75AADB\", border = \"white\",\n      xlab = \"Waiting time to next eruption (in mins)\",\n      main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n이 코드는 아래 그림과 같이 사용자의 입력에 반응하여 갯수만큼 histogram을 만드는 간단한 shiny application을 만들어 낸다.\n\n이 코드를 shinylive를 사용해 정적인 페이지를 만드는 방법은 2가지가 있는데 하나는 별도의 웹페이지로 만들어내는 것이고, 다른 하나는 이 기술 블로그 같은 quarto 블로그 페이지에 내부 콘텐츠로 심는 것이다.\n먼저 별도의 웹페이지를 만드는 방법은 다음과 같다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive via Web page",
    "text": "shinylive via Web page\n별도의 정적 웹페이지에서 shiny를 제공하려면, 이전에 설치했던 shinylive 패키지를 사용하여 app.R을 웹페이지로 변환하는 과정이 필요하다.\n내 문서(Documents)의 shinylive라는 폴더를 만들고 이 안에 app.R을 저장했을 때를 기준으로, export 함수의 사용 예시는 다음과 같다.\n\n# library(shinylive)\nshinylive::export('~/Documents/shinylive', '~/Documents/shinylive_out')\n\n이 코드를 실행하면 shinylive와 동일한 위치, 즉 내 문서(Documents)에 shinylive_out이라는 폴더를 새롭게 만들고 그 안에 shinylive 패키지를 사용해 변환된 wasm 버전의 shiny 코드를 생성한다.\n이 shinylive_out 폴더의 내용물을 확인해보면 다음과 같으며 이전 글에서 언급했던 webr, serviceworker 등이 포함되어 있는 것을 확인할 수 있다.\n\n조금 더 구체적으로 export 함수는 현재 R studio를 실행하고 있는 로컬 PC에서 shinylive 패키지의 파일들, 즉 shiny와 관련된 라이브러리 파일들을 out 디렉토리에 추가하는 역할을 한다.\n\n이제 이 폴더의 내용물을 기준으로 github page등을 만들면 shiny 를 제공하는 정적인 웹페이지를 제공할 수 있으며 그 결과는 아래의 명령어를 통해 미리 확인해 볼 수 있다.\n\n\n\n\n\n\ngithub page\n\n\n\n깃허브 페이지 배포를 위해서는 이전에 작성했던  pkgdown의 글 을 참고하길 권장하며, 이를 위해 shinylive_out 대신 docs 폴더로 결과를 내보내길 권장한다.\n\n\n\nhttpuv::runStaticServer(\"~/Documents/shinylive_out\")"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive in Quarto",
    "text": "shinylive in Quarto\nquarto 블로그에 shiny application을 추가하기 위해서는 별도의 extension을 사용해야한다. quarto extension은 quarto의 기능을 확장하는 별도의 패키지로, 기본 R에 R 패키지를 사용해 기능을 추가하는 것과 유사하다.\n먼저 Rstudio의 터미널에서 다음 코드를 실행하여 quarto extenstion을 추가해야 한다.\nquarto add quarto-ext/shinylive\nquarto 블로그에 shiny 를 심기 위해서 별도의 파일을 만들 필요는 없으며, {shinylive-r}이라는 코드 블록을 사용한다. 추가로 index.qmd의 yaml에 shinylive 를 설정해야만 한다.\nfilters: \n  - shinylive\n이후 shinylive-r 블록에 앞서 만든 app.R 의 내용을 작성한다.\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n아래는 실제 코드 블록이 어플리케이션으로 실행되는 결과이며 slider를 움직일때 반응하여 histogram을 그리는 것을 확인할 수 있다.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#정리",
    "href": "posts/2024-03-04-shinylive/index.html#정리",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "정리",
    "text": "정리\nshinylive는 wasm을 활용해 깃허브 페이지 또는 quarto 블로그 같은 정적 페이지에서 shiny를 실행할 수 있게 하는 기능으로 각각 R 패키지와 quarto extension을 통해 사용할 수 있다.\n물론 아직 나온지 1년이 되지 않은 기능인만큼 모든 기능이 제공되는 것은 아니며 정적 페이지를 사용하는 만큼 별도의 shiny server를 활용하는 것에 비하면 단점이 있기도 하다.\n그러나 shiny 사용법이나 간단한 통계 분석을 소개하고, 이를 웹사이트에서 별도의 R 설치 없이도 바로 실습할 수 있다는 점에서 많이 사용되고 있으며 앞으로도 더 많은 기능이 추가될 것으로 기대된다.\n이 블로그에 사용한 코드는 링크에서 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html",
    "href": "posts/2024-03-14-process-macro/index.html",
    "title": "Process macro 소개",
    "section": "",
    "text": "Process macro에 대해 알아보고 R에서 사용가능한 패키지를 소개합니다."
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#매개효과",
    "href": "posts/2024-03-14-process-macro/index.html#매개효과",
    "title": "Process macro 소개",
    "section": "매개효과",
    "text": "매개효과\n매개효과 분석은 설명변수가 반응변수에 영향을 미치는 경로, 매커니즘을 확인하기 위한 분석방법입니다. 단순매개모형(4번 모델)은 다음과 같은 다이아그램으로 표현할 수 있습니다.\n\n\n\n\nFigure.1\n\n\n\n먼저 R에서 process macro를 사용하려면 패키지를 설치하거나 파일을 다운받아야 합니다. 패키지로는 가톨릭대학교 문건웅 교수님이 만든 processR이라는 패키지를 다음과 같이 다운로드하고 불러올 수 있습니다.\n\ndevtools::install_github(\"cardiomoon/processR\")\nlibrary(processR)\n\nprocessR 패키지를 사용하려면 lavaan 패키지가 필요합니다. 아래 코드로 다운로드하고 불러올 수 있습니다.\n\ninstall.packages(\"lavaan\")\nlibrary(lavaan)\n\n아래 코드로 processR 패키지에서 지원하는 모델의 번호를 확인할 수 있습니다.\n\npmacro$no\n\n [1]  0.0  1.0  2.0  3.0  4.0  4.2  5.0  6.0  6.3  6.4  7.0  8.0  9.0 10.0 11.0\n[16] 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 28.0 29.0\n[31] 30.0 31.0 35.0 36.0 40.0 41.0 45.0 49.0 50.0 58.0 59.0 60.0 61.0 62.0 63.0\n[46] 64.0 65.0 66.0 67.0 74.0 75.0 76.0 25.0 26.0 27.0 58.2  4.3\n\n\n직접 다운받아 사용하시려면 process macro를 개발한 Andrew F. Hayes가 제공하는 파일을 여기서 내려받을 수 있습니다. process.R파일을 실행시키거나 분석을 진행할 R파일 상단에 source(\"process.R\")코드를 실행하면 함수를 사용할 수 있습니다. processR패키지와 process.R파일은 서로 다른 도구이니 혼동하지 않도록 주의해야 합니다. 이 포스트에서 processR 패키지에서 제공하는 함수는 코드 상단에 # processR로, process.R에서 제공하는 함수는 # process.R로 주석을 달아놓겠습니다.\n예시 데이터로 단순매개효과를 설명해보겠습니다.\n미국의 1,338명의 의료비용에 대한 데이터입니다.\n\ncost &lt;- read.csv(\"Medical_Cost.csv\")\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.924\n\n\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.552\n\n\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.462\n\n\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.471\n\n\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.855\n\n\n\n\n\nprocess.R의 process()함수를 실행해보겠습니다. 인자는 다음과 같습니다.\n\ndata = 데이터셋\nx = 설명변수\ny = 반응변수\nm = 매개변수\nmodel = 모델번호\nboot = 부트스트래핑 횟수\ntotal = 총효과 출력(0이면 출력하지 않음)\n\n\ncost$sex &lt;- ifelse(cost$sex == \"male\", 1, 0)\ncost$smoker &lt;- ifelse(cost$smoker == \"yes\", 1, 0)\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", m = \"bmi\", model = 4, boot = 0, total = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 4      \n    Y : charges\n    X : smoker \n    M : bmi    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: bmi\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.0038    0.0000   37.2152    0.0188    1.0000 1336.0000    0.8910\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant   30.6518    0.1870  163.8953    0.0000   30.2849   31.0187\nsmoker      0.0567    0.4133    0.1371    0.8910   -0.7541    0.8674\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8111    0.6579 50238769.3992 1283.9234    2.0000 1335.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -3459.0955  998.2795   -3.4651    0.0005 -5417.4628 -1500.7282\nsmoker   23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\nbmi        388.0152   31.7875   12.2065    0.0000   325.6564   450.3741\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.7873    0.6198 55804130.1996 2177.6149    1.0000 1336.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant  8434.2683  229.0142   36.8286    0.0000  7985.0017  8883.5348\nsmoker   23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\nDirect effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\n\nIndirect effect(s) of X on Y:\n       Effect\nbmi   21.9825\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n우선 Figure.1에서 보았던 경로의 이름을 정하겠습니다.\\(X → M : a\\)\\(M → Y : b\\)\\(X → Y : c'\\) 단순매개모형에서 설명변수가 0, 1로 이루어진 변수일때, \\(a = [\\bar{M}|(X = 1)] - [\\bar{M}|(X = 0)] = 0.0567\\)이며 X가 1일때 M의 평균과 X가 0일때 M의 평균의 차이와 같고 lm(bmi ~ smoker, data = cost)의 기울기와 같습니다.\\(b = [\\hat{Y}|(M = m, X = x)] - [\\hat{Y}|(M = m - 1, X = x)] = 388.0152\\)이며 이는 lm(charges ~ smoker + bmi, data = cost)에서 bmi의 기울기와 같고 아래처럼 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\ncost.1 &lt;- cost\ncost.1$bmi &lt;- cost.1$bmi - 1\n\npredict(model, cost)[1] - predict(model, cost.1)[1]\n\n       1 \n388.0152 \n\n\n\\(c' = [\\hat{Y}|(X = x, M = m)] - [\\hat{Y}|(X = x - 1, M = m)] = 23593.9810\\)이며 lm(charges ~ smoker + bmi, data = cost)의 smoker의 기울기와 같고 다음과 같이 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\nsmoke1 &lt;- cost\nsmoke1$smoker &lt;- 1\n\nsmoke0 &lt;- cost\nsmoke0$smoker &lt;- 0\n\npredict(model, smoke1)[1] - predict(model, smoke0)[1]\n\n       1 \n23593.98 \n\n\n단순매개모형에서 \\(ab\\)를 간접효과, \\(c'\\)을 직접효과, 이 둘을 더한 값을 \\(c\\)(총효과)라고 하며 총효과는 lm(charges ~ smoker, data = cost)의 기울기와 같습니다. 간접효과는 매개변수를 통했을 때 흡연자는 비흡연자보다 의료비용이 21.9825만큼 높다는 것을 의미하며, 직접효과는 매개변수가 고정되어있을 때 흡연자는 의료비용이 23593.981만큼 더 높다는 것을 의미합니다. 이제 processR 패키지를 실행해보겠습니다.\n\n# processR\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", M = \"bmi\")\nmeanSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\n \nY\nM\nY\n\n\n\n\ncharges\nbmi\nadjusted\n\n\n\n\nsmoker(X) = 0\nMean\n8434.268\n30.652\n8438.77\n\n\n\nSD\n5993.782\n6.043\n\n\n\nsmoker(X) = 1\nMean\n32050.232\n30.708\n32032.751\n\n\n\nSD\n11541.547\n6.319\n\n\n\n\nMean\n13270.422\n30.663\n\n\n\n\nSD\n12110.011\n6.098\n\n\n\n\n\n\n\nAdjusted mean은 \\(adjusted\\;mean(\\bar{Y}^*) = i_{Y} + b\\bar{M} + c'X\\)로 계산할 수 있습니다. 설명변수가 0일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 0\\)이고 설명변수가 1일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 1\\)로 계산할 수 있습니다. 보정평균은 \\(X\\)일때 평균적인 \\(M\\)의 값을 가지는 사람은 보정평균만큼의 \\(Y\\)를 갖는다는 것을 의미합니다.\n아래처럼 각 계수를 깔끔하게 출력하는 함수도 존재합니다.\n\n# processR\nmodelsSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\nbmi(M)\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\na\n0.057\n0.413\n0.137\n.891\n\nc'\n23593.981\n480.180\n49.136\n&lt;.001\n\n\nbmi(M)\n\n\n\n\n\n\nb\n388.015\n31.787\n12.207\n&lt;.001\n\n\nConstant\niM\n30.652\n0.187\n163.895\n&lt;.001\n\niY\n-3459.096\n998.279\n-3.465\n.001\n\n\nObservations\n\n1338\n\n\n1338\n\n\nR2\n\n0.000\n\n\n0.658\n\n\nAdjusted R2\n\n-0.001\n\n\n0.657\n\n\nResidual SE\n\n6.100 ( df = 1336)\n\n\n7087.931 ( df = 1335)\n\n\nF statistic\n\nF(1,1336) = 0.019, p = .891\n\n\nF(2,1335) = 1283.923, p &lt; .001\n\n\n\n\n\n\n간접효과, 직접효과, 총효과를 다음 함수로 출력할 수 있습니다.\n\n# processR\nmodel &lt;- tripleEquation(labels = labels)\nsemfit &lt;- sem(model = model, data = cost)\n\nmedSummaryTable(semfit)\n\n\n\n\n\nEffect\nEquation\nestimate\n95% CI\n\n\n\nindirect\n(a)*(b)\n21.983\n(-292.098 to 336.063)\n\n\ndirect\nc\n23593.981\n(22653.900 to 24534.062)\n\n\ntotal\ndirect+indirect\n23615.964\n(22624.816 to 24607.111)\n\n\nprop.mediated\nindirect/total\n0.001\n(-0.012 to 0.014)"
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#조절효과",
    "href": "posts/2024-03-14-process-macro/index.html#조절효과",
    "title": "Process macro 소개",
    "section": "조절효과",
    "text": "조절효과\n조절효과는 설명변수가 반응변수에 미치는 영향이 다른 변수에 의해 변화될 때, 이 변화를 조절효과라고 하며, 이러한 영향을 주는 변수를 조절변수라고 합니다.\n단순조절효과(1번모델)는 다음의 다이아그램으로 나타낼 수 있습니다.\n\n\n\n\nFigure.2\n\n\n\nprocess()함수로 단순조절효과를 알아보겠습니다. plot 인자는 출력결과 하단에 테이블을 만드어줍니다.\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", w = \"age\", model = 1, plot = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : charges\n    X : smoker \n    W : age    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8495    0.7217 40903347.7298 1153.1995    3.0000 1334.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -2091.4206  582.5654   -3.5900    0.0003 -3234.2647  -948.5764\nsmoker   22385.5487 1278.7311   17.5061    0.0000 19877.0057 24894.0917\nage        267.2489   13.9285   19.1872    0.0000   239.9247   294.5731\nInt_1       37.9887   31.0950    1.2217    0.2220   -23.0116    98.9890\n\nProduct terms key:\nInt_1  :  smoker  x  age      \n\nTest(s) of highest order unconditional interaction(s):\n      R2-chng         F       df1       df2         p\nX*W    0.0003    1.4925    1.0000 1334.0000    0.2220\n----------\nFocal predictor: smoker (X)\n      Moderator: age (W)\n\nData for visualizing the conditional effect of the focal predictor:\n     smoker       age    charges\n     0.0000   22.0000  3788.0555\n     1.0000   22.0000 27009.3554\n     0.0000   39.0000  8331.2870\n     1.0000   39.0000 32198.3946\n     0.0000   56.0000 12874.5186\n     1.0000   56.0000 37387.4338\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n단순조절효과의 계수는 lm(charges ~ smoker + age + smoker * charges, data = cost)의 계수와 동일합니다.\n\\(\\hat{Y} = i_{Y} + b_{1}X + b_{2}W + b_{3}XW\\)일때, \\(b_{1} = 23385.55\\), \\(b_{2} = 267.25\\), \\(b_{3} = 37.99\\)이며 각 계수를 다음과 같은 의미를 가지고 있습니다.\n\n\\(b_{1} = W\\)가 \\(0\\)일때 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{X→Y} = b_{1} + b_{3}W\\)로 계산합니다.\n\\(b_{2} = X\\)가 \\(0\\)일때 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{W→Y} = b_{2} + b_{3}X\\)로 계산합니다.\n\\(b_{3} = W\\)가 한 단위 바뀔 때, \\(X\\)의 한 단위 변화가 \\(Y\\)에 영향을 미치는 정도의 차이입니다.\n\nproceeR 패키지로 확인해보겠습니다.\n\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", W = \"age\")\nmodel &lt;- lm(charges ~ smoker + age + smoker * age, data = cost)\n\n# processR\nm.summary &lt;- modelsSummary(list(model), labels = labels)\nmodelsSummaryTable(m.summary)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\nc1\n22385.549\n1278.731\n17.506\n&lt;.001\n\n\nage(W)\nc2\n267.249\n13.929\n19.187\n&lt;.001\n\n\nsmoker:age(X:W)\nc3\n37.989\n31.095\n1.222\n.222\n\n\nConstant\niY\n-2091.421\n582.565\n-3.590\n&lt;.001\n\n\nObservations\n\n1338\n\n\nR2\n\n0.722\n\n\nAdjusted R2\n\n0.721\n\n\nResidual SE\n\n6395.573 ( df = 1334)\n\n\nF statistic\n\nF(3,1334) = 1153.199, p &lt; .001"
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html",
    "href": "posts/2024-05-17-patchwork/index.html",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "",
    "text": "데이터 시각화는 데이터 분석에서 중요한 역할을 한다. 다행히 R은 이 방면에서는 ggplot2를 필두로 다른 프로그래밍 언어들 이상의 뛰어난 여러 기능들을 사용할 수 있다는 장점이 있다.\n한편 데이터 시각화는 제작 이후 색상이나 레이아웃 등의 추가적인 커스텀 수정을 필요로 하기도 한다. 이를 위해 R 내에서 할 수 있다면 더할 나위 없이 좋지만 떄로는 단순한 작업을 위해 여러줄 코드를 사용하는 것보다 간단히 ppt 같은 외부 프로그램을 활용하는 것이 더 간편한 경우도 많다.\n이전의 다른 아티클에서 officer 패키지를 활용해 MS powerpoint로 벡터 이미지를 만들고 편집하는 방법을 소개하였는데, 이번 글에서는 여러 장의 이미지를 대상으로 R에서 할 수 있는 고급 방법들과 이에 쓰이는 R 패키지를 소개한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#data-visualization",
    "href": "posts/2024-05-17-patchwork/index.html#data-visualization",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "",
    "text": "데이터 시각화는 데이터 분석에서 중요한 역할을 한다. 다행히 R은 이 방면에서는 ggplot2를 필두로 다른 프로그래밍 언어들 이상의 뛰어난 여러 기능들을 사용할 수 있다는 장점이 있다.\n한편 데이터 시각화는 제작 이후 색상이나 레이아웃 등의 추가적인 커스텀 수정을 필요로 하기도 한다. 이를 위해 R 내에서 할 수 있다면 더할 나위 없이 좋지만 떄로는 단순한 작업을 위해 여러줄 코드를 사용하는 것보다 간단히 ppt 같은 외부 프로그램을 활용하는 것이 더 간편한 경우도 많다.\n이전의 다른 아티클에서 officer 패키지를 활용해 MS powerpoint로 벡터 이미지를 만들고 편집하는 방법을 소개하였는데, 이번 글에서는 여러 장의 이미지를 대상으로 R에서 할 수 있는 고급 방법들과 이에 쓰이는 R 패키지를 소개한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#result",
    "href": "posts/2024-05-17-patchwork/index.html#result",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "result",
    "text": "result\n이번 글에서 소개하는 방법들을 적용한 ppt 결과물을 먼저 소개한다.\n\n우선 위 이미지는\n\nR에서 ggplot2를 사용하여 만든 시각화를\n\ncowplot을 이용하여 박스로 감싸고\n\npatchwork와 를 사용해 시각화와 설명을 위한 텍스트를 레이아웃에 따라 배치한 뒤\n\nofficer를 활용하여 와이드스크린(혹은 16:9) 해상도 크기의 MS powerpoint로 만들어 낸 결과물이다.\n\n이 결과물들은 벡터 그래픽스를 활용한 만큼, 다음 이미지처럼 ppt에서 편리하게 커스텀 수정이 가능하다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#patchwork",
    "href": "posts/2024-05-17-patchwork/index.html#patchwork",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "patchwork",
    "text": "patchwork\n예시에서 사용할 이미지는 ggplot2의 mtcars 데이터셋을 사용하는 patchwork의 예시 코드를 사용한다. ggplot2과 각 차트에 대해서는 별도로 설명하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nMerc 450SE\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\nMerc 450SL\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n\n\nMerc 450SLC\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\nLincoln Continental\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\nChrysler Imperial\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\nDodge Challenger\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\nAMC Javelin\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\nPontiac Firebird\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\nFord Pantera L\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\nmtcars\n\n\n\nlibrary(ggplot2)\n\np1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp))\np2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))\np3 &lt;- ggplot(mtcars) + geom_bar(aes(gear)) + facet_wrap(~cyl)\np4 &lt;- ggplot(mtcars) + geom_bar(aes(carb))\np5 &lt;- ggplot(mtcars) + geom_violin(aes(cyl, mpg, group = cyl))\np6 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + facet_wrap( ~ cyl)\n\npatchwork는 여러 개의 ggplot 결과물들을 하나(한장)의 그래픽에 간단하게 배치할 수 있게 하는 R 패키지이다. 유사한 목적으로 patchwork외에 gridExtra나 cowplot등의 다른 패키지도 사용할 수 있다.\npatchwork의 사용법은 크게 +, |, ( ), /로 구성된다.\n| (vertical bar)\n먼저 | 는 여러 이미지를 하나의 행에 배치하는 역할을 한다.\n\np1 | p2 | p3 | p4\n\n\npatchwork - vertical bar\n\n\n\n\n+\n두번째로 +는 여러 이미지를 배치하는데 이때 행과 열은 grid 형태로, 행 순서로 채우는 방식을 사용한다.\n\np1 + p2 + p3 + p4\n\n\npatchwork - plus\n\n\n\n\n이때 이미지 배치를 특별히 지정을 하기 위해서는 plot_layout이라는 함수를 사용한다.\n\np1 + p2 + p3 + p4 + p5 +\n  plot_layout(ncol = 3, byrow = FALSE)\n\n\npatchwork - plot_layout\n\n\n\n\n/\n이어서 /를 사용하면 이미지를 열로 이어서 배치할 수 있다.\n\np1 / p2 \n\n\npatchwork - slash\n\n\n\n\n( )\n마지막으로 ( )를 사용하면 이미지를 하나의 그룹으로 묶어서 배치할 수 있다.\n\np1 | (p2 / p3)\n\n\npatchwork - parenthesis\n\n\n\n\n물론 이 외에도 patchwork는 다양한 기능을 제공하는데, 자세한 내용은 공식 문서를 참고하자.\n이를 활용해서 이제 앞에서 만들었던 예시 이미지 6개를 한장의 ppt에 배치해보자.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\n\npatchwork - combined"
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#cowplot",
    "href": "posts/2024-05-17-patchwork/index.html#cowplot",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "cowplot",
    "text": "cowplot\n이어서 cowplot을 사용해 이미지 사이에 캡션과 박스를 추가하는 방법을 다루겠다.\n우선 첫 이미지 3장을 표현하는 가상의 캡션을 list 형태로 생성한다. 참고로 &lt;br&gt;은 줄넘김을 의미한다.\n내용은 lorem ipsum을 활용했다.\n\ntext &lt;- list(\n  p1 = \"Lorem ipsum dolor sit amet &lt;br&gt; consectetur adipiscing elit.\",\n  p2 = \"Integer lectus risus, &lt;br&gt; tincidunt eget felis non.\",\n  p3 = \"Cras varius sapien et est consectetur porttitor.\"\n)\n\n이를 이전 combined_plot에 추가한다.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  ( text$p1 | text$p2 | text$p3 ) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\n# combined_plot \n# ERROR !!\n\n그러나 이 상태로는 text 오브젝트가 ggplot 결과가 아닌 단순 텍스트만을 포함하기 때문에 에러가 발생한다. 이를 해결하기 위해 cowplot의 ggdraw 함수를 사용한다.\nggdraw\n우선 cowplot은 ggplot2의 결과물에 annotation, theme를 추가하는 기능등을 제공하는 R 패키지로, ggdraw는 ggplot2의 결과물에 추가적인 그래픽을 그릴 수 있게 제일 상위 레벨에 레이어를 추가한다고 생각하면 편하다.\n\nscatter &lt;- ggplot(mpg, aes(displ, cty)) +\n  geom_point() +\n  theme_minimal_grid()\n\ndraft &lt;- ggdraw(scatter) + \n  draw_label(\"Draft\", colour = \"#80404080\", size = 120, angle = 45)\n\nscatter | draft\n\n\ncowplot - ggdraw\n\n\n\n\n이 ggdraw를 사용해 이전의 text 내용 중 첫번째 라벨(p1)을 label로 갖는 ggplot 오브젝트를 생성하고 이를 combined_plot에 추가한다.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  ( \n    ggdraw() + \n      labs(subtitle = text$p1) + \n      theme_void() +\n      theme(\n        text = element_text(size = 8),\n        plot.subtitle = ggtext::element_textbox_simple(\n          hjust = 0,\n          halign = 0,\n          margin = margin(3, 0, 0, 0)\n        ),\n        plot.margin = margin(0, 0, 0, 0)\n      ) \n  ) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\n\ncowplot - combined with caption\n\n\n\n\n이어서 남은 라벨을 추가하기 전, 라벨 커스텀에 반복적으로 쓰이는 기능들을 별도의 함수로 만들어 사용하자. 추가로 caption과 그래프를 동일한 1:1:1의 높이로 할당하지 않고 caption 부분을 줄이기 위해 plot_layout의 height로 높이를 조절한다.\n\n\n\ncowplot - caption function\n\n\n\n\n다음은 각 시각화를 박스(테두리)로 감싸는 방법을 다룬다. 이를 위해 각 시각화에 ggdraw를 사용하여 레이어를 만들고, 그 레이어에 draw_line 함수를 사용해 (0,0) 부터 (1,1)을 지나는 직선을 추가하는 방법을 사용한다.\n추가로 각 시각화에 text 속성을 조절하기 위해 theme 함수를 사용한다.\n\ntext_theme &lt;- theme(\n  text = element_text(size = 6), \n  axis.text = element_text(size = 6), \n  axis.title = element_text(size = 6),\n  axis.title.x = element_text(size = 6), \n  axis.title.y = element_text(size = 6), \n  plot.title = element_text(size = 6),\n  legend.text = element_text(size = 6),\n  legend.title = element_text(size = 6)\n)\n\n(\n  p1 | \n  ggdraw(p1 + text_theme) +\n    draw_line(\n      x = c(0, 1, 1, 0, 0), \n        y = c(0, 0, 1, 1, 0), \n        color = \"black\", \n        size = 0.5\n    )\n)\n\n\ncowplot - box\n\n\n\n\n이전과 마찬가지로 (반복되는) 박스를 만드는 기능들을 함수로 만들어 사용하자.\n\n\n\ncowplot - box function"
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#officer",
    "href": "posts/2024-05-17-patchwork/index.html#officer",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "officer",
    "text": "officer\n이제 officer 패키지를 사용해 위에서 만든 그래프를 ppt에 추가해보자.\n기본적인 officer에 대한 소개는 이전 아티클을 참고하면 좋다.\nofficer에서는 read_pptx 함수로 ppt 오브젝트를 생성하는데 이때 읽을 파일을 입력하지 않으면 너비와 높이가 4:3 비율인 새로운 오브젝트를 생성하여 사용한다.\n만약 이를 그대로 사용한다면 다음 그림과 같이 애써만든 레이아웃이 깨지는 상황이 발생할 수 있기 때문에, ppt에서 임의의 사이즈를 갖는 템플릿을 만들고 이를 파일로 읽어 사용한다.\n\n\n\n\n\n\n\nppt를 생성한 다음, 페이지 설정에서 16:9 혹은 와이드 스크린으로 변경하는 방법도 있지만, 이 방법 또한 마찬가지로 그래프 요소들을 다시 배치해야 한다는 점은 동일하다.\n\n\n\n\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  remove_slide(1) |&gt;\n  add_slide() |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")\n\n위 코드에서 2번째 줄 remove_slide 함수를 사용하지 않으면, 기존 템플릿의 슬라이드 이후 에 ggplot 결과를 담는 슬라이드를 만들기 때문에 아래와 같이 불필요한 첫페이지를 가지고 시작하게 된다.\n\n한편 remove_slide와 add_slide를 둘 다 제거하고 ph_with으로 이미지만 더하게 되면 아래와 같이 템플릿의 제목과 새로 추가한 제목이 겹쳐서 보여지게 된다.\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")\n\n\n그러므로 템플릿을 사용하는 경우에는 remove_slide와 add_slide를 활용하는 것을 권장한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#summary",
    "href": "posts/2024-05-17-patchwork/index.html#summary",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "summary",
    "text": "summary\n이번 아티클에서는 patchwork와 cowplot을 사용해 여러 그래프를 하나로 합치고 약간의 커스텀을 거쳐, officer를 사용해 ppt에 추가하는 방법을 알아보았다. 이처럼 R의 기능과 ppt를 연결하는 방법은 다양하게 활용할 수 있으며, 이를 통해 보다 효율적인 작업을 할 수 있을 것이다.\n\n최종 코드는 다음과 같다.\n\nCodelibrary(ggplot2)\nlibrary(patchwork)\nlibrary(cowplot)\nlibrary(officer)\n\np1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp))\np2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))\np3 &lt;- ggplot(mtcars) + geom_bar(aes(gear)) + facet_wrap(~cyl)\np4 &lt;- ggplot(mtcars) + geom_bar(aes(carb))\np5 &lt;- ggplot(mtcars) + geom_violin(aes(cyl, mpg, group = cyl))\np6 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + facet_wrap( ~ cyl)\n\ntext &lt;- list(\n  p1 = \"Lorem ipsum dolor sit amet &lt;br&gt; consectetur adipiscing elit.\",\n  p2 = \"Integer lectus risus, &lt;br&gt; tincidunt eget felis non.\",\n  p3 = \"Cras varius sapien et est consectetur porttitor.\"\n)\n\ncap &lt;- function(text){\n  ggdraw() + \n    labs(subtitle = text) +\n    theme_void() +\n    theme(\n      text = element_text(size = 8),\n      plot.margin = margin(0, 0, 0, 0)\n    )\n}\n\ntext_theme &lt;- theme(\n  text = element_text(size = 6), \n  axis.text = element_text(size = 6), \n  axis.title = element_text(size = 6),\n  axis.title.x = element_text(size = 6), \n  axis.title.y = element_text(size = 6), \n  plot.title = element_text(size = 6),\n  legend.text = element_text(size = 6),\n  legend.title = element_text(size = 6)\n)\n\nwith.box &lt;- function(p){\n  ggdraw(p + text_theme) +\n    cowplot::draw_line(\n      x = c(0, 1, 1, 0, 0), \n      y = c(0, 0, 1, 1, 0), \n      color = \"black\", \n      size = 0.5\n    ) \n}\n\ncombined_plot &lt;- (with.box(p1) | with.box(p2) | with.box(p3)) /\n  ( cap(text$p1 + text_theme) | cap(text$p2 + text_theme) | cap(text$p3 + text_theme) ) /\n  (with.box(p4) | with.box(p5) | with.box(p6)) +\n  plot_layout(heights = c(5, 0.1, 5)) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")"
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html",
    "title": "the Extended DLNM 소개",
    "section": "",
    "text": "지연 효과와 비선형 관계를 모두 고려한 모델인 DLNM의 확장 버전, the Extended DLNM에 대해 소개하고, R의 dlnm 패키지를 이용하여 the Extended DLNM을 적합시키는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#data",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#data",
    "title": "the Extended DLNM 소개",
    "section": "3.1 Data",
    "text": "3.1 Data\nthe Extended DLNM의 R 실습에서 사용될 데이터는 dlnm 패키지에 포함되어 있는 drug와 nested 데이터셋입니다.\ndrug 데이터는 환자별 시간에 따른 약물 복용량의 효과에 대한 데이터입니다. 해당 실험은 200명의 무작위 추출된 환자들에 대해 진행되었으며, 각 환자들은 4주 중 무작위로 선정된 2주동안 투약을 받게 되고, 약물 복용량은 매주 달라집니다. 아래 데이터를 보면, 각 환자별로 주별 복용량(day1.7, day8.14, day15.21, day22.28)이 기록되어 있으며, out에 28일째에 측정된 outcome 값이 기록되어 있습니다.\n\nhead(drug, 5)\n\n  id out sex day1.7 day8.14 day15.21 day22.28\n1  1  46   M      0       0       40       37\n2  2  50   F      0      47       55        0\n3  3   7   F     56      22        0        0\n4  4  70   M     91       0        0       87\n5  5  -3   F      0      42       28        0\n\n\nnested 데이터는 시간에 따른 노출 요인(exposure)과 암 사이의 연관성에 대한 nested case-control study 데이터입니다. 300명의 case 집단과 300명의 control 집단에 대한 데이터가 포함되어 있습니다. 아래 데이터의 case에 case(1)/control(0) 여부가 기록되어 있으며, exp15-exp60은 15세부터 65세까지 5년 간격으로 평균 노출 요인(exposure)을 구한 값입니다. 만약, 환자의 나이(age)가 컬럼에 해당하는 나이보다 적다면 해당 컬럼에는 NA가 입력됩니다. 예를 들어, 아래 데이터의 4번 환자는 52세이기 때문에 exp55 이후의 컬럼에는 NA가 입력되어 있습니다.\n\nhead(nested, 5)\n\n  id case age riskset exp15 exp20 exp25 exp30 exp35 exp40 exp45 exp50 exp55\n1  1    1  81     240     5    84    34    45   128    81    14    52    11\n2  2    1  69     129    11     8    25     6     8    12    19    60    16\n3  3    1  73     180    14    15     7    69    10   143    18    19    44\n4  4    0  52      19    10    16     5    30    24    33    14   122    NA\n5  4    0  66      96    10    16     5    30    24    33    14   122     2\n  exp60\n1    16\n2    10\n3    23\n4    NA\n5    11\n\n\nThe matrix of exposure histories\nDLNM을 적합시키기 전, 데이터의 형태를 matrix of exposure histories 형태로 바꿔주어야 합니다.\ndrug 데이터의 matrix of exposure histories에서 lag0에 해당하는 값은 28일째의 약물 복용량입니다. 즉, lag0-lag6은 마지막 주의 약물 복용량, lag7-lag13은 셋째 주의 약물 복용량입니다. drug 데이터의 주별 약물 복용량 값을 7번씩 반복하여 다음과 같은 matrix of exposure histories를 만들어줍니다.\n\nQdrug &lt;- as.matrix(drug[,rep(7:4, each=7)])\ncolnames(Qdrug) &lt;- paste(\"lag\", 0:27, sep=\"\")\nQdrug[1:3,1:14]\n\n  lag0 lag1 lag2 lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1   37   37   37   37   37   37   37   40   40   40    40    40    40    40\n2    0    0    0    0    0    0    0   55   55   55    55    55    55    55\n3    0    0    0    0    0    0    0    0    0    0     0     0     0     0\n\n\nnested 데이터의 matrix of exposure histories에서 lag0에 해당하는 시점은 환자의 나이마다 다릅니다. 예를 들어, 52세 환자의 lag0 시점은 52년의 값이고, 65세 환자의 lag0 시점은 65년의 값입니다. 이런 경우, matrix of exposure histories를 만들기 까다로운데, dlnm 패키지에서는 이런 경우의 matrix of exposure histories를 만들어주는 exphist()라는 함수를 제공합니다. exphist() 함수는 다음과 같이 이용합니다.\n\nexphist(exp, times, lag, fill=0)\n\nexp에는 exposure profile(관측 첫 시점부터 매 시점의 exposure 값)을 입력합니다. times에는 lag0에 해당하는 시점을 입력합니다. times에 입력된 시점부터 시점을 거슬러가며 각 시차별 exposure 값이 구해집니다. lag에는 maximum lag 또는 lag range를 입력합니다. fill에는 어떤 시차에 해당하는 exposure 값이 존재하지 않을 때 채울 값을 입력합니다.\nexphist() 함수를 이용하여 다음과 같은 matrix of exposure histories를 만들어줍니다. nested 데이터의 평균 노출 요인 값에 0, 0, 0을 추가하고 각 데이터를 5번씩 반복하여 1년부터 65년까지에 해당하는 exposure profile을 만들고 이 값을 exp에 넣습니다. times에는 각 환자별 나이(age)를 넣습니다. nested 데이터를 이용한 분석에서는 lag3부터 lag40까지의 데이터만 사용할 예정이기 때문에 lag에는 다음과 같이 lag range를 벡터로 넣어줍니다. nested 데이터의 matrix of exposure histories는 다음과 같습니다.\n\nQnest &lt;- t(apply(nested, 1, function(sub) exphist(rep(c(0,0,0,sub[5:14]),\n                                                      each=5), sub[\"age\"], lag=c(3,40))))\ncolnames(Qnest) &lt;- paste(\"lag\", 3:40, sep=\"\")\nQnest[1:3,1:11]\n\n  lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1    0    0    0    0    0    0    0     0     0     0     0\n2    0   10   10   10   10   10   16    16    16    16    16\n3    0    0    0    0    0   23   23    23    23    23    44"
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#a-simple-dlm",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#a-simple-dlm",
    "title": "the Extended DLNM 소개",
    "section": "3.2 A simple DLM",
    "text": "3.2 A simple DLM\ndrug 데이터와 dlnm 패키지의 함수를 이용하여 약물 복용량과 outcome 사이의 관계에 대해 분석해보고자 합니다.\n먼저, 약물 복용량에 대한 cross-basis matrix를 만들어야 합니다. crossbasis() 함수를 이용합니다. crossbasis()의 첫 번째 인수에는 matrix of exposure histories를 넣어줍니다. lag에는 lag-response 차원에서 시차를 얼마나 고려할지 lag period를 입력합니다. maximum lag 또는 lag range를 입력해야하며, minimum lag는 기본값이 0으로 지정되어 있습니다. 이때, lag period는 matrix of exposure histories의 시차 범위와 일치해야 합니다. argvar에는 exposure-response 차원에서 사용할 basis function, arglag에는 lag-response 차원에서 사용할 basis function을 입력합니다.\ndrug 데이터의 cross-basis는, exposure-response 차원에서 simple linear function을 이용하고, lag-response 차원에서 natural cubic spline을 이용합니다. 이 때, natural cubic spline에서 lag9, 18을 knots로 이용하는데, 이는 lag9, 18을 기준으로 구간을 나눠 각 구간에서 cubic regression model을 적합시킨다는 의미입니다.\nsummary() 함수를 통해 cross-basis matrix의 세부 사항을 확인할 수 있습니다.\n\ncbdrug &lt;- crossbasis(Qdrug, lag=27, argvar=list(\"lin\"),\n                     arglag=list(fun=\"ns\",knots=c(9,18)))\nsummary(cbdrug)\n\nCROSSBASIS FUNCTIONS\nobservations: 200 \nrange: 0 to 100 \nlag period: 0 27 \ntotal df:  4 \n\nBASIS FOR VAR:\nfun: lin \nintercept: FALSE \n\nBASIS FOR LAG:\nfun: ns \nknots: 9 18 \nintercept: TRUE \nBoundary.knots: 0 27 \n\n\n위에서 생성된 cross-basis 객체인 cbdrug를 회귀식에 포함시키고, 성별에 대한 효과를 보정하여 단순 선형 회귀 분석을 진행합니다. 모형을 통해 추정된 약물 복용량 및 그 시차에 대한 효과를 crosspred() 함수를 통해 확인할 수 있습니다. crosspred()의 첫 번째 인수에는 cross-basis 객체를, 두 번째 인수에는 cross-basis 객체를 사용한 모델을 입력합니다. 다음 코드에서, crosspred()의 at은 약물 복용량이 0:20*5(즉, 0, 5, 10, 15, …, 100)일 때의 outcome을 예측하라는 뜻입니다.\n\nmdrug &lt;- lm(out~cbdrug+sex, drug)\npdrug &lt;- crosspred(cbdrug, mdrug, at=0:20*5)\n\ncrosspred 객체인 pdrug에 저장된 effect summaries는 다음과 같이 추출될 수 있습니다.\n\nwith(pdrug,cbind(allfit,alllow,allhigh)[\"50\",])\n\n  allfit   alllow  allhigh \n30.29584 20.12871 40.46298 \n\n\n위 코드는 약물 복용량이 50일때의 overall cumulative effects 추정치(allfit)와 95% 신뢰구간(alllow, allhigh) 추출한 것입니다. 이때, overall cumulative effects는 lag period인 28일동안 약물 복용량이 50으로 유지되었을 때 outcome의 전체적인 증가량 또는 약물 복용량 50이 28일 뒤 미치는 총 영향으로 해석될 수 있습니다.\n위에서처럼 all-로 시작하는 객체들을 통해서는 전반적인 추정값을 확인할 수 있고, mat-으로 시작하는 객체들을 통해서는 특정 약물 복용량과 시차의 조합에 따른 추정 결과를 확인할 수 있습니다. 다음 코드는 lag3에서의 약물 복용량이 20일 때, outcome의 증가량을 추출한 것입니다. ?crosspred를 통해 crosspred()의 더 많은 기능을 확인할 수 있습니다.\n\npdrug$matfit[\"20\",\"lag3\"]\n\n[1] 1.118139\n\n\nplot() 함수를 이용해 추정 결과를 시각화할 수 있습니다.\n\npar(mfrow=c(1,3))\nplot(pdrug, zlab=\"Effect\", xlab=\"Dose\", ylab=\"Lag (days)\")\nplot(pdrug, var=60, ylab=\"Effect at dose 60\", xlab=\"Lag (days)\", ylim=c(-1,5))\nplot(pdrug, lag=10, ylab=\"Effect at lag 10\", xlab=\"Dose\", ylim=c(-1,5))\n\n\n\n\n\n\n\n첫 번째 그래프는 회귀 모델을 통해 추정한 exposure-lag-response 관계를 3차원 공간에 그려놓은 것입니다. 약물 복용량과 시차의 변화에 따라 effect가 어떻게 달라지는지 확인할 수 있습니다. 그래프에 따르면 약물 복용량의 효과는 복용 후 첫 번째 날에 나타나고 15-20일 후에 사라지는 경향이 있음을 알 수 있습니다.\n두 번째와 세 번째 그래프는 약물 복용량이 60일 때의 lag-response curve와 lag가 10일 때 exposure-response curve를 그린 것입니다. 각각 var=60, lag=10을 지정하여 3차원상의 첫 번째 그래프의 단면을 자른 것과 같습니다. cross-basis matrix를 만들 때 basis function을 지정했던대로, lag-response 차원에서는 natural cubic spline, exposure-response 차원에서는 simple linear function 형태로 나타나는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#a-more-complex-dlnm",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#a-more-complex-dlnm",
    "title": "the Extended DLNM 소개",
    "section": "3.3 A more complex DLNM",
    "text": "3.3 A more complex DLNM\nneseted 데이터와 dlnm 패키지의 함수를 이용하여 노출 요인(exposure)에 대한 장기간의 노출이 암에 어떤 영향을 미치는지 분석하고자 합니다.\n최근 3년(lag0-2)간의 노출은 암에 영향을 주지 않는다고 가정해봅시다. 따라서 matrix of exposure histories에는 lag3부터의 데이터만 있으면 됩니다.\n먼저, cross-basis matrix를 만듭니다. crossbasis() 함수를 이용합니다. lag3부터 lag40까지의 데이터만 이용할 것이므로, lag에 c(3,40)을 넣어줍니다. exposure-response 차원에서는 basis 함수로 quadratic spline을 사용하고, exposure-lag 차원에서는 natural cubic spline을 사용합니다. quadratic spline의 자유도(df)와 차수(degree)를 입력해야하며, single knot는 별도로 지정하지 않으면 중앙값으로 지정됩니다. natural cubic spline은 intercept=F를 입력하여 intercept를 제외합니다. 위에서 lag0-2는 고려하지 않는다고 하였으므로, intercept를 제외함으로써 위 가정과 일치하게 시차 차원에서의 null effect를 예측할 수 있습니다.\n\ncbnest &lt;- crossbasis(Qnest, lag=c(3,40), argvar=list(\"bs\",degree=2,df=3),\n                     arglag=list(fun=\"ns\",knots=c(10,30),intercept=F))\nsummary(cbnest)\n\nCROSSBASIS FUNCTIONS\nobservations: 600 \nrange: 0 to 1064 \nlag period: 3 40 \ntotal df:  9 \n\nBASIS FOR VAR:\nfun: bs \nknots: 15 \ndegree: 2 \nintercept: FALSE \nBoundary.knots: 0 1064 \n\nBASIS FOR LAG:\nfun: ns \nknots: 10 30 \nintercept: FALSE \nBoundary.knots: 3 40 \n\n\n앞에서와 마찬가지로, cross-basis 객체를 회귀 모형에 포함시킵니다. nested 데이터는 nested case-control study 데이터이기 때문에, conditional logistic regression을 이용합니다. survival 패키지의 clogit() 함수를 이용합니다.\ncrosspred 객체를 만들 때, cen=0을 입력하여 reference value를 0으로 지정합니다. 즉, 노출 요인에 따른 암의 OR은 exposure=0일 때를 기준으로 계산됩니다.\n\nlibrary(survival)\nmnest &lt;- clogit(case~cbnest+strata(riskset), nested)\npnest &lt;- crosspred(cbnest, mnest, cen=0, at=0:20*5)\n\neffect summaries는 pnest를 통해 확인할 수 있습니다. 이번에는 allRR-이나 matRR-로 시작하는 객체를 추출하여 OR 추정값을 확인할 수 있습니다.\n\nwith(pnest,cbind(allRRfit,allRRlow,allRRhigh)[\"50\",])\n\n  allRRfit   allRRlow  allRRhigh \n 32.061676   2.561702 401.276652 \n\n\n\npnest$matRRfit[\"50\",\"lag5\"]\n\n[1] 1.058661\n\n\n앞에서와 마찬가지로, plot() 함수를 통해 추정 결과를 시각화할 수 있습니다.\n\npar(mfrow=c(1,3))\nplot(pnest, zlab=\"OR\", xlab=\"Exposure\", ylab=\"Lag (years)\")\nplot(pnest, var=50, ylab=\"OR for exposure 50\", xlab=\"Lag (years)\", xlim=c(0,40))\nplot(pnest, lag=5, ylab=\"OR at lag 5\", xlab=\"Exposure\", ylim=c(0.95,1.15))\n\n\n\n\n\n\n\n첫 번째 그래프는 노출 요인과 암 사이의 exposure-lag-response 관계를 3차원 상에서 시각화한 것입니다. 그래프에 따르면 노출 초기엔 암의 위험성을 증가시키다가 점차 감소한다는 사실을 확인할 수 있습니다.\n두 번째와 세 번째 그래프는 exposure이 50일 때의 lag-response curve와 lag가 5일 때 exposure-response curve를 그린 것입니다. 두 번째 그래프를 통해 노출 후 10-15년이 지났을 때 암의 위험성이 가장 크게 증가하며, 30년이 지났을 때쯤부턴 거의 원 상태로 되돌아온다는 사실을 확인할 수 있습니다. 세 번째 그림을 통해 노출 요인의 양이 20으로 증가할 때까지는 위험성이 크게 증가하지만 그 이후로는 위험성이 천천히 증가함을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#extended-prediction-summaries",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#extended-prediction-summaries",
    "title": "the Extended DLNM 소개",
    "section": "3.4 Extended prediction summaries",
    "text": "3.4 Extended prediction summaries\ncrosspred()의 at과 lag 인자를 통해 특정 exposure 값 또는 lag에서의 effect summaries를 얻을 수 있습니다. 이때, nested 데이터의 matrix of exposure histories를 만들 때 사용한 함수인 exphist()를 사용한다면, 특정 exposure history에서의 effect summaries를 얻을 수 있습니다.\n예를 들어, ’3.3 A more complex DLNM’에서 이용한 nested 데이터를 다시 분석해봅시다. 5년 동안의 exposure 값이 10이고, 그 이후 5년 간은 0, 그 이후 10년간은 13일 때의 effect summary를 구하고자 할 때, 아래와 같이 exposure history를 만들 수 있습니다.\n\nexpnested &lt;- rep(c(10,0,13), c(5,5,10))\nhist &lt;- exphist(expnested, time=length(expnested), lag=c(3,40))\nhist\n\n   lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13 lag14 lag15 lag16\n20   13   13   13   13   13   13   13     0     0     0     0     0    10    10\n   lag17 lag18 lag19 lag20 lag21 lag22 lag23 lag24 lag25 lag26 lag27 lag28\n20    10    10    10     0     0     0     0     0     0     0     0     0\n   lag29 lag30 lag31 lag32 lag33 lag34 lag35 lag36 lag37 lag38 lag39 lag40\n20     0     0     0     0     0     0     0     0     0     0     0     0\n\n\ntime 인자에는 expnested의 길이를 입력하고, lag는 lag3부터 40까지만 고려한다고 입력합니다. 이때, expnested의 길이는 20이기 때문에 lag20-40의 exposure 값은 0으로 입력됩니다.\n이렇게 만들어진 exposure history를 crosspred()의 at 인자에 넣어줍니다.\n\npnesthist &lt;- crosspred(cbnest, mnest, cen=0, at=hist)\nwith(pnesthist, c(allRRfit,allRRlow,allRRhigh))\n\n      20       20       20 \n3.503928 1.240109 9.900351 \n\n\n위의 코드를 통해 해당 exposure history에서의 OR 추정치는 3.5임을 알 수 있습니다.\n이 방법을 통해 여러 개의 time-varying exposure histories를 만들어 effect summaries를 구할 수 있습니다. ’3.2 A simple DLM’에서 이용한 drug 데이터를 다시 분석해봅시다. 환자의 2주 동안의 약물 복용량이 10이고, 그 후 1주간은 50이고, 그 후 1주간은 복용하지 않았을 때, 각 시점에서의 effect summary를 구해봅시다. 아래와 같이 각 시점에서의 exposure history을 만들어줍니다.\n\nexpdrug &lt;- rep(c(10,50,0,20),c(2,1,1,2)*7)\ndynhist &lt;- exphist(expdrug, lag=27)\ndynhist[1:10,]\n\n   lag0 lag1 lag2 lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1    10    0    0    0    0    0    0    0    0    0     0     0     0     0\n2    10   10    0    0    0    0    0    0    0    0     0     0     0     0\n3    10   10   10    0    0    0    0    0    0    0     0     0     0     0\n4    10   10   10   10    0    0    0    0    0    0     0     0     0     0\n5    10   10   10   10   10    0    0    0    0    0     0     0     0     0\n6    10   10   10   10   10   10    0    0    0    0     0     0     0     0\n7    10   10   10   10   10   10   10    0    0    0     0     0     0     0\n8    10   10   10   10   10   10   10   10    0    0     0     0     0     0\n9    10   10   10   10   10   10   10   10   10    0     0     0     0     0\n10   10   10   10   10   10   10   10   10   10   10     0     0     0     0\n   lag14 lag15 lag16 lag17 lag18 lag19 lag20 lag21 lag22 lag23 lag24 lag25\n1      0     0     0     0     0     0     0     0     0     0     0     0\n2      0     0     0     0     0     0     0     0     0     0     0     0\n3      0     0     0     0     0     0     0     0     0     0     0     0\n4      0     0     0     0     0     0     0     0     0     0     0     0\n5      0     0     0     0     0     0     0     0     0     0     0     0\n6      0     0     0     0     0     0     0     0     0     0     0     0\n7      0     0     0     0     0     0     0     0     0     0     0     0\n8      0     0     0     0     0     0     0     0     0     0     0     0\n9      0     0     0     0     0     0     0     0     0     0     0     0\n10     0     0     0     0     0     0     0     0     0     0     0     0\n   lag26 lag27\n1      0     0\n2      0     0\n3      0     0\n4      0     0\n5      0     0\n6      0     0\n7      0     0\n8      0     0\n9      0     0\n10     0     0\n\n\nexphist() 함수의 time 인자가 지정되지 않았을 경우에는 모든 time point에 대하여 exposure history가 만들어집니다. 이렇게 만들어진 exposure histories를 crosspred()의 at 인자에 넣어줍니다.\n\npdyndrug &lt;- crosspred(cbdrug, mdrug, at=dynhist)\n\n아래 코드로 그린 plot을 통해 각 시점에서의 exposure history의 effect summary를 확인할 수 있습니다.\n\nplot(pdyndrug,\"overall\", ylab=\"Effect\", xlab=\"Time (days)\", ylim=c(-10,27),\n     xlim=c(1,50), yaxt=\"n\")\naxis(2, at=-1:5*5)\npar(new=TRUE)\nplot(expdrug, type=\"h\", xlim=c(1,50), ylim=c(0,300), axes=F, ann=F)\naxis(4, at=0:6*10, cex.axis=0.8)\nmtext(\"Dose\", 4, line=-1.5, at=30, cex=0.8)"
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#applying-user-defined-functions",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#applying-user-defined-functions",
    "title": "the Extended DLNM 소개",
    "section": "3.5 Applying user-defined functions",
    "text": "3.5 Applying user-defined functions\ncross-basis 객체를 생성할 때, exposure와 lag 차원에서 사용할 basis function을 사용자 정의 함수로 지정할 수 있습니다. 이때, 사용자 정의 함수의 첫 번째 인자는 반드시 \\(X\\)값이어야하고, return 값은 변환된 벡터 혹은 행렬이어야 합니다.\n’3.3 A more complex DLNM’의 세 번째 plot을 보면, exposure-response 차원에서의 quadratic spline은 log 함수와 비슷한 형태임을 알 수 있습니다. cross-basis 객체를 생성할 때, exposure-response 차원에서의 basis function을 quadratic spline이 아닌 log 함수를 이용해봅시다.\n다음과 같이 log 함수를 정의합니다.\n\nmylog &lt;- function(x) log(x+1)\n\n위에서 정의한 mylog 함수를 crossbasis() 함수에 넣어줍니다.\n\ncbnest2 &lt;- crossbasis(Qnest, lag=c(3,40), argvar=list(\"mylog\"),\n                      arglag=list(fun=\"ns\",knots=c(10,30),intercept=F))\nsummary(cbnest2)\n\nCROSSBASIS FUNCTIONS\nobservations: 600 \nrange: 0 to 1064 \nlag period: 3 40 \ntotal df:  3 \n\nBASIS FOR VAR:\nfun: mylog \n\nBASIS FOR LAG:\nfun: ns \nknots: 10 30 \nintercept: FALSE \nBoundary.knots: 3 40 \n\n\n3.3에서 정의한 cross-basis cbnest와 비교하여 자유도가 9에서 3으로 감소한 것을 확인할 수 있습니다.\n아래 plot을 통해 추정 결과를 비교해봅시다.\n\nmnest2 &lt;- clogit(case~cbnest2+strata(riskset), nested)\npnest2 &lt;- crosspred(cbnest2, mnest2, cen=0, at=0:20*5)\n\npar(mfrow=c(1,3))\nplot(pnest2, zlab=\"OR\", xlab=\"Exposure\", ylab=\"Lag (years)\")\nplot(pnest2, var=50, ylab=\"OR for exposure 50\", xlab=\"Lag (years)\", xlim=c(0,40))\nlines(pnest, var=50, lty=2)\nplot(pnest2, lag=5, ylab=\"OR at lag 5\", xlab=\"Exposure\", ylim=c(0.95,1.15))\nlines(pnest, lag=5, lty=2)\n\n\n\n\n\n\n\n점선은 3.3에서 추정한 결과를 나타낸 것입니다. exposure-response 차원에서 basis function으로 log 함수를 사용했을 때와 quadratic spline을 사용했을 때의 결과가 유사함을 확인할 수 있습니다.\n’3.2 A simple DLM’의 두 번째 plot을 보면, lag-response 차원에서의 natural cubic spline은 지수적으로 감소하는 형태를 보입니다. 다음과 같이 exponential decay 함수를 정의합니다.\n\nfdecay &lt;- function(x,scale=5) {\n  basis &lt;- exp(-x/scale)\n  attributes(basis)$scale &lt;- scale\n  return(basis)\n}\n\n다음과 같이 crossbasis() 함수에 인자로 넣어주고, plot을 그려 결과를 확인합니다.\n\ncbdrug2 &lt;- crossbasis(Qdrug, lag=27, argvar=list(\"lin\"),\n                      arglag=list(fun=\"fdecay\",scale=6))\nsummary(cbdrug2)\n\nCROSSBASIS FUNCTIONS\nobservations: 200 \nrange: 0 to 100 \nlag period: 0 27 \ntotal df:  1 \n\nBASIS FOR VAR:\nfun: lin \nintercept: FALSE \n\nBASIS FOR LAG:\nfun: fdecay \nscale: 6 \n\nmdrug2 &lt;- lm(out~cbdrug2+sex, drug)\npdrug2 &lt;- crosspred(cbdrug2, mdrug2, at=0:20*5)\n\npar(mfrow=c(1,3))\nplot(pdrug2, zlab=\"Effect\", xlab=\"Dose\", ylab=\"Lag (days)\")\nplot(pdrug2, var=60, ylab=\"Effect at dose 60\", xlab=\"Lag (days)\", ylim=c(-1,5))\nlines(pdrug, var=60, lty=2)\nplot(pdrug2, lag=10, ylab=\"Effect at lag 10\", xlab=\"Dose\", ylim=c(-1,5))\nlines(pdrug, lag=10, lty=2)\n\n\n\n\n\n\n\n점선은 3.2에서 추정한 결과를 나타낸 것입니다. lag-response 차원에서 basis function으로 exponential decay 함수를 사용했을 때와 natural cubic spline을 사용했을 때의 결과가 유사함을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "",
    "text": "2x2 table에서 민감도와 특이도는 본래 McNemar’s test를 통해 \\(z = \\frac{(f_{21} - f_{12})}{\\sqrt{f_{12} + f_{21}}}\\)로 정의된 z값의 p-value를 확인하는 방법으로 진행되었습니다. 하지만 위 방식으로는 민감도와 특이도의 신뢰 구간에 대한 정보를 알 수 없기에, 신뢰 구간까지 정할 수 있는 방법인 Wald confidence interval을 적용함으로써 더 많은 정보를 얻을 수 있습니다."
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#분산의-기본-식",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#분산의-기본-식",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.1. 분산의 기본 식",
    "text": "2.1. 분산의 기본 식\n두 확률 변수 \\(p_1\\)과 \\(p_2\\)의 차이의 분산은 다음과 같이 계산됩니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\text{Var}(p_2) + \\text{Var}(p_1) - 2\\text{Cov}(p_2, p_1)\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#각-분산의-계산",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#각-분산의-계산",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.2. 각 분산의 계산",
    "text": "2.2. 각 분산의 계산\n우리는 \\(p_1\\)과 \\(p_2\\)를 비율로 간주할 수 있습니다. 이 비율들의 분산은 다음과 같이 계산됩니다:\n\\[\n\\text{Var}(p_1) = \\frac{p_1(1 - p_1)}{n} = \\frac{p_{12} + p_{11}}{n} \\cdot \\left(1 - \\frac{p_{12} + p_{11}}{n}\\right)\n\\]\n\\[\n\\text{Var}(p_2) = \\frac{p_2(1 - p_2)}{n} = \\frac{p_{21} + p_{11}}{n} \\cdot \\left(1 - \\frac{p_{21} + p_{11}}{n}\\right)\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#공분산-textcovp_2-p_1-의-계산",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#공분산-textcovp_2-p_1-의-계산",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.3. 공분산 \\(\\text{Cov}(p_2, p_1)\\) 의 계산",
    "text": "2.3. 공분산 \\(\\text{Cov}(p_2, p_1)\\) 의 계산\n\\(p_2\\) 와 \\(p_1\\) 의 공분산은 다음과 같이 정의됩니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\text{Cov}(p_{11} + p_{21}, p_{11} + p_{12})\n\\]\n이를 확장하면 다음과 같은 네 가지 항으로 분리할 수 있습니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\text{Cov}(p_{11}, p_{11}) + \\text{Cov}(p_{11}, p_{12}) + \\text{Cov}(p_{21}, p_{11}) + \\text{Cov}(p_{21}, p_{12})\n\\]\n공분산은 다음과 같이 정리할 수 있습니다.\n\\[\n\\text{Var}(p_{11}) = \\frac{p_{11}(1 - p_{11})}{n}\n\\]\n\\[\n\\text{Cov}(p_{11}, p_{12}) = -\\frac{p_{11}p_{12}}{n}\n\\]\n\\[\n\\text{Cov}(p_{11}, p_{21}) = -\\frac{p_{11}p_{21}}{n}\n\\]\n\\[\n\\text{Cov}(p_{21}, p_{12}) = -\\frac{p_{21}p_{12}}{n}\n\\]\n이를 바탕으로 \\(\\text{Cov}(p_2, p_1)\\)를 계산하면:\n\\[\n\\text{Cov}(p_2, p_1) = \\frac{p_{11}(1 - p_{11} - p_{12} - p_{21}) - p_{21}p_{12}}{n}\n\\]\n여기서 \\(p_{11} + p_{12} + p_{21} + p_{22} = 1\\)임을 이용하여, 공분산을 다음과 같이 단순화할 수 있습니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\frac{p_{11}p_{22} - p_{21}p_{12}}{n}\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#최종적으로-textvarp_2---p_1-의-유도",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#최종적으로-textvarp_2---p_1-의-유도",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.4. 최종적으로 \\(\\text{Var}(p_2 - p_1)\\) 의 유도",
    "text": "2.4. 최종적으로 \\(\\text{Var}(p_2 - p_1)\\) 의 유도\n이제 분산 공식을 대입하여 최종적으로 \\(p_2 - p_1\\)의 분산을 계산합니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\text{Var}(p_2) + \\text{Var}(p_1) - 2\\text{Cov}(p_2, p_1)\n\\]\n따라서 \\(p_2 - p_1\\)의 분산은 다음과 같습니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\frac{(p_{12} + p_{21}) - (p_{21} - p_{12})^2/n}{n}\n\\]\n\\[\n\\text{Var}(p_2 - p_1) = \\frac{(b + c) - (b - c)^2/n}{n}\n\\]\n이를 적용한 민감도의 차이에 대한 신뢰 구간은 다음과 같이 계산됩니다. \\[\n\\ CI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm z_{1-\\alpha/2} \\cdot \\frac{1}{n} \\cdot \\sqrt{b + c - \\frac{(b - c)^2}{n}} \\right] \\\n\\]\n\n  if ( (ci.method == \"wald\") & (cont.corr == FALSE) ) {\n    # sensitivity\n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    sens.diff.se &lt;- sqrt((b+c) - ((b-c)**2) / n) / n\n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}\n\n만약 이항분포로 얻어진 값을 정규분포에 맞도록 조정하기 위해 continuity correction을 진행한다면 wald에서는 확률 \\(p\\) 하나당 \\(\\frac{1}{2n}\\)만큼 분산을 늘려, 아래와 같은 값을 갖게 된다.\n\\[\nCI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm \\left( z_{1-\\alpha/2} \\cdot \\frac{1}{n} \\sqrt{b + c - \\frac{(b - c)^2}{n} }+ \\frac{1}{n} \\right) \\right].\n\\]\n\n  if ( (ci.method == \"wald\") & (cont.corr == TRUE) ) {\n    # sensitivity\n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    sens.diff.se &lt;- (sqrt((b+c) - ((b-c)**2) / n) / n) + 1/n\n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#agresti-신뢰-구간agresti-ci",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#agresti-신뢰-구간agresti-ci",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "3.1. Agresti 신뢰 구간(Agresti CI)",
    "text": "3.1. Agresti 신뢰 구간(Agresti CI)\nWald 신뢰 구간을 수정한 방법 중 하나로, 샘플 크기에 특정 상수를 더한 후 위 wald와 같은 방식으로 전체 샘플 크기 𝑛에 1에서 4까지의 상수를 더한 후 이 방법들을 비교했습니다. 그 결과, n+2를 사용하는 것이 표준 Wald 신뢰 구간과 비교했을 때 표본 증가의 효과로 신뢰 구간의 성능(coverage probability)을 향상시킴을 확인했습니다. \\[\n\\ CI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm z_{1-\\alpha/2} \\cdot \\frac{1}{n+2} \\cdot \\sqrt{(b+0.5) + (c+0.5) - \\frac{(b - c)^2}{n+2}} \\right] \\\n\\]\n\n  if (ci.method == \"agresti-min\") {\n    k &lt;- 0.5\n    # sensitivity    \n    b &lt;- tab$diseased[1,2]+k; c &lt;- tab$diseased[2,1]+k; n &lt;- tab$diseased[3,3]+4*k\n    sens.diff.se &lt;- (sqrt((b+c) - ((b-c)**2) / n) / n) \n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#tango",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#tango",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "3.2. Tango",
    "text": "3.2. Tango\nTango는 귀무가설과 함수는 두 비율 간의 차이와, 주어진 정보 행렬(\\(b\\), \\(c\\), \\(n\\))을 기반으로 likelihood를 계산해 신뢰구간을 계산하는 방법입니다. R에서는 scoreci.mp라는 별도의 함수를 사용해 진행합니다.\n\n  if (ci.method == \"tango\") {\n    # sensitivity    \n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    tango &lt;- scoreci.mp(b, c, n, conf.level=1-alpha)    \n    sens.diff.se &lt;- NA    \n    sens.diff.cl &lt;- sort(c(tango$conf.int[1], tango$conf.int[2]))\n    if ( (tango$conf.int[1] &gt; sens.diff) | (tango$conf.int[2] &lt; sens.diff))\n      sens.diff.cl &lt;- sort(-1*sens.diff.cl)}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#일반적인-민감도-특이도-검정",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#일반적인-민감도-특이도-검정",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "4.1. 일반적인 민감도, 특이도 검정",
    "text": "4.1. 일반적인 민감도, 특이도 검정\n\nlibrary(DTComPair)\n\nLoading required package: PropCIs\n\nt1 &lt;- read.tab.paired(18, 14, 0, 18,\n                      18, 12, 2, 18)\nt1\n\nTwo binary diagnostic tests (paired design)\n\nTest1: 'Noname 1'\nTest2: 'Noname 2'\n\nDiseased:\n           Test1 pos. Test1 neg. Total\nTest2 pos.         18         14    32\nTest2 neg.          0         18    18\nTotal              18         32    50\n\nNon-diseased:\n           Test1 pos. Test1 neg. Total\nTest2 pos.         18         12    30\nTest2 neg.          2         18    20\nTotal              20         30    50\n\nsesp.diff.ci(t1, ci.method=\"wald\", cont.corr=FALSE)\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.06349803 0.15554615 0.40445385 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.06928203 -0.33579029 -0.06420971 \n\n$ci.method\n[1] \"wald\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\nsesp.diff.ci(t1, ci.method=\"wald\", cont.corr=TRUE)\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.08349803 0.11634687 0.44365313 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.08928203 -0.37498957 -0.02501043 \n\n$ci.method\n[1] \"wald\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] TRUE\n\nsesp.diff.ci(t1, ci.method=\"agresti-min\")\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.06444681 0.15368658 0.40631342 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.06954236 -0.33630053 -0.06369947 \n\n$ci.method\n[1] \"agresti-min\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\nsesp.diff.ci(t1, ci.method=\"tango\")\n\n$sensitivity\n    test1     test2      diff   diff.se  diff.lcl  diff.ucl \n0.3600000 0.6400000 0.2800000        NA 0.1747417 0.4166512 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000          NA -0.34470882 -0.06111243 \n\n$ci.method\n[1] \"tango\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\n\n아래와 같은 방법으로 answer, test1, test2를 열로 가지고, 각 결과 데이터가 1,0(1은 diseased sample, 2는 non-diseased sample)로 표시되어 있는 경우에는 바로 paired table을 제작할 수 있습니다.\n\nlibrary(DTComPair)\ntb &lt;- tab.paired(answer, test1, test2, data = na.omit(sample_data))"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#민감도-특이도의-비열등성-검정",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#민감도-특이도의-비열등성-검정",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "4.2. 민감도, 특이도의 비열등성 검정",
    "text": "4.2. 민감도, 특이도의 비열등성 검정\n어떤 테스트의 민감도와 특이도가 대조 테스트에 비해 떨어지지 않는다는 것을 확인하기 위해서는 비열등성 검정을 사용해야 합니다. 이 때는 sesp.diff.ci 함수를 통해 구한 standard error 값과 두 민감도/특이도의 차이를 통해 p value를 계산합니다. 아래 예시에서는 민감도에 대한 비열등성 마진(sens_margin)을 5%, 특이도에 대한 비열등성 마진(spec_margin)을 10%로 설정했습니다.\n민감도를 예로 들자면 귀무가설은 아래와 같고, 아래 수식에 따라 p value를 계산합니다.\n\\[H_0 \\colon \\text{test1 sensitivity} - \\text{test2 sensitivity} \\leq -\\text{ sensitivity margin}\\] \\[p-value = 1 - Φ\\left(\\frac{\\text{test1 sensitivity} - \\text{test2 sensitivity} + \\text{sensitivity margin}}{\\text{sensitivity diff.SE}}\\right)\\]\n\nlibrary(DTComPair)\nt1 &lt;- read.tab.paired(18, 14, 0, 18,\n                      18, 12, 2, 18)\nt1.wald &lt;- sesp.diff.ci(t1, ci.method=\"wald\", cont.corr=FALSE)\n\nsens_margin &lt;-  0.05\nspec_margin &lt;-  0.1\n\np_value_sensitivity &lt;- pnorm((t1.wald$sensitivity['diff'] + sens_margin) / t1.wald$sensitivity['diff.se'], lower.tail = FALSE)\np_value_specificity &lt;- pnorm((t1.wald$specificity['diff'] + spec_margin) / t1.wald$specificity['diff.se'], lower.tail = FALSE)\np_value_sensitivity\n\n        diff \n1.012589e-07 \n\np_value_specificity\n\n     diff \n0.9255427 \n\n\n따라서 민감도에 대해서는 위 귀무가설이 기각 되었음으로 test1의 민감도가 test2보다 열등하지 않다고 말할 수 있지만, 특이도에 대해서는 귀무가설이 기각되지 않았기 때문에 test1의 특이도가 test2에 비해 열등하지 않다는 결론을 내릴 수 없습니다."
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html",
    "href": "posts/2024-10-08 Survey/index.html",
    "title": "Survey design 모델에서의 통계",
    "section": "",
    "text": "이 문서에서는 Survey 모델에 대한 개요와 이를 jstable 패키지, forestploter package를 사용하여 필요한 모형들을 제작하는 방법을 알아보겠습니다"
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#survey-model의-특징",
    "href": "posts/2024-10-08 Survey/index.html#survey-model의-특징",
    "title": "Survey design 모델에서의 통계",
    "section": "Survey model의 특징",
    "text": "Survey model의 특징\n층화\n국민건강영양조사의 경우 무작위로 표본을 추출하지 않고, 층화, 군집화, 가중치 등의 방법을 사용하여 표본을 추출합니다. 층화에 대해 먼저 알아보겠습니다. 인구 집단을 성별, 연령대, 지역 등으로 층화하여 각 층에서 독립적으로 표본을 추출합니다. 이렇게 하는 경우 특정 집단이 과소 또는 과대 대표되는 문제를 방지할 수 있습니다.\n군집화\n가구나 지역단위로 표본을 추출하는 군집화 방법또한 사용합니다. 군집화된 데이터는 같은 군집 내의 사람들 간에 상관성이 존재할 수 있기 때문에, 이를 반영할 수 있도록 데이터를 추출하여 사용합니다.\n가중치\n각 표본을 추출한 이후에는 그 표본이 전체 인구를 어느정도 반영하는지를 보는 가중치를 제공하여야합니다. 특정 연령대나 지역에서 표본이 과대 추출되는 경우 가중치를 낮추는 등의 방법으로 전체 인구를 보다 정확히 대변할 수 있습니다.\n유한 모집단 보정\n이는 국민건강영양조사와 같이 전체 인구집단과 표본의 차이가 큰 경우에는 크게 중요하지 않을 수도 있으나, 표본의 크기가 전체 모집단에 비해 클 때 모집단이 가지는 변동성을 조정할 필요하 있습니다. 따라서 이런 경우 Survey 모델에서는 보정이 필요합니다. FPC를 구하는 공식을 보시면 모집단이 커질 수록 변동성이 커진다는 것을 알 수 있습니다.\n\\[\nFPC = \\sqrt{\\frac{N - n}{N - 1}}\n\\]"
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#survey-모델을-가지고-모형-만들기",
    "href": "posts/2024-10-08 Survey/index.html#survey-모델을-가지고-모형-만들기",
    "title": "Survey design 모델에서의 통계",
    "section": "Survey 모델을 가지고 모형 만들기",
    "text": "Survey 모델을 가지고 모형 만들기\nSurvey 모델을 통한 선형회귀 분석\n간단하게 국민건강영양조사에서 공개된 자료로 선형회귀 분석을 직접 진행해보겠습니다. 이 포스트에서 다루는 통계는 코드 실행과 통계학적 모형 설명을 위해 사용하는 것임으로, 엄격한 inclusion, exclusion criteria 과정 등을 적용하지 않아 결과는 현실과 상이할 수 있음을 미리 밝힙니다.\n국민건강영양조사의 2012년 안검사 데이터를 사용해보겠습니다. 국민건강영양조사 사이트에서 데이터를 다운 받은 이후 haven, data.table 라이브러리를 사용하여 자료를 datatable로 바꾸겠습니다.\n\nlibrary(survey)\nlibrary(haven)\nsas_data &lt;- read_sas(\"hn12_eye.sas7bdat\")\nhn12&lt;-data.table(sas_data)\n\n이후 교육, 성별, 나이에 따른 백내장 진단 여부를 알아보도록 하겠습니다. Survey 모델의 경우, 앞서 말씀드린 특징들로 인해 데이터를 사용하기 이전에 Survey design모델이 필요합니다.\n\nhn12_cat&lt;- hn12[E_DH2_dg %in% c(1,2) &!is.na(edu)&!is.na(sex)&!is.na(age)]\nhn12_cat$E_DH2_dg[hn12_cat$E_DH2_dg == 2] &lt;- 0\nsurvey_hn12 &lt;- svydesign(id = ~psu, strata = ~kstrata, weights = ~wt_itvex, data = hn12_cat)\n\n분석을 위해 간단하게, 성별, 나이, 교육에서 결측치를 제거하였고 백내장 진단 여부에 대해서는 무응답과 같은 변수를 제거하고 진단 여부를 (1.예,2.아니오)만 사용하기로 하였고, 0,1을 가지는 이항변수로 만들기 위해 2.아니오는 0으로 코딩하는 과정을 거쳤습니다.\n모델 디자인의 경우, svydesign이라는 함수를 사용하게 되는데, 표본조사를 제공하는 곳에서 보통 id, strata, weights에 사용할 수 있는 변수들에 대한 설명이 같이 첨부되어있습니다. 국민건강영양조사에서 제공하는 안대에 따라 psu, kstrata, wt_itvex변수를 코딩하였습니다. 앞서 말씀드린바와 같이 국민건강영양조사에서는 전체 인구가 표본에 비해 크기 때문에 fpc 변수가 필요하지 않지만, 모델에 따라 fpc변수를 svydesign에 추가할 수 있습니다. 백내장 진단여부를 이항변수로 코딩했기 때문에 앞선 포스트에서 살펴본 glm모델을 사용하여 데이터를 살펴보도록 하겠습니다. 이후 jstable 패키지를 이용해 glm모델을 살펴볼 수 있는 테이블을 만들어보겠습니다\n\nds&lt;-svyglm(E_DH2_dg~age+as.factor(edu)+as.factor(sex), design=survey_hn12, family =quasibinomial())\nsvyregress.display(ds)\n\n$first.line\n[1] \"Logistic regression predicting E_DH2_dg- weighted data\\n\"\n\n$table\n                       crude OR.(95%CI)   crude P value adj. OR.(95%CI)   \nage                    \"1.15 (1.13,1.16)\" \"&lt; 0.001\"     \"1.15 (1.13,1.16)\"\nas.factor(edu): ref.=1 NA                 NA            NA                \nas.factor(edu)2        \"0.3 (0.22,0.41)\"  \"&lt; 0.001\"     \"0.86 (0.58,1.3)\" \nas.factor(edu)3        \"0.19 (0.14,0.27)\" \"&lt; 0.001\"     \"1 (0.68,1.46)\"   \nas.factor(edu)4        \"0.17 (0.11,0.24)\" \"&lt; 0.001\"     \"1.22 (0.78,1.91)\"\nas.factor(sex): 2 vs 1 \"1.71 (1.39,2.1)\"  \"&lt; 0.001\"     \"1.47 (1.12,1.93)\"\n                       adj. P value\nage                    \"&lt; 0.001\"   \nas.factor(edu): ref.=1 NA          \nas.factor(edu)2        \"0.479\"     \nas.factor(edu)3        \"0.997\"     \nas.factor(edu)4        \"0.392\"     \nas.factor(sex): 2 vs 1 \"0.006\"     \n\n$last.lines\n[1] \"No. of observations = 3891\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n테이블의 결과를 살펴 보았을 때 나이는 통계적으로 유의미하게 백내장의 발생과 관련이 있고, 교육수준은 거의 없는 것으로 알 수 있습니다. 성별에 따른 차이도 p-value가 0.05보다는 크지만 0.06으로 통계적으로 유의미하지는 않지만 조금의 관련성이 있다는 것을 알 수 있습니다. 이번에는 strata별로 볼 수 있는 table을 만들어보도록 하겠습니다.\n\nhn12_cat$E_DH2_dg&lt;-as.numeric(hn12_cat$E_DH2_dg)\nhn12_cat$age&lt;-as.numeric(hn12_cat$age)\nsvyCreateTableOne2(strata = 'sex', vars = c('E_DH2_dg','age'), data = survey_hn12)\n\n          level 1                      2                      p        test\nn         \"\"    \"10311810.38\"          \"11404537.11\"          \"\"       \"\"  \nE_DH2_dg  \"\"    \"       0.09 ± 0.29\"  \"       0.15 ± 0.36\"  \"&lt;0.001\" \"\"  \nage       \"\"    \"      54.92 ± 10.74\" \"      56.64 ± 11.52\" \"&lt;0.001\" \"\"  \n          sig \nn         NA  \nE_DH2_dg  \"**\"\nage       \"**\"\n\nsvyCreateTableOneJS(strata = 'sex', strata2 = 'edu', vars = c('E_DH2_dg','age'), data = survey_hn12)\n\n$table\n          level 1                    2                    3                   \nn         \"\"    \"2112878.74\"         \"1499859.85\"         \"3737738.11\"        \nE_DH2_dg  \"\"    \"      0.16 ± 0.37\" \"      0.10 ± 0.30\" \"      0.08 ± 0.27\"\nage       \"\"    \"     63.35 ± 9.70\" \"     57.76 ± 9.59\" \"     52.83 ± 9.84\"\n          4                    p        test sig  1                   \nn         \"2961333.68\"         \"\"       \"\"   NA   \"4381837.88\"        \nE_DH2_dg  \"      0.06 ± 0.23\" \"&lt;0.001\" \"\"   \"**\" \"      0.30 ± 0.46\"\nage       \"     50.09 ± 9.11\" \"&lt;0.001\" \"\"   \"**\" \"     66.23 ± 9.29\"\n          2                    3                    4                   \nn         \"1694071.75\"         \"3575372.10\"         \"1753255.37\"        \nE_DH2_dg  \"      0.09 ± 0.28\" \"      0.05 ± 0.21\" \"      0.05 ± 0.21\"\nage       \"     55.89 ± 7.87\" \"     49.75 ± 7.83\" \"     47.47 ± 7.20\"\n          p        test sig \nn         \"\"       \"\"   NA  \nE_DH2_dg  \"&lt;0.001\" \"\"   \"**\"\nage       \"&lt;0.001\" \"\"   \"**\"\n\n$caption\n[1] \"Stratified by sex() & edu- weighted data\"\n\n\nsvyCreateTableOne2같은 경우에는 보다 단순한 데이터에서 하나의 층으로 테이블을 만들고 싶을 때 사용할 수 있으며, svyCreateTableOneJS의 경우에는 층화를 두 층 이상, 그리고 조금 더 다양한 옵션들이 가능합니다. 유사한 방식으로 진단여부와 같은 이항분포뿐만 아니라 안압과 같은 연속변수에도 적용이 가능합니다. 녹내장 환자를 에시로 하면 아래와 같이 할 수 있습니다.\n\nhn12_glau&lt;-hn12[!(E_Gr_p%in% c(NA, 888, 999))&!is.na(E_Gl_p)&!is.na(edu)&!is.na(sex)&!is.na(age)]\nsurvey_hn12 &lt;- svydesign(id = ~psu, strata = ~kstrata, weights = ~wt_itvex, data = hn12_glau)\nds2&lt;-svyglm(E_Gr_p~age+as.factor(edu)+as.factor(sex), design=survey_hn12)\nsvyregress.display(ds2)\n\n$first.line\n[1] \"Linear regression predicting E_Gr_p- weighted data\\n\"\n\n$table\n                       crude coeff.(95%CI)   crude P value\nage                    \"0.01 (0,0.01)\"       \"0.142\"      \nas.factor(edu): ref.=1 NA                    NA           \nas.factor(edu)2        \"0.11 (-0.26,0.48)\"   \"0.552\"      \nas.factor(edu)3        \"0.06 (-0.27,0.38)\"   \"0.725\"      \nas.factor(edu)4        \"0 (-0.38,0.38)\"      \"0.995\"      \nas.factor(sex): 2 vs 1 \"-0.32 (-0.52,-0.11)\" \"0.002\"      \n                       adj. coeff.(95%CI)    adj. P value\nage                    \"0.01 (0,0.02)\"       \"0.059\"     \nas.factor(edu): ref.=1 NA                    NA          \nas.factor(edu)2        \"0.16 (-0.22,0.54)\"   \"0.399\"     \nas.factor(edu)3        \"0.22 (-0.18,0.62)\"   \"0.287\"     \nas.factor(edu)4        \"0.16 (-0.31,0.63)\"   \"0.496\"     \nas.factor(sex): 2 vs 1 \"-0.31 (-0.51,-0.12)\" \"0.002\"     \n\n$last.lines\n[1] \"No. of observations = 5339\\nAIC value = 27920.3506\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\nSurvey 모델을 통한 생존 분석\n이번에는 미국의 국민건강영양조사의 데이터를 가지고 생존분석을 진행해보도록 하겠습니다. 사이트에서 Mortality, 와 기본 조사 데이터를 다운 받은 이후, 이전과 유사한 방법으로 제공된 weight, id, strata를 통해 design모델을 만들어보겠습니다.\n\nlibrary(readr)\ndemo &lt;- read_xpt(\"DEMO_J.XPT\")\nmortality &lt;- read_fwf(\"NHANES_2017_2018_MORT_2019_PUBLIC.dat\",\n         col_types = \"iiiiiiii\",\n         fwf_cols(SEQN = c(1,6),\n                  ELIGSTAT = c(15,15),\n                  MORTSTAT = c(16,16),\n                  UCOD_LEADING = c(17,19),\n                  DIABETES = c(20,20),\n                  HYPERTEN = c(21,21),\n                  PERMTH_INT = c(43,45),\n                  PERMTH_EXM = c(46,48)),\n         na = c(\"\", \".\")\n)\nhead(mortality)\n\n# A tibble: 6 × 8\n   SEQN ELIGSTAT MORTSTAT UCOD_LEADING DIABETES HYPERTEN PERMTH_INT PERMTH_EXM\n  &lt;int&gt;    &lt;int&gt;    &lt;int&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n1 93703        2       NA           NA       NA       NA         NA         NA\n2 93704        2       NA           NA       NA       NA         NA         NA\n3 93705        1        0           NA       NA       NA         18         18\n4 93706        1        0           NA       NA       NA         35         34\n5 93707        2       NA           NA       NA       NA         NA         NA\n6 93708        1        0           NA       NA       NA         19         18\n\nmortality$PERMTH_EXM[mortality$PERMTH_EXM == \".\"] &lt;- NA\nmortality$PERMTH_EXM &lt;- as.numeric(mortality$PERMTH_EXM)\nmortality$MORTSTAT &lt;- as.numeric(mortality$MORTSTAT)\nmerged_data &lt;- merge(demo, mortality, by = \"SEQN\")\nmerged_data_clean &lt;- merged_data[complete.cases(merged_data[, c(\"MORTSTAT\", \"PERMTH_EXM\", \"RIDAGEYR\", \"DMDEDUC2\", \"RIAGENDR\",'INDFMIN2')]), ]\nsurvey_nhanes &lt;- svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, data = merged_data_clean, nest = TRUE)\n\n이번에는 생존 분석을 진행할 예정이기 때문에 사망여부인 변수 MORTSTAT과 관찰기관 변수인 PERMTH_EXM을 사용하여 survey cox모델을 만들어보겠습니다. 기존의 Cox모델과 함수 작성방법이 유사하며 svycoxph함수를 사용하여, 사망이벤트와 나이(RIDAGEYR), 교육(DMDEDUC2), 성별(RIAGENDR), 가정소득수준(IDFMIN2)의 관련성을 알아보겠습니다.\n\nds3 &lt;- svycoxph(Surv(PERMTH_EXM, MORTSTAT) ~ RIDAGEYR + DMDEDUC2 + RIAGENDR+INDFMIN2, design = survey_nhanes)\nsvycox.display(ds3)\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\n\n\n$table\n         crude HR(95%CI)    crude P value adj. HR(95%CI)     adj. P value\nRIDAGEYR \"1.08 (1.06,1.1)\"  \"&lt; 0.001\"     \"1.08 (1.06,1.1)\"  \"&lt; 0.001\"   \nDMDEDUC2 \"0.74 (0.6,0.9)\"   \"0.003\"       \"0.79 (0.67,0.93)\" \"0.005\"     \nRIAGENDR \"0.68 (0.38,1.19)\" \"0.172\"       \"0.59 (0.35,1)\"    \"0.05\"      \nINDFMIN2 \"0.98 (0.92,1.03)\" \"0.404\"       \"0.98 (0.95,1.02)\" \"0.356\"     \n\n$metric\n                       [,1] [,2] [,3] [,4]\n&lt;NA&gt;                     NA   NA   NA   NA\nNo. of observations 4991.00   NA   NA   NA\nNo. of events        124.00   NA   NA   NA\nAIC                 1401.96   NA   NA   NA\n\n$caption\n[1] \"Survey cox model on time ('PERMTH_EXM') to event ('MORTSTAT')\"\n\n\n결과를 대략 살펴보면 조정 이후 나이와 성별 교육 수준이 모두 통게적으로 유의미하게 관련이 있다는 것을 확인할 수 있습니다. 이번에는 인종, 나이(50세 이상 이하), 미국 시민권(Naturalized여부) 등의 그룹에 대해 subgroup analysis를 진행해보도록 하겠습니다.\n\nmerged_data_mod&lt;- merged_data %&gt;% \n  mutate(gender = factor(RIAGENDR, labels = c(\"Male\", \"Female\")),\n         MORTSTAT = as.numeric(MORTSTAT == 1),\n         age_group = ifelse(RIDAGEYR &gt;= 50, \"Over 50\", \"Under 50\"),\n         race_ethnicity = factor(case_when(\n           RIDRETH1 == 1 | RIDRETH1 == 2 ~ \"Hispanic\",\n           RIDRETH1 == 3 ~ \"Non-Hispanic White\",\n           RIDRETH1 == 4 ~ \"Non-Hispanic Black\")),\n         citizenship_status = factor(case_when(\n           DMDCITZN == 1 ~ \"U.S. Citizen\",\n           DMDCITZN == 2 ~ \"Naturalized Citizen\"\n         )),\n  )\nmerged_data_mod&lt;-merged_data_mod[complete.cases(merged_data_mod[, c(\"MORTSTAT\", \"PERMTH_EXM\", \"RIDAGEYR\",  \"RIAGENDR\",\"RIDRETH1\", \"DMDCITZN\",\"age_group\", \"race_ethnicity\", \"citizenship_status\")]), ]\nTableSubgroupMultiCox(Surv(PERMTH_EXM, MORTSTAT) ~ gender, var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, time_eventrate = 365 , line = FALSE)\n\n                     Variable Count Percent Point Estimate Lower Upper\ngender                Overall  4413     100           0.65  0.45  0.94\n1                   age_group  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n2                     Over 50  2387    54.1           0.69  0.47     1\n3                    Under 50  2026    45.9           0.59   0.1  3.56\n4              race_ethnicity  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n5                    Hispanic  1250    28.3           1.79  0.54  5.94\n6          Non-Hispanic Black  1266    28.7           0.55  0.26  1.17\n7          Non-Hispanic White  1897      43           0.61  0.38  0.96\n8          citizenship_status  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n9         Naturalized Citizen   504    11.4           2.46  0.26 23.63\n10               U.S. Citizen  3909    88.6           0.63  0.43  0.92\n       gender=Male gender=Female P value P for interaction\ngender         4.8           2.7   0.022              &lt;NA&gt;\n1             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.869\n2              8.5           4.9    0.05              &lt;NA&gt;\n3              0.5           0.3   0.569              &lt;NA&gt;\n4             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.202\n5              0.7           1.3   0.342              &lt;NA&gt;\n6              4.1           2.3   0.122              &lt;NA&gt;\n7              8.2             4   0.035              &lt;NA&gt;\n8             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.232\n9              0.4           1.2   0.436              &lt;NA&gt;\n10             5.5           2.9   0.016              &lt;NA&gt;\n\nTableSubgroupMultiGLM(MORTSTAT ~ gender,var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, family = \"binomial\")\n\n                           Variable Count Percent   OR Lower Upper P value\ngenderFemale                Overall  4413     100 0.64  0.44  0.93    0.02\n1                         age_group  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n2                           Over 50  2387    54.1 0.67  0.46  0.99   0.043\n3                          Under 50  2026    45.9 0.58   0.1   3.5   0.557\n4                    race_ethnicity  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n5                          Hispanic  1250    28.3  1.8  0.54     6    0.34\n6                Non-Hispanic Black  1266    28.7 0.54  0.25  1.16   0.114\n7                Non-Hispanic White  1897      43 0.59  0.37  0.95    0.03\n8                citizenship_status  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n9               Naturalized Citizen   504    11.4 2.49  0.26 24.15    0.43\n10                     U.S. Citizen  3909    88.6 0.62  0.42  0.91   0.014\n             P for interaction\ngenderFemale              &lt;NA&gt;\n1                        0.881\n2                         &lt;NA&gt;\n3                         &lt;NA&gt;\n4                        0.192\n5                         &lt;NA&gt;\n6                         &lt;NA&gt;\n7                         &lt;NA&gt;\n8                        0.235\n9                         &lt;NA&gt;\n10                        &lt;NA&gt;\n\n\n이번에는 TableSubgroupMultiCox 함수를 사용하면, 원하는 테이블을 얻을 수 있다는 것을 알 수 있습니다. Subgroup별로 남성과 여성의 사망률의 차이를 직관적이게 table로 확인할 수 있습니다. Subgroup analysis의 경우 하지만, forestplot도 필요한 경우들이 많기 때문에 forestploter 패키지를 이용하여 만들어보도록 하겠습니다."
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#forestploter를-이용한-forestplot만들기",
    "href": "posts/2024-10-08 Survey/index.html#forestploter를-이용한-forestplot만들기",
    "title": "Survey design 모델에서의 통계",
    "section": "Forestploter를 이용한 forestplot만들기",
    "text": "Forestploter를 이용한 forestplot만들기\n우선, 필요한 경우 forestploter를 R에서 설치합니다(Link Text에서 더 상세한 사용법에 대해서 알 수 있습니다) 이후 TableSubgroupMultiCox 함수를 이용하여 데이터를 불러온 이후, 변수명을 제외한 나머지 행들을 numeric으로 바꿔줍니다.\n\nlibrary(grid)\nlibrary(forestploter)\ndf&lt;- TableSubgroupMultiCox(Surv(PERMTH_EXM, MORTSTAT) ~ gender, var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, line = FALSE)\ndf&lt;-data.table(df)\nnum_cols&lt;- names(df)[-c(1)]\ndf[, (num_cols) := lapply(.SD, as.numeric), .SDcols = num_cols]\n\n이후 가독성을 위해 subgroup 변수들은 띄어쓰기를 진행하고, NA 값들은 빈칸으로 바꾸어줍니다. 또한 표준 오차값을 변수를 통해 지정하여 주고, plot의 line을 그리기 위한 공간을 테이블에 확보해줍니다. HR값 또한 변수내에서 계산을 통해 행을 추가합니다. 이후 원하는 테마를 지정한 이후, 위와 같이 코드를 실행한다면 forest plot을 얻을 수 있습니다.\n\ndf$Variable &lt;- ifelse(is.na(df$Count), \n                      df$Variable,\n                      paste0(\"   \", df$Variable))\ndf$Count &lt;- ifelse(is.na(df$Count), \"\", df$Count)\ndf$se &lt;- (log(df$Upper) - log(df$'Point Estimate'))/1.96\ndf$` ` &lt;- paste(rep(\" \", 20), collapse = \" \")\ndf$`HR (95% CI)` &lt;- ifelse(is.na(df$se), \"\",\n                           sprintf(\"%.2f (%.2f to %.2f)\",\n                                   df$'Point Estimate', df$Lower, df$Upper))\ntm &lt;- forest_theme(base_size = 10,\n                   refline_col = \"red\",\n                   arrow_type = \"closed\",\n                   footnote_gp = gpar(col = \"blue\", cex = 0.6))\n\nrefline_col will be deprecated, use refline_gp instead.\n\np &lt;- forest(df[,c(1:2, 12:13)],\n            est = df$'Point Estimate',\n            lower = df$Lower, \n            upper = df$Upper,\n            sizes = df$se,\n            ci_column = 3,\n            ref_line = 1,\n            arrow_lab = c(\"Female Better\", \"Male Better\"),\n            xlim = c(0, 4),\n            ticks_at = c(0.5, 1, 2, 3),\n            footnote = \"Example data using NHANES\",\n            theme = tm)\nplot(p)"
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html",
    "href": "posts/2024-10-17-TTE/index.html",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "",
    "text": "임상연구에서 exposure와 outcome 간의 인과관계를 밝히는 gold standard는 randomized controlled trial(RCT)입니다. 그러나 임상현장 및 임상연구자의 현실적인 제약들로 인해 모든 임상연구를 RCT로 시행할 수는 없으며, 대부분의 연구는 real world data(RWD)를 기반으로 이루어집니다. Target trial emulation이란, 목표로 하는 가상의 RCT(target trial)를 설정한 후, RWD를 대상으로 이 RCT를 모사하여 인과성을 추론하는 연구기법입니다.  이 글에서는 Clone-Censor-Weight method를 소개하고, 이를 이용하여 Target trial emulation을 R에서 구현하는 과정을 살펴봅니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#introduction",
    "href": "posts/2024-10-17-TTE/index.html#introduction",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "",
    "text": "임상연구에서 exposure와 outcome 간의 인과관계를 밝히는 gold standard는 randomized controlled trial(RCT)입니다. 그러나 임상현장 및 임상연구자의 현실적인 제약들로 인해 모든 임상연구를 RCT로 시행할 수는 없으며, 대부분의 연구는 real world data(RWD)를 기반으로 이루어집니다. Target trial emulation이란, 목표로 하는 가상의 RCT(target trial)를 설정한 후, RWD를 대상으로 이 RCT를 모사하여 인과성을 추론하는 연구기법입니다.  이 글에서는 Clone-Censor-Weight method를 소개하고, 이를 이용하여 Target trial emulation을 R에서 구현하는 과정을 살펴봅니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method",
    "href": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Clone-Censor-Weight method",
    "text": "Clone-Censor-Weight method\nClone-censor-weight method에서는  1) eligibile한 모든 대상자를 clone하여 치료군(Treatment)과 비교군(Control)에 할당하고,  2) 실제 대상자가 치료군에 속한 경우라면 대조군에 있던 clone은 치료 시작 시점에 artificial censoring되며(치료군의 대상자 중 정의된 치료시작시점 이후에 치료가 시작된 경우도 artificial censoring),  3) 해당 baseline characteristics를 가진 대상자가 관찰시간에 따라 artificial censoring되지 않고 남아있을 확률의 역수를 가중치로 하여 분석 하게 됩니다.\nImmortal time bias\nimmortal time bias는 추적관찰 연구에서 Follow up start period와 Treatment start period가 다를 때 발생하는 bias입니다. 연구대상자가 치료군에 속하기 위해서는 Treatment start period까지 생존(event 미발생)해 있어야 합니다. 즉, 대상자가 치료군에 속해있다면 Follow up start period와 Treatment start period 사이의 시간은 event가 발생할 수 없는 immortal time이 되는 것입니다. 따라서 immortal time 구간은 치료를 받지 않은 시기임에도 time to event를 계산할 때 치료를 받은 시기로 산입되는 misclassification bias가 생기고, 치료군은 치료시작시점까지 생존한 환자들이 선택되어 selection bias가 발생합니다.\n\n\n\n\n\nFigure 1: Immortal time bias\n\n\nImmortal time bias는 약물 복용의 효과나 질환의 경과 중에 시행하는 시술의 효과를 평가하는 연구 등에서 치료의 효과를 overestimation할 수 있어 중요하게 고려해야 하는 bias입니다. \n지금까지는 immortal time bias를 보정하기 위해 Landmark analysis나 Time-dependent survival analysis 등을 사용했습니다. 하지만 Landmark method는 어느 시점을 landmark로 결정할지에 대한 문제가 있으며 실제 치료나 약물복용을 정확히 반영하지 못한다는 단점이 있습니다. Clone-censor-weight 과정을 거치면 실제로 치료를 받은 대상자의 clone이 immortal time 동안 비교군에 있다가 치료 시작시점에 censor되므로, bias를 보정할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#실습-데이터-구성",
    "href": "posts/2024-10-17-TTE/index.html#실습-데이터-구성",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "실습 데이터 구성",
    "text": "실습 데이터 구성\n데이터 소개\n실습에 사용할 데이터는 국민건강보험공단에서 제공하는 예시 데이터입니다. 실습에서는 개인별 기본정보(성별 등)를 담고 있는 bnc, 사망정보를 담고 있는 bnd, 진료명세서 데이터(상병명)인 m20, 약물처방 데이터인 m60 데이터를 사용할 것입니다. bnd 데이터에는 사망일자 변수(DTH_YYYYMM)가 연도와 월 까만 있고 날짜는 없어, 분석을 위해 모든 대상자의 사망일자를 사망한 월의 말일로 변환하여 데이터를 불러오겠습니다.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(survival))\nsuppressPackageStartupMessages(library(boot))\nsuppressPackageStartupMessages(library(DT))\n\ninst &lt;- fread(\"data/nsc2_inst_1000.csv\")\nbnc &lt;- fread(\"data/nsc2_bnc_1000.csv\") \nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\") \nm20 &lt;- fread(\"data/nsc2_m20_1000.csv\") \nm30 &lt;- fread(\"data/nsc2_m30_1000.csv\") \nm40 &lt;- fread(\"data/nsc2_m40_1000.csv\") \nm60 &lt;- fread(\"data/nsc2_m60_1000.csv\") \ng1e_0915 &lt;- fread(\"data/nsc2_g1e_0915_1000.csv\") \n\nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\")[, Deathdate := (lubridate::ym(DTH_YYYYMM) %&gt;% lubridate::ceiling_date(unit = \"month\") - 1)][]\n\nTarget trial 설정\n이 실습의 Target trial에서는 2011년 1월 1일부터 2015년 12년 31까지 위염 및 십이지장염(공단 질병분류기호 K29.0~K29.9)을 진단받은 환자들 중, Proton pump inhibitor (PPI) 복용에 따라 위염 및 십이지장염의 재발 위험이 달라지는지를 보고자 합니다. 즉, time-zero는 위염의 진단 시점이고, 관심 event는 위염의 재발입니다. 본 실습은 Clone-censor-weight 과정을 소개하기 위한 글이므로, 임상적 타당성은 고려하지 않았음을 미리 밝힙니다.\nData set 구성\n먼저 처방정보(m60)데이터에서 PPI에 해당하는 처방코드(code.ppi)가 입력된 case들을 선택한 뒤, 가장 마지막으로 PPI를 처방받은 행을 선택하겠습니다(m60.drug).\n\ncode.ppi &lt;-  c(\"367201ACH\", \"367201ATB\", \"367201ATD\", \"367202ACH\", \"367202ATB\", \n               \"367202ATD\", \"498001ACH\", \"498002ACH\", \"509901ACH\", \"509902ACH\", \n               \"670700ATB\", \"204401ACE\", \"204401ATE\", \"204402ATE\", \"204403ATE\", \n               \"664500ATB\", \"640200ATB\", \"664500ATB\", \"208801ATE\", \"208802ATE\", \n               \"656701ATE\", \"519201ATE\", \"519202ATE\", \"656701ATE\", \"519203ATE\", \n               \"222201ATE\", \"222202ATE\", \"222203ATE\", \"181301ACE\", \"181301ATD\", \n               \"181302ACE\", \"181302ATD\", \"181302ATE\", \"621901ACR\", \"621902ACR\", \n               \"505501ATE\")\n\nm60.drug &lt;- m60[GNL_NM_CD %in% code.ppi][order(MDCARE_STRT_DT, TOT_MCNT), .SD[.N], keyby = \"RN_KEY\"] %&gt;%\n  .[order(-MDCARE_STRT_DT), .SD[1], by = \"RN_INDI\"] %&gt;% .[, .SD, .SDcols = c(\"RN_INDI\", \"MDCARE_STRT_DT\")]\n\n이후 진료명세서(m20) 데이터에서, 2011년 1월 1일 이후 주상병과 첫 번째 부상병에서 K29 코드가 포함된 행을 선택하겠습니다. 그리고 한 환자에서 처음으로 진단명이 입력된 시점을 first_date(위염의 처음 진단 ; time zero)로 가정하고, 마지막으로 진단명이 입력된 시점을 recurr_date(위염이 재발한 시점 ; event 발생 시점)로 가정하겠습니다. 데이터를 m60.drug와 합쳐서, first_date 이후 처음으로 PPI가 처방된 시점을 treat_date 변수로 코딩하겠습니다(데이터 kk).\n\nkk &lt;- m20[(grepl('K29', SICK_SYM1) | grepl('K29', SICK_SYM2)) & (MDCARE_STRT_DT &gt;= 20110000), \n          .SD, .SDcols = c(\"RN_INDI\", \"MDCARE_STRT_DT\")] %&gt;% \n  .[order(MDCARE_STRT_DT), .SD, keyby=\"RN_INDI\"] %&gt;%\n  .[, .(first_date = min(MDCARE_STRT_DT, na.rm = TRUE),\n        recurr_date = ifelse(.N &gt; 1, max(MDCARE_STRT_DT, na.rm = TRUE), NA_integer_)), \n    keyby = \"RN_INDI\"]  %&gt;% \n  m60.drug[, .(RN_INDI, MDCARE_STRT_DT)][., on = \"RN_INDI\"] %&gt;% \n  .[, treat_date := ifelse(MDCARE_STRT_DT &gt; first_date & (is.na(recurr_date) | MDCARE_STRT_DT &lt; recurr_date),\n                           MDCARE_STRT_DT,\n                           NA)]\nkk$MDCARE_STRT_DT &lt;- NULL\n\n이후 분석에서 공변량으로 사용할 성별 정보를 기본정보(bnc) 데이터에서 추가하고, 날짜 변수의 class를 변환하겠습니다.\n\nkk &lt;- merge(kk, bnc[, .(SEX = SEX[1]), keyby = \"RN_INDI\"], by = \"RN_INDI\")\n\nkk[, `:=`(recurr_date = as.Date(as.character(recurr_date), format = \"%Y%m%d\"),\n          treat_date = as.Date(as.character(treat_date), format = \"%Y%m%d\"),\n          first_date = as.Date(as.character(first_date), format = \"%Y%m%d\")\n                                )]\n\n이제 PPI 처방 여부(treatment ; 0,1)와 위염 재발 여부(recurr ; 0,1)를 새로운 변수로 만들고, 사망정보(bnd) 데이터에서 관찰기간 내 사망 여부와 사망 날짜를 확인하겠습니다. 사망했다면 사망일을, 그렇지 않다면 2015-12-31을 Obs_day로 코딩하고, 위염의 진단부터 재발까지(event가 발생한)의 시간을 gastritis_day로 코딩합니다. 마지막으로, Obs_day와 gastritis_day 중 먼저 온 것 ; event가 발생한 시점 또는 관찰이 종료된 시점을 FU_time으로 코딩합니다.\n\nkk[, `:=` (treatment = ifelse(is.na(treat_date), 0, 1),\n           recurr = ifelse(is.na(recurr_date), 0, 1))]\n\nkk.death &lt;- bnd[, .(RN_INDI, Deathdate, BTH_YYYY)][kk, on=\"RN_INDI\", ] %&gt;% \n  .[, `:=`(Age = year(first_date) - as.integer(substr(BTH_YYYY, 1, 4)),\n           death = as.integer(!is.na(Deathdate)),\n           Obs_day = as.integer(pmin(Deathdate, as.Date(\"2015-12-31\"), na.rm=T)-first_date),\n           treat_day = as.integer(treat_date-first_date))]\n\nkk.death[, `:=` (gastritis_day = as.integer(recurr_date - first_date))]\nkk.death[, FU_day := pmin(Obs_day, gastritis_day, na.rm = T)]\n\nori &lt;- kk.death[, .(RN_INDI, treatment, treat_day, recurr, Age, SEX, FU_day)]\n\n이제 분석을 위한 데이터(ori)가 구성되었습니다. 실제 연구라면 이전에 위염을 진단받은 환자를 배제하거나, treat_date로부터 특정 시간 이내에 PPI 처방 이력이 있는 환자를 제외하는 등의 다양한 조건을 설정할 수 있겠습니다.\n\ndatatable(ori, rownames = F, caption = \"Original data\", options = list(scrollX = T))"
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method-적용",
    "href": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method-적용",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Clone-Censor-Weight method 적용",
    "text": "Clone-Censor-Weight method 적용\nClone-Censor-Weight method를 이해하기 위해 다음 그림을 설명하겠습니다.\n\n\n\n\n\nFigure 2: Clone-Censor-Weight model\n\n\n그림에서 세모 표시는 치료를, X 표시는 event 발생을, 괄호 표시는 censor(follow up loss 등)을 나타냅니다.  일반적으로 Clone-Censor-Weight design에서는 ‘grace time’을 설정하는데, 실제로 치료를 받은 대상자라고 하더라도 grace time(그림에서는 180일)이 경과된 시점에 치료를 받았다면 치료를 받지 않았다고 간주하는 것입니다. 즉, 치료군에 속하기 위해서는 추적관찰 시작 시점으로부터 grace time 이내에 치료를 받아야 합니다. 이 정의에 따르면 그림의 A~E는 치료를 받은 대상자이지만, F~H는 치료를 받지 않은 대상자가 됩니다. \n\nA~E는 Grace time 이전에 치료를 받았으므로 치료군이 됩니다. A~E의 clone은 비교군에 할당될 것이고, 비교군의 입장에서는 치료를 받은 시점에 protocol violation이 발생한 것이므로 artificial censoring됩니다.\nF~H는 Grace time 이후에 치료를 받았으므로 비교군이 됩니다. 이 때, 치료군에 할당된 F~H의 clone은 grace time인 180일에 artificial censoring됩니다.\nI, J, M은 치료 자체를 받지 않았으므로 비교군이 되고, 이들의 clone은 grace time인 180일에 artificial censoring 됩니다.\nK와 L은 치료를 받지 않았으면서 grace time 이전에 follow up이 종료되었습니다. 이런 경우에는 clone에서 artificial censoring이 일어나지 않습니다.\n\n대상자의 clone에서 protocol violation에 의해 발생한 censoring은 실제 RCT에서 일어나지 않는 Artificial censoring입니다(일반적인 생존분석에서 follow up period 내에 event가 발생하지 않는 censoring과 다릅니다). Artificial censoring은 관찰 시간에 따라 발생 확률이 달라지고, 대상자의 특성(성별, 나이, 과거력 등의 공변량)에 따라서도 발생 확률이 달라집니다. Artificial censoring은 Clone이라는 연구 디자인에 의해 발생한 것이고, 만일 특정 대상자의 clone이 어떤 구간에서 artificial censoring될 확률이 높다면 실제로는 censoring 이후에 event가 발생할 수 있음에도 불구하고 그 event를 관찰할 수 없게 됩니다. 따라서 시간에 따라 artificial censoring이 되지 않고 관찰대상으로 남아 있을 확률을 공변량을 보정하여 구하고, 그 역수를 가중치로 부여합니다(Inverse Probability of Censoring Weighting).  이렇게 하면 1) 동일한 대상자에서 시간구간 (5,6)까지 censor되지 않을 확률이 0.8이라면 가중치 1.25를 부여하고, (10,12)까지 censor되지 않을 확률이 0.5라면 가중치 2.0을 부여하여 시간에 따라 censor될 확률을 동일하게 맞추고, 2) 공변량(예를 들면 성별)이 다른 대상자에서 시간구간 (5,6)까지 censor되지 않을 확률이 0.7이라면 이 확률에 대한 성별의 영향을 보정합니다. 만일 real world에서 중년 여성이 남성에 비해 위염 치료를 위해 PPI를 많이 처방받는다고 하면, CCW design에서 중년여성은 남성에 비해 control arm의 artificial censoring이 많이 될 것입니다. RCT에서는 treatment group과 control group에서 성별의 분포가 동일하므로, 이를 모방하기 위해서는 weight를 주어야 합니다. 이후 분석에는 대상자별 가중치를 고려한 생존분석을 시행합니다.\nClone\n먼저 전체 대상자를 Clone 하여 Treatment arm(PPI arm)과 Control arm을 만들고, 각 arm에서 follow up time과 outcome(event)을 입력하겠습니다.\nControl arm_follow up time, outcome(event) 설정\n최종 데이터인 ori를 control arm으로 할당합니다(arm=Control). 치료를 받은 대상자(Case1)에서는 control group의 clone이 치료를 시작한 날(treat_day)에 artificial censoring되므로, treat_day를 follow-up time(fup)으로 설정하고 outcome(위염 재발)은 발생하지 않은 것으로 설정합니다. 치료를 받지 않은 대상자(grace time 이후에 받은 대상자 포함, Case2)에서는 기존의 outcome(recurr)과 추적기간(FU_day)을 그대로 입력합니다.\n\n#Arm \"Control\": no treatment within 180days\nori_control&lt;-ori  # We create a first copy of the dataset: \"clones\" assigned to the control (no treatment) arm\nori_control$arm&lt;-\"Control\"\n\n#Case 1: Patients receive PPI within 180days (scenarii A to E)\n#they are still alive and followed-up until treatment\nori_control$outcome[ori_control$treatment==1 & ori_control$treat_day &lt;= 180] &lt;- 0\n\nori_control$fup[ori_control$treatment==1 & ori_control$treat_day &lt;= 180]&lt;-ori_control$treat_day[ori_control$treatment==1 & ori_control$treat_day &lt;= 180]\n\n#Case 2: Patients do not receive PPI within 180days (either no treatment or treatment after 6 months): \n#we keep their observed outcomes and follow-up times (scenarii F to M)\nori_control$outcome[ori_control$treatment==0  | (ori_control$treatment==1 & ori_control$treat_day &gt; 180)] &lt;- ori_control$recurr[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt; 180)]\n\nori_control$fup[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt;180)] &lt;- ori_control$FU_day[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt;180)]\n\n예를 들어 45461번 대상자는 39일째 PPI를 복용했으므로 Control arm에서 39일까지 follow up하였으며, outcome은 발생하지 않았습니다.\n\ndatatable(ori_control, rownames = F, caption = \"ori_control; fup & outcome\", options = list(scrollX = T))\n\n\n\n\n\nTreatment(PPI) arm_follow up time, outcome(event) 설정\n이번에는 ori를 treatment arm으로 할당합니다(arm=PPI). 치료군의 정의를 만족하는 대상자(Case1)에서는 기존 추적기간과 outcome을 유지합니다. Case2와 같이 치료를 받지 않고 grace time 이전에 추적이 종료된 대상자도 마찬가자입니다. Case3와 같이 180일 이후에 PPI를 복용했거나 치료 없이 180일 이상 추적관찰 된 대상자는, 대상자의 clone이 treatment arm에서 180일째 artificial censoring되므로 follow-up time(fup)을 180일로 설정하고 outcome은 발생하지 않은 것으로 설정합니다\n\n#Arm \"ppi\": Treatment within 180days\nori_ppi&lt;-ori # We create a second copy of the dataset: \"clones\" assigned to the PPI arm\nori_ppi$arm&lt;-\"PPI\"\n\n#Case 1: Patients receive PPI within 180 days : \n#we keep their observed outcomes and follow-up times\n\nori_ppi$outcome[ori_ppi$treatment==1\n                    & ori_ppi$treat_day &lt;=180]&lt;-ori_ppi$recurr[ori_ppi$treatment==1\n                                                                             & ori_ppi$treat_day &lt;=180]\n\nori_ppi$fup[ori_ppi$treatment==1\n                & ori_ppi$treat_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==1\n                                                                           & ori_ppi$treat_day &lt;=180]\n\n#Case 2: Patients die or are lost to follow-up before 180days without recieving treatment: \n#we keep their observed outcomes and follow-up times (scenarii K and L)\n\nori_ppi$outcome[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$recurr[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\nori_ppi$fup[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\n#Case 3: Patients do not receive PPI within 180days and are still alive or \n#at risk at 6 months (scenarii F-J and M)\n# they are considered alived and their follow-up time is 6 months\n\nori_ppi$outcome[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==0  & ori_ppi$treat_day &gt;180)|(ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-0\n\nori_ppi$fup[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==0 & ori_ppi$treat_day &gt;180)|(ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-180\n\n예를 들어 24053번 대상자는 PPI를 복용하지 않았고 297일간 추적관찰하였으므로, Treatment arm에 180일간 follow up 하였습니다.\n\ndatatable(ori_ppi, rownames = F, caption = \"ori_ppi; fup & outcome\", options = list(scrollX = T))\n\n\n\n\n\nControl arm_censoring status, follow-up time uncensored 설정\n이제 각 arm에 있는 대상자들이 artificial censor되는지 여부(censoring)와, censor 되기 전까지의 follow-up time(fup_uncensored)을 변수로 만들겠습니다. fup_uncensored는 최댓값을 grace time인 180일로 정합니다. Control arm의 경우 180일 이전에 PPI를 복용한 대상자만 censoring이 되고, 나머지 대상자는 censoring 되지 않습니다.\n\n#Arm \"Control\": no treatment within 6 months\n\n#Case 1: Patients receive treatment within 6 months: \n#they are censored in the control group at time of treatment (scenarii A to E)\nori_control$censoring[ori_control$treatment==1 & ori_control$treat_day &lt;=180]&lt;-1\n\nori_control$fup_uncensored[ori_control$treatment==1 & ori_control$treat_day &lt;=180]&lt;-(ori_control$treat_day[ori_control$treatment==1 & ori_control$treat_day &lt;=180])\n\n#Case 2: Patients die or are lost to follow-up before 180days : \n#we keep their follow-up time but they are uncensored (scenarii K and L)\nori_control$censoring[ori_control$treatment==0 & ori_control$FU_day &lt;=180]&lt;-0\n\nori_control$fup_uncensored[ori_control$treatment==0 & ori_control$FU_day &lt;=180]&lt;-ori_control$FU_day[ori_control$treatment==0 & ori_control$FU_day &lt;=180]\n\n#Case 3: Patients do not receive PPI within 6 months and are still alive or \n#at risk at 6 months : (scenarii F-J and M)\n# they are considered uncensored and their follow-up time is 180days\nori_control$censoring[(ori_control$treatment==0 & ori_control$FU_day &gt;180) | (ori_control$treatment==1 & ori_control$treat_day &gt;180)]&lt;-0\n\nori_control$fup_uncensored[(ori_control$treatment==0 & ori_control$FU_day &gt;180) | (ori_control$treatment==1 & ori_control$treat_day &gt;180)]&lt;- 180\n\n\ndatatable(ori_control, rownames = F, caption = \"ori_control\", options = list(scrollX = T))\n\n\n\n\n\nTreatment(PPI) arm_censoring status, follow-up time uncensored 설정\n다음으로, tretment arm에서도 동일하게 censoring status, follow-up time uncensored 변수를 만들겠습니다. Treatment arm에서는 PPI를 복용하지 않은 대상자와, grace time인 180일 이후 복용한 대상자에서 censoring이 발생합니다.\n\n#Arm \"PPI\": PPI within 6 months\n\n#Case 1: Patients receive treatment within 180days : \n# they are uncensored in the ppi arm and remain at risk of \n# censoring until time of treatment (scenarii A to E)\nori_ppi$censoring[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180]&lt;-0\n\nori_ppi$fup_uncensored[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180]&lt;-(ori_ppi$treat_day[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180])\n\n#Case 2: Patients die or are lost to follow-up before 180days : \n#we keep their follow-up times but they are uncensored (scenarii K and L)\nori_ppi$censoring[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-0\n\nori_ppi$fup_uncensored[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\n#Case 3: Patients do not receive ppi within 180days and are still alive \n#or at risk at 6 months (scenarii F-J and M): \n# they are considered censored and their follow-up time is 180days\nori_ppi$censoring[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==1 & ori_ppi$treat_day &gt;180)]&lt;-1\n\nori_ppi$fup_uncensored[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-180\n\n마지막으로, 분석을 위해 control arm과 treatment(PPI) arm을 하나의 데이터(dat)로 합치겠습니다. 합친 데이터에서는 모든 대상자가 두 명씩 존재할 것입니다.\n\ndat &lt;-rbind(ori_control, ori_ppi)\ndatatable(dat[dat$RN_INDI == 45461, ], rownames = F, caption = \"dat ; original + clone\", options = list(scrollX = T))\n\n\n\n\n\nTime-split data의 생성\n이제 데이터를 event가 발생한 시점마다 자르는(split) 작업을 하겠습니다. 혼란을 피하기 위해 데이터 이름을 tab으로 새로 생성하겠습니다. event가 발생하거나, artificial censor가 발생한 모든 시점은 fup 변수에 포함되어 있습니다. 따라서 fup 변수의 시간대별로 구간을 생성하기 위해 times 데이터 프레임을 만듭니다.\n\ntab &lt;- dat\n#Dataframe containing the time of events and an ID for the times of events\nt_events&lt;-sort(unique(tab$fup))\ntimes&lt;-data.frame(\"tevent\"=t_events,\"ID_t\"=seq(1:length(t_events)))\n\ntevent는 event가 발생한 시점이고, ID_t는 해당 시점이 전체 발생 시점들 중 몇 번째에 해당하는지를 나타냅니다.\n\ndatatable(times, rownames = F, caption = \"times ; dataframe for fup\", options = list(scrollX = T))\n\n\n\n\n\n먼저 treatment arm인 tab_s에 대해, Survival 패키지의 survSplit 함수를 이용하여 t_event에 해당하는 시점마다 구간을 나누고 시간 순으로 정렬합니다. 이렇게 만들어진 data.long은 구간별로 outcome, 즉 위염의 재발이 일어났는지를 보여줍니다. 다음으로 tab_s를 동일하게 t_event에 따라 split하되, 구간별로 outcome이 아니라 artificial censoring이 일어났는지를 보겠습니다(코드에서 event=“censoring”으로 입력합니다). data.long과 data.long.cens는 동일한 데이터(tab_s)를 동일한 시간구간(t_event)별로 나눈 것이므로, data.long.cens의 censoring status를 data.long과 합치면 구간별로 outcome 발생 여부와 censoring 여부를 확인할 수 있습니다.\n\ntab_s&lt;-tab[tab$arm==\"PPI\",]\n  \n#Creation of the entry variable (Tstart, 0 for everyone)\ntab_s$Tstart&lt;-0\n  \n#Splitting the dataset at each time of event until the event happens and sorting it\ndata.long&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"outcome\",id=\"ID\") \ndata.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  \n#Splitting the original dataset at each time of event and sorting it\n#until censoring happens. This is to have the censoring status at each time of event \ndata.long.cens&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"censoring\",id=\"ID\") \ndata.long.cens&lt;-data.long.cens[order(data.long.cens$ID,data.long.cens$fup),] \n  \n#Replacing the censoring variable in data.long by the censoring variable obtained\n# in the second split dataset\ndata.long$censoring&lt;-data.long.cens$censoring\n  \n#Creating Tstop (end of the interval) \ndata.long$Tstop&lt;-data.long$fup\n  \n#Merge and sort\ndata.long&lt;-merge(data.long,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\ndata.long&lt;-data.long[order(data.long$ID,data.long$fup),] \ndata.long$ID_t[is.na(data.long$ID_t)]&lt;-0\n\n각 구간의 끝은 fup 이므로, Tstart에 대응되는 Tstop 변수에 fup을 입력하고, 위에서 만든 times와 merge하게 되면 대상자별로 각 행이 몇 번째 구간인지를 알 수 있습니다(ID_t : 구간번호). 이 때, times는 1부터 시작하므로 tstart가 0, tstop이 1인 행은 ID_t가 NA로 출력되므로 해당 구간번호를 0으로 바꾸어줍니다. data.long을 보면, RN_INDI별로 [tstart, tstop]으로 나누어진 시간 구간이 있고, 해당 구간의 outcome과 censoring이 있습니다.\n\ndatatable(data.long, rownames = F, caption = \"data.long ; time-splitted data for control arm\", options = list(scrollX = T))\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n동일한 작업을 control arm인 tab_c에 대해서도 수행합니다.\n\ntab_c&lt;-tab[tab$arm==\"Control\",]\n  \n#Creation of the entry variable (Tstart, 0 for everyone)\ntab_c$Tstart&lt;-0\n  \n  \n#Splitting the dataset first at each time of event\n#until the event happens \ndata.long2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"outcome\",id=\"ID\") \ndata.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n  \n#Splitting the original dataset at each time of event\n#until censoring happens \ndata.long.cens2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"censoring\",id=\"ID\") \ndata.long.cens2&lt;-data.long.cens2[order(data.long.cens2$ID,data.long.cens2$fup),] \n  \n#Replacing the censoring variable in data.long by the censoring variable obtained\n# in the second split dataset\ndata.long2$censoring&lt;-data.long.cens2$censoring\n  \n#Creating Tstop (end of the interval)\ndata.long2$Tstop&lt;-data.long2$fup\n  \n#Merge and sort\ndata.long2&lt;-merge(data.long2,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\ndata.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \ndata.long2$ID_t[is.na(data.long2$ID_t)]&lt;-0\n\n예시로, RN_INDI가 45461인 대상자를 살펴보겠습니다. 이 대상자는 39일째 PPI를 처방받은 사람으로, 관찰 767일째에 위염의 재발이 발생했습니다.\n\ndatatable(dat[dat$RN_INDI == 45461, ], rownames = F, caption = \"RN_INDI=45461\", options = list(scrollX = T))\n\n\n\n\n\nTreatment arm에서 45461은 artificial censor 되지 않으며, 마지막 구간인 (766,767)에서 outcome이 발생합니다.\n\ndatatable(data.long[data.long$RN_INDI == 45461, ], rownames = F, caption = \"45461 in treatment arm\", options = list(scrollX = T))\n\n\n\n\n\nControl arm에서는 치료가 시작된 39일에 protocol violation에 의해 artificial censor되며, outcome은 발생합니다.\n\ndatatable(data.long2[data.long2$RN_INDI == 45461,], rownames = F, caption = \"45461 in control arm\", options = list(scrollX = T))\n\n\n\n\n\n이제 weight 계산을 위해 data.long과 data.long2를 하나로 합칩니다.\n\n#Final dataset\ndata&lt;-rbind(data.long,data.long2)\ndata_final&lt;-merge(data,times,by=\"ID_t\",all.x=T)\ndata_final&lt;-data_final[order(data_final$ID,data_final$fup),]\n\nCensoring weight 계산\n이제 Cox model을 이용하여 구간별로 대상자가 artificial censoring 되지 않고 관찰대상으로 남아 있을 확률을 구하겠습니다. 확인하고자 하는 것은 censoring이므로, Surv 함수 안에 들어갈 event는 outcome이 아니라 censoring입니다. 보정할 covariate는 대상자별 연령(Age)과 성별(SEX)입니다. 동일한 대상자가 treatment arm에 있을 때와 control arm에 있을 때 censoring되는 여부가 다르고, raw data(ori)에서 치료를 받은 군과 받지 않은 군의 분포가 다르므로 weight는 각각의 arm에서 따로 구해야 합니다.  Treatment arm부터 확률을 구해봅니다.\n\ndata.long&lt;-data_final[data_final$arm==\"PPI\",]\n# Cox model\nms_cens&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long)\n\n데이터에서 공변량인 Age와 SEX를 추출한 행렬(design_mat)을 만들고, Cox model인 ms_cens의 회귀계수를 beta에 저장합니다. design_mat과 beta를 곱하여 lin_pred을 구합니다.\n\n#Design matrix\ndesign_mat&lt;-as.matrix(data.long[,c(\"Age\",\"SEX\")])\n#Vector of regression coefficients\nbeta&lt;-coef(ms_cens)\n  \n#Calculation of XB (linear combineation of the covariates)\ndata.long$lin_pred&lt;-design_mat%*%beta\n  \n#Estimating the cumulative hazard (when covariates=0)\ndat.base&lt;-data.frame(basehaz(ms_cens,centered=F))\nnames(dat.base)&lt;-c(\"hazard\",\"t\")\ndat.base&lt;-unique(merge(dat.base,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n\ndat.base에는 covariate(Age, SEX)가 모두 0일 때, 시간에 따른 hazard가 저장됩니다.\n\ndatatable(dat.base, rownames = F, caption = \"baseline hazard\", options = list(scrollX = T))\n\n\n\n\n\ndata.long에 시간구간에 따른 hazard를 병합하고, 대상자의 covariate 정보가 포함된 lin_pred과의 연산을 통해 해당 구간에 artificial censor되지 않고 남아 있을 확률인 P_uncens를 구합니다. 이후 outcome에 대한 Cox regression을 할 때 사용될 weight는 P_uncens의 역수입니다.\n\n#Merging and reordering the dataset\ndata.long&lt;-merge(data.long,dat.base,by=\"ID_t\",all.x=T)\ndata.long&lt;-data.long[order(data.long$RN_INDI,data.long$fup),]\ndata.long$hazard&lt;-ifelse(is.na(data.long$hazard),0,data.long$hazard)\n  \n#Estimating the probability of remaining uncensored at each time of event\ndata.long$P_uncens&lt;-exp(-(data.long$hazard)*exp(data.long$lin_pred))  \n  \n#Weights are the inverse of the probability of remaining uncensored\ndata.long$weight_Cox&lt;-1/data.long$P_uncens\n\n동일한 과정을 통해 Control arm에 대해서도 weight를 구합니다.\n\ndata.long2&lt;-data_final[data_final$arm==\"Control\",]\n  \n#Cox model\nms_cens2&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long2)\n  \n#Design matrix\ndesign_mat2&lt;-as.matrix(data.long2[,c(\"Age\",\"SEX\")])\n#Vector of regression coefficients\nbeta2&lt;-coef(ms_cens2)\n  \n#Calculation of XB (linear combineation of the covariates)\ndata.long2$lin_pred&lt;-design_mat2%*%beta2\n  \n#Estimating the cumulative hazard (when covariates=0)\ndat.base2&lt;-data.frame(basehaz(ms_cens2,centered=F))\nnames(dat.base2)&lt;-c(\"hazard\",\"t\")\n  \ndat.base2&lt;-unique(merge(dat.base2,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n#Merging and reordering the dataset\ndata.long2&lt;-merge(data.long2,dat.base2,by=\"ID_t\",all.x=T)\ndata.long2&lt;-data.long2[order(data.long2$RN_INDI,data.long2$fup),]\ndata.long2$hazard&lt;-ifelse(is.na(data.long2$hazard),0,data.long2$hazard)\n  \n#Estimating the probability of remaining uncensored at each time of event\ndata.long2$P_uncens&lt;-exp(-(data.long2$hazard)*exp(data.long2$lin_pred))\n  \n#Weights are the inverse of the probability of remaining uncensored\ndata.long2$weight_Cox&lt;-1/data.long2$P_uncens\ndata.long2$weight_Cox[data.long2$ID_t==0]&lt;-1\n\n이제 data.long과 data.long2에 대상자별, 시간 구간별로 weight_Cox 변수가 생성되었습니다.\n예를 들어, 아까 살펴본 RN_INDI가 45461인 대상자를 보겠습니다. Control arm에 할당된 이 대상자의 clone의 시간에 따른 분석 가중치 ; 해당 구간에 censor지 않고 남아 있을 확률의 역수는 아래와 같습니다.\n\ndatatable(data.long2[data.long2$RN_INDI == 45461, c(\"RN_INDI\", \"Tstart\", \"Tstop\", \"ID_t\", \"weight_Cox\")], rownames = F, caption = \"Weight for Cox model\", options = list(scrollX = T))\n\n\n\n\n\n45461 대상자의 경우에는 control arm에서만 artificial censor가 되고, treatment arm에서는 censor가 되지 않습니다. 따라서 두 arm을 합쳐서 weight를 구하게 되면 별도로 구한 결과와 비교하여 다음과 같은 차이가 발생합니다.\n\n\n\n\n\n\n\n\nCox model을 이용한 분석\n이제 가중치가 계산된 최종 데이터(data.long.Cox)를 가지고, outcome(위염 재발)에 대한 생존분석을 시행하겠습니다. 따라서 Surv 함수에 들어갈 event는 censoring이 아니라 outcome이 됩니다. Cox model에 weights가 반영됩니다.  비례위험가정이 위배될 가능성이 있어, 먼저 Kaplan-Meier cuve의 RMST(Restricted Mean Survival Time)를 이용하여 두 arm 사이의 1년 발생율을 비교해보겠습니다.\n\ndata.long.Cox&lt;-rbind(data.long,data.long2)\n\nemul_Cox_s &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1, data=data.long.Cox[data.long.Cox$arm==\"PPI\",],weights = weight_Cox)\nS1 &lt;- summary(emul_Cox_s, times = 365)$surv\nfit.tableM &lt;- summary(emul_Cox_s, rmean=365)$table\nRMST1 &lt;- fit.tableM[\"rmean\"] # Estimated RMST in the treatment arm\n  \nemul_Cox_c &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1, data=data.long.Cox[data.long.Cox$arm==\"Control\",],weights = weight_Cox)\nS0 &lt;- summary(emul_Cox_c, times = 365)$surv\nfit.tableM2 &lt;- summary(emul_Cox_c, rmean=365)$table\nRMST0 &lt;- fit.tableM2[\"rmean\"] # Estimated RMST in the control arm\n\nDiff_surv&lt;-S1-S0 #Difference in 1 year survival\nDiff_RMST&lt;-RMST1-RMST0 #Difference in RMST\n\nDiff_surv\n\n[1] -0.03980216\n\nDiff_RMST\n\n    rmean \n-8.770445 \n\n\n마지막으로, Cox model을 이용한 Hazard ratio를 구해보겠습니다.\n\nCox_w &lt;- coxph(Surv(Tstart,Tstop, outcome) ~ arm, data=data.long.Cox, weights=weight_Cox)\nHR&lt;-exp(Cox_w$coefficients)\nsummary(Cox_w)\n\nCall:\ncoxph(formula = Surv(Tstart, Tstop, outcome) ~ arm, data = data.long.Cox, \n    weights = weight_Cox)\n\n  n= 283192, number of events= 669 \n\n          coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)\narmPPI 0.27392   1.31510  0.05942   0.20553 1.333    0.183\n\n       exp(coef) exp(-coef) lower .95 upper .95\narmPPI     1.315     0.7604     0.879     1.967\n\nConcordance= 0.54  (se = 0.029 )\nLikelihood ratio test= 21.19  on 1 df,   p=4e-06\nWald test            = 1.78  on 1 df,   p=0.2\nScore (logrank) test = 21.38  on 1 df,   p=4e-06,   Robust = 1.36  p=0.2\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\nHR\n\n  armPPI \n1.315104 \n\n\nHazard ratio가 1.31(95% CI 0.879-1.967, p-value 0.183)으로 계산됩니다.\nControl arm과 treatment arm은 서로 독립적인 데이터가 아니므로, 일반적인 confidence interval을 신뢰하기 어려워 censor-weight 과정을 다음과 같이 bootstrap으로 시행할 수 있습니다. 코드의 실행 속도를 빠르게 하기 위해 boot 함수에서 CPU의 개수를 지정하여 multicore 연산을 시행할 수 있습니다.\n\nfboot &lt;- function(dat, indices) {\n  t&lt;-dat[dat$arm==\"Control\",]\n  t1&lt;-dat[dat$arm==\"PPI\",]\n  tab0 &lt;- t[indices,] # allows boot to select sample\n  select&lt;-tab0$RN_INDI\n  tab1&lt;-t1[t1$RN_INDI %in% select,] \n  tab&lt;-rbind(tab0,tab1)\n  \n  t_events&lt;-sort(unique(tab$fup))\n  times&lt;-data.frame(\"tevent\"=t_events,\"ID_t\"=seq(1:length(t_events)))\n  \n  tab_s&lt;-tab[tab$arm==\"PPI\",]\n  tab_s$Tstart&lt;-0\n  \n  data.long&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", \n                       start=\"Tstart\", event=\"outcome\",id=\"ID\") \n  data.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  \n  data.long.cens&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", \n                            start=\"Tstart\", event=\"censoring\",id=\"ID\") \n  data.long.cens&lt;-data.long.cens[order(data.long.cens$ID,data.long.cens$fup),] \n\n  data.long$censoring&lt;-data.long.cens$censoring\n  \n  data.long$Tstop&lt;-data.long$fup\n  \n  data.long&lt;-merge(data.long,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\n  data.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  data.long$ID_t[is.na(data.long$ID_t)]&lt;-0\n  \n  tab_c&lt;-tab[tab$arm==\"Control\",]\n  tab_c$Tstart&lt;-0\n  \n  data.long2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", \n                        start=\"Tstart\", event=\"outcome\",id=\"ID\") \n  data.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n\n  data.long.cens2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", \n                             start=\"Tstart\", event=\"censoring\",id=\"ID\") \n  data.long.cens2&lt;-data.long.cens2[order(data.long.cens2$ID,data.long.cens2$fup),] \n  \n  data.long2$censoring&lt;-data.long.cens2$censoring\n  data.long2$Tstop&lt;-data.long2$fup\n\n  data.long2&lt;-merge(data.long2,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\n  data.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n  data.long2$ID_t[is.na(data.long2$ID_t)]&lt;-0\n  \n  data&lt;-rbind(data.long,data.long2)\n  data_final&lt;-merge(data,times,by=\"ID_t\",all.x=T)\n  data_final&lt;-data_final[order(data_final$ID,data_final$fup),]\n  \n  data.long&lt;-data_final[data_final$arm==\"PPI\",]\n  \n  ms_cens&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long) \n  design_mat&lt;-as.matrix(data.long[,c(\"Age\",\"SEX\")])\n\n  beta&lt;-coef(ms_cens)\n  \n  data.long$lin_pred&lt;-design_mat%*%beta\n  \n  dat.base&lt;-data.frame(basehaz(ms_cens,centered=F))\n  names(dat.base)&lt;-c(\"hazard\",\"t\")\n  dat.base&lt;-unique(merge(dat.base,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n  data.long&lt;-merge(data.long,dat.base,by=\"ID_t\",all.x=T)\n  data.long&lt;-data.long[order(data.long$RN_INDI,data.long$fup),]\n  data.long$hazard&lt;-ifelse(is.na(data.long$hazard),0,data.long$hazard)\n  \n  data.long$P_uncens&lt;-exp(-(data.long$hazard)*exp(data.long$lin_pred))  \n\n  data.long$weight_Cox&lt;-1/data.long$P_uncens\n\n  data.long2&lt;-data_final[data_final$arm==\"Control\",]\n  \n  ms_cens2&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long2)\n  summary(ms_cens2)\n  \n  design_mat2&lt;-as.matrix(data.long2[,c(\"Age\",\"SEX\")])\n\n  beta2&lt;-coef(ms_cens2)\n  \n  data.long2$lin_pred&lt;-design_mat2%*%beta2\n  \n  dat.base2&lt;-data.frame(basehaz(ms_cens2,centered=F))\n  names(dat.base2)&lt;-c(\"hazard\",\"t\")\n  \n  \n  dat.base2&lt;-unique(merge(dat.base2,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n  data.long2&lt;-merge(data.long2,dat.base2,by=\"ID_t\",all.x=T)\n  data.long2&lt;-data.long2[order(data.long2$RN_INDI,data.long2$fup),]\n  data.long2$hazard&lt;-ifelse(is.na(data.long2$hazard),0,data.long2$hazard)\n  \n  data.long2$P_uncens&lt;-exp(-(data.long2$hazard)*exp(data.long2$lin_pred))\n  \n  data.long2$weight_Cox&lt;-1/data.long2$P_uncens\n  data.long2$weight_Cox[data.long2$ID_t==0]&lt;-1\n  \n  data.long.Cox&lt;-rbind(data.long,data.long2)\n  \n  emul_Cox_s &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1,\n                        data=data.long.Cox[data.long.Cox$arm==\"PPI\",],weights = weight_Cox)\n  S1&lt;- summary(emul_Cox_s, times = 365)$surv\n  fit.tableM &lt;- summary(emul_Cox_s, rmean=365)$table\n  RMST1 &lt;- fit.tableM[\"rmean\"]\n  \n  emul_Cox_c &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1,\n                        data=data.long.Cox[data.long.Cox$arm==\"Control\",],weights = weight_Cox)\n  S0&lt;- summary(emul_Cox_c, times = 365)$surv\n  fit.tableM2 &lt;- summary(emul_Cox_c, rmean=365)$table\n  RMST0 &lt;- fit.tableM2[\"rmean\"] \n  \n  Diff_surv&lt;-S1-S0 \n  Diff_RMST&lt;-RMST1-RMST0 \n  \n  Cox_w &lt;- coxph(Surv(Tstart,Tstop, outcome) ~ arm,\n                 data=data.long.Cox, weights=weight_Cox)\n  HR&lt;-exp(Cox_w$coefficients) \n  \n  res&lt;-c(Diff_surv,Diff_RMST,HR)\n  \n  return(res)\n  \n}\n\nresults &lt;- boot(data=dat, statistic=fboot, R=100, parallel = \"multicore\", ncpus = 40)\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = dat, statistic = fboot, R = 100, parallel = \"multicore\", \n    ncpus = 40)\n\n\nBootstrap Statistics :\n       original       bias    std. error\nt1* -0.03980216 -0.003960884  0.05664037\nt2* -8.77044466 -0.527749924  8.65111431\nt3*  1.31510388  0.026720376  0.25511206\n\n# 95% confidence intervals for each measure\nboot.ci(results,type=\"norm\",index=1) #Difference in survival\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 1)\n\nIntervals : \nLevel      Normal        \n95%   (-0.1469,  0.0752 )  \nCalculations and Intervals on Original Scale\n\nboot.ci(results,type=\"norm\",index=2) #Difference in RMST\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 2)\n\nIntervals : \nLevel      Normal        \n95%   (-25.199,   8.713 )  \nCalculations and Intervals on Original Scale\n\nboot.ci(results,type=\"norm\",index=3) #HR\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 3)\n\nIntervals : \nLevel      Normal        \n95%   ( 0.788,  1.788 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#conclusion",
    "href": "posts/2024-10-17-TTE/index.html#conclusion",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Conclusion",
    "text": "Conclusion\nClone-censor-weight method는 Eligibility가 있다고 판단된 모든 환자를 clone하여 치료군과 비교군에 할당하므로 두 군 간의 비교성을 극대화할 수 있다는 장점이 있습니다. 따라서 RCT를 시행하기 어려운 환자군 또는 질환군에 대한 연구에 있어 RWD 바탕으로 RCT를 모방할 수 있습니다.  또한 immortal time biase에 대한 보정이 가능하며, censoring weight를 계산할 때 여러 공변량을 고려할 수 있어 연구대상자가 특정 시점에 특정 치료를 시행할지 여부를 반영할 수 있다는 장점이 있습니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#reference",
    "href": "posts/2024-10-17-TTE/index.html#reference",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Reference",
    "text": "Reference\nFigure 1 Jiannong Liu, Eric D. Weinhandl, David T. Gilbertson, Allan J. Collins, Wendy L. St Peter, Issues regarding ‘immortal time’ in the analysis of the treatment effects in observational studies, Kidney International, Volume 81, Issue 4, 2012, Pages 341-350.\nFigure 2 Maringe, C., Benitez Majano, S., Exarchakou, A., Smith, M., Rachet, B., Belot, A., & Leyrat, C. (2020). Reflection on modern methods: trial emulation in the presence of immortal-time bias. Assessing the benefit of major surgery for elderly lung cancer patients using observational data. Int J Epidemiol, 49(5), 1719-1729.\n[3] Chen, A., Ju, C., Mackenzie, I. S., MacDonald, T. M., Struthers, A. D., Wei, L., & Man, K. K. C. (2023). Impact of beta-blockers on mortality and cardiovascular disease outcomes in patients with obstructive sleep apnoea: a population-based cohort study in target trial emulation framework. Lancet Reg Health Eur, 33, 100715.\n\n실습데이터가 필요하신 분께서는 Reply 달아주시면 데이터를 보내드리겠습니다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html",
    "href": "posts/2024-11-20-DeLongsMethod/index.html",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "우리는 ROC 곡선에서 AUC 값을 구할 수 있다. AUC 값이 1에 가까우면 모델의 성능이 좋다라고 하며, 0.5에 가까워질 수록 성능이 나쁘다- 라고 한다. 그렇다면, ROC에서 신뢰구간은 어떻게 구할까? 특정 AUC값과 비교하여 p value를 구하려면 어떻게 해야 할까?\n이를 위해 DeLong의 AUC 표준 오차 구하는 방법을 소개하려 한다. (E. DeLong, 1988)\n\n\n\nROC 곡선은 이진 분류 문제에서 모델의 성능을 평가하는데 사용되는 시각적 도구이다.\n다양한 임계값에서의 민감도와 False Positive Rate의 관계를 시각화한 것으로,\n\n민감도(Sensitivity)를 Y축으로, 1 - 특이도(False Positive Rate)을 X축으로 하여 관계를 그린 그래프이다.\n\n민감도(Sensitivity, True Positive Rate)\n\n실제 양성 사례를 얼마나 잘 분류하는지/ 실제 양성인 샘플을 양성으로 올바르게 분류한 비율 \\[\nSens = \\frac{True Positives(TP)}{True Positives(TP) + False Negatives(FN)}\n\\]\n\n\n특이도(Specificity, True Negative Rate)\n\n실제 음성 사례를 얼마나 잘 분류하는지/ 실제 음성인 샘플을 음성으로 올바르게 분류한 비율\n\nFalse Positive Rate(FPR) : 1-Spec, 실제 음성인 샘플을 잘못 양성으로 분류한 비율\n\n\\[\nSpec = \\frac{True Negatives(TN)}{True\\ Negatives(TN) + False \\ Positives(FP)}\n\\]\n이상적인 ROC 곡선\n\n이상적인 분류는 ROC가 (0,1)을 지날 때, 즉 FPR이 0이고, TPR이 1인 경우.\n무작위 분류는 ROC가 y=x일 때, TPR = FPR인 경우.\n\nAUC(Area Under the Curve)\n\n\nAUC는 ROC 곡선 아래 면적을 의미한다.\n\nAUC가 1에 가까울수록 성능이 좋은 모델. 0.5에 가까울수록 성능이 안좋은 모델.\n\n\n\n\n\n\nFigure 1: ROC\n\n\n\n\n아래와 같이 ROC Curve를 추정할 수 있다.\ntest를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자.\n\n\\[\n\\text{for any real number z,} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\]\n\\[\nSens(z) = \\frac{1}{m}\\sum_{i=1}^{m}{I(X_i \\geq z)},\n\\]\n\\[\nSpec(z) = \\frac{1}{n}\\sum_{j=1}^{n}{I(Y_i &lt; z)},\n\\]\n\\[\nI(A) =\n\\begin{cases}\n1, &  A \\text{ is true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n이때 실수 z가 variable 내 가능한 값들 내에서 움직인다면, ROC curve는 [1 - spec(z)]를 X로, Sens(z)를 Y로 갖는 plot이라 할 수 있다. 만약 z가 가능한 최대값보다 크다면 curve는 (0,0)을, 최솟값보다 작다면(1,1)을 지날 것이다. Sens(z) = 1 - Spec(z)라면 y=x 위, 45도 선 위에 놓일 것이다.\n\n\n\n\n\n\nFigure 2: AUC\n\n\n\n\n\nROC curve는 위와 같이 구할 수 있다. 그럼 이의 넓이: AUC 값은 어떻게 구할까? 보통, 곡선 아래 넓이는 trapezoidal rule을 통해 구한다. 고등학교 때 배운 적분을 떠올리면 된다. 수많은 사다리꼴로 쪼개어 넓이를 근사하던 기억을 되살려 보자.\n\n\n\n\n\n\nFigure 3: Trapezes\n\n\n여기서, Mann-Whitney two sample statistic에 따르면, ROC curve 아래 넓이를 구할때, trapezodial rule로 구한 넓이는 Mann-Whitney two sample statistic으로 구한 넓이로 대체할 수 있다.\n\n\nMann-Whitney statistic는 확률\\(\\theta\\)를 예측한다. \\(C_2\\)에서 무작위 추출한 값이 \\(C_1\\)의 값보다 같거나 작을 확률을 추정한다. (test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자)\n\n\n\\[\n\\hat{\\theta} = \\frac{1}{mn}\\sum_{j=1}^{n}\\sum_{i=1}^{m}{\\psi(X_i, Y_j)}\n\\]\n\\[\n\\psi(X,Y) = \\begin{cases}\n1 &  \\ Y &lt; X \\\\\n1/2 & \\  Y=X \\\\\n0 & \\ Y&gt;X\n\\end{cases}\n\\]\n\n모든 (X, Y) 쌍에 대해 X &gt; Y이면 1을, X = Y이면 \\(\\frac{1}{2}\\), X &lt; Y 이면 0을 부여하여 확률을 구한다.\n직관적으로, ROC curve와 AUC 값은 곧 모델의 추정이 옳을 확률이며, 이의 성능의 최고값은 1, 최저값은 1/2이라는 점을 고민하면 위의 추정은 그럴듯 하다.\n\n\\[\nE(\\hat{\\theta}) = Pr(X&gt;Y) + \\frac{1}{2}Pr(X= Y)\n\\]\n\n그럼, 확률(AUC)은 위와 같이 정리할 수 있다.\n\n이제, AUC값의 신뢰성을 측정하기 위해서는 SE(standard Error)를 계산하는 것이 필요하다. 이는 DeLong(1988)이 제시한 방법을 참고하자.\nξ를 각각의 집단 간 공분산이라 하자.\nξ₁₀은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j, Y_k\\)간 공분산,\nξ₀₁은 \\(C_2\\)의 \\(Y_j\\)와 \\(C_1\\)의 \\(X_i, Y_k\\)간 공분산,\nξ₁₁은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j\\)간 자기공분산이다.\nξ₁₀, ξ₀₁, ξ₁₁의 기대값은 아래와 같다.\n\\[\nξ_{10} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_k)]-\\theta^2,\n\\]\n\\[\nξ_{01} = E[\\psi(X_i, Y_j) \\psi(X_k,Y_j)]-\\theta^2,\n\\]\n\\[\nξ_{11} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_j)]-\\theta^2,\n\\]\n기댓값들을 이용하여 AUC 추정치의 분산을 계산할 수 있다.\n\\[\nvar(\\hat{\\theta}) = \\frac{(n-1)ξ_{10}+(m-1)ξ_{01}}{mn} + \\frac{ξ_{11}}{mn}\n\\]\n이와 같은 방법으로 단일 표본 집합에서의 AUC의 표준 오차를 구할 수 있다.\n\n이제, 단일 표본 집합이 아닌 다른 표본집합 r과 s에 대해 다뤄보자. 여러 표본 집합이 있을 경우, 각 표본 간의 상호 공분산 또한 고려되어야한다. 두 표본 집합 r과 s에 대해 AUC의 공분산 계산은 아래와 같다.\n\n\\[\nξ_{10}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{01}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{11}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n이제, 아래 식을 통해 표본 집합의 AUC 값 간의 공분산을 계산할 수 있다.\n\\[\ncov(\\hat{\\theta^r},\\hat{\\theta^s}) = \\frac{(n-1)ξ_{10}^{rs}+(m-1)ξ_{01}^{rs}}{mn} + \\frac{ξ_{11}^{rs}}{mn}\n\\] 이 수식을 통해 여러 표본 집합 간의 공분산을 반영하여 AUC의 표준 오차를 더 정확하게 추정할 수 있다.\n\n이를 바탕으로 우리가 궁금한 값 “표준 오차”에 접근해 보자. (Hoeffding_1948, Bamber_1975, Sen_1960)과 같은 분들 덕분에, 우리는 AUC 표준 오차를 보다 정확히 추정할 수 있다.\n\n\\[\nV^r_{10}(X_i) = \\frac{1}{n} \\sum_{j=1}^n\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (i= 1,2,...,m)\n\\]\n\\[\nV^r_{01}(Y_j) = \\frac{1}{m} \\sum_{i=1}^m\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (j= 1,2,...,m)\n\\]\n\n\n\\(V^r_{10}(X_i)\\)은 집합r에서 값을 기반으로 한 분산이다.\n\n\\(\\psi(X_i^r,Y_j^r)\\)는 \\(X_i^r,Y_j^r\\)의 관계를 나타내며, 이를 통해 분산을 구한다. \\(V^r_{01}(Y_j)\\)은 집합 \\(Y^r_j\\)에서의 분산이다.\n두 값을 통해 각 표본 집합에 대한 분산을 따로 계산한 후, 이를 결합하여 최종적으로 표준 오차를 추정할 수 있다. 각 분산 값이 표본 크기에 따라 가중평균되며, AUC 표준 오차 S는 아래와 같다.\n\\[\n\\\\\nS = \\frac{1}{m}S_{10} + \\frac{1}{n}S_{01}\n\\\\\n\\]\n\n표준 오차를 통해 우리는 AUC 값이 얼마나 신뢰할 수 있는지 평가할 수 있으며, 이는 모델의 성능을 명확히 이해하는데 중요한 역할을 한다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#roc-curve-recevier-operating-characteristic-curve",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#roc-curve-recevier-operating-characteristic-curve",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "ROC 곡선은 이진 분류 문제에서 모델의 성능을 평가하는데 사용되는 시각적 도구이다.\n다양한 임계값에서의 민감도와 False Positive Rate의 관계를 시각화한 것으로,\n\n민감도(Sensitivity)를 Y축으로, 1 - 특이도(False Positive Rate)을 X축으로 하여 관계를 그린 그래프이다.\n\n민감도(Sensitivity, True Positive Rate)\n\n실제 양성 사례를 얼마나 잘 분류하는지/ 실제 양성인 샘플을 양성으로 올바르게 분류한 비율 \\[\nSens = \\frac{True Positives(TP)}{True Positives(TP) + False Negatives(FN)}\n\\]\n\n\n특이도(Specificity, True Negative Rate)\n\n실제 음성 사례를 얼마나 잘 분류하는지/ 실제 음성인 샘플을 음성으로 올바르게 분류한 비율\n\nFalse Positive Rate(FPR) : 1-Spec, 실제 음성인 샘플을 잘못 양성으로 분류한 비율\n\n\\[\nSpec = \\frac{True Negatives(TN)}{True\\ Negatives(TN) + False \\ Positives(FP)}\n\\]\n이상적인 ROC 곡선\n\n이상적인 분류는 ROC가 (0,1)을 지날 때, 즉 FPR이 0이고, TPR이 1인 경우.\n무작위 분류는 ROC가 y=x일 때, TPR = FPR인 경우.\n\nAUC(Area Under the Curve)\n\n\nAUC는 ROC 곡선 아래 면적을 의미한다.\n\nAUC가 1에 가까울수록 성능이 좋은 모델. 0.5에 가까울수록 성능이 안좋은 모델.\n\n\n\n\n\n\nFigure 1: ROC"
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#empirical-roc-curve",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#empirical-roc-curve",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "아래와 같이 ROC Curve를 추정할 수 있다.\ntest를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자.\n\n\\[\n\\text{for any real number z,} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\]\n\\[\nSens(z) = \\frac{1}{m}\\sum_{i=1}^{m}{I(X_i \\geq z)},\n\\]\n\\[\nSpec(z) = \\frac{1}{n}\\sum_{j=1}^{n}{I(Y_i &lt; z)},\n\\]\n\\[\nI(A) =\n\\begin{cases}\n1, &  A \\text{ is true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n이때 실수 z가 variable 내 가능한 값들 내에서 움직인다면, ROC curve는 [1 - spec(z)]를 X로, Sens(z)를 Y로 갖는 plot이라 할 수 있다. 만약 z가 가능한 최대값보다 크다면 curve는 (0,0)을, 최솟값보다 작다면(1,1)을 지날 것이다. Sens(z) = 1 - Spec(z)라면 y=x 위, 45도 선 위에 놓일 것이다.\n\n\n\n\n\n\nFigure 2: AUC"
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#auc",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#auc",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "ROC curve는 위와 같이 구할 수 있다. 그럼 이의 넓이: AUC 값은 어떻게 구할까? 보통, 곡선 아래 넓이는 trapezoidal rule을 통해 구한다. 고등학교 때 배운 적분을 떠올리면 된다. 수많은 사다리꼴로 쪼개어 넓이를 근사하던 기억을 되살려 보자.\n\n\n\n\n\n\nFigure 3: Trapezes\n\n\n여기서, Mann-Whitney two sample statistic에 따르면, ROC curve 아래 넓이를 구할때, trapezodial rule로 구한 넓이는 Mann-Whitney two sample statistic으로 구한 넓이로 대체할 수 있다.\n\n\nMann-Whitney statistic는 확률\\(\\theta\\)를 예측한다. \\(C_2\\)에서 무작위 추출한 값이 \\(C_1\\)의 값보다 같거나 작을 확률을 추정한다. (test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자)\n\n\n\\[\n\\hat{\\theta} = \\frac{1}{mn}\\sum_{j=1}^{n}\\sum_{i=1}^{m}{\\psi(X_i, Y_j)}\n\\]\n\\[\n\\psi(X,Y) = \\begin{cases}\n1 &  \\ Y &lt; X \\\\\n1/2 & \\  Y=X \\\\\n0 & \\ Y&gt;X\n\\end{cases}\n\\]\n\n모든 (X, Y) 쌍에 대해 X &gt; Y이면 1을, X = Y이면 \\(\\frac{1}{2}\\), X &lt; Y 이면 0을 부여하여 확률을 구한다.\n직관적으로, ROC curve와 AUC 값은 곧 모델의 추정이 옳을 확률이며, 이의 성능의 최고값은 1, 최저값은 1/2이라는 점을 고민하면 위의 추정은 그럴듯 하다.\n\n\\[\nE(\\hat{\\theta}) = Pr(X&gt;Y) + \\frac{1}{2}Pr(X= Y)\n\\]\n\n그럼, 확률(AUC)은 위와 같이 정리할 수 있다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#standard-error",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#standard-error",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "이제, AUC값의 신뢰성을 측정하기 위해서는 SE(standard Error)를 계산하는 것이 필요하다. 이는 DeLong(1988)이 제시한 방법을 참고하자.\nξ를 각각의 집단 간 공분산이라 하자.\nξ₁₀은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j, Y_k\\)간 공분산,\nξ₀₁은 \\(C_2\\)의 \\(Y_j\\)와 \\(C_1\\)의 \\(X_i, Y_k\\)간 공분산,\nξ₁₁은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j\\)간 자기공분산이다.\nξ₁₀, ξ₀₁, ξ₁₁의 기대값은 아래와 같다.\n\\[\nξ_{10} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_k)]-\\theta^2,\n\\]\n\\[\nξ_{01} = E[\\psi(X_i, Y_j) \\psi(X_k,Y_j)]-\\theta^2,\n\\]\n\\[\nξ_{11} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_j)]-\\theta^2,\n\\]\n기댓값들을 이용하여 AUC 추정치의 분산을 계산할 수 있다.\n\\[\nvar(\\hat{\\theta}) = \\frac{(n-1)ξ_{10}+(m-1)ξ_{01}}{mn} + \\frac{ξ_{11}}{mn}\n\\]\n이와 같은 방법으로 단일 표본 집합에서의 AUC의 표준 오차를 구할 수 있다.\n\n이제, 단일 표본 집합이 아닌 다른 표본집합 r과 s에 대해 다뤄보자. 여러 표본 집합이 있을 경우, 각 표본 간의 상호 공분산 또한 고려되어야한다. 두 표본 집합 r과 s에 대해 AUC의 공분산 계산은 아래와 같다.\n\n\\[\nξ_{10}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{01}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{11}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n이제, 아래 식을 통해 표본 집합의 AUC 값 간의 공분산을 계산할 수 있다.\n\\[\ncov(\\hat{\\theta^r},\\hat{\\theta^s}) = \\frac{(n-1)ξ_{10}^{rs}+(m-1)ξ_{01}^{rs}}{mn} + \\frac{ξ_{11}^{rs}}{mn}\n\\] 이 수식을 통해 여러 표본 집합 간의 공분산을 반영하여 AUC의 표준 오차를 더 정확하게 추정할 수 있다.\n\n이를 바탕으로 우리가 궁금한 값 “표준 오차”에 접근해 보자. (Hoeffding_1948, Bamber_1975, Sen_1960)과 같은 분들 덕분에, 우리는 AUC 표준 오차를 보다 정확히 추정할 수 있다.\n\n\\[\nV^r_{10}(X_i) = \\frac{1}{n} \\sum_{j=1}^n\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (i= 1,2,...,m)\n\\]\n\\[\nV^r_{01}(Y_j) = \\frac{1}{m} \\sum_{i=1}^m\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (j= 1,2,...,m)\n\\]\n\n\n\\(V^r_{10}(X_i)\\)은 집합r에서 값을 기반으로 한 분산이다.\n\n\\(\\psi(X_i^r,Y_j^r)\\)는 \\(X_i^r,Y_j^r\\)의 관계를 나타내며, 이를 통해 분산을 구한다. \\(V^r_{01}(Y_j)\\)은 집합 \\(Y^r_j\\)에서의 분산이다.\n두 값을 통해 각 표본 집합에 대한 분산을 따로 계산한 후, 이를 결합하여 최종적으로 표준 오차를 추정할 수 있다. 각 분산 값이 표본 크기에 따라 가중평균되며, AUC 표준 오차 S는 아래와 같다.\n\\[\n\\\\\nS = \\frac{1}{m}S_{10} + \\frac{1}{n}S_{01}\n\\\\\n\\]\n\n표준 오차를 통해 우리는 AUC 값이 얼마나 신뢰할 수 있는지 평가할 수 있으며, 이는 모델의 성능을 명확히 이해하는데 중요한 역할을 한다."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html",
    "href": "posts/2024-12-13-rollmerge/index.html",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "data.table은 대용량의 데이터를 처리하는 데 있어 빠른 속도와 메모리 효율을 보여주는 패키지이다. 또한 dplyr와 비교하여, dplyr에서 지원하지 않는 기능도 가지고 있는데, 그 중 하나인 rolling join을 소개한다.\n\n일반적으로 join이라 함은 원하는 재료집합이 2개 이상일때, 이를 인결하여 새로운 집합을 만드는 연산을 의미한다. 당연히 join 연산을 할 때마다 집합이 확장되며 컬럼의 수가 증가하게 된다. join 연산의 경우 일반적으로는 equality condition을 사용한다. 즉 사용되는 집합이 일치하는 경우에만 연산이 이루어진다. 보통 이러한 조건 때문에 inner join, outter join 등을 사용하게 된다.\n\nrolling join은 inequality condition을 사용한다. 병합의 기준이 되는 컬럼 내에서 값을 탐색할 때, 다음과 같은 단계를 따를 수 있다.\n① 일단 일치하는 값이 있는지 확인하고, 없으면 선택한 방향을 따라 탐색한다. ② 탐색 범위에 기준 값에 가장 가까운 값이 있으면 그 값이 존재하는 행과 merge를 실행한다. ③ 탐색 범위 내에 값이 존재하지 않으면 병합을 실행하지 않는다.\n일반적으로 두 data.table object를 병합할 때에는 다음과 같이 실행할 수 있다. 여기서 on = 을 활용해서 기준이 되는 컬럼을 지정할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2)]\n\n그러나 rolling join을 실행하고 싶은 경우, 다음과 같이 작성할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf] # roll option = c(Inf, -Inf, number, \"nearest\")"
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#join",
    "href": "posts/2024-12-13-rollmerge/index.html#join",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "일반적으로 join이라 함은 원하는 재료집합이 2개 이상일때, 이를 인결하여 새로운 집합을 만드는 연산을 의미한다. 당연히 join 연산을 할 때마다 집합이 확장되며 컬럼의 수가 증가하게 된다. join 연산의 경우 일반적으로는 equality condition을 사용한다. 즉 사용되는 집합이 일치하는 경우에만 연산이 이루어진다. 보통 이러한 조건 때문에 inner join, outter join 등을 사용하게 된다."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#rolling-join",
    "href": "posts/2024-12-13-rollmerge/index.html#rolling-join",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "rolling join은 inequality condition을 사용한다. 병합의 기준이 되는 컬럼 내에서 값을 탐색할 때, 다음과 같은 단계를 따를 수 있다.\n① 일단 일치하는 값이 있는지 확인하고, 없으면 선택한 방향을 따라 탐색한다. ② 탐색 범위에 기준 값에 가장 가까운 값이 있으면 그 값이 존재하는 행과 merge를 실행한다. ③ 탐색 범위 내에 값이 존재하지 않으면 병합을 실행하지 않는다.\n일반적으로 두 data.table object를 병합할 때에는 다음과 같이 실행할 수 있다. 여기서 on = 을 활용해서 기준이 되는 컬럼을 지정할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2)]\n\n그러나 rolling join을 실행하고 싶은 경우, 다음과 같이 작성할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf] # roll option = c(Inf, -Inf, number, \"nearest\")"
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#roll-옵션",
    "href": "posts/2024-12-13-rollmerge/index.html#roll-옵션",
    "title": "data.table의 rolling join",
    "section": "1. roll = 옵션",
    "text": "1. roll = 옵션\n아래는 실습에 사용할 데이터를 전처리하는 코드이다. 실습 데이터는 다음 링크에서 다운받을 수 있으며, 4개 파일을 사용하여 실습을 진행할 것이다.\n\nlibrary(data.table);library(magrittr)\n\nbnc &lt;- fread(\"data/nsc2_bnc_1000.csv\") \nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\")[, Deathdate := (lubridate::ym(DTH_YYYYMM) %&gt;% lubridate::ceiling_date(unit = \"month\") - 1)][]\nm20 &lt;- fread(\"data/nsc2_m20_1000.csv\") \nm40 &lt;- fread(\"data/nsc2_m40_1000.csv\")[SICK_CLSF_TYPE %in% c(1, 2, NA)] \n\ncode.HTN &lt;- paste(paste0(\"I\", 10:15), collapse = \"|\")\ndata.start &lt;- m20[like(SICK_SYM1, code.HTN) & (MDCARE_STRT_DT &gt;= 20060101), .(Indexdate = min(MDCARE_STRT_DT)), keyby = \"RN_INDI\"]\n\n## Previous disease: Among all sick code\nexcl &lt;- m40[(MCEX_SICK_SYM %like% code.HTN) & (MDCARE_STRT_DT &lt; 20060101), .SD[1], .SDcols = c(\"MDCARE_STRT_DT\"), keyby = \"RN_INDI\"]\n\n## Merge: left anti join\ndata.incl &lt;- data.start[!excl, on = \"RN_INDI\"][, Indexdate := as.Date(as.character(Indexdate), format = \"%Y%m%d\")][]\ndata.asd &lt;- merge(bnd, bnc[, .(SEX = SEX[1]), keyby = \"RN_INDI\"], by = \"RN_INDI\") %&gt;% \n  merge(data.incl, by = \"RN_INDI\") %&gt;% \n  .[, `:=`(Age = year(Indexdate) - as.integer(substr(BTH_YYYY, 1, 4)),\n           Death = as.integer(!is.na(DTH_YYYYMM)),\n           Day_FU = as.integer(pmin(as.Date(\"2015-12-31\"), Deathdate, na.rm =T) - Indexdate))] %&gt;% .[, -c(\"BTH_YYYY\", \"DTH_YYYYMM\", \"Deathdate\")] \n\n\ncode.cci &lt;- list(\n  MI = c(\"I21\", \"I22\", \"I252\"),\n  CHF = c(paste0(\"I\", c(\"099\", 110, 130, 132, 255, 420, 425:429, 43, 50)), \"P290\"),\n  Peripheral_VD = c(paste0(\"I\", 70, 71, 731, 738, 739, 771, 790, 792), paste0(\"K\", c(551, 558, 559)), \"Z958\", \"Z959\"),\n  Cerebro_VD = c(\"G45\", \"G46\", \"H340\", paste0(\"I\", 60:69)),\n  Dementia = c(paste0(\"F0\", c(0:3, 51)), \"G30\", \"G311\"),\n  Chronic_pulmonary_dz = c(\"I278\", \"I279\", paste0(\"J\", c(40:47, 60:67, 684, 701, 703))),\n  Rheumatologic_dz = paste0(\"M\", c(\"05\", \"06\", 315, 32:34, 351, 353, 360)),\n  Peptic_ulcer_dz = paste0(\"K\", 25:28),\n  Mild_liver_dz = c(\"B18\", paste0(\"K\", c(700:703, 709, 713:715, 717, 73, 74, 760, 762:764, 768, 769)), \"Z944\"),\n  DM_no_complication = paste0(\"E\", c(100, 101, 106, 108:111, 116, 118:121, 126, 128:131, 136, 138:141, 146, 148, 149)),\n  DM_complication = paste0(\"E\", c(102:105, 107, 112:115, 117, 122:125, 127, 132:135, 137, 142:145, 147)),\n  Hemi_paraplegia = paste0(\"G\", c(\"041\", 114, 801, 802, 81, 82, 830:834, 839)),\n  Renal_dz = c(\"I120\", \"I131\", paste0(\"N\", c(\"032\", \"033\", \"034\", \"035\", \"036\", \"037\", \"052\", \"053\", \"054\", \"055\", \"056\", \"057\",\n                                             18, 19, 250)), paste0(\"Z\", c(490:492, 940, 992))),\n  Malig_with_Leuk_lymphoma = paste0(\"C\", c(paste0(\"0\", 0:9), 10:26, 30:34, 37:41, 43, 45:58, 60:76, 81:85, 88, 90, 97)),\n  Moderate_severe_liver_dz = c(paste0(\"I\", c(85, 859, 864, 982)), paste0(\"K\", c(704, 711, 721, 729, 765:767))),\n  Metastatic_solid_tumor = paste0(\"C\", 77:80),\n  AIDS_HIV = paste0(\"B\", c(20:22, 24))\n)\ncciscore &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 6, 6, 2)\nnames(cciscore) &lt;- names(code.cci)\n\n지금부터는 실제 의료 데이터를 활용하여 roll merge가 사용될 수 있는 상황에 대해 알아보고, 이를 코드로 적용해 보겠다. 데이터는 성균관대학교 바이오헬스규제학과 강의에 사용된 건강보험공단 데이터를 사용하였다. 일단 roll merge를 시행할 데이터에 대해 알아보자.\nroll = Inf\n\n\n[1] \"병력 진단 기준 날짜(Indexdate)가 있는 data.asd 데이터\"\n\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI   COD1   COD2   SEX  Indexdate   Age Death Day_FU\n     &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1:   13546                   1 2006-08-08    45     0   3432\n2:   23682                   1 2008-09-22    53     0   2656\n3:   36714                   2 2010-01-19    64     0   2172\n4:   39217                   1 2013-04-02    56     0   1003\n5:   46621                   1 2011-03-24    51     0   1743\n6:   51049                   1 2006-11-23    21     0   3325\n\n\n[1] \"병력 진단 날짜(MDCARE_STRT_DT)가 있는 m40 데이터\"\n\n\n   RN_INDI        RN_KEY MDCARE_STRT_DT FORM_CD MCEX_SICK_SYM\n     &lt;int&gt;         &lt;i64&gt;          &lt;int&gt;   &lt;int&gt;        &lt;char&gt;\n1:  596535 2002120187152       20021202       3          J209\n2:  615374 2002121012274       20021202       3          J209\n3: 1005547 2002120808216       20021202       3          J209\n4:  226594 2002120381612       20021202       3          J209\n5:  204930 2002120790182       20021202       3          J209\n6:  798943 2002040446183       20020401       3          J209\n   DETAIL_TMSG_SUBJ_CD SICK_CLSF_TYPE STD_YYYY\n                &lt;char&gt;          &lt;int&gt;    &lt;int&gt;\n1:                                 NA     2002\n2:                                 NA     2002\n3:                                 NA     2002\n4:                                 NA     2002\n5:                                 NA     2002\n6:                                 NA     2002\n\n\n첫 번째 데이터는 환자별로 병력 진단 기준일이 되는 날짜가 적혀 있다. 그리고 두 번째 데이터는 환자가 병력 진단을 받았을 경우 해당 날짜가 적혀 있다. 우리는 이 데이터를 가지고, 나름의 기준을 세워서 두 데이터를 병합하는 것이 목적이다. 만약 ’첫 번째 데이터의 Indexdate를 기준으로 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면 병력이 존재하는 것으로 간주’하려면 어떻게 해야 할까?\n병합을 하는 기준이 비교하는 두 날짜가 완벽히 일치하는 것이기 아니기 때문에 일반적인 join method를 사용할 수 없다. 이러한 경우에 roll merge를 사용할 수 있다.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  data.asd[, MDCARE_STRT_DT := Indexdate]\n  dt &lt;- m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][, MDCARE_STRT_DT := as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\")][, .(RN_INDI, MDCARE_STRT_DT, Incidate = MDCARE_STRT_DT)]  \n  dt[, .SD[1], keyby = c(\"RN_INDI\", \"MDCARE_STRT_DT\")][data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = Inf]})\n\nprint(head(info.cci[[10]], n = 5))\n\n   RN_INDI MDCARE_STRT_DT   Incidate   COD1   COD2   SEX  Indexdate   Age Death\n     &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;\n1:   13546     2006-08-08       &lt;NA&gt;                   1 2006-08-08    45     0\n2:   23682     2008-09-22       &lt;NA&gt;                   1 2008-09-22    53     0\n3:   36714     2010-01-19 2009-12-26                   2 2010-01-19    64     0\n4:   39217     2013-04-02 2013-04-02                   1 2013-04-02    56     0\n5:   46621     2011-03-24 2011-02-16                   1 2011-03-24    51     0\n   Day_FU\n    &lt;int&gt;\n1:   3432\n2:   2656\n3:   2172\n4:   1003\n5:   1743\n\n\n주의할 점은 data.table의 on=으로 병합을 시도할 경우에는 대상 컬럼이 이름이 같아야 한다는 것이다. 위의 코드에서는 Indexdate, incidate와 값이 같은 MDCARE_STRT_DT 컬럼을 대상으로 roll merge를 시도하였다.\nroll = Inf로 옵션을 주었기 때문에 dt의 날짜를 기준으로 data.asd를 연결할 때 정확히 일치하는 날짜가 없다면 data.asd의 날짜를 뒤로 밀어서 일치하는 날짜를 찾는다. 결과적으로 data.asd의 Indexdate 기준으로 앞 날짜에 m40의 Incidate가 존재한다면 병합이 되는 로직이라고 할 수 있다.\nroll merge를 사용하지 않는다면 cartesian = T 옵션을 사용하여 모든 가능한 조합을 허용하여 merge하고 그후에 조건에 맞게 필터링하는 과정을 거쳐야 한다. 다음은 rolling join을 사용하지 않은 코드의 예시이다.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  result &lt;- merge(data.asd[, .(RN_INDI, Indexdate)],\n                  m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][order(MDCARE_STRT_DT), .SD[1], keyby = \"RN_INDI\"][, .(RN_INDI, Incidate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))],\n                  by = \"RN_INDI\", all.x = T)\n  result[Indexdate &lt; Incidate, Incidate := NA]\n  return(result)\n})\nprint(head(info.cci[[10]], n = 5))\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI  Indexdate   Incidate\n     &lt;int&gt;     &lt;Date&gt;     &lt;Date&gt;\n1:   13546 2006-08-08       &lt;NA&gt;\n2:   23682 2008-09-22       &lt;NA&gt;\n3:   36714 2010-01-19 2005-12-09\n4:   39217 2013-04-02 2007-11-08\n5:   46621 2011-03-24 2005-09-20\n\n\n일단 merge를 수행한 이후, Incidate가 가장 빠른 첫 번째 날짜를 채택하여, 그 날짜가 Indexdate보다 뒤에 있을 경우 NA로 바꾸는 방식으로 필터링하였다. 이렇게 작업하여도 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면이라는 조건을 만족하는 행을 필터링하는 데에는 문제가 없지만 로직의 차이로 인해 Incidate에 적힌 날짜가 다른 것을 알 수 있다. 또한 필터링 과정 때문에 코드가 길어져 가독성이 좋지 않다.\nroll = -Inf\n\n\n[1] \"병력 진단 기준 날짜(Indexdate)가 있는 data.asd 데이터\"\n\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI   COD1   COD2   SEX  Indexdate   Age Death Day_FU MDCARE_STRT_DT\n     &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;         &lt;Date&gt;\n1:   13546                   1 2006-08-08    45     0   3432     2006-08-08\n2:   23682                   1 2008-09-22    53     0   2656     2008-09-22\n3:   36714                   2 2010-01-19    64     0   2172     2010-01-19\n4:   39217                   1 2013-04-02    56     0   1003     2013-04-02\n5:   46621                   1 2011-03-24    51     0   1743     2011-03-24\n6:   51049                   1 2006-11-23    21     0   3325     2006-11-23\n\n\n[1] \"병력 진단 날짜(MDCARE_STRT_DT)가 있는 m40 데이터\"\n\n\n   RN_INDI        RN_KEY MDCARE_STRT_DT FORM_CD MCEX_SICK_SYM\n     &lt;int&gt;         &lt;i64&gt;          &lt;int&gt;   &lt;int&gt;        &lt;char&gt;\n1:  596535 2002120187152       20021202       3          J209\n2:  615374 2002121012274       20021202       3          J209\n3: 1005547 2002120808216       20021202       3          J209\n4:  226594 2002120381612       20021202       3          J209\n5:  204930 2002120790182       20021202       3          J209\n6:  798943 2002040446183       20020401       3          J209\n   DETAIL_TMSG_SUBJ_CD SICK_CLSF_TYPE STD_YYYY\n                &lt;char&gt;          &lt;int&gt;    &lt;int&gt;\n1:                                 NA     2002\n2:                                 NA     2002\n3:                                 NA     2002\n4:                                 NA     2002\n5:                                 NA     2002\n6:                                 NA     2002\n\n\n두 번째 예시는 동일한 데이터를 활용할 것이지만, 이번에는 ’첫 번째 데이터의 Indexdate를 기준으로 이후의 모든 날짜에서 발병 기록이 한 번이라도 있으면 발병한 것으로 간주’하려면 어떻게 해야 할까?\n\ndata.asd[, MDCARE_STRT_DT := Indexdate]\ninfo.MI &lt;- m40 %&gt;% \n  .[like(MCEX_SICK_SYM, paste(code.cci[[\"MI\"]], collapse = \"|\")), .(RN_INDI, MDCARE_STRT_DT = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"), MIdate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))] %&gt;%\n  .[data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = -Inf] \n\nprint(info.MI[40:50])\n\n    RN_INDI MDCARE_STRT_DT     MIdate   COD1   COD2   SEX  Indexdate   Age\n      &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt;\n 1:  484978     2006-02-04 2006-02-04                   2 2006-02-04    63\n 2:  505228     2013-04-02       &lt;NA&gt;                   2 2013-04-02    45\n 3:  517447     2011-10-05       &lt;NA&gt;                   2 2011-10-05    61\n 4:  518792     2011-01-09       &lt;NA&gt;                   1 2011-01-09    50\n 5:  529690     2013-01-09       &lt;NA&gt;                   2 2013-01-09    58\n 6:  530990     2015-11-25       &lt;NA&gt;                   1 2015-11-25    63\n 7:  540586     2008-06-03       &lt;NA&gt;                   1 2008-06-03    59\n 8:  546772     2006-07-24       &lt;NA&gt;                   2 2006-07-24    19\n 9:  551252     2008-05-20       &lt;NA&gt;                   2 2008-05-20    73\n10:  559420     2014-12-22       &lt;NA&gt;                   2 2014-12-22    52\n11:  562142     2013-03-27 2013-03-27                   1 2013-03-27    53\n    Death Day_FU\n    &lt;int&gt;  &lt;int&gt;\n 1:     0   3617\n 2:     0   1003\n 3:     0   1548\n 4:     0   1817\n 5:     0   1086\n 6:     0     36\n 7:     0   2767\n 8:     0   3447\n 9:     0   2781\n10:     0    374\n11:     0   1009\n\n\nroll = -Inf로 옵션을 주었기 때문에 m40의 날짜를 기준으로 data.asd를 연결할 때 정확히 일치하는 날짜가 없다면 data.asd의 날짜를 앞으로 당겨서 일치하는 날짜를 찾는다. 결과적으로 data.asd의 Indexdate 기준으로 뒤 날짜에 m40의 Incidate가 존재한다면 병합이 되는 로직이라고 할 수 있다. (여기서는 시작 날짜도 포함하였다.)\nroll merge를 사용하지 않는다면 cartesian = T 옵션을 사용하여 모든 가능한 조합을 허용하여 merge하고 그후에 조건에 맞게 필터링하는 과정을 거쳐야 한다. 다음은 rolling join을 사용하지 않은 코드의 예시이다.\n\ninfo.MI &lt;- merge(data.asd[, .(RN_INDI, Indexdate)],\n                 m40[like(MCEX_SICK_SYM, paste(code.cci[[\"MI\"]], collapse = \"|\"))][order(MDCARE_STRT_DT), .SD[1], keyby = \"RN_INDI\"][, .(RN_INDI, MIdate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))],\n                 by = \"RN_INDI\", all.x = T) %&gt;%\n  .[Indexdate &lt; MIdate, MIdate := NA]\n\nprint(info.MI[40:50])\n\nKey: &lt;RN_INDI&gt;\n    RN_INDI  Indexdate     MIdate\n      &lt;int&gt;     &lt;Date&gt;     &lt;Date&gt;\n 1:  484978 2006-02-04 2006-02-04\n 2:  505228 2013-04-02       &lt;NA&gt;\n 3:  517447 2011-10-05       &lt;NA&gt;\n 4:  518792 2011-01-09       &lt;NA&gt;\n 5:  529690 2013-01-09       &lt;NA&gt;\n 6:  530990 2015-11-25       &lt;NA&gt;\n 7:  540586 2008-06-03       &lt;NA&gt;\n 8:  546772 2006-07-24       &lt;NA&gt;\n 9:  551252 2008-05-20       &lt;NA&gt;\n10:  559420 2014-12-22       &lt;NA&gt;\n11:  562142 2013-03-27 2013-03-27\n\n\n일단 merge를 수행한 이후, MIdate의 첫 번째 날짜를 채택하여, 그 날짜가 Indexdate보다 뒤에 있을 경우 NA로 바꾸는 방식으로 필터링하였다. 이렇게 작업하여도 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면이라는 조건을 만족하는 행을 필터링하는 데에는 문제가 없지만 로직의 차이로 인해 MIdate에 적힌 날짜가 다른 것을 알 수 있다. 또한 필터링 과정 때문에 코드가 길어져 가독성이 좋지 않다.\n정리하면 result_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf]에서 roll의 방향은 Inf 옵션일 때에는 dt1을 기준으로 dt2의 컬럼 값을 더 큰값으로 바꾸며 탐색하며, -Inf 옵션일 때에는 dt1을 기준으로 dt2의 컬럼 값을 더 작은 값으로 바꾸며 탐색한다. roll = 옵션에는 숫자도 줄 수 있는데, 이 경우 정해준 컬럼 값 기준 숫자 범위 내에서만 탐색한다. roll = nearest 옵션에서는 양방향 탐색을 진행하되, 가장 가까운 값을 찾아서 merge를 시도한다."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#rollends-옵션",
    "href": "posts/2024-12-13-rollmerge/index.html#rollends-옵션",
    "title": "data.table의 rolling join",
    "section": "2. rollends = 옵션",
    "text": "2. rollends = 옵션\nrollends = 옵션을 활용하여 rolling을 시작한 경계와 끝 경계에서 어떤 동작을 취할 지 지정할 수 있다. 아래 코드 예시와 같이, rollends 옵션은 두 개의 boolean 값을 가진다. 첫 번째 index는 rolling 시작 경계값에 대한 처리이며, 두 번째 index는 rolling 끝 경계값에 대한 처리이다.\n\nresult_dt &lt;- dt1[dt2, on = .(id), roll = Inf, rollends = c(TRUE, TRUE)]\n\n즉 양 끝의 값을 포함할 것인지, 버리고 NA를 취할 것인지에 대한 조정이라고 보면 된다. rollends =옵션이 Inf일 경우 (T, F), -Inf일 경우 (F, T), 숫자일 경우 (F, F)이다. 이전에 사용했던 건강보험공단 데이터로 실행해보면서 알아보자.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  data.asd[, MDCARE_STRT_DT := Indexdate]\n  dt &lt;- m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][, MDCARE_STRT_DT := as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\")][, .(RN_INDI, MDCARE_STRT_DT, Incidate = MDCARE_STRT_DT)]  \n  dt[, .SD[1], keyby = c(\"RN_INDI\", \"MDCARE_STRT_DT\")][data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = Inf, rollends = c(T, F)]})\n\nprint(head(info.cci[[10]], n = 5))\n\n   RN_INDI MDCARE_STRT_DT   Incidate   COD1   COD2   SEX  Indexdate   Age Death\n     &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;\n1:   13546     2006-08-08 2007-03-12                   1 2006-08-08    45     0\n2:   23682     2008-09-22       &lt;NA&gt;                   1 2008-09-22    53     0\n3:   36714     2010-01-19 2009-12-26                   2 2010-01-19    64     0\n4:   39217     2013-04-02 2013-04-02                   1 2013-04-02    56     0\n5:   46621     2011-03-24 2011-02-16                   1 2011-03-24    51     0\n   Day_FU\n    &lt;int&gt;\n1:   3432\n2:   2656\n3:   2172\n4:   1003\n5:   1743\n\n\nroll = Inf이므로 Incidate 기준으로 Indexdate는 Incidate보다 큰 값이 탐지되어야 merge를 할 수 있다. 하지만 rollends = (T, F) 로 되어 있으므로 첫 번째 행에서 시작 바운더리인 기준값보다 작아도 roll merge가 일어난 모습을 볼 수 있다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html",
    "href": "posts/2025-01-03-competingrisk/index.html",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "경쟁 위험(Competing risk)은 한 종류의 이벤트가 발생하면 다른 이벤트가 발생할 수 없는 상황으로 정의됩니다. 예를 들어, 암 재발이 event of interest 인 경우 사망이 competing risk로 작용할 수 있습니다 (사망한 암 환자에게는 암 재발이라는 event가 발생 할 수 없기 때문). 경쟁 위험을 무시하면 모집단의 생존 함수나 이벤트 발생률을 과대 혹은 과소평가 할 위험이 생기기에 이를 고려한 분석이 필요한 경우들이 있습니다. 여러 상황들을 살펴보고, 어떤 지표들을 이용했을 때 어떤 해석이 가능한지 알아보도록 하겠습니다.\n\n\\[\nS(t) = \\prod_{i: t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n\\]\n\nS(t): 생존 함수\n\\(t_i\\): 사건 발생 시점\n\\(d_i\\): 시점 \\(t_i\\)에서 발생한 사건 수\n\\(n_i\\): 시점 \\(t_i\\)에서 위험에 노출된 대상 수\n결국 생존 함수란 각 사건 발생 시점에서의 생존 확률(어떤 이벤트도 발생하지 않을 확률)을 누적 곱하여 계산한 시간 \\(t\\)까지 생존할 확률을 뜻합니다.\n\n\\[\nh_k(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t, \\epsilon = k \\mid T \\geq t)}{\\Delta t}\n\\]\n\n\\(\\lambda_k(t)\\): 이벤트 유형 \\(k\\)의 원인별 위험 함수\nT: 이벤트 발생 시간\n\\(\\epsilon\\): 이벤트 유형 지표(Ex. \\(\\epsilon\\) = 1: event of interest, \\(\\epsilon\\) = 2: competing risk )\n결국 위험 함수란 각 시점에서 event k 가 발생할 확률을 뜻합니다.\n\n\\[\nCIF_k(t) = \\int_{0}^{t} h_k(u) S(u) \\, du\n\\]\n이벤트 k에 대한 누적발생률 함수란 결국 시점 마다 생존해 있을 확률 \\(S(t)\\) 와 그 시점 에서 k가 발생할 위험 \\(h_k(t)\\)를 곱하여 합산한 값으로, 시점 t에 이번트 k의 누적 발생률을 뜻하게 됩니다.\n\n경쟁 위험이 없는 생존 분석에서는 대게 Kaplan-Meier 추정치를 사용하지만, 이는 앞서 말했던 예시와 같이 이벤트 발생률을 과대 혹은 과소 평가할 위험이 있기 때문에 경쟁 위험이 있는 경우 적합하지 않은 경우가 많습니다. 암 재발과 사망의 예시를 생각해보겠습니다. Kaplan-Meier 추정치의 경우 사망한 환자의 경우 censoring 된 것으로 처리가 되어 “이 환자의 경우 이후 정보는 알 수 없다”고 간주를 하고 계산하는 추정치입니다. 하지만, 현실에서는 사망한 환자의 경우 이후 암 재발이 일어날 가능성이 0이라는 것을 알 수 있습니다. 반면 CIF의 경우 수식에서도 볼 수 있듯이 사망이 발생해버린 사람은 이미 사건이 발생한 상태로 처리가 되며, 경쟁 이벤트가 발생했다는 \\(CIF_2(t)\\)라는 누적발생률 함수에 따로 기록되고 있는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#생존-함수",
    "href": "posts/2025-01-03-competingrisk/index.html#생존-함수",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nS(t) = \\prod_{i: t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n\\]\n\nS(t): 생존 함수\n\\(t_i\\): 사건 발생 시점\n\\(d_i\\): 시점 \\(t_i\\)에서 발생한 사건 수\n\\(n_i\\): 시점 \\(t_i\\)에서 위험에 노출된 대상 수\n결국 생존 함수란 각 사건 발생 시점에서의 생존 확률(어떤 이벤트도 발생하지 않을 확률)을 누적 곱하여 계산한 시간 \\(t\\)까지 생존할 확률을 뜻합니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#위험-함수",
    "href": "posts/2025-01-03-competingrisk/index.html#위험-함수",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nh_k(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t, \\epsilon = k \\mid T \\geq t)}{\\Delta t}\n\\]\n\n\\(\\lambda_k(t)\\): 이벤트 유형 \\(k\\)의 원인별 위험 함수\nT: 이벤트 발생 시간\n\\(\\epsilon\\): 이벤트 유형 지표(Ex. \\(\\epsilon\\) = 1: event of interest, \\(\\epsilon\\) = 2: competing risk )\n결국 위험 함수란 각 시점에서 event k 가 발생할 확률을 뜻합니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#누적발생률-함수cif",
    "href": "posts/2025-01-03-competingrisk/index.html#누적발생률-함수cif",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nCIF_k(t) = \\int_{0}^{t} h_k(u) S(u) \\, du\n\\]\n이벤트 k에 대한 누적발생률 함수란 결국 시점 마다 생존해 있을 확률 \\(S(t)\\) 와 그 시점 에서 k가 발생할 위험 \\(h_k(t)\\)를 곱하여 합산한 값으로, 시점 t에 이번트 k의 누적 발생률을 뜻하게 됩니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#왜-경쟁-위험-분석에서-cif를-사용할까",
    "href": "posts/2025-01-03-competingrisk/index.html#왜-경쟁-위험-분석에서-cif를-사용할까",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "경쟁 위험이 없는 생존 분석에서는 대게 Kaplan-Meier 추정치를 사용하지만, 이는 앞서 말했던 예시와 같이 이벤트 발생률을 과대 혹은 과소 평가할 위험이 있기 때문에 경쟁 위험이 있는 경우 적합하지 않은 경우가 많습니다. 암 재발과 사망의 예시를 생각해보겠습니다. Kaplan-Meier 추정치의 경우 사망한 환자의 경우 censoring 된 것으로 처리가 되어 “이 환자의 경우 이후 정보는 알 수 없다”고 간주를 하고 계산하는 추정치입니다. 하지만, 현실에서는 사망한 환자의 경우 이후 암 재발이 일어날 가능성이 0이라는 것을 알 수 있습니다. 반면 CIF의 경우 수식에서도 볼 수 있듯이 사망이 발생해버린 사람은 이미 사건이 발생한 상태로 처리가 되며, 경쟁 이벤트가 발생했다는 \\(CIF_2(t)\\)라는 누적발생률 함수에 따로 기록되고 있는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#fine-gray-method",
    "href": "posts/2025-01-03-competingrisk/index.html#fine-gray-method",
    "title": "Competing Risk Analysis",
    "section": "Fine-Gray Method",
    "text": "Fine-Gray Method\n경쟁위험이 존재하는 경우 생존 함수인 \\(S(t)\\)가 \\(1-CIF(t)\\)로 치환되지 않기 때문에 전통적 위험함수로는 \\(CIF(t)\\)를 추정할 수 없다는 한계가 있습니다. (\\(S(t)+CIF_1(t)+CIF_2(t)+....+CIF_k(t)=1\\)이기 때문) 따라서, Fine-Gray Method 에서는 \\(1-CIF(t)\\)의 시간에 따른 변화량을 추적하는 함수인 Subdistribution hazard function을 새로 추정합니다.\n하위 분포 위험의 경우 아래의 식과 같이 계산할 수 있습니다. \\[\nh_k^{sd}(t)\n\\;=\\;\n-\\,\\frac{d}{dt} \\,\\ln\\!\\bigl\\{1 - CIF_k(t)\\bigr\\}\n\\;=\\;\n\\frac{h_k^{cs}(t) S(t)}{\\,1 - CIF_k(t)\\,}.\n\\] 하위 분포 위험의 경우 \\(h_k^{sd}(t)\\)로 표기를 하고 이는 \\(CIF_k(t)\\)의 변화량을 추정할 수 있는 위험 함수입니다. 또한 여기서 \\(h_k^{cs}(t)\\)의 경우 cause-specific hazard function으로, 앞서 정의한 위험함수와 같은 의미를 가집니다.(subdistrivution hazard function과 구분하기 위해 이렇게 지칭하도록 하겠습니다). 그렇다면 \\(h_k^{sd}(t)\\)는 어떻게 해석을 할 수 있을지 알아보겠습니다. 우선 \\(S(t)\\)가 \\(1-CIF(t)\\)로 치환이 되는 경쟁위험이 없는 상황에서는 cause-specific hazard function과 완전히 동일하다는 것을 알 수 있습니다. 하지만 그렇지 않은 경우 cause-specific hazard function에 \\(\\frac {S(t)}{\\,1 - CIF_k(t)\\,}\\)을 곱한 값이 된다는 것을 알 수 있고, 이는 관심 event가 발생하지 않은 모든 사람이 분모 즉, risk set에 포함된다는 것을 알 수 있습니다. 이는 곧 아래의 수식으로 표현될 수 있습니다.\n\\[\nh_k^{sd}(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t+\\Delta t, \\text{Cause} = k \\mid T \\geq t \\cup \\text{(Cause } \\neq k \\text{ 발생)})}{\\Delta t}\n\\] 예를 들어, event of interest가 암의 재발이고 comepting risk가 사망인 경우 전통적 생존 분석에서는 시간 t에서 살아 있는 환자들이 risk set이 된다면 subdistribution hazard function 에서는 시간 t에서 살아있는 사람들과 시간 t전에 competing risk를 경험한(사망한)사람들 또한 risk set에 포함된다는 차이가 있습니다. Fine-Gray Method에서는 이 subdistribution hazard function을 이용하여 \\(\\beta\\)값을 추정하여 \\(CIF(t)\\)에 대한 보다 직접적인 추정을 가능하게 하는 방법입니다. Fine-Gray Method 분석법을 실행하는 방법과 전통적 생존 분석과 어떻게 비교하여 어떻게 수치들을 해석할 수 있는지를 알아보겠습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#fine-gray-method-와-전통적-생존함수-비교",
    "href": "posts/2025-01-03-competingrisk/index.html#fine-gray-method-와-전통적-생존함수-비교",
    "title": "Competing Risk Analysis",
    "section": "Fine-Gray Method 와 전통적 생존함수 비교",
    "text": "Fine-Gray Method 와 전통적 생존함수 비교\n다음과 같은 상황을 한번 생각해보겠습니다. 심부전 환자에서 중증 우울증 진단 여부에 따른 심장질환 관련 사망여부가 달라지는지 관심이 있는 상황을 가정해보겠습니다. 이 경우 CVD death가 Event of Interest가 되고 non-CVD death가 Competing risk가 될 것입니다. (non-CVD death로 사망하는 경우 심장 질환 관련 사망으로 사망하는지 관찰할 수 없기 때문). 우울증 진단 여부가 심장 질환 관련 사망에 관련이 없지만, 자살율을 크게 증가시켜 non-CVD death를 많이 발생시킨다고 한번 가정해보도록 하겠습니다. 가정에 따라 데이터를 한번 만들어보겠습니다.\n\nN &lt;- 10000\n# 우울증 여부를 배정\nDepression_status &lt;- c(rep(0, N/2), rep(1, N/2))\n#임의로 성별 배정\nSex &lt;- sample(c(0,1), size = N, replace = TRUE)\n\n# hazard를 가정에 맞게 정의\nlambda_cvd &lt;- 0.2  # CVD death 여부는 우울증과 무관하게 매년 0.2\nlambda_non_cvd_no_depression &lt;- 0.1  # Non-CVD death의 경우 비우울증 인구에서는 0.1\nlambda_non_cvd_depression &lt;- 0.5      # Non-CVD death rate가 자살율의 증가로 인해 우울증 인구에서 0.5\n\nCVD_time &lt;- numeric(N)\nnon_CVD_time &lt;- numeric(N)\nTime &lt;- numeric(N)\nEvent &lt;- numeric(N)\n\n# Simulate times\nfor (i in 1:N) {\n\n  CVD_time[i] &lt;- rexp(1, rate = lambda_cvd)\n  \n  if (Depression_status[i] == 0) {\n    non_CVD_time[i] &lt;- rexp(1, rate = lambda_non_cvd_no_depression)\n  } else {\n    non_CVD_time[i] &lt;- rexp(1, rate = lambda_non_cvd_depression)\n  }\n  if (CVD_time[i] &lt; non_CVD_time[i]) {\n    Time[i] &lt;- CVD_time[i]\n    Event[i] &lt;- 1  \n  } else {\n    Time[i] &lt;- non_CVD_time[i]\n    Event[i] &lt;- 2  \n  }\n}\n\nsample_data &lt;- data.frame(\n  ID = 1:N,\n  Time = round(Time, 2),\n  Event = Event,\n  Sex = ifelse(Sex == 1, \"Male\", \"Female\"),\n  Depression = ifelse(Depression_status == 1, \"Yes\", \"No\"),\n  CVD_time = round(CVD_time, 2)  \n)\n\nhead(sample_data, n= 15)\n\n   ID Time Event    Sex Depression CVD_time\n1   1 3.73     2 Female         No     9.03\n2   2 5.74     2 Female         No     9.50\n3   3 3.47     1 Female         No     3.47\n4   4 5.96     1   Male         No     5.96\n5   5 0.79     2   Male         No     2.80\n6   6 6.27     1 Female         No     6.27\n7   7 0.06     1 Female         No     0.06\n8   8 2.74     2 Female         No     3.49\n9   9 4.01     1   Male         No     4.01\n10 10 1.43     2 Female         No     5.07\n11 11 0.08     1   Male         No     0.08\n12 12 2.65     1 Female         No     2.65\n13 13 5.22     1 Female         No     5.22\n14 14 1.42     1 Female         No     1.42\n15 15 2.00     1 Female         No     2.00\n\n\n데이터에서 Time은 Death(CVD, non-CVD 포함)가 발생한 시점입니다. R의 cmprsk 패키지를 통해 fine gray method로 분석을 진행하고, 전통적 생존 분석또한 함께 진행해보겠습니다.\n\nlibrary(dplyr);library(survival);library(jstable)\nsample_data &lt;- sample_data %&gt;%\n  mutate(\n    Sex_num = ifelse(Sex == \"Male\", 1, 0),\n    Depression_num = ifelse(Depression == \"Yes\", 1, 0)\n  )\nsurv_object_csh &lt;- Surv(time = sample_data$Time, event = sample_data$Event == 1)\ncox_csh &lt;- coxph(surv_object_csh ~ Depression_num, data = sample_data)\ncox2.display(cox_csh)\n\n$table\n               HR(95%CI)         P value\nDepression_num \"1.03 (0.97,1.1)\" \"0.351\"\n\n$metric\n                        [,1] [,2]\n&lt;NA&gt;                      NA   NA\nNo. of observations 10000.00   NA\nNo. of events        4750.00   NA\nAIC                 76672.56   NA\n\n$caption\n[1] \"Cox model on time ('NA, surv_object_csh, character(0)') to event ('surv_object_csh')\"\n\n\nexp(coef= beta)즉 Hazard ratio가 1이고 p-value가 0.936으로, 우울증 여부가 CVD-death에 영향을 미치지 않는 다는 것을 알 수 있습니다. 이는 데이터를 만들때의 가정과 동일하기 때문에 전통적 생존분석은 이를 잘 반영한다는 것을 알 수 있습니다. 이번엔 competing risk를 고려한 fine gray method로 분석을 해보겠습니다.\n\nlibrary(cmprsk)\nftime &lt;- sample_data$Time\nfstatus &lt;- sample_data$Event\ncovariates &lt;- sample_data %&gt;% select(Depression_num)\nfg_model &lt;- crr(ftime = ftime, fstatus = fstatus, cov1 = covariates)\nsummary(fg_model)\n\nCompeting Risks Regression\n\nCall:\ncrr(ftime = ftime, fstatus = fstatus, cov1 = covariates)\n\n                coef exp(coef) se(coef)     z p-value\nDepression_num -1.05     0.352   0.0333 -31.4       0\n\n               exp(coef) exp(-coef)  2.5% 97.5%\nDepression_num     0.352       2.84 0.329 0.375\n\nNum. cases = 10000\nPseudo Log-likelihood = -41790 \nPseudo likelihood ratio test = 1190  on 1 df,\n\n\nexp(coef) 즉 subdistribution hazard ratio가 0.363이고, p value 가 &lt;0.01임으로 우울증이 있는 사람에게는 CVD-death가 덜 발생한다고 해석할 수 있습니다. 우울증 여부가 CVD-death에 영향을 미치지 않게 데이터를 만들었는데 왜 이런 결과가 나왔는지 그리고 서로 다르게 나온 두 결과를 각각 어떻게 해석해야 되는지를 알아보겠습니다. 우울증 환자와 비 우울증 환자에서 CVD-death에 대한 CIF를 한번 그려보도록 하겠습니다.\n\ngroup &lt;- sample_data$Depression_num\ncif_cvd &lt;- cuminc(ftime, fstatus, group = group, cencode = 0)\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"purple\", lty = 1, lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\"),\n       col = c(\"blue\", \"purple\"),\n       lty = 1,\n       lwd = 2)\n\n\n\n\n\n\n\n우울증 여부가 CVD-death에 영향을 미치지 않는다 하더라고, 우울증 환자에게서 CVD-death가 덜 발생한다는 것을 관찰할 수 있습니다. 이는 우울증 환자에서 이미 non-CVD death를 경험하여 CVD-death를 경험할 수 없기 때문이라는 것을 CVD, non-CVD death에 대한 CIF로 알아볼 수 있습니다.\n\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"red\", lty = 1, lwd = 2)\nlines(cif_cvd[[3]]$time, cif_cvd[[3]]$est, col = \"skyblue\", lty = 1, lwd = 2)\nlines(cif_cvd[[4]]$time, cif_cvd[[4]]$est, col = \"pink\", lty = 1, lwd = 2)\n\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\", \n                  \"Non-Depression - Non-CVD Death\", \"Depression - Non-CVD Death\"),\n       col = c(\"blue\", \"red\", \"skyblue\", \"pink\"),\n       lty = 1, lwd = 2, bty = \"n\", cex = 0.8)\n\n\n\n\n\n\n\n실제로 Depression 군에서는 non-CVD death가 급격하게 많이 발생하여, CVD-death의 CIF가 낮게 나오고 있다는 것을 확인 할 수 있습니다. non-CVD death가 발생하지 않은 경우를 가정하여 만들어 놓은 CVD_time변수로 Depression군과 non-Depression 군의 CIF를 비교해보도록 하겠습니다.\n\nlibrary(cmprsk)\nftime &lt;- sample_data$CVD_time\nfstatus &lt;- 1\ngroup &lt;- sample_data$Depression_num\ncif_cvd &lt;- cuminc(ftime, fstatus, group = group, cencode = 0)\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"purple\", lty = 1, lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\"),\n       col = c(\"blue\", \"purple\"),\n       lty = 1, lwd = 2, bty = \"n\", cex = 0.8)\n\n\n\n\n\n\n\n두개의 그래프가 거의 일치하는 것을 확인할 수 있습니다. 즉 Competing risk의 존재는, 실제로 우울증이 CVD death에 영향을 미치지 않더라고, CIF에는 변화를 줄 수 있다는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#how-to-interpret",
    "href": "posts/2025-01-03-competingrisk/index.html#how-to-interpret",
    "title": "Competing Risk Analysis",
    "section": "How to Interpret",
    "text": "How to Interpret\n지금까지 얻은 정보를 종합하여, cause-specific hazard와 subdistribution hazard 함수가 각각 어떤 질문에 적합한지, 어떤 내용을 설명하는지 정리해보겠습니다. Cause-specific hazard function은 “A라는 변수가 실제로 B에 영향을 미치는가?”와 같은 원인적(etiologic) 질문에 더 적합한 함수입니다. 예를 들어, “우울증이 CVD 사망을 유발하는가?”라는 질문에 답하기 위해서는 cause-specific hazard function을 사용하는 것이 더 적절할 수 있습니다. 반면, subdistribution hazard ratio는 “우울증이 CVD 사망의 발생률에 어떤 영향을 미치는가?”와 같은 예후(prognostic) 질문에 더 적합한 함수입니다. 이는 subdistribution hazard ratio가 Cumulative Incidence Function (CIF)을 직접적으로 추정하기 때문에, CIF 함수와 긴밀하게 연관되어 있습니다. 따라서 예후를 예측하는 측면에서는 Fine-Gray 방법을 이용한 분석이, 원인을 분석하는 측면에서는 전통적인 생존 분석이 더 적절합니다. 두 분석 결과를 함께 고려함으로써, 경쟁 위험이 존재하는 상황에서 관심 있는 사건에 영향을 미치는 요인들에 대한 포괄적인 분석을 수행할 수 있습니다.\nPS. Censoring이 존재하는 경우\nSubdistribution hazard function을 살펴볼 때, censoring이 존재하지 않고 환자를 이벤트가 발생할 때까지 관찰할 수 있다는 가정(complete data로 통칭)하에 risk set이 정의가 되었습니다. Censoring이 존재하는 경우 risk set이 어떻게 존재하는지 알아보겠습니다. Complete data의 경우 competing risk가 발생한 환자도, 발생한 시점 외에도 계속 한명의 존재로 risk set에 남아있다는 것을, 앞선 subdistribution hazard function의 유도과정에서 살펴보았습니다. Censoring이 생기는 경우 IPCW(Inverse probability censor weighting)이라는 방법을 통해, censoring이 된 환자의 예후를 반영하는 방법을 fine-gray method에서 사용하고 있습니다. 시점 t에서 100명이 생존한 와중에 60명이 censoring되고 40명에게 event가 발생하거나, 관측되거나 했다고 가정해봅시다. Censoring이라는 과정이 환자의 event 발생여부나 baseline에 관계없이 발생한다는 가정이 있다면 남은 40명에게 각각 2.5배의 가중치를 준다면, censoring이 발생하지 않고 100명을 계속 관측하는 것과 동일한 결과를 얻을 수 있다는 것이 IPCW의 방법론입니다. 따라서 시점 t이전에 competing event가 발생한 사람의 경우 여전히 한명으로 risk set에 기여를 하게 되고, 시점 t에서 censoring된 사람들은 2.5배의 가중치를 받아 risk set과 event set에 반영이 됩니다. 즉 competing risk가 발생한 환자들의 경우 여전히 한명으로 risk set에 기여하지만, 실질적으로 기여하는 가중치는 적어진다는 것을 알 수 있습니다. 이를 반영한 것이 survival package의 finegray함수이며, 이를 이용하여 finegray method를 통한 분석 또한 가능합니다.\n\nlibrary(survival)\ndata &lt;- mgus2\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))\npdata &lt;- finegray(Surv(etime, event) ~ ., data=data)\nhead(pdata)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death fgstart fgstop\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1       0     35\n2  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      35     44\n3  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      44     47\n4  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      48     52\n5  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      53     56\n6  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      56     57\n  fgstatus      fgwt\n1        0 1.0000000\n2        0 0.9990449\n3        0 0.9980368\n4        0 0.9959629\n5        0 0.9905896\n6        0 0.9873022\n\n\n데이터에서 볼 수 있듯이 competing risk가 발생한 환자의 경우 추적 종료까지 risk set에 기여도는 낮아지지만, 끝까지 포함되어 있다는 것을 알 수 있습니다. 또한 얻은 pdata로 기존의 생존분석과 같은 함수를 사용하면, 가중치가 반영되어 fine gray method로 구한 subdistribution hazard function에 대한 coefficient값을 얻을 수 있다는 것을 알 수 있습니다.\n\nfgfit &lt;- coxph(Surv(fgstart, fgstop, fgstatus) ~ age+sex,\n               weight=fgwt, data=pdata, model = T)\nsummary(fgfit)\n\nCall:\ncoxph(formula = Surv(fgstart, fgstop, fgstatus) ~ age + sex, \n    data = pdata, weights = fgwt, model = T)\n\n  n= 41775, number of events= 115 \n\n          coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)   \nage  -0.017302  0.982847  0.007022  0.005528 -3.13  0.00175 **\nsexM -0.259757  0.771239  0.187049  0.181707 -1.43  0.15285   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     0.9828      1.017    0.9723    0.9936\nsexM    0.7712      1.297    0.5402    1.1012\n\nConcordance= 0.548  (se = 0.026 )\nLikelihood ratio test= 7.28  on 2 df,   p=0.03\nWald test            = 11.19  on 2 df,   p=0.004\nScore (logrank) test = 7.58  on 2 df,   p=0.02,   Robust = 9.28  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서 새로운 영상 장비의 도입이 기존 방법보다 우수한지 평가하는 것은 매우 중요한 과정이다. 이를 위해 흔히 사용되는 연구 디자인이 MRMC 연구이며, 이는 여러 명의 판독자가 여러 증례를 평가하는 방식으로 진행된다. 이러한 MRMC 연구에서의 가장 큰 고민 중 하나는 “몇 명의 판독자와 몇 개의 증례가 필요한가?” 하는 문제이다.\n즉, 연구의 신뢰성을 확보하면서도 불필요한 비용과 시간을 줄이기 위해 표본 크기를 적절하게 설정하는 것이 매우 중요하다.\n\nMRMC 연구에서는 판독자와 증례를 두 가지 주요 요소로 고려해야 한다.\n일반적으로 판독자가 많아질수록 데이터의 변동성이 줄어들고 연구의 신뢰성이 높아지지만, 판독자의 시간과 연구 비용을 고려해야 한다. 마찬가지로 증례 수가 많을수록 통계적 검정력이 향상되지만, 데이터 수집 및 분석 비용이 증가하게 된다.\n또한, MRMC 연구에서는 다음과 같은 요소들이 표본 크기 결정에 영향을 미친다.\n\n\n효과 크기(Effect Size, \\(d\\))\n\n비교하고자 하는 두 판독 조건 간의 차이\n연구 설계에서 “임상적으로 의미 있는 차이”를 미리 정의하는 것이 중요\n\n\n\n검정력(Statistical Power, \\(1 - \\beta\\))\n\n실제로 차이가 있을 때 이를 검출할 확률\n일반적으로 80% 이상을 목표로 설정\n\n\n\n유의수준(Significance Level, \\(\\alpha\\))\n\nType I 오류의 허용 범위\n보통 5%로 설정\n\n\n\n분산 성분(Variance Components)\n\n판독자 간 변동성, 증례 간 변동성, 판독자와 증례 간 상호작용 등의 변동 요인을 반영\n\n\n\n의료 영상 분석에서는 병변의 유무를 파악하는 것이 중요하다.\n그 성능을 평가하는 대표적인 방법으로는 ROC(Receiver Operating Characteristic) 분석이 있는데, 이는 병변의 단순 유무를 파악하는 데 사용된다.\n그러나 영상 판독 연구에서는 단순히 병변의 유무뿐만 아니라 병변의 위치(Localization)까지 고려하는 것이 중요하다.\n이때 FROC(Free-Response ROC) 분석이 사용된다.\nFROC 분석을 활용한 MRMC 연구에서는 판독자가 병변의 위치까지도 설정하도록 하여,병변 탐지(Detection)뿐만 아니라 위치 정확도(Localization Accuracy)도 평가된다.\n권장되는 평가 지표로는 가중 AFROC(Weighted AFROC, wAFROC) 가 있으며,\n모든 병변은 동일한 가중치를 갖는 것이 일반적이다(특정 임상적인 이유가 있다면 병변마다 다른 가중치를 부여할 수 있음)."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#why-is-sample-size-estimation-important",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#why-is-sample-size-estimation-important",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서 새로운 영상 장비의 도입이 기존 방법보다 우수한지 평가하는 것은 매우 중요한 과정이다. 이를 위해 흔히 사용되는 연구 디자인이 MRMC 연구이며, 이는 여러 명의 판독자가 여러 증례를 평가하는 방식으로 진행된다. 이러한 MRMC 연구에서의 가장 큰 고민 중 하나는 “몇 명의 판독자와 몇 개의 증례가 필요한가?” 하는 문제이다.\n즉, 연구의 신뢰성을 확보하면서도 불필요한 비용과 시간을 줄이기 위해 표본 크기를 적절하게 설정하는 것이 매우 중요하다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서-표본-크기-추정의-핵심-개념",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서-표본-크기-추정의-핵심-개념",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "MRMC 연구에서는 판독자와 증례를 두 가지 주요 요소로 고려해야 한다.\n일반적으로 판독자가 많아질수록 데이터의 변동성이 줄어들고 연구의 신뢰성이 높아지지만, 판독자의 시간과 연구 비용을 고려해야 한다. 마찬가지로 증례 수가 많을수록 통계적 검정력이 향상되지만, 데이터 수집 및 분석 비용이 증가하게 된다.\n또한, MRMC 연구에서는 다음과 같은 요소들이 표본 크기 결정에 영향을 미친다.\n\n\n효과 크기(Effect Size, \\(d\\))\n\n비교하고자 하는 두 판독 조건 간의 차이\n연구 설계에서 “임상적으로 의미 있는 차이”를 미리 정의하는 것이 중요\n\n\n\n검정력(Statistical Power, \\(1 - \\beta\\))\n\n실제로 차이가 있을 때 이를 검출할 확률\n일반적으로 80% 이상을 목표로 설정\n\n\n\n유의수준(Significance Level, \\(\\alpha\\))\n\nType I 오류의 허용 범위\n보통 5%로 설정\n\n\n\n분산 성분(Variance Components)\n\n판독자 간 변동성, 증례 간 변동성, 판독자와 증례 간 상호작용 등의 변동 요인을 반영"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서의-성능-평가",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서의-성능-평가",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서는 병변의 유무를 파악하는 것이 중요하다.\n그 성능을 평가하는 대표적인 방법으로는 ROC(Receiver Operating Characteristic) 분석이 있는데, 이는 병변의 단순 유무를 파악하는 데 사용된다.\n그러나 영상 판독 연구에서는 단순히 병변의 유무뿐만 아니라 병변의 위치(Localization)까지 고려하는 것이 중요하다.\n이때 FROC(Free-Response ROC) 분석이 사용된다.\nFROC 분석을 활용한 MRMC 연구에서는 판독자가 병변의 위치까지도 설정하도록 하여,병변 탐지(Detection)뿐만 아니라 위치 정확도(Localization Accuracy)도 평가된다.\n권장되는 평가 지표로는 가중 AFROC(Weighted AFROC, wAFROC) 가 있으며,\n모든 병변은 동일한 가중치를 갖는 것이 일반적이다(특정 임상적인 이유가 있다면 병변마다 다른 가중치를 부여할 수 있음)."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#hypothesis-testing-in-mrmc-studies",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#hypothesis-testing-in-mrmc-studies",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. Hypothesis Testing in MRMC Studies",
    "text": "1. Hypothesis Testing in MRMC Studies\nMRMC 연구에서 새로운 영상 기술(또는 판독 조건)이 기존 방법보다 우수한지 검증하려면 통계적 검정을 수행해야 한다.\n이를 위해 귀무가설(\\(H_0\\))과 대립가설(\\(H_1\\))을 설정하고, 효과 크기(Effect Size)를 기반으로 표본 크기를 결정한다.\n\n\n귀무가설(\\(H_0\\)): 두 판독 조건(예: 기존 방법 vs. 새로운 방법)의 성능 차이가 없다.\n\n즉, \\[\\theta_1 = \\theta_2\\] (ROC-AUC 또는 wAFROC-AUC 값이 동일)\n\n\n\n대립가설(\\(H_1\\)): 두 판독 조건의 성능 차이가 있다.\n\n즉, \\[\\theta_1 \\neq \\theta_2\\]"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#observed-vs.-anticipated-effect-size",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#observed-vs.-anticipated-effect-size",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. Observed vs. Anticipated Effect-Size",
    "text": "2. Observed vs. Anticipated Effect-Size\n표본 크기를 결정하려면 두 판독 조건 간의 차이를 수량화한 효과 크기(Effect Size, \\(d\\))를 추정해야 한다.\n이때 관측된 효과 크기(Observed Effect-Size)와 예상 효과 크기(Anticipated Effect-Size)를 구분해야 한다.\n✔ 관측된 효과 크기(\\(d_{\\text{obs}}\\)):\n- 파일럿 연구(Pilot Study)에서 얻은 효과 크기의 추정값\n- 작은 표본 크기로 인해 오차가 포함될 가능성이 있음\n✔ 예상 효과 크기(\\(d_{\\text{ant}}\\)):\n- 본 연구(Pivotal Study)의 표본 크기를 결정하기 위해 설정하는 값\n- 신뢰구간(Confidence Interval, CI)을 고려하여 설정해야 함\n예상 효과 크기를 설정할 때 다음과 같은 접근법이 가능하다.\n\n\n신뢰구간의 하한을 사용 (보수적 접근법)\n\nType II 오류를 줄여 검정력을 확보 가능\n그러나 불필요하게 많은 판독자 및 증례가 필요할 수 있음\n\n\n\n신뢰구간의 중앙값을 사용 (균형적 접근법)\n\n너무 보수적이지 않으면서도 무리한 연구 설계를 방지 가능\n\n\n\n관측된 효과 크기 그대로 사용 (위험 부담 있음)\n\n파일럿 연구 표본이 작으면 변동성이 크므로 추천되지 않음"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#variance-components-and-their-impact-on-power",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#variance-components-and-their-impact-on-power",
    "title": "Sample Size Estimation in MRMC",
    "section": "3. Variance Components and Their Impact on Power",
    "text": "3. Variance Components and Their Impact on Power\nMRMC 연구에서 표본 크기를 결정할 때, 효과 크기(Effect Size)뿐만 아니라 분산 성분(Variance Components)이 통계적 검정력(Statistical Power)에 미치는 영향을 고려해야 한다.\n분산 성분은 판독자 간 변동성(Reader Variability), 증례 간 변동성(Case Variability), 판독자-처치 간 변동성(Treatment-Reader Interaction Variability) 등을 반영하며, 연구 설계에서 중요한 요소이다.\n3.1. 통계적 검정력(Statistical Power)과 분산 성분의 관계\nMRMC 연구에서 검정력은 효과 크기(\\(d\\))와 분산 성분의 함수로 결정된다. 통계적 검정력은 비중심 모수(Non-centrality Parameter, \\(\\lambda\\)) 에 의해 결정되며, 다음과 같은 조건에서 검정력이 증가한다.\n검정력은 다음과 같은 확률로 정의된다.\n\\[\n\\text{Power} = P\\left(F &gt; F_{\\alpha, \\text{ndf}, \\text{ddf}} \\mid \\lambda \\right)\n\\]\n여기서,\n- \\(F\\) : 검정 통계량 (F-statistic)\n- \\(F_{\\alpha, \\text{ndf}, \\text{ddf}}\\) : 유의수준 \\(\\alpha\\)에서의 F-분포 임계값 (분자 자유도 \\(\\text{ndf}\\), 분모 자유도 \\(\\text{ddf}\\))\n- \\(\\lambda\\) : 비중심 모수(Non-centrality Parameter)\n비중심 모수 \\(\\lambda\\) 는 다음과 같이 정의된다.\n\\[\n\\lambda = \\frac{J K d^2}{\\sigma^2_{\\text{total}}}\n\\]\n여기서,\n- \\(J\\) : 판독자 수(Readers)\n- \\(K\\) : 증례 수(Cases)\n- \\(d\\) : 효과 크기(Effect Size)\n- \\(\\sigma^2_{\\text{total}}\\) : 총 분산 성분 (전체 변동성)\n3.2. 분산 성분이 검정력에 미치는 영향\n(1) 분자가 클 경우 → 검정력 증가\n\n\n예상 효과 크기(Anticipated Effect-Size, \\(d_{\\text{ant}}\\))가 클 경우\n\n\n효과 크기가 클수록 귀무가설(\\(H_0\\))을 기각할 가능성이 높아진다.\n\n\n판독자 수(\\(J\\)) 또는 증례 수(\\(K\\))가 클 경우\n\n판독자(\\(J\\)) 또는 증례(\\(K\\))가 많을수록 추정값의 변동성이 감소하여 통계적 검정력이 증가한다.\n이는 큰 표본 크기가 귀무가설 기각 확률을 증가시키는 일반적인 원리와 동일하다.\n\n\n(2) 분모가 작을 경우 → 검정력 증가\n통계적 검정력의 분모는 다음과 같이 구성된다.\n\\[\n\\sigma^2_{\\epsilon} + \\sigma^2_{\\tau RC} + K\\sigma^2_{\\tau R} + J\\sigma^2_{\\tau C}\n\\]\n이때, 각 분산 성분이 작아질수록 검정력이 증가한다.\n\n\n잔차 변동성(\\(\\sigma^2_{\\epsilon} + \\sigma^2_{\\tau RC}\\)) 감소 → 검정력 증가\n\n이 두 항은 Jackknife Pseudovalues의 잔차 변동성을 나타낸다.\n잔차 변동성이 작을수록 비중심 모수(Non-centrality Parameter, \\(\\lambda\\))가 증가하여 검정력이 증가한다.\n\n\n\n처치-판독자 변동성(\\(\\sigma^2_{\\tau R}\\)) 감소 → 검정력 증가\n※ 처치 : 비교하고자하는 판독 조건을 의미;의료 영상 분석의 예시에서는 기존 방식과 AI 보조 영상 판독가 될 수 있음\n\n판독자 자체의 변동성(\\(\\sigma^2_R\\))은 모든 처치 조건에서 동일한 영향을 미치므로 검정력에 영향을 주지 않는다.\n처치-판독자(Treatment-Reader) 분산 성분(\\(\\sigma^2_{\\tau R}\\))은 노이즈로 작용하여 효과 크기 추정을 방해하고 검정력을 감소시킨다.\n따라서 이 값이 작을수록 표본 크기 추정의 정확도가 높아진다.\n\n\n\n\n처치-증례 변동성(\\(\\sigma^2_{\\tau C}\\)) 감소 → 검정력 증가\n\n증례 자체의 변동성(\\(\\sigma^2_C\\))은 모든 처치 조건에서 동일한 영향을 미치므로 검정력에 영향을 주지 않는다.\n그러나 처치-증례 변동성(\\(\\sigma^2_{\\tau C}\\))은 효과 크기 추정치에 노이즈를 추가하여 검정력을 감소시킨다.\n이 항은 판독자 수(\\(J\\))와 곱해지므로, 보통의 \\(J \\ll K\\)인 연구에서는 그 영향이 크지 않다.\n\n\n\n📌 **즉, MRMC 연구 설계에서 표본 크기를 결정할 때, 단순히 숫자를 늘리는 것이 아니라, 이러한 분산 성분을 고려해야 한다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#using-pilot-studies",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#using-pilot-studies",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. Using Pilot Studies",
    "text": "1. Using Pilot Studies\n파일럿 연구(Pilot Study)는 본 연구에서 필요한 효과 크기(\\(d_{\\text{obs}}\\)) 및 분산 성분을 추정하는 역할을 한다. - 파일럿 연구에서 얻은 정보는 RJafroc 패키지의 SsSampleSizeKGiven() 함수를 활용해 표본 크기를 결정하는 데 사용된다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#sample-size-estimation-using-rjafroc",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#sample-size-estimation-using-rjafroc",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. Sample Size Estimation Using RJafroc",
    "text": "2. Sample Size Estimation Using RJafroc\n2.1. 함수 개요\n\n# SsSampleSizeKGivenJ()\n# SsSampleSizeKGivenJ(dataset, FOM, J, effectSize, method, analysisOption)\n\n\ndataset : Pilot Data\nFOM : 평가 지표 (“ROC-AUC” in ROC Studies)\nJ : 판독자 수\nK : 증례 수\neffectSize : 효과 크기 (파일럿 연구에서 추정)\nmethod : 분석 방법 (“DBM” 또는 “OR”)\nanalysisOption : 분산 분석 Option (“RRRC”, “FRRC”, “RRFC”)\n2.2. 실제 예제\n일반적인 연구 설계에선 판독자수는 제한된 자원으로 인해 고정되거나, 대략적인 좁은 범위에서 결정되는 경우가 많다.\n이에 특정 판독자 수에서 검정력을 만족하는 최소한의 증례 수를 찾는 것이 일반적이고, RJafroc의 SsSampleSizeKGivenJ 함수도 그러하다.\n본 예시 코드에서는 판독자수가 6~13명 정도의 범위일때, 검정력이 80% 이상이 되는 최소한의 \\(K\\) 값을 찾는다.\nlibrary(RJafroc)\n\npilot_data &lt;- dataset02  # 예제 데이터셋 사용\n\n\ntarget_power &lt;- 0.8 # 목표 검정력 \nJ_vals &lt;- 6:13 # 판독자 수 범위 지정 \noptimal_K_results &lt;- data.frame(J = integer(), K = integer(), Power = numeric())  \n\n# 목표 검정력을 만족하는 최소 K(증례수) 도출 \nfor (J in J_vals) {\n  ret &lt;- SsSampleSizeKGivenJ(\n    dataset = pilot_data,\n    FOM = \"Wilcoxon\",\n    J = J,\n    analysisOption = \"RRRC\",\n    alpha = 0.05,\n    desiredPower = target_power\n  )\n  optimal_K_results &lt;- rbind(optimal_K_results, \n                             data.frame(J = J, K = ret$K, Power = signif(ret$powerRRRC, 3)))\n}\n\nrmarkdown::paged_table(optimal_K_results)\n\n  \n\n\n위 코드를 실행하면 판독자가 많아짐에 따라 같은 효과 크기에서도 더 작은 증례수로 목표 검정력을 확보할 수 있음을 확인 가능하다.\nSsSampleSizeKGivenJ() 함수를 이용하면 파일럿 연구에서 추정된 효과 크기와 목표 검정력을 바탕으로 MRMC 연구의 판독자수에 따른 적절한 증례 수를 합리적으로 결정할 수 있게 된다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#froc와-roc-연구의-차이점",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#froc와-roc-연구의-차이점",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. FROC와 ROC 연구의 차이점",
    "text": "1. FROC와 ROC 연구의 차이점\nFROC 연구에서 효과 크기 변환이 필요한 이유를 이해하려면, ROC와 wAFROC의 차이를 먼저 살펴보아야 한다.\nROC 연구의 특징\n\n\n판독자가 병변이 있는지(Yes/No)만 결정 → 위치 정보는 고려되지 않음\n\n성능 평가 지표: ROC-AUC (0.5 ~ 1)\n\n무작위 성능 ≈ 0.5 (랜덤으로 판별할 경우)\n완벽한 성능 ≈ 1\n\n\n\nFROC 연구의 특징\n\n\n판독자가 병변이 있는 위치까지 특정해야 함 → 탐지뿐만 아니라 위치 정확도(Localization Accuracy)도 고려됨\n\n성능 평가 지표: wAFROC-AUC (0 ~ 1)\n\n무작위 성능 ≈ 0 (무작위로 위치를 선택하면 병변을 정확히 찾을 확률이 0에 가까움)\n완벽한 성능 ≈ 1\n\n\n\n\n✔ ROC-AUC는 0.5 ~ 1 범위에서 변화하는 반면, wAFROC-AUC는 0 ~ 1 범위에서 변화한다.\n✔ 따라서 판독 능력의 차이가 비슷하더라도, 수치적인 AUC 차이는 서로 다르게 나타날 수 있다.\n✔ 따라서 ROC 연구에서 정의한 효과 크기를 FROC 연구에서 적용하려면 변환 과정이 필요하다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환을-위한-rsmradiological-search-model-적용",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환을-위한-rsmradiological-search-model-적용",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. 변환을 위한 RSM(Radiological Search Model) 적용",
    "text": "2. 변환을 위한 RSM(Radiological Search Model) 적용\nFROC 연구에서 ROC 효과 크기를 wAFROC 효과 크기로 변환하려면Radiological Search Model (RSM)을 활용해야 한다.\nRSM 모델의 핵심 개념\nRSM 모델은 판독자의 탐색(Search)과 의사결정(Decision Making) 과정을 모델링하며,\n이를 위해 3가지 주요 매개변수를 사용한다.\n\n\nμ (mu): 병변과 비병변 간의 신호 대비 (Signal-to-Noise Ratio, SNR)\n\nROC에서 병변을 감지하는 능력(Detection Ability)을 나타냄\n값이 클수록 병변과 비병변을 더 잘 구별할 수 있음\n\n\n\nλ (lambda): 비병변을 오탐(False Positive)할 가능성\n\n값이 클수록 판독자가 비병변(non-lesion)을 병변으로 잘못 판단할 확률이 증가\n\n\n\n\nν (nu): 실제 병변을 정확히 찾는 확률 (Localization Accuracy)\n\n값이 클수록 판독자가 병변을 정확한 위치에 마킹할 가능성이 증가"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-과정",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-과정",
    "title": "Sample Size Estimation in MRMC",
    "section": "3. 변환 과정",
    "text": "3. 변환 과정\n변환 계수(Scaling Factor) 계산\n변환 계수는 ROC 효과 크기(\\(\\Delta\\)ROC-AUC)를 FROC 효과 크기(\\(\\Delta\\)wAFROC-AUC)로 변환하기 위한 비율이며, 다음 절차를 통해 도출된다.\n\n1. 파일럿 데이터에서 RSM 모델 적합(fitting) 및 AUC 계산\n✔ 파일럿 데이터에서 RSM 모델을 적합(fit)하여 ROC-AUC 및 wAFROC-AUC 값을 도출한다.\n✔ 이를 통해 현재의 \\(\\mu\\), \\(\\lambda\\), \\(\\nu\\) 값을 얻는다.\n2. \\(\\mu\\)를 단계적으로 증가시키면서 ROC-AUC와 wAFROC-AUC 변화를 측정\n✔ 모델의 탐지 능력이 향상된다고 가정하고 \\(\\mu\\) 값을 일정한 간격(예: \\(\\Delta \\mu = 0.01\\))으로 증가시킨다.\n✔ \\(\\mu\\)가 증가함에 따라 ROC-AUC와 wAFROC-AUC도 증가하게 된다.\n✔ 그러나 \\(\\lambda\\)와 \\(\\nu\\)는 \\(\\mu\\)와 상관성이 있기 때문에, \\(\\mu\\)만 단순 증가시키면 변환이 정확하지 않다.\n✔ 이를 해결하기 위해, 내재적 파라미터 변환을 적용하여 \\(\\lambda\\)와 \\(\\nu\\)도 함께 변화하도록 조정한다.\n3. 내재적 파라미터 변환 (Util2Intrinsic() 사용)\n✔ \\(\\mu\\) 값이 변화하면 \\(\\lambda\\)와 \\(\\nu\\)도 함께 조정되어야 하는데, 직접적으로 조정하기 어렵다.\n✔ 따라서 물리적(Physical) 파라미터 (\\(\\mu, \\lambda, \\nu\\))를 내재적(Intrinsic) 파라미터 (\\(\\lambda_i, \\nu_i\\))로 변환한다.\n\\[\n\\lambda_i = \\lambda \\times \\mu\n\\]\n\\[\n\\nu_i = -\\log(1 - \\nu) / \\mu\n\\]\n✔ 변환 후, \\(\\mu\\)를 증가시키면서도 \\(\\lambda\\)와 \\(\\nu\\)를 올바르게 유지할 수 있는 상태가 된다.\n4. 각 단계에서 ROC-AUC 변화량 (\\(\\Delta\\)ROC-AUC)과 wAFROC-AUC 변화량 (\\(\\Delta\\)wAFROC-AUC) 비교\n✔ \\(\\mu\\) 값을 증가시킨 후, 해당하는 새로운 ROC-AUC와 wAFROC-AUC를 계산한다.\n✔ 그러나 이 상태에서는 여전히 기존 \\(\\lambda\\)와 \\(\\nu\\)를 사용하고 있으므로, 이를 새로운 \\(\\mu\\) 값에 맞게 변환해야 한다.\n내재적 → 물리적 변환 (Util2Physical() 사용)\n✔ 증가된 \\(\\mu\\) 값(\\(\\mu_{\\text{new}}\\))에 대해 내재적 파라미터를 다시 물리적 파라미터로 변환하여 \\(\\lambda\\)와 \\(\\nu\\)를 조정한다.\n\\[\n\\lambda = \\frac{\\lambda_i}{\\mu_{\\text{new}}}\n\\]\n\\[\n\\nu = 1 - \\exp(-\\nu_i \\times \\mu_{\\text{new}})\n\\]\n✔ 즉, 새로운 \\(\\mu\\) 값에서 RSM 모델을 다시 적합하여, 해당하는 ROC-AUC 및 wAFROC-AUC를 구한다.\n✔ 이를 반복 수행하여 \\(\\Delta\\)ROC-AUC과 \\(\\Delta\\)wAFROC-AUC을 측정한다.\n5. 선형 회귀를 통해 변환 계수(Scaling Factor) 결정\n✔ 여러 번의 \\(\\mu\\) 증가 단계에 대해 \\(\\Delta\\)ROC-AUC(독립 변수)와 \\(\\Delta\\)wAFROC-AUC(종속 변수) 사이의 관계를 분석한다.\n✔ 이를 선형 회귀(linear regression)를 사용하여 분석하면 변환 계수(Scaling Factor)를 구할 수 있다.\n✔ 선형 회귀 분석에서 \\(\\Delta\\)ROC-AUC를 독립 변수, \\(\\Delta\\)wAFROC-AUC를 종속 변수로 설정하면 기울기(Slope)가 변환 계수가 된다.\n✔ 위 과정을 통해 ROC 연구에서의 효과 크기를 FROC 연구에서 사용할 수 있도록 변환할 수 있다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-후-표본-크기-추정",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-후-표본-크기-추정",
    "title": "Sample Size Estimation in MRMC",
    "section": "4. 변환 후 표본 크기 추정",
    "text": "4. 변환 후 표본 크기 추정\n변환 계수를 적용하고 나면, ROC 연구와 동일한 방법으로 FROC 연구에서 필요한 표본 크기를 계산할 수 있다.\n\nROC 연구에서 정의한 효과 크기(ΔROC-AUC)를 변환하여 wAFROC 효과 크기(ΔwAFROC-AUC)로 변환\n변환된 wAFROC 효과 크기를 기반으로, SsSampleSizeKGivenJ() 함수를 이용하여 샘플 크기 계산"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#reference",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#reference",
    "title": "Sample Size Estimation in MRMC",
    "section": "Reference",
    "text": "Reference\nhttps://cran.r-project.org/web/packages/RJafroc/RJafroc.pdf\nhttps://dpc10ster.github.io/RJafrocBook/"
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html",
    "href": "posts/2025-02-28-reg2/index.html",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "",
    "text": "2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#들어가며",
    "href": "posts/2025-02-28-reg2/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "",
    "text": "2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#generalized-linear-models-glms",
    "href": "posts/2025-02-28-reg2/index.html#generalized-linear-models-glms",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "1. Generalized Linear Models (GLMs)",
    "text": "1. Generalized Linear Models (GLMs)\n1.1. Linear Model 한계\n\n1장에서 본 Linear Regression Model은 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (3) 오차항의 독립성(Independence) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정에서 비롯된 모델이었고, Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)를 통해 오차항의 등분산성(Homoscedasticity) 가정이 깨진 data에 대해서도 Linear model로부터 얻은 모델 parameter의 분산을 robust하게 추정할 수 있었습니다. 그러나, 위에서 언급하였듯 outcome of single yes/no, outcome of single K-way, count 등 많은 data는 반응 변수 Y가 정규분포를 따르지 않거나 등분산성 가정, 선형성에 위배됩니다. 각각에 대해서 좀 더 설명하자면, 어떤 사건이나 행동이 일어나거나 그렇지 않은 경우를 고려하는 이진 데이터(binary data, outcome of single yes/no)의 경우, \\(Y \\in \\{0, 1\\}\\)로 제한되며 이를 \\(Y \\in \\mathbb{R}\\)인 정규분포로 가정하는 것은 옳지 않습니다. 특정 기간 동안 발생하는 사건의 횟수 등, 이진 분류처럼 discrete한 종속변수 값을 가지는 카운트 데이터(count data) 또한 discrete(정수) 값만 갖으며, 이 두 경우는 종종 분산이 평균(모델의 예측)에 비례하는 형태를 갖을 수 있고, 이는 당연하게도 등분산성 가정을 위배합니다.\n이러한 데이터의 경우 단순히 독립변수의 선형결합 형태, 또는 기하학적으로는 Hyper plane 형태로 모델을 만들면, 비선형적인 (이진 데이터 등) 위 같은 경우에 대해서는 올바르게 고려하지 못할 것입니다. 이러한 기존의 Linear Regression 모델의 한계를 극복하고, (종속변수의) 다양한 형태의 데이터를 모델링하기 위해 여러 함수를 설계함으로써 유연성을 확장한 Generalized Linear Models이 개발되었습니다. Generalized Linear Models(GLMs)의 중요 구성 요소들과 원리를 간략히 설명해보자면, 선형 결합으로 바로 종속변수를 예측하는 대신, non-linear한 Link Function에 넣어 최종적으로 예측함으로써 non-linear한 종속변수에도 fit 할 수 있고, 이에 따라 종속변수의 분포가 정규분포가 아닌 다른 분포(Exponential Family)도 포함할 수 있도록 하였으며, 이 Exponential Family와 Variance function구성은 종속변수의 분산이 모델의 예측값(종속변수의 mean)마다 다를 수 있도록 합니다. 이를 통해 Generalized Linear Models는 위 네 개의 Linear Regression 가정 중 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정을 깼으며, 위에서 Linear Model의 한계로 언급한 데이터들을 고려할 수 있는 모델입니다.\n1.2. GLM 정의 및 수학적 표현\n\nGLM은 세 가지 구성 요소 (Random component, Systematic component, Link function)으로 정의 되며, 이때 Random component는 Y를 Exponential Family로, Systematic component는 Linear predictor와 Link function으로 구성됩니다. 어떻게 Generalized Linear Models가 설계되었는지 component들을 하나하나 자세히 다뤄보겠습니다.\nLinear predictor\n\nLinear predictor \\(\\eta\\)는 말그대로 Linear Model처럼 모델 parameter \\(\\boldsymbol{\\beta}\\)와 독립변수 \\(\\mathbf{X}\\)의 선형 결합으로, 기존에는 \\(\\eta\\)로 바로 \\(\\mathbf{y}\\)를 추정하여 non-linear한 종속변수를 고려하지 못하였었다면, GLM은 \\(\\eta\\)를 계산한 후, 이 값을 non-linear한 Link function에 input하여 최종적으로 종속변수를 예측합니다. 중요한 점은, 이는 단순히 종속변수를 transform한 뒤(로그 등) 이전처럼 선형적으로 추정하는 Transformation (with LM)과 다르다는 점입니다. 가장 큰 차이점은 Transformation을 함으로써 종속변수의 sample space에서 boundaries에 있는 값들은 정의가 되지 않고(로그는 0에서 정의되지 않음.), 이후 바로 Linear Model을 사용하기 위해선 종속변수가 변형 이후 반드시 linearity와 variance의 homogeneity가 거의 보장되어야 합니다.(기존 LM을 사용하기 때문에 이때 사용한 가정 또한 필요하게 됩니다.) GLM의 Linear predictor (선형 예측자) \\(\\eta\\)의 식은 다음과 같습니다: \\[\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_p x_{pi}\\]\nLink function (링크 함수)\n\nLink function \\(g(\\mu_i)\\)는 non-linear하고 미분 및 inverse(역)가 가능한 함수로 정의되며, 종속변수의 평균 \\(E(Y_i) = \\mu_i\\)를 선형 예측자 \\(\\eta_i\\)와 연결하여 간접적으로 독립변수 및 모델 parameter의 선형결합과 종속변수를 mapping하는 역할을 합니다.\\[g(\\mu_i) = \\eta_i\\]\nVariance function (분산 함수)\n\n분산 함수는 평균 \\(\\mu_i\\)에 따라 종속변수의 분산이 어떻게 변하는지를 나타냅니다. 이를 통해 간접적으로 독립변수에 따라 분산이 다르게 나오는 것을 반영할 수 있으며 식은 아래와 같고,\\[\\mathrm{Var}(Y_i) = \\phi V(\\mu_i)\\]\n여기서 \\(\\phi\\)는 dispersion parameter로, 일반적으로 특정 분포에 따라 다르게 정의됩니다. (예: Poisson 분포에서는 \\(\\phi = 1\\)).\nExponential Family\n\nGLM은 종속변수의 분포로 Gaussian(또는 정규분포)를 포함한, 더욱 general한 Exponential Family을 고려하며, 이 분포는 다음과 같은 일반 형태를 가집니다.\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\right\\}\n\\]\n즉 GLM은 LM과 다르게 linear predictor, link function, variance function을 설계함으로써 종속변수가 더욱 general한 분포인 exponential family distribution인 경우에도 잘 mapping할 수 있도록 하는 모델이라고 볼 수 있습니다. 위 식에서 의미론적으로 각 parameters를 해석하면 \\(\\theta\\)는 canonical parameter로 분포의 위치를 나타내는 파라미터, \\(\\phi\\)는 dispersion parameter로 분산과 관련된 파라미터, \\(b(\\theta)\\)는 평균과 분산 관계를 정의하는 함수입니다. 이 분포에 대해 \\(E(Y) = b'(\\theta) = \\mu\\), \\(\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)\\)이라는 특성이 증명 가능하고, 이는 “2. GLMs 추정”에서 모델 \\(\\beta\\)를 추정하는 과정에 필요하기 때문에 아래에서 증명할 것입니다. 이보다 더욱 general한 분포로 (dispersion parameter 관련) exponential dispersion family가 있습니다.\n다음으로 넘어가기 전에 간단하게 잘 알려져있는 Exponential Family의 예시인 정규분포, 이항분포, 포아송분포, 감마분포가 이에 포함됨을 확인해보겠습니다.\n(1) 정규분포 (Normal Distribution)\n정규분포의 확률밀도함수는 다음과 같습니다: \\[\n    f(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(y - \\mu)^2}{2\\sigma^2} \\right\\}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; \\mu, \\sigma^2) = \\exp\\left\\{ \\frac{y\\mu - \\frac{\\mu^2}{2}}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\mu\\),\nDispersion parameter: \\(\\phi = \\sigma^2\\),\n\n\\(b(\\theta) = \\frac{\\theta^2}{2}\\),\n\n\\(b'(\\theta) = \\theta = \\mu\\),\n\n\\(b''(\\theta) = 1\\),\n\n\\(c(y, \\phi) = -\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log(2\\pi\\phi)\\).\n\n(2) 이항분포 (Binomial Distribution)\n이항분포의 확률질량함수는 다음과 같습니다: \\[\n    f(y; n, p) = \\binom{n}{y} p^y (1-p)^{n-y}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; n, p) = \\exp\\left\\{ y \\log\\left(\\frac{p}{1-p}\\right) + n \\log(1-p) + \\log\\binom{n}{y} \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\log\\left(\\frac{p}{1-p}\\right)\\),\nDispersion parameter: \\(\\phi = 1\\),\n\n\\(b(\\theta) = n \\log(1 + e^\\theta)\\),\n\n\\(b'(\\theta) = e^\\theta = \\lambda\\),\n\n\\(b''(\\theta) = e^\\theta = \\lambda\\),\n\n\\(c(y, \\phi) = \\log\\binom{n}{y}\\).\n\n(3) 포아송분포 (Poisson Distribution)\n포아송분포의 확률질량함수는 다음과 같습니다: \\[\n    f(y; \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; \\lambda) = \\exp\\left\\{ y \\log\\lambda - \\lambda - \\log(y!) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\log\\lambda\\),\nDispersion parameter: \\(\\phi = 1\\),\n\n\\(b(\\theta) = e^\\theta\\),\n\n\\(b'(\\theta) = \\frac{n e^\\theta}{1 + e^\\theta} = np\\),\n\n\\(b''(\\theta) = \\frac{n e^\\theta}{(1 + e^\\theta)^2} = np(1-p)\\),\n\n\\(c(y, \\phi) = -\\log(y!)\\).\n\n(4) 감마분포 (Gamma Distribution)\n감마분포의 확률밀도함수는 다음과 같습니다: \\[\n    f(y; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} y^{k-1} e^{-\\frac{y}{\\theta}}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; k, \\theta) = \\exp\\left\\{ -\\frac{y}{\\theta} + (k-1)\\log y - k\\log\\theta - \\log\\Gamma(k) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = -\\frac{1}{\\theta}\\),\nDispersion parameter: \\(\\phi = \\frac{1}{k}\\),\n\n\\(b(\\theta) = -\\log(-\\theta)\\),\n\n\\(b'(\\theta) = \\frac{1}{\\theta} = \\mu\\),\n\n\\(b''(\\theta) = \\frac{1}{\\theta^2} = \\mu^2\\),\n\n\\(c(y, \\phi) = (k-1)\\log y - \\log\\Gamma(k)\\).\n\n위에서 \\(b(\\theta)\\)는 한 번 미분하면 mean, 두 번 미분하면 variance의 term과 관련됨을 언급하였고 위 4개의 분포에서 원래 알고 계신 mean, variance와 \\(b'(\\theta)\\), \\(b''(\\theta)\\)가 dispersion parameter를 고려하면 일치한 것을 확인하실 수 있습니다. 이는 cumulant generating function의 일부이기 때문이며, 따라서 평균과 분산 관계를 정의하는 항이라고 언급하였던 것입니다.\nCanonical Link\n\nCanonical link는 GLM에서 통계적 성질을 최적화하기 위해 사용되는 링크 함수(link function)로, 다음과 같이 정의됩니다:\n\\[\ng(\\mu_i) = g(b'(\\theta_i)) = \\theta_i = \\eta_i\n\\] 이 식의 의미는 결국 아래 식과 같습니다. \\[\ng = (b')^{-1}\n\\]\n아래에서 확인하겠지만, Binomial 분포의 경우 canonical link는 logit 함수이고, Poisson 분포의 경우 canonical link는 log 함수이며, Canonical link를 사용하면 MLE(Maximum Likelihood Estimation) 과정이 단순화되고, efficient한 추정치를 얻을 수 있기 때문에 link function은 거의 항상 Canonical link로 정의합니다. 또한, canonical하지 않은 link function의 경우에도 위 Exponential Family distribution에서 식 조작을 통해 canonical link 형태를 만들 수 있습니다.\n1.3. GLM 예시\n\n위 철학에 따라, data가 따르는 Exponential Family 중 특정 분포가 정해지면, 이에 해당하는 보통 사용하는 Link function(Canonical link), Variance function가 정해져 있고 결국 모델이 특정되며, GLM은 이렇게 특정될 수 있는 모든 모델에서 공통적으로 parameter와 그 variance를 추정해내는 general한 모델이라고 생각할 수 있습니다. 여기에서 다루지는 않겠지만, 사실 특정한 형태의 data에서 가능한 link function은 여러 개이며, 이에 따라 variance function도 여러 가지가 가능할 수 있습니다. 그러나 효율성과 computation cost를 고려하여 보통 사용되는 function forms는 정해져 있다고 알아두시면 좋을 것 같습니다. 아래 예시 중 대표적으로 Binomial 예시에서는 link function이 0 이상 1 이하의 정의역에서 실수 전체(for linear predictor)를 map할 수 있는 미분 및 역이 가능한 함수이면 되지만, 보통 logit function이 사용됩니다. 헷갈릴 수 있지만 아래 Exponential Family 중 친숙한 분포의 예시를 직관적인 관점에서 고려하여 위에서 얻은 Exponential Family의 form과 같은 결과가 나옴을 보시면 좋을 것 같습니다.\n아래의 예시에서 linear predictor는 공통이므로 미리 정의하고 각각의 link function, variance function은 어떻게 특정되는지를 보겠습니다.\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}\n\\]\nwhere \\(\\eta_i\\)는 linear predictor, \\(\\beta_0, \\beta_1, ..., \\beta_p\\)는 regression coefficients(parameters), \\(x_{1i}, ..., x_{pi}\\)는 predictor variables(독립변수) 입니다.\nBinomial Case\n\nBinomial Data, 즉 종속변수가 \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\)인 data의 경우, 종속변수의 기댓값의 sample space 또한 0~1이며, 우리가 모델링하고 싶은 값이 \\(Y_i/n_i\\)인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, \\(E(Y_i / n_i) = p_i\\)이고, 분산은 \\(\\frac{1}{n_i} p_i (1 -p_i)\\) 입니다. \\(Y_i/n_i\\)의 variance 식에 \\(Y_i/n_i\\)의 mean이 들어감을 알 수 있고, 기존 LM에서는 이렇게 관측치에 따라 다르게 variance를 고려할 수 없었지만, GLM에서는 이 관계를 variance function을 통해 고려할 수 있으며, 식은 다음과 같습니다: \\[\nV(\\mu_i) = \\mu_i (1 - \\mu_i)\n\\] 또한, \\(Y_i / n_i\\)와 linear predictor를 matching 해줄 수 있는 미분가능한 function을 link로 고려해야 하고, Binomial에서는 non-linear link function으로 대부분 logit 함수를 사용합니다. (이때, logit function의 inverse는 sigmoid function입니다.)\n\\[g(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1 - \\mu_i} \\right)\\]\n이 식은 위에서 확인한 이항분포의 canonical parameter와 같은 형태임을 알 수 있습니다.\nPoisson Case\n\nPoisson Data, 즉 종속변수가 \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\)인 data이고 우리가 모델링하고 싶은 값이 종속변수 \\(Y_i\\)인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, \\(E(Y_i) = \\lambda_i\\)이고 분산은 \\(\\lambda_i\\) 이므로, 마찬가지로 \\(Y_i\\)의 variance 식에 \\(Y_i\\)의 mean이 들어감을 알 수 있고, variance function은 다음과 같습니다:\n\\[\nV(\\mu_i) = \\mu_i\n\\] 또한, \\(Y_i\\)의 sample space는 0 이상의 실수로, 이와 linear predictor를 matching 해줄 수 있는 미분가능한 non-linear function을 link로 고려해야 하고, Binomial에서는 link function으로 대부분 log 함수를 사용합니다. (inverse는 지수 함수.) \\[\ng(\\mu_i) = \\log(\\mu_i)\n\\]이 식은 위에서 확인한 포아송분포의 canonical parameter와 같은 형태임을 알 수 있습니다. 위 두 예시에서는 어떻게 GLMs의 구성 요소들이 선택되는지 직관적으로 보았고, 이는 이해를 돕기 위한 해석이었으며 이미 위에서 Exponential Family에 포함됨을 보일 때 같은 결과가 나왔다는 것을 보시면 됩니다. 위 Binomial Data의 GLM은 Logistic Regression, Poisson Data의 GLM은 Poisson Regression으로도 불립니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#glms-추정",
    "href": "posts/2025-02-28-reg2/index.html#glms-추정",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "2. GLMs 추정",
    "text": "2. GLMs 추정\n위 내용들을 통해서 GLMs가 어떻게 비정규분포를 갖는 종속변수를 고려해서 잘 작동하며, 어떠한 함수(Link function, Variance function, Exponential Family, Canonical Link)가 어떠한 수식과 철학으로 GLM을 구성하고 있는지 확인할 수 있었습니다. GLMs은 거의 대부분의 고려 가능한 data 분포가 Exponential Family를 따르며, 이에 대해 일관적인 form과 parameter estimation이 가능하기 때문에 아주 powerful한 Regression Model입니다. 그러나 어떻게 Exponential Family를 따르는 data를 다룰 수 있는지는 확인할 수 있었지만, 어떻게 Regression Model의 parameter와 그 분산을 추정할 수 있는지는 다루지 않았습니다. Linear Model에서는 closed-form solution을 쉽게 찾을 수 있었지만, GLM은 대부분의 경우(있는 경우도 있습니다.) 이러한 closed-form이 없어 컴퓨터 프로그램으로 여러 번에 걸쳐 추정할 수 있도록 알고리즘을 구현하여 이를 추정합니다. 실제로 이 estimation의 수식과 실제 구현 과정을 다루기 위해서는 긴 증명 과정을 거치는데, 최대한 중요한 부분은 빠지지 않으면서 증명해보겠습니다. 우선, MLE로 모델을 추정하는 과정을 증명하기 위해 필요한 두 가지 유용한 성질을 살펴보겠습니다. (Derivatives of Log Likelihood’s, Exponential Family 성질)\n2.1. Derivatives of Log Likelihood’s 성질\n\n확률변수 \\(Y\\)의 밀도 함수 \\(f(y; \\theta)\\)가 주어지며, 이때 \\(\\theta\\)는 스칼라 매개변수라고 가정하겠습니다. 또, \\(\\ell\\)이 \\(\\theta\\)에 대해 최소 두 번 미분 가능하다고 가정하면, 단일 관측치 \\(Y\\)에 대한 로그 가능도(log likelihood) 함수 \\(\\ell(\\theta; Y)\\)에 대해서 함수의 첫 번째 및 두 번째 도함수는 다음과 같습니다.\n\n\n첫 번째 도함수: \\[ \\ell' = \\frac{d\\ell}{d\\theta} \\]\n\n\n두 번째 도함수: \\[ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} \\]\n\n\n이때, 이 두 함수들은 다음 관계식이 성립합니다.\n\\[\nE \\{ \\ell'(\\theta; Y) \\} = 0\n\\]\n\\[\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n\\]\n이를 증명해보겠습니다. 의지를 잃지 않기 위해 위 식들의 의미를 스포하자면, MLE를 통해 모델을 추정할 때 보통 log likelihood를 모델의 parameter로 미분한 식이 0(또는 영벡터)이 되도록 하는 parameter를 찾음으로써 이를 수행하는데, 첫 번째 식은 이 미분한 식(score function)의 기댓값(평균)이 0이라는 의미이고, 두 번째 식은 첫 번째 식에서 mean이 0이었으므로 왼쪽항의 제곱 안에 -0을 넣어주면 \\[\nE\\left[ \\{ \\ell'(\\theta; Y) - E [ \\ell'(\\theta; Y) ] \\}^2 \\right]\n\\]\n가 되어 분산 term이 되고, 따라서 분산은 이차 도함수의 기댓값의 음수와 같다는 의미입니다. 이러한 성질들을 이용해서 앞으로 Likelihood 기반의 다양한 모델 추정을 수행할 수 있게 됩니다.\n(1) Prove \\(E \\{ \\ell'(\\theta; Y) \\} = 0\\).\n우선, 확률 분포는 모든 범위에서의 적분 또는 누적합이 1이므로,\n\\[\n1 = \\int f(y; \\theta) dy\n\\]\n입니다. 이제 양변을 \\(\\theta\\)에 대해 미분한 후 미분과 적분의 순서를 바꾸면,\n\\[\n0 = \\frac{d}{d\\theta} \\int f(y; \\theta) dy \\\\\n=\\int \\frac{d}{d\\theta} f(y; \\theta) dy\n\\]\n입니다. 여기서 수학적 증명 과정에서 굉장히 자주 사용되는 skill \\(\\frac{d}{d\\theta} f(y; \\theta) = \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta)\\)를 사용하면\n\\[\n0 = \\int \\frac{d}{d\\theta} f(y; \\theta) dy  \\\\\n= \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy \\\\\n= \\int \\ell'(\\theta; Y) f(y; \\theta) dy \\\\\n= E \\{ \\ell'(\\theta; Y) \\}\n\\] 입니다. 의미를 다시 해석해보면, 어떠한 분포를 따르는 \\(Y\\)와 이의 매개변수 \\(\\theta\\)에 대해서, 우리는 MLE를 통해 log likelihood 함수 \\(\\ell(\\theta; Y)\\)를 \\(\\theta\\)로 미분하였을 때 0이 나오도록 하는 \\(\\hat{\\theta}\\)를 찾음으로써 parameter를 estimate합니다. 위 (1)은 이러한 \\(\\ell'(\\theta; Y)\\)의 기댓값은 \\(Y\\)의 분포가 이계도함수가 존재한다면 어떤 분포이건 관계 없이 0임을 보인 것입니다.\n(2) Prove \\(\\ell'' = \\frac{d^2\\ell}{d\\theta^2}\\).\n동일한 논리를 따라 위 식을 한 번 더 미분하면,\n\\[\n0 = \\frac{d}{d\\theta} \\left[ \\int \\frac{d}{d\\theta} \\{ \\log f(Y; \\theta) \\} f(y; \\theta) dy \\right]\n\\]\n입니다. 두 함수의 곱 형태의 미분이며 둘 다 \\(\\theta\\)를 포함하므로 이를 전개하고 마찬가지로 기댓값으로 표기하면,\n\\[\n0 = \\int \\frac{d^2}{d\\theta^2} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy + \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} \\frac{d}{d\\theta} f(y; \\theta) dy \\\\\n= E \\{ \\ell''(\\theta; Y) \\} + E \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right]\n\\]\n이고, 따라서 아래 식이 증명되었습니다.\n\\[\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n\\]\n위 증명 과정에서 미분과 적분 연산자의 교환을 정당화하는 과정의 설명이 생략되었지만 exponential family에선 문제가 없고, Y가 discrete한 경우는 적분을 누적합으로 바꿔주면 된다고 얘기해두며 마무리 하겠습니다. 또한, 위 증명에서는 \\(\\theta\\)가 1차원 스칼라 변수라고 가정했지만, 다차원 매개변수에 대해서도 동일한 결과가 성립됩니다. 식의 의미를 마지막으로 되짚어보면, log likelihood의 일차 도함수는 기대값이 0이고, 이 일차 도함수의 공분산 행렬은 이차 도함수 행렬의 기대값의 음수에 해당합니다. 이 값은 피셔 정보 행렬(Fisher Information Matrix)이라고도 불립니다. (함수의 기댓값이라는 말이 어색하게 들릴 수도 있는데, 애초에 모든 랜덤(확률)변수는 어떠한 관측치에 대해서 실수를 output하는 함수임을 되새기면 좋을 것 같습니다.)\n2.2 Exponential Family 성질\n\n이번에는 위에서 증명한 수식을 통해서 Exponential Family를 소개할 때 언급한 \\(E(Y) = b'(\\theta) = \\mu\\), \\(\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)\\)을 증명할 것입니다. Exponential Family distribution은 다음과 같은 일반적인 형식으로 정의됩니다.\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\}\n\\] 때문에 log likelihood는 단순하게 아래와 같이 도출됩니다.\n\\[\n\\ell(y; \\theta, \\phi) = \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\n\\] 이제 이 log likelihood의 derivatives를 계산하면 다음과 같습니다:\n\n첫 번째 도함수 (Score Function): \\[\n\\ell' (y; \\theta, \\phi) = \\frac{y - b'(\\theta)}{a(\\phi)}\n\\]\n두 번째 도함수 (Observed Information): \\[\n\\ell'' (y; \\theta, \\phi) = \\frac{-b''(\\theta)}{a(\\phi)}\n\\]\n\n이 두 함수를 통해서 위에서 유도한 두 공식을 활용하면 다음 두 수식을 얻을 수 있습니다.\n\\[\nE \\left\\{ \\frac{Y - b'(\\theta)}{a(\\phi)} \\right\\} = 0\n\\]\n\\[\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{b''(\\theta)}{a(\\phi)}\n\\] 이때, 식을 잘 보면 첫 번째 식은 결국 \\[E[Y - b'(\\theta)] = E[Y] - b'(\\theta) = 0\\] 이 되어 \\(E\\{ Y \\} = b'(\\theta)\\)을 얻을 수 있고, 두 번째 식에서 \\[\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{E[(Y - b'(\\theta))^2]}{E[a(\\phi)^2]} = \\frac{\\operatorname{Var}(Y)}{a(\\phi)^2} = \\frac{b''(\\theta)}{a(\\phi)}\n\\] 이므로, \\(\\text{Var}(Y) = b''(\\theta) a(\\phi)\\)임을 보일 수 있습니다.\n증명한 수식을 다시 한 번 확인하면,\n\\[\nE(Y) = b'(\\theta) = \\mu\n\\]\n\\[\nVar(Y) = a(\\phi) V(\\mu_i) = a(\\phi)b''(\\theta)\n\\]\n2.3. GLMs’ parameter 추정식 유도\n\n이제 필요한 식이 준비되었으니, 위에서 계속 다루고 있는 log likelihood을 이용해서 MLE estimation으로 GLMs’ parameter을 추정하는 과정을 살펴볼 것입니다. 이때, 추정 과정은 계속 언급한대로 Exponential Family distribution을 따르는 종속변수에 대해서 log likelihood의 model parameter에 대해 미분한 식이 (parameter가 벡터이므로, 좀더 엄밀하게 정의해야 하지만, 의미는 같으니 이렇게 얘기하겠습니다.) 0이 되게 하는 parameter를 찾음으로써 수행되며, 이 때의 함수 (log likelihood의 1차 도함수)를 앞으로는 score function이라고 부르겠습니다.\n우리는 MLE estimation을 통해 여러 Exponential Family distributions에 대해 통일된 estimation algorithm으로 parameter를 추정할 수 있습니다. (이러한 분포 가정 마저 없다면, 3장 GEE에서 보겠지만 분포에 대한 직접적 가정없이 cumulative generating function 등 몇 함수 만으로 Likelihood를 고려하는 Quasi-likelihood Estimation의 개념으로 이어집니다.)\n주어진 data가 \\((y_1, ... , y_n)\\)일 때, 위에서부터 계속 사용해왔던 log-likelihood function은 다음과 같습니다.\n\\[\nl = \\sum_{i=1}^{n} \\left( \\frac{y_i \\theta_i - b(\\theta_i)}{\\phi_i} + c(y_i, \\phi_i) \\right)\n\\]\n지금까지 우리는 \\(\\theta\\)로 log likelihood를 다뤘지만, 추정해야 하는 parameter는 \\(\\boldsymbol{\\beta}\\)입니다. \\(\\boldsymbol{\\beta}\\)는 벡터이기 때문에 이 중 하나의 파라미터 \\(\\beta_j\\)에 대해 먼저 log likelihood를 미분해보면 아래와 같은 식이 나옵니다. (\\(\\text{Var}(y) = \\phi_i V(\\mu_i)\\), \\(\\frac{\\partial \\mu}{\\partial \\eta} = \\frac{1}{g'(\\mu_i)}\\))임은 위에서 보았습니다. g는 link function이었습니다.)\n\\[\n\\frac{\\partial l}{\\partial \\beta_j} = s(\\beta_j)\n\\]\n\\[\n= \\left(\\frac{\\partial l}{\\partial \\theta} \\right) \\left(\\frac{\\partial \\theta}{\\partial \\mu} \\right) \\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right) \\left(\\frac{\\partial \\eta}{\\partial \\beta_j} \\right)\n\\]\n\\[\n= \\frac{y_i - \\mu_i}{\\text{Var}(y_i)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) x_{ij}, \\quad \\text{or}\n\\]\n\\[\n= \\sum_{i=1}^{n} \\frac{y_i - \\mu_i}{\\phi_i V(\\mu_i)} \\times \\frac{x_{ij}}{g'(\\mu_i)} = 0\n\\]\n식이 혼란스러울 수 있는데, 이는 단지 chain rule을 이용해서 \\(\\beta\\)에 대한 \\(\\ell\\)의 기울기를 구하는 과정이며, 위에서 GLM을 구성하는 과정을 차근차근 복기하면 각각의 변화율은 다음과 같이 구할 수 있기 때문에 최종 식을 얻을 수 있었음을 알 수 있습니다.\n\\[\n\\frac{\\partial l}{\\partial \\theta} =\n\\frac{y - b'(\\theta)}{a(\\phi)} =\n\\frac{y - \\mu}{a(\\phi)}\n\\]\n\\[\n\\frac{\\partial \\theta}{\\partial \\mu} =\n\\frac{1}{b''(\\theta)} =\n\\frac{1}{V(\\mu)} =\n\\frac{a(\\phi)}{\\text{Var}(y)}\n\\]\n\\[\n\\frac{\\partial \\eta}{\\partial \\beta_j} = x_{ij}\n\\]\n이제 이 score function의 음의 미분(또는 분산)의 기댓값을 전개하면 다음과 같습니다.\n\\[\n- E \\left( \\frac{\\partial^2 l}{\\partial \\beta_j \\partial \\beta_k}\n\\right) = E \\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right)\n\\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right]\n\\]\n\\[\n= E \\left[ \\left( \\frac{y - \\mu}{\\text{Var}(y)} \\right)^2\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right]\n\\]\n\\[\n= E \\left[ \\frac{\\text{Var}(y)}{\\text{Var}(y)^2}\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right]\n\\]\n\\[\n= \\frac{1}{\\text{Var}(y)} \\left(\\frac{\\partial \\mu}{\\partial \\eta}\n\\right)^2 x_{ij} x_{ik}\n\\]\n위 term은 Fisher Information matrix라고도 부르며, 이 term이 분산의 기댓값인 이유를 생각해보면, 이전에 구한 derivatives of log likelihood의 성질들에 의해 \\(\\ell\\)의 negative 2차 도함수의 기댓값은 1차 도함수의 기댓값의 square와 같고, 이 1차 도함수의 기댓값이 0이므로 이는 분산과 같습니다. 정리하자면, score function의 미분식이 score function의 분산과 기댓값이 같으므로, 미분을 직접하는 대신 분산으로 근사 후 식을 전개한 것이며, 이 때 근사한 이 Matrix를 Fisher Information matrix라고 부릅니다.\n이들을 한 번에 벡터와 행렬 연산으로 표현하면 다음과 같습니다:\n\\[\n\\frac{\\partial l}{\\partial \\beta} = X^\\top A (y - \\mu)\n\\]\n\\[\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) =\n-X^T W X\n\\]\n\\[\nwhere, W \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right)^2,\n\\]\n\\[\n\\text{and }A \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right).\n\\]\n이제 우리는 score function와 그 미분, 또는 log likelihood의 1차 도함수와 (negative) 2차 도함수의 추정식을 얻었습니다. 사실, \\(X^\\top A (y - \\mu)\\)가 0이 되도록 하는 parameter \\(\\beta\\)만 찾으면 parameter 추정이 끝나지만, 이는 closed-form solution이 존재하지 않기 때문에 그 미분(2차 도함수)식을 이용해 근사적으로 구할 수 있는 알고리즘을 최종적으로 다룰 것입니다. (물론 후에 보겠지만 이 Fisher Information matrix는 분산과도 관련이 있습니다.) 즉, 우리는 추정식을 유도하는 것은 완성했지만, 실제로 알고리즘을 설계하여 어떻게 이를 추정할 지에 대해서는 모르는 상태이고, 때문에 최종적으로 GLM을 제안한 학자가 소개하였으며 대부분의 패키지에서 이 GLM을 estimate하기 위해 사용하고 있는 method인 IRLS(Iteratively Reweighted Least Squares) Algorithm을 살펴볼 것입니다. (negative) 2차 도함수의 추정식(or Fisher Matrix) \\(X^T W X\\)을 유도한 이유는, 이 알고리즘에서 필요로 하기 때문이고, 위 식에서 \\(W, A\\)는 식이 복잡해보이지만, 그저 observations(data) 하나 당 GLM 모델의 구성요소를 통해 determinant하게 미리 계산되어 대각성분으로 각각 들어가는 term임을 명심하시면 좋을 것 같습니다. (이전 설명에서, 종속변수의 분포로 Exponential Family 중 특정 분포가 정해지면, 이에 따라 Link function(Canonical link), Variance function가 정해져 모델이 특정된다고 설명드린 적이 있고, 위 \\(W, A\\)모두 이 두 함수로 이루어진 식이기 때문에 관측치마다 각각 넣으면 determinant하게 하나의 값이 나오는 식인 것입니다.)\n2.4. GLMs’ parameter 추정 (IRLS)\n\n위에서 언급하였듯, GLM의 MLE estimation은 비선형 최적화 문제이기 때문에 공통된 framework에서 사용할 수 있는 closed-form solution이 존재하지 않으며, 대신 여러 최적화 방법을 사용할 수 있습니다. 뉴턴-랩슨 방법(Newton-Raphson Method)은 2차 도함수(Hessian Matrix)를 사용하여 score function을 수렴시키지만, Hessian Matrix \\(H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}\\)를 직접 구해야 하고, 이는 계산이 복잡하여 computation cost가 큽니다. Fisher Scoring은 Newton Method에서 Hessian Matrix 대신 이를 근사하는 Fisher Information Matrix를 사용합니다. 이는 이전에 Derivatives of log likelihood 의 성질이랑 추정식 유도에서 모두 보았던대로 2차 도함수가 1차 도함수의 분산(또는 제곱)와 기댓값이 같다는 수학적 성질을 토대로 \\[\nE \\left(\n\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right]\n= X^T W X\\\n\\] 가 만족함을 확인하였기 때문에, MLE추정에서 Newton-Raphson Method보다 computation cost가 합리적인 method라고 생각해 볼 수 있습니다. 이외에도 경사 하강법(Gradient Descent) 알고리즘은 1차 도함수(Gradient)만 사용하여 특정 값만큼 조금씩 점진적으로 parameter를 움직여 최적점을 찾는 방법입니다. 모델이 매우 복잡해서 2차 도함수를 계산하기 힘든 딥러닝에서는 많은 경우 이를 발전시킨 여러 methods로 iterativaly하게 parameter를 추정합니다. (이렇게 iterative하게 model’s parameter를 움직이면서 추정하는 과정이 AI에서 얘기하는 learning입니다.)\n위에서 얘기한 IRLS(Iteratively Reweighted Least Squares) Algorithm는 이 Fisher Scoring의 알고리즘적 변형으로, Fisher Scoring의 식을 가중 최소제곱(Weighted Least Squares) 문제로 치환하여, 이 문제에서 사용하는 IRLS 알고리즘으로 GLM의 parameter 해를 구하는 method입니다. 우선 Newton-Raphson Method, Fisher Scoring에 대한 이야기를 간단하게 하고, 자세하게 어떻게 IRLS가 GLM의 parameter를 추정하는지 보겠습니다.\nNewton-Raphson Method & Fisher Scoring\n\nGLM (Generalized Linear Model)의 파라미터 추정을 위한 최적화 과정은 우선 log likelihood function의 최대화를 목적으로 합니다. 이때, Newton-Raphson Method와 그 변형인 Fisher Scoring은 모두 이를 위한 알고리즘입니다.\nNewton-Raphson 방법은 다음과 같은 일반적인 업데이트 식을 갖습니다:\n\\[\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]^{-1} \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}},\n\\] 이후의 method들은 모두 이 method에서 기인하므로, 위 식의 의미를 이해하는 것은 매우 중요합니다. 위 식이 어떻게 안정적으로 \\(\\boldsymbol{\\beta}\\)를 수렴시킬 수 있는지 2차 테일러 전개를 통해 수식적으로 좀 더 명확히 볼 수 있지만, 여기서는 좀더 직관적으로 가볍게 이해해보겠습니다.\nNewton-Raphson 방법은 어떠한 함수 \\(f(x)\\)에 대해서 함수의 해, 즉 \\(f(x) = 0\\)을 만족하는 \\(x\\)를 찾기 위한 반복적인 근사 방법입니다.(informal한 증명이므로 1-dimension case로 보겠습니다.) 이 방법은 현재 점에서 함수의 접선을 그려 \\(x\\)축과 만나는 점을 다음 근사해로 사용합니다. 예를 들어, 초기값 \\(x_0\\)에서 접선을 그리면 그 접선의 방정식은 \\(y = f'(x_0)(x - x_0) + f(x_0)\\)입니다. 이 접선이 \\(x\\) 축과 만나는 점은 \\(y = 0\\)일 때이므로, 이를 대입하여 접선이 0이 되는 점을 구해보면 \\(x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\\)가 됩니다. 그러나 접선은 함수의 선형 approximation이므로 이 접선이 0이 되는 점 \\(x_1\\)이 실제 함수에서도 곧바로 0이 되지는 않습니다. 따라서 이 과정을 반복하여 특정 단계에서 \\(f(x_n)\\)이 0에 충분히 가까워지면, \\(x_n\\)을 근사해로 채택합니다.\n이 방법이 작동하는 이유는 접선의 기울기 \\(f'(x_n)\\)이 함수의 곡률을 반영하기 때문입니다. 곡률이 클수록(기울기가 가파를수록) 업데이트의 크기가 작아지고, 곡률이 작을수록 업데이트의 크기가 커집니다. 또한, Newton-Raphson 방법은 2차 수렴(Quadratic Convergence) 속도를 가집니다. 이는 오차가 반복마다 제곱으로 줄어들기 때문에 매우 빠르게 해에 수렴한다는 의미입니다.\n이정도로 간단하게 Newton-Raphson method를 이해할 수 있고, 다시 돌아와서 위 식에서는 multi-dimention 상황에서 해를 찾고 싶은 함수가 score function, 즉 \\(\\frac{\\partial  \\ell}{\\partial \\beta}\\)인 경우이기 때문에 위처럼 식이 구성되었다는 것을 알 수 있습니다.(f의 미분이 분모로 들어간 term은 행렬에서 역행렬 -1과 같은 의미라고 보시면 됩니다.) 첨언하자면, 이 경우 Newton-Raphson method는 두 가지 중요 조건이 붙는데, 언급만 하자면 Hessian matrix가 Positive-definite (볼록) 해야 하며, 초기값이 최적점에 충분히 가까워야 합니다.\nFisher Scoring은 Hessian 행렬 대신 Fisher Information 행렬 \\(\\mathcal{I}(\\boldsymbol{\\beta})\\)를 사용하여 다음과 같이 업데이트합니다:\n\\[\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\left( \\mathcal{I}(\\boldsymbol{\\beta}^{(t)}) \\right)^{-1} \\mathbf{S}(\\boldsymbol{\\beta}^{(t)}),\n\\]\n여기서 \\[\n\\mathbf{S}(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}}\n\\] 는 Score function으로 구한 (Fisher) score 벡터이고, \\[\n\\mathcal{I}(\\boldsymbol{\\beta}) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right] = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]\n\\] 는 Fisher information matrix입니다.\n즉, Fisher Scoring 방법은 뉴턴-랩슨 방법(Newton-Raphson Method)에서 Hessian Matrix \\(H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}\\) 를 사용하는 대신, Fisher information matrix를 사용해서 업데이트를 수행함으로써 parameter를 estimation하는 매커니즘입니다. IRLS는 GLM에서 이 Fisher Scoring와 거의 일치하다 봐도 무방하며, 단순히 위에서 추정한 \\(\\mathbf{S}(\\boldsymbol{\\beta})\\)와 \\(\\mathcal{I}(\\boldsymbol{\\beta})\\)를 Fisher Scoring 공식에 넣으면 weighted least squares problme(가중 최소제곱 문제)와 완전히 유사해지기 때문에, 이 문제를 해결하는 방식으로 parameter를 추정한다는 의미라고 생각하시면 될 것 같습니다. 좀 더 수식과 같이 자세하게 설명드리겠습니다.\nIRLS (Iteratively Reweighted Least Squares) Algorithm\n\n앞서, 2.3.에서 log likelihood의 gradient(1차 도함수, Score function)와 expected Hessian(Fisher Information matrix)가 각각\n\\[\n\\frac{\\partial l}{\\partial \\beta} = X^T A (y - \\mu)\n\\]\n\\[\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = X^T W X\n\\]\n\\[\nwhere, \\quad A = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) \\; and \\quad W = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\n\\]\n임을 보았습니다. 이 결과들을 Fisher Scoring method의 식에 대입하면,\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n\\]\n이고, 이 equation은 가중 최소제곱 문제(Weighted Least Squares, WLS)의 parameter 추정식과 일치하기 때문에 비선형 GLM의 parameter 추정을 WLS problem으로 치환할 수 있음을 알 수 있습니다.\n위의 업데이트 식은 아래 working response \\(z\\)를 정의하면\n\\[\nz = X \\beta^{(t)} + W^{-1} A (y - \\mu),\n\\]\n아래와 같이 표현할 수 있습니다.\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n\\] \\[\n= \\left( X^T W X \\right)^{-1}\\left( X^T W X \\right)\\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T W W^{-1} A (y - \\mu)\n\\] \\[\n= \\left( X^T W X \\right)^{-1}X^T W \\left( X \\beta^{(t)} + W^{-1} A (y - \\mu) \\right)\n\\]\n\\[\n= \\left( X^T W X \\right)^{-1} X^T W z\n\\]\n또 강조하지만, 이는 가중치 행렬 \\(W\\)에 따라 각 관측치의 기여도를 달리하는 선형 회귀 문제(가중 최소제곱 문제)의 정규방정식과 동일합니다:\n\\[\n(X^T W X) \\beta = X^T W z.\n\\]\n따라서 해당 정규방정식의 \\(\\beta\\)를 가중 최소제곱 문제 방식으로 풀어냄으로써 추정치 \\(\\beta^{(t+1)}\\)를 구할 수 있으며, 이는 현재 단계의 추정치 \\(\\beta^{(t)}\\)에서의 예측값과 오차를 반영한 새로운 업데이트가 됩니다.\nIRLS 구체적 절차\n\n정리하자면, IRLS(Iteratively Reweighted Least Squares) 알고리즘은 위의 아이디어를 바탕으로 GLM의 최대우도추정 문제를 반복적으로 가중 최소제곱 문제로 전환하여 해결합니다. 구체적인 단계는 다음과 같습니다:\n(1) 초기화: 초기 파라미터 \\(\\beta^{(0)}\\)를 설정합니다.\n(2) 현재 단계 계산:\n(2.1) 예측값 계산: 현재 추정치 \\(\\beta^{(t)}\\)을 이용하여 선형 예측치 \\(\\eta^{(t)} = X \\beta^{(t)}\\)를 구하고, link 함수의 역함수를 통해 \\(\\mu^{(t)} = g^{-1}(\\eta^{(t)})\\)를 계산합니다.\n(2.2) 가중치 및 보조 행렬 계산: 정의한 식에 따라\n\\[\nA^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t)}\n\\]\n\\[\nW^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t) 2}\n\\]\n을 계산합니다.\n(2.3) Working Response 구성:\n\\[\nz^{(t)} = X \\beta^{(t)} + \\left( W^{(t)} \\right)^{-1} A^{(t)} (y - \\mu^{(t)}).\n\\]\n(3) 가중 최소제곱 문제 해결:\n위의 working response와 가중치 행렬을 사용하여 정규방정식\n\\[\n(X^T W^{(t)} X) \\beta^{(t+1)} = X^T W^{(t)} z^{(t)}\n\\] 을 풀어 새로운 추정치 \\(\\beta^{(t+1)}\\)를 구합니다.\n(4) 수렴 판단 및 반복:\n\\(||\\beta^{(t+1)} - \\beta^{(t)}||\\)(L1 norm, 쉽게는 절댓값)가 미리 설정한 임계값 이하가 될 때까지 2번과 3번의 단계를 반복하고, 수렴이 되었다면 이 \\(\\beta^{(t+1)}\\) 값이 GLM의 parameter에 대한 IRLS의 최종 Estimation 결과입니다. 결국, Fisher Scoring에서 대입한 수식이 결국 가중 최소제곱 문제로 귀착됨을 통해, IRLS 알고리즘은 각 반복마다 선형 회귀 문제와 유사한 방식으로 parameter를 업데이트합니다.\n이 IRLS의 소프트웨어 구현에 대한 첨언을 하자면, 보통 IRLS에서는 각 단계마다 위 3단계와 같이 아래 정규방정식을 풉니다:\n\\[\n(X^T W X) \\beta = X^T W z.\n\\]\n이때 직접 \\((X^T W X)^{-1}\\)를 구해 업데이트하는 방법은 계산적으로 불안정할 수 있습니다. 특히, 데이터의 규모가 크거나 \\(X\\) 행렬이 ill-conditioned(조건수가 열악한)인 경우에는 직접 역행렬을 계산하는 과정에서 수치적인 문제가 발생할 위험이 큽니다. 따라서, 구현의 영역이기 때문에 더이상 나열하지는 않겠지만 실제는 역행렬을 direct하게 구하는 대신, QR 분해나 Cholesky 분해 같은 선형대수 기법을 활용하여 안정적으로 선형 시스템을 풀 수 있습니다.\n2.5. GLMs’ parameter Variance\n\n앞서 IRLS(Iteratively Reweighted Least Squares) 알고리즘으로 GLM의 파라미터를 추정하는 과정을 살펴보았습니다. 이때 우리는 MLE(최대우도추정)을 IRLS(반복적으로 가중 최소제곱)문제로 전환하는 과정을 거쳤는데, 최종적으로 구해지는 추정치 \\(\\hat{\\boldsymbol{\\beta}}\\)의 분산에 대한 추정까지 마쳐야 유의성 검정 등의 분석을 수행할 수 있을 것입니다. GLM에서 최대우도추정(MLE)을 사용해 얻은 \\(\\hat{\\boldsymbol{\\beta}}\\)는, 이론적으로 위에서 언급한 Fisher 정보 행렬(Fisher information matrix)의 역행렬로써 구할 수 있습니다:\n\\[ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = \\bigl(\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})\\bigr)^{-1}, \\]\n여기서 \\(\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})\\)는 \\(\\hat{\\boldsymbol{\\beta}}\\)에서의 (observed 혹은 expected) Fisher 정보 행렬입니다.(3장에서 이 이유에 대해 살펴볼 것입니다.) 모델의 parameter를 추정하는 과정에서, 각 관측치 \\(i\\)에 대해 \\(\\mathrm{Var}(y_i)\\)와 \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\)가 포함된 특정 가중치 \\(W^{(t)}\\)가 등장하였었고, 반복(step)마다 업데이트되는 정규방정식을 풀어감으로써 추정치 \\(\\beta^{(t+1)}\\)를 얻었습니다. 이후 최종 수렴 시점(\\(t \\to \\infty\\))에서, 우리는 \\(\\hat{\\boldsymbol{\\beta}} = \\beta^{(\\infty)}\\)에 도달하게 되고, 이 시점에서 계산된 Hessian(또는 Fisher information) matrix \\(\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X}\\)에 대해서, 이 행렬의 역수가 분산이 되는 것입니다.\n즉, 실제 계산 시에는 아래와 같은 모양이 됩니다.\n\\[ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, \\]\n왜 최종적으로 추정된 모델의 분산이 수렴된 time step에서의 Fisher information matrix로 추정할 수 있는지에 대해서는 3장의 M-estimation에서 증명할 것입니다. 여기서 미리 강조할 것은, 위에서 얻는 분산의 추정값은 GLM에서의 기본 가정들이었던 독립성, 분산 함수의 형태 등이 성립한다는 조건 위에서 도출된 것이므로, 이러한 기본 가정이 어느 정도 엄격히 맞아떨어지는 상황(정확한 포아송 분포를 따르는 count data, 명시적으로 독립적인 개별 관측치 등)이라면 괜찮지만, 현실의 data에서는 이질분산, 클러스터 내 상관, 과산포(overdispersion) 등으로 인해 이 기본 가정들이 깨질 수 있습니다. 다행히도, GLM에서도 모델이 consist할 때(GLM의 위로부터 추정된 parameter 자체는 consist합니다.) HC(Heteroskedasticity-Consistent) se와, 아래에서 clustered data에서 고려할 수 있는 버전인 Cluster-robust standard errors를 사용하여 더욱 robust하게 분산을 추정할 수 있습니다. 때문에 아래에서는 Cluster-robust se를 OLS 버전으로 소개드린 후, GLM에서 사용하기 위해 \\(\\hat{\\mathbf{W}}\\) 행렬 \\(\\hat{\\mathbf{A}}\\) 행렬 등의 구조가 어떻게 수식적으로 첨가되어 LM(Linear Model)에서의 식과는 살짝 다른 모습을 취하게 되는지 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#cluster-robust-standard-errors",
    "href": "posts/2025-02-28-reg2/index.html#cluster-robust-standard-errors",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "3. Cluster-Robust Standard Errors",
    "text": "3. Cluster-Robust Standard Errors\n3.1. Clustered Data 정의\n\nClustered data란 데이터 내에서 동일 그룹에 속하는 관측치들이 상관관계를 가지는 경우를 의미합니다. 예를 들어, 한 환자의 여러 진료 기록이 서로 상관되어 있을 수 있습니다. 이 때, cluster간에는 상관관계가 없고 cluster 내의 데이터들은 상관관계가 있다는 가정하에 Cluster-robust standard errors나 3장의 GEE, GLMM model이 개발되었습니다. 의료 분석 상황의 예시로는 대표적으로 데이터의 각 환자 당 여러 시간 또는 주기에 걸쳐 측정한 데이터, 여러 학교나 병원과 같은 단체에서 얻은 데이터들을 한 번에 고려하는 경우가 있을 것입니다. 또한, cluster간에 상관관계가 있거나 cluster 안에 cluster가 있는 hierarchical의 경우도 있지만, 이에 대한 공식들은 위에서 고려하는 1차적인 상황을 이해하면 쉽게 이해할 수 있으며, 의료 분석에서 고려하는 피험자 내 관측치 간 상관관계, 병원 내 관측치 간 상관관계 등을 고려해야 하는 상황은 이번 블로그에서 이야기 할 1차적인 clustered 상황임을 알아두시면 좋을 것 같습니다. 이 observations간의 상관관계에 대한 이야기와, 이때 사용해야 하는 Regression Models에 대한 설명은 3장에서 GEE, GLMM과 함께 더욱 자세하게 다뤄볼 예정입니다.\n확실한 것은, 비선형 분포를 추정할 수 있는 GLM이나, OLS에서 안정적인 parameter 분산 추정 method였던 HC(Heteroskedasticity-Consistent) 표준오차는 관측치 간의 독립성을 가정하였었고, 이는 위와 같은 data를 다룰 때에는 깨져야 하는 가정이라는 것입니다. 이제 설명드릴 Cluster-robust standard errors는 HC se와 형태가 매우 비슷하며, 같은 철학으로 clustered data에서 robust한 모델 분산 추정 method입니다. 이때 기억하셔야 할 부분은, 1장에서는 HC se의 안정성을 Linear Regression에 대해서 고려하였고 R의 sandwich 패키지를 통해 구현할 수 있음을 보았는데, 이번 장에서 다루고 있는 GLM에서도 이 HC se, Cluster-robust se를 모두 사용할 수 있다는 것입니다. 이때 식이 LM과 GLM에서 살짝 다른데, 우선 Linear Model에서의 Cluster-robust se에 대해서 설명드리고, GLM에서는 무엇이 다른지 보겠습니다.\n실제 소프트웨어의 구현에 대해서 첨언하자면, R의 sandwich 패키지나 대부분의 패캐지에서는 이 두 robust 분산 추정의 계산 및 검정을 LM, GLM 모두에 사용 가능하고, 이 패키지들은 들어오는 모델의 객체가 LM, GLM임을 분류한 뒤 각각에 맞는 살짝 변형된 식으로 추정한다고 생각하시면 될 것 같습니다.\n3.2. Cluster-robust standard errors 정의 및 수학적 표현\n\nCluster-robust standard errors는 클러스터 내 상관관계를 고려하여 분산을 추정합니다. 이를 통해 클러스터 간 독립성은 유지하되, 클러스터 내 관측치 간 상관관계가 존재할 때도 일관된 추정치를 제공합니다. LM에서 Cluster-robust standard error를 구하는 식은 다음과 같습니다:\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n위 식에서 \\(g\\)는 클러스터 인덱스, \\(\\mathbf{\\hat{u}}_g\\)는 클러스터 \\(g\\)의 잔차 벡터입니다.이 식은 1장에서의 HC0과 아주 유사하며, 가운데 meat항 (두 \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\) 사이에 있는 항)만 달라졌음을 알 수 있습니다. 이는 실제로 HC0에서, 위에서 설명드린 가정인 1차적 clustered 구조(클러스터 간 독립성을 가정하지만, 클러스터 내 관측치들 간 상관관계는 허용)를 고려해서 \\(\\Phi\\) 항만 바뀌었음을 짐작해볼 수 있습니다. 이제 이 Cluster-robust standard errors의 철학에 대해서 구체적으로 살펴보겠습니다.\n3.3. Cluster-robust standard errors 수학적 표현\n\nLM에서는 이전에 봤던대로 parameter의 분산을 유도하면 다음과 같습니다:\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n여기서 \\(\\Phi\\)는 오차항의 공분산 행렬을 나타냈었습니다. HC0 (Heteroskedasticity-Consistent 0)에서는 모든 관측치가 서로 독립임을 가정하였기 때문에 (Heteroskedasticity를 고려하였지 dependent case를 고려하지는 않았었습니다.) 이에 따라 \\(\\Phi\\)는 대각행렬로 표현되며,\n\\[\n\\Phi_{\\text{HC0}} = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)\n\\]\n결과적으로 분산 추정량은 개별 관측치에 대해아래와 같이 계산하였었습니다.\n\\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\] 위 식은 1장의 HC0 식과 같은 식입니다. 표현이 어색하다고 느끼시는 분을 위해 이전에 사용한 식을 가져오면 \\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n이며, 위에서부터 얘기하고 있는 \\(\\hat{u}\\)는 이 error term \\(e\\)와 비슷한(잔차이기 때문에 사실 의미는 다릅니다) 의미입니다. HC0에서는 각 관측치만을 고려하기 때문에 \\(\\hat{u}_i\\)는 \\(e_i\\)와 같고, 길이가 1인 벡터, 즉 scalar이기 때문에 제곱을 사용하였지만 위 cluster-robust 식에서 사용한 \\(\\hat{u}_g\\)는 클러스터 g에 해당하는 모든 관측치를 한 줄로 나열한 임의의 길이의 벡터이기 때문에 제곱이 아니라 \\(\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top\\) term을 사용한 것입니다.\n이에 대한 이해를 바탕으로 Cluster-robust se의 meat term을 생각해보면, Cluster-robust에서는 cluster간은 독립적이고, cluster안의 관측치들은 상관관계를 가질 수 있다고 가정하기 때문에 각 cluser에 대해서 \\(\\Phi_i\\)를 \\(\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top\\)로 각각 구한 후, 전체 \\(\\Phi\\)는 아래와 같이 block diagonal 구조로 넣어준다고 이해할 수 있습니다. (block 행렬은 행렬을 특정한 block으로 나누었을 때 대각선 이외의 모든 행렬 블록이 영행렬인 행렬을 의미하며, cluster간의 독립을 block diagonal 구조로 고려하였다고 이해하면 됩니다.)\n\\[\n\\Phi_{\\text{cluster}} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & \\cdots & 0 \\\\\n0 & \\Phi_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\Phi_G\n\\end{pmatrix}\n\\]\n즉, 여기서 각 \\(\\Phi_g = E[\\mathbf{u}_g \\mathbf{u}_g^\\top]\\)는 클러스터 \\(g\\) 내의 오차의 공분산 행렬이고, 각 cluster에 대해 잔차\\(\\hat{\\mathbf{u}}_g\\)를 사용하여\n\\[\n\\widehat{\\text{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\] 와 같이 추정합니다.\n위 Cluster-robust의 meat항에 대한 이해는 비슷하게 3장에서도 필요하기 때문에 예시를 통해 좀더 직관적으로 보여드리겠습니다. 우선 이 중앙항은 다음과 같고,\n\\[\n\\mathbf{B} = \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g\n\\]\n이는 행렬로 보면 위에서 보신 \\(\\Phi\\)항과 같이 block diagonal 형태를 갖습니다. 3개의 cluster가 있고, 각 cluster 내 관측치 수가 2, 3, 1개라고 가정하면 각각의 \\(\\Phi\\)는 다음과 같고,\n\\[\n\\Phi_1 = \\mathbf{X}_1^\\top \\hat{\\mathbf{u}}_1 \\hat{\\mathbf{u}}_1^\\top \\mathbf{X}_1 =\n\\begin{pmatrix}\n\\sigma_{11}^2 & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}^2\n\\end{pmatrix}\n\\]\n\\[\n\\Phi_2 = \\mathbf{X}_2^\\top \\hat{\\mathbf{u}}_2 \\hat{\\mathbf{u}}_2^\\top \\mathbf{X}_2 =\n\\begin{pmatrix}\n\\sigma_{33}^2 & \\sigma_{34} & \\sigma_{35} \\\\\n\\sigma_{34} & \\sigma_{44}^2 & \\sigma_{45} \\\\\n\\sigma_{35} & \\sigma_{45} & \\sigma_{55}^2\n\\end{pmatrix}\n\\]\n\\[\n\\Phi_3 = \\mathbf{x}_6 \\hat{u}_6^2 \\mathbf{x}_6^\\top\n\\]\n로 표현될 수 있으며, 결국 Cluster-robust의 중앙 term은\n\\[\n\\mathbf{\\Phi} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & 0 \\\\\n0 & \\Phi_2 & 0 \\\\\n0 & 0 & \\Phi_3\n\\end{pmatrix}\n\\] 가 될 것입니다.\n정리하자면, HC0는 \\(\\Phi\\)가 대각행렬인 경우로, 개별 관측치의 Heteroskedasticity 만을 고려하며\n\\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nCluster-Robust 분산 추정량은 clusr별 \\(\\Phi\\)가 block diagonal 구조로, Heteroskedasticity와 cluster 내의 상관관계를 반영합니다.\n\\[\nE\\left[\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}})\\right] = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n3.4. In GLMs..\n\n이미 위(3장에서) Cluster-robust standard errors가 어떤 원리로부터 유도되며, OLS 환경(Linear Model)에서의 공식이 어떻게 생겼는지 살펴보았습니다. 또한 HC(Heteroskedasticity-Consistent) se 역시 기본 가정(등분산, 독립성 등)이 약화되었을 때도 일관된 추정을 제공하기 위해 Robust(샌드위치) 분산 추정량을 쓰게 된다는 것을 보았습니다. GLM에서도 LM과 같이 위 두 robust한 분산 추정치 식을 사용하여 Fisher information matrix의 역행렬로 분산을 추정하는 대신, 더욱 안정적으로 분산을 추정할 수 있습니다. GLM의 경우, 단순 OLS와 달리 \\(\\hat{\\mathbf{W}}, \\hat{\\mathbf{A}}\\) 등 추가적인 항이 존재하고, 이 행렬들이 실제 분산 추정 과정에 반영됩니다. 이 때문에 “bread”(양쪽에 곱해지는 행렬)와 “meat”(중간에 오는 분산·잔차 구조) 부분이 LM에서의 표기와는 형태가 조금 달라집니다. 즉, 원리는 동일하되, link & variance function으로 부터 비롯된 미분 항(\\(\\mathbf{A}\\))과 가중치 항(\\(\\mathbf{W}\\))이 반영되어야 한다는 점만 다릅니다. 1장에서 소개했던 HC0를 떠올리면, LM의 경우\n\\[ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}, \\]\n로 식이 구성되었습니다. GLM의 경우에는 \\(\\hat{\\mathbf{W}}\\)가 \\(\\mathbf{X}\\)와 상호작용하여 분산 추정에 들어가므로, 실제로는 다음과 같은 형태를 가집니다. (식은 패키지나 저자별 표기 차이에 따라 다소 달라질 수 있습니다). \\[ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n \\mathbf{x}_i \\Bigl(\\hat{u}_i^2 \\Bigr) \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}. \\]\n여기서 \\(\\hat{u}_i\\)는 단순 잔차가 아니라, 펄슨(pearson) 잔차 등 비선형적인 GLM 설정에 맞춰 적절히 조정된 형태일 수 있습니다. 구현별로 이탈도(deviance) 잔차를 사용할 수도 있고, 핵심은 “관측치별 잔차의 크기”를 통해 이질분산성을 추정하는 것입니다. 여기서는 앞뒤의 \\((\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}\\)이 bread(빵)이고, 가운데 잔차 \\(\\hat{u}_i\\hat{u}_i^\\top\\)가 meat(고기) 역할을 한다고 보면 됩니다. 철학적으로 해석하면, GLM에서도 LM에서와 동일하게 HC se는 “각 관측치별 오차분산”이 서로 다르더라도 일관된 추정을 제공하기 위하는 목적이며, 식은 (1) 잔차(오차항) 부분은 그대로 meat로 넣고, (2) 정보를 제공하는 bread에는 \\(\\mathbf{X}\\)에 가중치의 의미를 가진 \\(\\hat{\\mathbf{W}}\\) 항을 추가하여 구성한 위 형태로 구성됩니다.\n클러스터링이 있는 데이터에 대하여, LM과 마찬가지로 GLM에서도 Cluster-robust se가 적용될 수 있습니다. 이미 섹션 3.2~3.3에서 보았듯, 클러스터 간에는 독립이지만 클러스터 내 관측치들 간에는 상관관계가 존재할 수 있으므로, \\(\\Phi\\) 행렬(오차의 공분산 구조)을 block diagonal 형태로 가정하고, 이를 샌드위치 가운데(meat)에 반영합니다.\nLM에서의 일반적 식은 다음과 같았습니다.\n\\[ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\]\nGLM에서는 동일한 철학으로, 단순히 \\(\\mathbf{X}\\) 대신 가중치를 고려해 \\(\\hat{\\mathbf{W}}^{1/2}\\mathbf{X}\\)와 같은 형태(혹은 관련 도함수 항)가 곱해지게 됩니다. 즉,\n\\[ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G (\\mathbf{X}_g^\\top \\hat{\\mathbf{W}}_g^{1/2}) \\, \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\, (\\hat{\\mathbf{W}}_g^{1/2} \\mathbf{X}_g) \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, \\]\n와 같은 꼴이 됩니다(마찬가지로 패키지마다 표기 방식이나 구현 세부가 약간씩 다를 수 있습니다).각 항들 또한 한 번 더 설명하자면, \\(\\hat{\\mathbf{u}}_g\\)는 클러스터 \\(g\\) 내 잔차 벡터(pearson 또는 deviance 잔차 등).\\(\\mathbf{X}_g\\)는 클러스터 \\(g\\)에 해당하는 행만 추출한 \\(\\mathbf{X}\\)의 서브 행렬. \\(\\hat{\\mathbf{W}}\\)는 클러스터 (\\(g\\))에 해당하는 마찬가지로 가중치 서브 행렬이고, 이때 1/2승을 한다는 의미는 이가 diagonal matrix이므로 이 경우에는 단순히 diagonal 성분들 각각을 루트 씌운 값입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#r-예시-glm-cluster-robust-se",
    "href": "posts/2025-02-28-reg2/index.html#r-예시-glm-cluster-robust-se",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "4. R 예시: GLM, Cluster-robust SE",
    "text": "4. R 예시: GLM, Cluster-robust SE\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. GLM모델의 분산과 cluster-robust 분산을 비교하시면서 해석하면 됩니다.\n\n## 필요한 패키지 설치 (필요시)\n## install.packages(\"sandwich\")\n## install.packages(\"lmtest\")\n## install.packages(\"nlme\")\n#\n## 데이터 불러오기\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환\n#\n## 기본 GLM (로지스틱 회귀)\n#glm_fit &lt;- glm(binary ~ age + Sex, \n#               data = Orthodont, \n#               family = binomial)\n#summary(glm_fit) \n## 클러스터-로버스트 표준오차 (Subject 기준)\n#library(sandwich)\n#library(lmtest)\n#cluster_se &lt;- vcovCL(glm_fit, cluster = ~ Subject)\n#coeftest(glm_fit, vcov = cluster_se)  # 결과 출력"
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#마무리하며",
    "href": "posts/2025-02-28-reg2/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "마무리하며",
    "text": "마무리하며\n이번 2장에서는 1장에서 다룬 Linear Model을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수에서도 분석할 수 있도록 확장한 Generalized linear model의 기본 개념과, 실제로 패키지에서 이 GLM의 parameter를 estimate할 때 사용하는 대표적인 알고리즘인 IRLS(Fisher scoring)을 수학적으로 상당히 깊게 살펴보았습니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고, GLMs에서도 이 둘을 사용할 수 있다는 것을 밝힌 뒤 그 변형된 수식을 보았습니다. 다음 3장에서는 아직 깨지 못한 가정이었던 오차항의 독립, 즉 data(observations)간의 correlation이 존재하는 경우 자체를 모델에 반영하기 위해 개발된 모델들인 GEE, GLMM에 대하여 어느 정도 살펴보고 (GLMM의 내용은 너무 길어지기 때문에 얕게 다룰 것입니다.), 모델의 분산을 robust하게 추정하기 위한 가장 general한 형태의 Sandwich estimator를 M-estimation의 개념과 함께 공부할 것입니다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html",
    "href": "posts/2025-03-13-Quarto/index.html",
    "title": "quarto 의 기초",
    "section": "",
    "text": "Quarto를 이용해 R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 강의할 예정입니다. 강의 내용을 미리 공유합니다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#시작하기-전에",
    "href": "posts/2025-03-13-Quarto/index.html#시작하기-전에",
    "title": "quarto 의 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n1\nQuarto는 Markdown을 기반으로 한 문서 작성 도구로, Python, R , Julia, and Observable 등 다양한 언어로 코드실행, 분석, 시각화를 포함한 컨텐츠를 만드는 툴이며 크게 3가지 활용법이 있다.\n\n문서(pdf, html, docx): 글쓰기, 분석 결과, 참고문헌 등 논문의 모든 작업을 Quarto으로 수행한다.\n프리젠테이션(pdf, html, pptx): R 코드나 분석결과가 포함된 프리젠테이션을 만든다. 기본 템플릿2 외에 xaringan3 패키지가 최근 인기를 끌고 있다.\n웹(html): 웹사이트나 블로그를 만든다. blogdown4 이나 distill5 패키지가 대표적이다. 이 글의 블로그도 distill로 만들었으며, 과거 차라투 홈페이지는 blogdown을 이용하였다.\n\n본 강의는 1의 가장 기초에 해당하는 강의로 간단한 문서를 작성하는 것을 목표로 한다. pdf 문서를 만들기 위해서는 추가로 LaTeX 문서작성 프로그램인 Tex Live를 설치해야 하며 본 강의에서는 생략한다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#qmd-문서-시작하기",
    "href": "posts/2025-03-13-Quarto/index.html#qmd-문서-시작하기",
    "title": "quarto 의 기초",
    "section": ".qmd 문서 시작하기",
    "text": ".qmd 문서 시작하기\nQuarto는 qmd 파일로 작성되며 Quarto6 패키지를 설치한 후, Rstudio에서 File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document… 의 순서로 클릭하여 시작할 수 있다.\n Rstudio File 메뉴7\n Quarto 시작 메뉴8\n문서의 제목과 저자 이름을 적은 후 파일 형태를 아무거나 고르면(나중에도 쉽게 수정 가능)확장자가 qmd인 문서가 생성될 것이다.\n다음은 각각 html, pdf, docx로 생성된 문서이다.\n\n\n\n\nhtml 문서\n\n\n\n\n\n\n\npdf 문서\n\n\n\n\n\n\n\nword 문서\n\n\n\n생각보다 간단하지 않은가? 이제 본격적으로 qmd 파일의 내용을 살펴보면서 어떻게 글과 코드를 작성하는지 알아보자. qmd는 크게 제목을 적는 YAML Header, 글을 쓰는 Markdown Text와 코드를 적는 Code Chunk로 나눌 수 있다.\n\n\n\n\nqmd 구성 예시"
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#yaml-header",
    "href": "posts/2025-03-13-Quarto/index.html#yaml-header",
    "title": "quarto 의 기초",
    "section": "YAML Header",
    "text": "YAML Header\nYAML은 YAML Ain’t Markup Language의 재귀형식의 이름을 갖고 있는 언어로 가독성에 초점을 두고 개발되었다. Quarto은 qmd의 시작 부분에 문서 형식을 설정하는 용도로 이 포맷을 이용한다. 다음은 기초 정보만 포함된 YAML이다.\n---\ntitle: \"My Document\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    css: styles.css\n---\nYAML 에 기본적으로 title, author, date 등을 작성할 수 있고, format을 통해 출력 형식 (e.g html, pdf, docx…) 과 각 형식에 맞는 다양한 옵션을 설정하여 문서를 꾸밀 수 있다. Table of Contents, Layout, Fonts 등 다양한 옵션의 설정이 가능하고, 여기서는 Toc 옵션에 대해 살펴볼 것이다. 자세한 format option은 Quarto reference를 참고하기 바란다.\ntoc\nQuarto 문서(.qmd)에서 ##을 사용해 제목을 작성하면, 자동으로 목차(TOC)에 포함된다. 하위 목차를 추가하려면 ###, ####처럼 #의 개수를 늘려 계층 구조를 만들 수 있다.\ntoc 옵션에는 toc-depth, toc-location, toc-title, toc-expand 가 있다. 이 문서의 yaml 부분의 toc 옵션을 살펴보면 다음과 같이 설정되어있다.\n---\ntitle: \"quarto 의 기초\"\nauthor: .\neditor: visual\nformat: \n  html: \n    toc-depth: 3\n    toc-expand: true\n    toc-location: left\n    toc-title: \"Quarto 알아보기\"\n---"
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#markdown-글쓰기",
    "href": "posts/2025-03-13-Quarto/index.html#markdown-글쓰기",
    "title": "quarto 의 기초",
    "section": "Markdown 글쓰기",
    "text": "Markdown 글쓰기\nMarkdown 은 이름에서 알 수 있듯이 마크다운(Markdown) 을 기반으로 만들어졌다. 마크다운은 문법이 매우 간단한 것이 특징으로 깃허브의 README.md가 대표적인 마크다운 문서이다. Quarto 는 Pandoc markdown 을 바탕으로 하며, quarto guide9에 흔히 쓰는 문법이 정리되어 있다.\n2 가지만 따로 살펴보겠다.\nInline R code\n문장 안에 분석 결과값을 적을 때, 분석이 바뀔 때마다 바뀐 숫자를 직접 수정해야 한다. 그러나 숫자 대신 `r &lt;코드&gt;` 꼴로 R 코드를 넣는다면 재분석시 그 숫자를 자동으로 업데이트 시킬 수 있다.\nThere were  `r nrow(cars)` cars studied\n\nThere were 50 cars studied\n\n수식\nLaTeX 문법을 사용하며 hwp 문서의 수식 편집과 비슷하다. inline 삽입은 $...$, 새로운 줄은 $$...$$ 안에 식을 적으면 된다.\nThis summation expression $\\sum_{i=1}^n X_i$ appears inline.\n\nThis summation expression \\(\\sum_{i=1}^n X_i\\) appears inline.\n\n$$\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n$$\n\\[\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\]\n수식 전반은 LaTeX math and equations10을 참고하기 바란다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#r-chunk",
    "href": "posts/2025-03-13-Quarto/index.html#r-chunk",
    "title": "quarto 의 기초",
    "section": "R chunk",
    "text": "R chunk\nQuarto 에서는 내장된 knitr 패키지을 이용하여 R에서 작성한 코드를 실행하고 그 결과를 실시간으로 출력하여 문서에 삽입할 수 있다.\nR chunk 생성하기\nR chunk 를 생성하는 방법은 위의 단추를 통해 생성하거나 \n혹은 다음과 같이 직접 타이핑하여 생성도 가능하다. \nR chunk 옵션\n.qmd 문서에서 R 코드가 들어가는 방식은 4가지이다.\n\n몰래 실행. 코드와 결과는 다 숨긴다\n실행. 코드와 결과를 모두 보여준다. - default\n실행. 코드는 숨기고 결과만 보여준다.\n실행하지 않음. 코드 보여주기만 한다.\n\ninclude, echo, eval 3가지 옵션으로 지정한다. - eval=F : 코드를 실행하지 않는다. - echo=F : 코드를 보여주지 않는다. - include=F : 실행 결과를 보여주지 않는다.\n코드 청크의 옵션은 YAML 에서 지정하여 문서 전체에 적용되게 할 수 있고, 각각 R 청크마다 #| 을 쳐서 각각 옵션을 변경할 수도 있다.\n최초 설정\n문서를 처음 생성 시 옵션을 따로 지정하지 않으면 다음의 값으로 실행된다.\ninclude = TRUE \necho = TRUE \neval = TRUE \n코드를 실행하고, 코드와 결과물 모두 문서에 보여준다.\n\nprint(\"Hello world\")\n\n[1] \"Hello world\"\n\n\n이를 잘 활용하여 R내에서 문서를 완성할 수 있다.\n생존곡선을 그릴 때를 생각해보자, 생존 곡선을 그릴 때는 먼저 survfit 함수를 통해 생존확률을 구해야 한다.\n이 때 survfit 함수는 결과를 보이지 않아도 되므로\n실행하고 결과를 보이지 않기\n다음은 이 html을 생성할 때 쓴 quarto 문서의 캡처본으로 \n현재 강의 화면인 html 에는 코드 및 결과를 보이지 않는다.\n실행하고 결과를 보여주기\n그러나 실행은 되었기 때문에 다음의 코드에서 fit3 에 대한 ggsurvplot 함수를 적용할 수 있었고 코드 및 실행 후 결과는 다음과 같다.\n\nggsurv &lt;- ggsurvplot(fit3, data = colon,\n  fun = \"cumhaz\", conf.int = F,\n  risk.table = F, risk.table.col=\"strata\",\n  ggtheme = theme_bw())\nggsurv\n\n\n\n\n\n\n\n이렇게 시행하여 echo = False 옵션까지 적용하면 quarto 내에서 분석을 시행하고 그에 대한 문서작성을 한번에 할 수 있다.\n이외에도 코드 청크에 다음과 같은 옵션을 적용 가능하다.\n\n\nmessage=F - 실행 때 나오는 메세지를 보여주지 않는다.\n\nwarning=F - 실행 때 나오는 경고를 보여주지 않는다.\n\nerror=T - 에러가 있어도 실행하고 에러코드를 보여준다.\n\nfig.height = 7 - 그림 높이, R로 그린 그림에만 해당한다.\n\nfig.width = 7 - 그림 너비, R로 그린 그림에만 해당한다.\n\nfig.align = 'center' - 그림 위치, R로 그린 그림에만 해당한다.\n\nr chunk 에 적용할 수 있는 전체 옵션은 knitr::opts_chunk$get 함수로 확인할 수 있다. `\n\nknitr::opts_chunk$get()\n\n$eval\n[1] TRUE\n\n$echo\n[1] TRUE\n\n$results\n[1] \"markup\"\n\n$tidy\n[1] FALSE\n\n$tidy.opts\nNULL\n\n$collapse\n[1] FALSE\n\n$prompt\n[1] FALSE\n\n$comment\n[1] NA\n\n$highlight\n[1] TRUE\n\n$size\n[1] \"normalsize\"\n\n$background\n[1] \"#F7F7F7\"\n\n$strip.white\n[1] TRUE\n\n$cache\n[1] FALSE\n\n$cache.path\n[1] \"index_cache/html/\"\n\n$cache.vars\nNULL\n\n$cache.lazy\n[1] TRUE\n\n$dependson\nNULL\n\n$autodep\n[1] FALSE\n\n$cache.rebuild\n[1] FALSE\n\n$fig.keep\n[1] \"high\"\n\n$fig.show\n[1] \"asis\"\n\n$fig.align\n[1] \"default\"\n\n$fig.path\n[1] \"index_files/figure-html/\"\n\n$dev\n[1] \"png\"\n\n$dev.args\nNULL\n\n$dpi\n[1] 96\n\n$fig.ext\nNULL\n\n$fig.width\n[1] 7\n\n$fig.height\n[1] 5\n\n$fig.env\n[1] \"figure\"\n\n$fig.cap\nNULL\n\n$fig.scap\nNULL\n\n$fig.lp\n[1] \"fig:\"\n\n$fig.subcap\nNULL\n\n$fig.pos\n[1] \"\"\n\n$out.width\nNULL\n\n$out.height\nNULL\n\n$out.extra\nNULL\n\n$fig.retina\n[1] 2\n\n$external\n[1] TRUE\n\n$sanitize\n[1] FALSE\n\n$interval\n[1] 1\n\n$aniopts\n[1] \"controls,loop\"\n\n$warning\n[1] FALSE\n\n$error\n[1] FALSE\n\n$message\n[1] FALSE\n\n$render\nNULL\n\n$ref.label\nNULL\n\n$child\nNULL\n\n$engine\n[1] \"R\"\n\n$split\n[1] FALSE\n\n$include\n[1] TRUE\n\n$purl\n[1] TRUE\n\n$fig.asp\nNULL\n\n$fenced.echo\n[1] FALSE\n\n$ft.shadow\n[1] FALSE\n\n\n다음은 필자가 논문을 quarto로 쓸 때 흔히 쓰는 디폴트 옵션이다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#figures",
    "href": "posts/2025-03-13-Quarto/index.html#figures",
    "title": "quarto 의 기초",
    "section": "Figures",
    "text": "Figures\n.qmd 문서에 그림이 들어가는 방법은 2가지가 있다.\n\nR 코드로 생성 : plot 함수, ggplot2 패키지 등\n외부 그림 삽입\n\n앞서도 언급했듯이 주의할 점은 그림이 만들어지는 방법에 따라 서로 다른 옵션이 적용된다는 것이다. 일단 전자부터 살펴보자.\nFigures with R\n\nR 코드에서 자체적으로 만든 그림은 전부 chunk 옵션의 지배를 받아 간단하다.\n\n#|fig-cap: \"scatterplot: cars\"\n#|fig-width: 8\n#|fig-height: 6\n\nplot(cars, pch = 18)\n\n\n\n\n\n\n\nExternal figures\n외부 그림은 R 코드로도 삽입할 수 있고 마크다운 문법을 쓸 수도 있는데, 어떤 방법을 쓰느냐에 따라 다른 옵션을 적용받는다는 것을 주의해야 한다. R에서는 knitr::include_graphics 함수를 이용하여 그림을 넣을 수 있고 이 때는 chunk 내부의 옵션이 적용된다.\n\nlibrary(knitr)\ninclude_graphics(\"https://www.tidyverse.org/images/tidyverse-default.png\")\n\n\n\ntidyverse logo\n\n\n\n같은 그림을 chunk없이 바로 마크다운에서 삽입할 수도 있다. 이 때는 YAML의 옵션이 적용된다.\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n\n\ntidyverse logo\n\n{ width=50% } 는 그림의 크기를 조절하는 옵션이며 R chunk에서도 같은 옵션 out.width=\"50%\"이 있다. 위치를 가운데로 조절하려면 &lt;center&gt;...&lt;/center&gt; 를 포함시키자.\n&lt;center&gt;\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n&lt;/center&gt;\n\n\n\ntidyverse logo\n\n\n개인적으로는 외부 이미지도 chunk 내부에서 읽는 것을 추천한다. chunk 내부의 옵션들이 마크다운의 그것보다 훨씬 체계적이고 쉬운 느낌이다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#tables",
    "href": "posts/2025-03-13-Quarto/index.html#tables",
    "title": "quarto 의 기초",
    "section": "Tables",
    "text": "Tables\n논문을 쓸 때 가장 귀찮은 부분 중 하나가 분석 결과를 테이블로 만드는 것으로, knitr::kable() 함수를 쓰면 문서 형태에 상관없이 Rmd에서 바로 테이블을 만들 수 있다. 아래는 데이터를 살펴보는 가장 간단한 예시이다.\n\n#|label: \"tables-mtcars\"\nknitr::kable(iris[1:5, ], caption = 'A caption', row.names = T)\n\n\nA caption\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\nepiDisplay 패키지의 regress.display, logistic.display 함수를 활용하면 회귀분석의 결과를 바로 테이블로 나타낼 수 있다.\n\n#|label: \"regtable\"\nmtcars$vs &lt;- as.factor(mtcars$vs)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmodel &lt;- glm(mpg ~ disp + vs + cyl, data = mtcars)\nmodel.display &lt;- epiDisplay::regress.display(model, crude = T, crude.p.value = T)\nmodel.table &lt;- model.display$table[rownames(model.display$table)!=\"\", ]\nkable(model.table, caption = model.display$first.line)\n\n\nLinear regression predicting mpg\n\n\n\n\n\n\n\n\n\n\ncrude coeff.(95%CI)\ncrude P value\nadj. coeff.(95%CI)\nP(t-test)\nP(F-test)\n\n\n\ndisp (cont. var.)\n-0.04 (-0.05,-0.03)\n&lt; 0.001\n-0.03 (-0.05,0)\n0.019\n&lt; 0.001\n\n\nvs: 1 vs 0\n7.94 (4.6,11.28)\n&lt; 0.001\n0.04 (-3.81,3.89)\n0.984\n0.334\n\n\ncyl: ref.=4\n\n\n\n\n0.041\n\n\n6\n-6.92 (-10.11,-3.73)\n&lt; 0.001\n-4.77 (-8.56,-0.98)\n0.016\n\n\n\n8\n-11.56 (-14.22,-8.91)\n&lt; 0.001\n-4.75 (-12.19,2.7)\n0.202\n\n\n\n\n\n\n테이블을 좀 더 다듬으려면 kableExtra 패키지가 필요하며, 자세한 내용은 cran 설명서11를 참고하기 바란다. html 문서의 경우 kable()외에도 다양한 함수들을 이용할 수 있는데, 대표적인 것이 rmarkdown::paged_table() 함수와 DT 패키지이다. 전자는 아래와 같이 YAML에서 테이블 보기의 기본 옵션으로 설정할 수도 있다.\n---\ntitle: \"Motor Trend Car Road Tests\"\noutput:\n  html_document:\n    df_print: paged\n---\nDT 패키지에 대한 설명은 Rstudio DT 홈페이지12를 참고하기 바란다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#마치며",
    "href": "posts/2025-03-13-Quarto/index.html#마치며",
    "title": "quarto 의 기초",
    "section": "마치며",
    "text": "마치며\n본 강의를 통해 Quarto으로 기본적인 문서를 만드는 법을 알아보았다. 본 강의에서는 시간 관계상 참고문헌 다는 법을 언급하지 않았는데 궁금하다면 Bibliographies and Citations13을 참고하자.\n이 내용까지 숙지한다면 Quarto으로 논문을 쓸 준비가 된 것이다. Quarto에 대한 전반적인 내용은 아래의 Quarto Cheet Sheet14에 잘 요약되어 있으니 그때그떄 확인하면 좋다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#footnotes",
    "href": "posts/2025-03-13-Quarto/index.html#footnotes",
    "title": "quarto 의 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://bioinformatics.ccr.cancer.gov/docs/btep-coding-club/CC2024/Quarto/GettingStarted_with_Quarto_orig.html↩︎\nhttps://rmarkdown.rstudio.com/lesson-11.html↩︎\nhttps://github.com/yihui/xaringan↩︎\nhttps://github.com/rstudio/blogdown↩︎\nhttps://rstudio.github.io/distill/↩︎\nhttps://github.com/quarto-dev/quarto-r↩︎\nhttps://github.com/rstudio/rstudio/issues/10966↩︎\nhttps://github.com/quarto-dev/quarto-r↩︎\nhttps://quarto.org/docs/authoring/markdown-basics.html↩︎\nhttps://www.latex-tutorial.com/tutorials/amsmath/↩︎\nhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html↩︎\nhttps://rstudio.github.io/DT/↩︎\nhttps://pandoc.org/MANUAL.html#citations↩︎\nhttps://rstudio.github.io/cheatsheets/quarto.pdf↩︎"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "Intraclass Correlation Coefficient (ICC)는 집단으로 구성된 단위들에 대해 수치 측정값이 주어졌을 때, 동일한 집단 내 단위들이 서로 얼마나 유사한지를 나타내는 기술 통계량이다. 주로 측정 신뢰도를 평가할 때 사용되며, 특히 동일한 대상에 대해 여러 관측자(또는 반복 측정)가 수행한 측정의 일관성을 평가하는 데 유용하다.\nICC는 일반적인 상관계수와 달리 짝지어진 관측값이 아니라, 그룹으로 구성된 데이터에 대해 계산된다는 점에서 차이가 있다.\n\n\nICC와 Cohen’s Kappa는 모두 측정자 간 일치도를 측정하는 도구지만, 적용되는 데이터의 특성과 해석 방식에서 차이가 있다.\n자료의 유형\n\nICC는 연속형 변수에 사용됨\nCohen’s Kappa는 범주형 변수에 사용됨\n\n즉 ICC는 “얼마나 수치들이 서로 비슷한가?”를 보는 것이고, Cohen’s kappa는 “서로 같은 범주로 분류했는가?”를 보는 것이다.\n평가방식\n\nICC는 분산을 기반으로 측정값 간의 상관 관계(유사성)를 분석함.\nCohen’s Kappa는 기대 일치도를 고려하여, 관측된 일치도와 우연히 일어날 수 있는 일치도의 차이를 보정함.\n\n즉, ICC는 총 변동성 중에서 집단 내 일관성에 기인하는 부분의 비율을 나타내는 것이고, Cohen’s Kappa는 실제 일치가 단순히 우연에 의한 것이 아님을 보장해 주는 것이다.\n측정자 수\n\nICC는 둘 이상의 평가자에게 적용할 수 있음. 측정 모델에 따라 다양한 변형등이 존재함.\nCohen’s Kappa는 전통적으로 두 평가자에 대한 일치도 측정에 사용되며, 다수 평가자일 경우에는 Fleiss’ kappa 등을 사용함.\n\n\n\n\n\n심리학, 의학, 교육학 등에서 테스트 재현성 또는 평가자 간 신뢰도 분석\n머신러닝/데이터 분석에서는 데이터 라벨링 일관성 확인\n반복 측정 설계에서 측정의 안정성 검증"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-vs.-cohens-kappa",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-vs.-cohens-kappa",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "ICC와 Cohen’s Kappa는 모두 측정자 간 일치도를 측정하는 도구지만, 적용되는 데이터의 특성과 해석 방식에서 차이가 있다.\n자료의 유형\n\nICC는 연속형 변수에 사용됨\nCohen’s Kappa는 범주형 변수에 사용됨\n\n즉 ICC는 “얼마나 수치들이 서로 비슷한가?”를 보는 것이고, Cohen’s kappa는 “서로 같은 범주로 분류했는가?”를 보는 것이다.\n평가방식\n\nICC는 분산을 기반으로 측정값 간의 상관 관계(유사성)를 분석함.\nCohen’s Kappa는 기대 일치도를 고려하여, 관측된 일치도와 우연히 일어날 수 있는 일치도의 차이를 보정함.\n\n즉, ICC는 총 변동성 중에서 집단 내 일관성에 기인하는 부분의 비율을 나타내는 것이고, Cohen’s Kappa는 실제 일치가 단순히 우연에 의한 것이 아님을 보장해 주는 것이다.\n측정자 수\n\nICC는 둘 이상의 평가자에게 적용할 수 있음. 측정 모델에 따라 다양한 변형등이 존재함.\nCohen’s Kappa는 전통적으로 두 평가자에 대한 일치도 측정에 사용되며, 다수 평가자일 경우에는 Fleiss’ kappa 등을 사용함."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-사용분야",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-사용분야",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "심리학, 의학, 교육학 등에서 테스트 재현성 또는 평가자 간 신뢰도 분석\n머신러닝/데이터 분석에서는 데이터 라벨링 일관성 확인\n반복 측정 설계에서 측정의 안정성 검증"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#초기-icc의-정의",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#초기-icc의-정의",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "초기 ICC의 정의",
    "text": "초기 ICC의 정의\nRonald Fisher가 제안한 초기 ICC는 다음과 같다.\n\\[\nr = \\sum_{n=1}^{N} \\frac{(x_{n,1} - \\bar{x})(x_{n,2} - \\bar{x})}{Ns^2}\n\\]\n\n데이터셋은 \\(N\\)개의 짝으로 구성됨\n각 개체 \\(n\\)에 대해 두 개의 측정값 \\((x_{n,1} , x_{n,2})\\)가 존재함\n\\(\\bar{x}\\)는 전체 데이터의 평균 \\[\n\\bar{x} = \\frac{1}{2N} \\sum_{n=1}^{N} (x_{n,1} + x_{n,2})\n\\]\n\\(s^2\\)은 전체 데이터의 분산 \\[\ns^2 = \\frac{1}{2N} \\left\\{ \\sum_{n=1}^{N} (x_{n,1} - \\bar{x})^2 + \\sum_{n=1}^{N} (x_{n,2} - \\bar{x})^2 \\right\\}\n\\]\n\n\n\n\nr 값 범위\n신뢰도의 정도\n\n\n\n\nr ≤ 0.00\nPoor\n\n\n0.00 &lt; r ≤ 0.20\nSlight\n\n\n0.20 &lt; r ≤ 0.40\nFair\n\n\n0.40 &lt; r ≤ 0.60\nModerate\n\n\n0.60 &lt; r ≤ 0.80\nSubstantial\n\n\n0.80 &lt; r ≤ 1.00\nAlmost Perfect"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#pearson-상관계수와-icc의-차이점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#pearson-상관계수와-icc의-차이점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Pearson 상관계수와 ICC의 차이점",
    "text": "Pearson 상관계수와 ICC의 차이점\nPearson 상관계수와 ICC는 모두 연속형 데이터를 비교할 때 사용하는 지표이지만, 그들이 측정하는 대상과 의미는 전혀 다르다.\n먼저 Pearson 상관계수는 두 변수 간의 선형 관계(일관된 증가/감소)를 측정한다. 예를 들어, 키와 몸무게처럼, 한 변수가 커질수록 다른 변수도 함께 커지는 경향이 있는지를 파악할 때 유용하다. 이때 값은 -1에서 +1 사이이며, 1에 가까울수록 완벽한 양의 선형 관계를 의미한다. 하지만 이 지표는 값 자체의 차이는 고려하지 않는다.\n반면 ICC는 동일한 대상을 여러 번 측정했을 때 그 값이 얼마나 비슷하게 나오는지를 측정하는 지표아다. 즉 서로 다른 평가자나 시간에 따라 측정값이 달라졌을 때, 그 변화가 개인의 특성 차이 때문인지, 아니면 측정자의 오차 때문인지를 분리할 수 있다. ICC 값은 일반적으로 0에서 1 사이에 있으며, 1에 가까울수록 일관되고 신뢰할 수 있는 측정이라고 해석할 수 있다.\n중요한 차이는 해석의 포인트에 있다:\nPearson 상관계수는 측정값 간의 선형 관계만을 보기 때문에, 두 평가자가 항상 10점 차이를 주더라도 여전히 상관계수는 1이 될 수 있다.\n하지만 ICC는 ’값 자체가 얼마나 유사한지’를 보며, 위와 같은 상황에서는 일치도가 낮다고 판단한다. 즉 Pearson은 “패턴의 일관성”, ICC는 “값의 일치도”를 본다고 이해할 수 있다.\n또한, Pearson 상관계수는 서로 다른 단위를 가진 두 변수(예: 키와 체중)에도 적용 가능하지만, ICC는 동일한 측정 단위 내에서만 적절하게 해석할 수 있다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#개-이상의-값을-가진-그룹에서의-icc",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#개-이상의-값을-가진-그룹에서의-icc",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "3개 이상의 값을 가진 그룹에서의 ICC",
    "text": "3개 이상의 값을 가진 그룹에서의 ICC\n데이터셋이 각 그룹당 3개의 측정값을 가지는 경우, ICC는 다음과 같이 확장된다. \\[\nr = \\frac{1}{3N s^2} \\sum_{n=1}^{N} \\left\\{\n(x_{n,1} - \\bar{x})(x_{n,2} - \\bar{x}) +\n(x_{n,1} - \\bar{x})(x_{n,3} - \\bar{x}) +\n(x_{n,2} - \\bar{x})(x_{n,3} - \\bar{x})\n\\right\\}\n\\]\n\n\\(\\bar{x}\\)는 전체 평균 \\[\n\\bar{x} = \\frac{1}{3N} \\sum_{n=1}^{N} (x_{n,1} + x_{n,2} + x_{n,3})\n\\]\n\\(s^2\\)는 전체 분산 \\[\ns^2 = \\frac{1}{3N} \\left\\{\n\\sum_{n=1}^{N} (x_{n,1} - \\bar{x})^2 +\n\\sum_{n=1}^{N} (x_{n,2} - \\bar{x})^2 +\n\\sum_{n=1}^{N} (x_{n,3} - \\bar{x})^2\n\\right\\}\n\\] 여기서, 그룹의 크기(K)가 커질수록, 계산 과정에서 고려해야 할 교차항의 수도 증가한다.\n\n위 공식을 일반화하면 다음과 같아진다. \\[\nr = \\frac{K}{K-1} \\cdot \\frac{1}{Ns^2} \\sum_{n=1}^{N} (\\bar{x}_n - \\bar{x})^2 - \\frac{1}{K-1}\n\\]\n\n\\(K\\)는 그룹당 데이터 개수\n\\(\\bar{x_n}\\)는 \\(n\\)번째 그룹의 평균\n\n\\(K=3\\)을 대입하면 위 공식과 완벽히 같아진다. 위 공식에 따르면, ICC값은 항상 \\(\\frac{-1}{K-1}\\)이상의 값을 가진다는 것을 알 수 있다. 따라서 ICC는 항상 \\(-1 \\leq r \\leq 1\\)의 범위 안에서 존재하지만, 데이터 개수(\\(K\\))가 많아질 수록 음수로 나올 가능성이 줄어든다.\n또한 충분히 큰 \\(K\\)에 대해서, 다음과 같이 근사할 수도 있다. \\[\nr = \\frac{K}{K-1} \\cdot \\frac{1}{Ns^2} \\sum_{n=1}^{N} (\\bar{x}_n - \\bar{x})^2\n\\]"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-해석-및-한계점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-해석-및-한계점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC의 해석 및 한계점",
    "text": "ICC의 해석 및 한계점\nICC는 총 분산 중에서 그룹 간 변동이 차지하는 비율로 해석할 수 있다.\n이상적인 데이터에서는 ICC 값이 0~1 사이에 있어야 하지만, 실제 샘플 데이터에서는 음수 값이 나올 수도 있다. 이는 Ronald Fisher가 ICC를 편향되지 않은 추정량으로 설계했기 때문이다. 따라서 모집단에서 ICC가 0일 경우, 표본 데이터에서는 음수 값이 나올 수 있다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss-1979",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss-1979",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Shrout & Fleiss (1979)",
    "text": "Shrout & Fleiss (1979)\nShrout & Fleiss는 초기의 ICC를 체계적으로 분류하였다. 3가지 모델 유형(Model 1, 2, 3), 2가지 측정 방식(1: single, k: 평균)으로 분류하여 총 6가지 유형의 ICC를 정의하였다.\n모델 종류\n\none-way random\n\n피험자 간의 변동만을 고려하는 모형\n피험자 간의 차이에 대한 평가지의 일치도를 평가할 때 사용함\n평가자의 효과는 고려하지 않고, 단순히 피험자 간의 일관성을 평가할 때 사용함\n\ntwo-way random\n\n피험자 간 변동뿐만이 아니라, 평가자 간의 변동도 고려하는 모형\n동일한 피험자에 대해 평가한 결과가 얼마나 일치하는지, 평가자들 간의 평가 결과가 얼마나 일관성 있는 지를 평가할 때 사용함\n\ntwo-way mixed\n\n피험자 간의 변동과 평가자 간의 변동을 고려하는 모형\n특정 평가자들이 고정되어 있을 때, 피험자 간의 일치도를 평가하는 데 사용함\n\n\n측정방식\n\n단일 측도(single)\n\n평가자 간에 얼마나 차이가 있는지 확인\n각 평가자에 의해 한 번의 측정이 일어난 경우\n\n평균 측도(average)\n\n평균값과 얼마나 차이가 있는지 확인\n각 평가자에 의해 여러 번 측정이 일어난 경우"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#mcgraw-wong-1996",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#mcgraw-wong-1996",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "McGraw & Wong (1996)",
    "text": "McGraw & Wong (1996)\nMcGraw & Wong은 Shrout & Fleiss의 체계를 확장하여 총 10가지 ICC 형태를 정의하였다. 다음과 같은 총 3가지의 분류 기준을 제시하였다.\n모델 종류\n\none-way random\ntwo-way random\ntwo-way mixed\n\n측정방식\n\n단일 측도(single)\n평균 측도(average)\n\n정의(Definition Agreement)\n\n일치도(consistency)\n\n상대적 순위/경향이 일치하는지를 의미\n평가자 간의 체계적인 차이는 무시하고, 변동성만 분석\n포함하는 오차 : 우연한 변동\n사용 상황 : 평가자가 고정되어 있고, 상대적 순위가 중요한 경우\n\n절대합의도(absolute agreement)\n\n두 평가자의 결과가 완전히 같은지를 의미\n평가자 간의 체계적인 차이까지도 고려\n포함하는 오차 : 우연한 변동, 평가자 간 편향\n사용 상황 : 정량적 측정이 실제 절대값의 일치도를 요구하는 경우"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-분류",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-분류",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC 분류",
    "text": "ICC 분류\n아래의 표는 Shrout & Fleiss와 McGraw & Wong의 기준에 따라 ICC를 정리한 것이다.\n\n\n\n\n\n\n\n\nMcGraw and Wong\nShrout and Fleiss\nFormulas for Calculating ICC\n\n\n\n\nOne-way random effects, absolute agreement, single rater/measurment\nICC(1,1)\n\\(\\frac{MS_R - MS_W}{MS_R +(k+1)MS_W}\\)\n\n\nTwo-way random effects, consistency, single rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E}\\)\n\n\nTwo-way random effects, absolute agreement, single rater/measurment\nICC(2,1)\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E + \\frac{k}{n} (MS_C - MS_E )}\\)\n\n\nTwo-way mixed effects, consistency , single rater/measurment\nICC(3,1)\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E}\\)\n\n\nTwo-way mixed effects, absolute agreement, single rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E + \\frac{k}{n} (MS_C - MS_E )}\\)\n\n\nOne-way random effects, absolute agreement, multiple rater/measurment\nICC(1,k)\n\\(\\frac{MS_R - MS_W}{MS_R}\\)\n\n\nTwo-way random effects, consistency, multiple rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R}\\)\n\n\nTwo-way random effects, absolute agreement, multiple rater/measurment\nICC(2,k)\n\\(\\frac{MS_R - MS_E}{MS_R + \\frac{MS_C -MS_E}{n}}\\)\n\n\nTwo-way mixed effects, consistency, multiple rater/measurment\nICC(3,k)\n\\(\\frac{MS_R - MS_E}{MS_R}\\)\n\n\nTwo-way mixed effects, absolute agreement, multiple rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R + \\frac{MS_C -MS_E}{n}}\\)\n\n\n\n\n\\(k\\) : 평가자 수\n\\(n\\) : 피험자 수\n\\(MS_R\\) : mean square for rows\n\\(MS_W\\) : mean square for residual sources of variance\n\\(MS_E\\) : mean square for error\n\\(MS_C\\) : mean square for columns"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#one-way-모델에서-consistency가-정의-되지-않는-이유",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#one-way-모델에서-consistency가-정의-되지-않는-이유",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "One-way 모델에서 Consistency가 정의 되지 않는 이유",
    "text": "One-way 모델에서 Consistency가 정의 되지 않는 이유\nOne-way random model은 평가자 효과를 모델에 포함하지 않기 때문이다. One-way 모델에서는 오직 피험자 간 차이만 고려하기 때문에 평가자 간 차이가 분산 구조에서 빠져있다. 따라서 평가자 간 일관성을 측정할 수 없다. 결과적으로 One-way 모델은 “Agreement”는 가능하지만 “Consistency”는 정의할 수 없다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-방식을-정하는-법",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-방식을-정하는-법",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC 방식을 정하는 법",
    "text": "ICC 방식을 정하는 법\n\n\n\nICC 방식을 정하는 과정\n\n\n위 그림은 ICC 유형을 선택하는 의사결정 흐름도를 시각적으로 잘 정리한 자료이다. ICC는 McGraw & Wong이 제안한 모델을 기준으로 총 10가지 유형이 존재한다. 각각의 선택은 연구 설계에 따라 달라지며, 다음과 같은 과정을 거치면 적절한 유형을 결정할 수 있다.\n첫 번째로, 연구 유형이 Test-Retest / Intra-rater Reliability인지, Inter-rater Reliability인지 알아본다. 만일 Test-Retest / Intra-rater Reliability라면, 같은 평가자가 같은 대상을 반복 측정하는 경우이기 때문에 Two-way Mixed Effects 모델을 사용한다. 반면에 Inter-rater Reliability라면 여러 평가자가 각 대상을 평가하는 경우이기 때문에, 다음 과정으로 넘어간다.\n연구 유형이 Inter-rater Reliability일 때 모든 피평가자가 같은 평가자에게 평가되지 않았다면, 일부 평가자만 일부 피평가자를 평가하므로 One-way Random Effects 모델을 사용한다. 만일 모든 피평가자가 같은 평가자에게 평가되었다면, 평가자가 무작위 집단이라면 Two-way Random Effects 모델을 사용하고, 특정 평가자에 한정된다면 Two-way Mixed Effects 모델을 사용한다. 위 과정을 통해 ICC의 모델 중 어떤 것을 사용할 지 선택할 수 있다.\n두 번째로 측정 프로토콜이 어떤 지 확인한다. 만일 한 번만 측정한다면 single rater/measurment을 사용하고, 여러 번 측정 후 평균을 사용한다면 multiple rater/measurment을 사용한다.\n마지막으로 연구에서 중요한 것이 절대적 일치인지, 경향성 일치인지 판단한다. 진단 일치나 채점 점수 등 수치 자체가 같아야 하는 경우라면 Absolute Agreement를 사용하고, 순위 유지가 중요한 경우라면 Consistency를 사용하면 된다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#대중적으로-사용되는-icc-방식",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#대중적으로-사용되는-icc-방식",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "대중적으로 사용되는 ICC 방식",
    "text": "대중적으로 사용되는 ICC 방식\n연구 현장에서 많이 쓰이는 ICC는 다음과 같다.\n\nICC(2,1): Two-way random effects, single rater/measurment, absolute agreement\nICC(2,k): Two-way random effects, multiple rater/measurment, absolute agreement\n\nTwo-way random effects 모델은 평가자와 피평가자 모두 무작위 샘플링된 것으로 간주한다. 따라서 평가자도 연구의 일반적인 모집단에서 랜덤하게 선택된 경우를 반영할 수 있어 일반화 가능성이 높다.\nAbsolute agreement는 단순히 측정값 간의 일관성만이 아니라, 측정값 자체가 정확히 일치하는지를 따진다. 이는 consistency보다 더 보수적이고 엄격한 방식이다. 또한 consistency는 평가자의 bias를 왜곡하는 반면, Absolute agreement는 systematic bias까지 감지할 수 있다.\n따라서 Two-way random effects, Absolute agreement를 사용하는 ICC(2,1), ICC(2,k)를 실제 연구에서 많이 사용한다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#랜덤-효과-모형에서의-icc",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#랜덤-효과-모형에서의-icc",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "랜덤 효과 모형에서의 ICC",
    "text": "랜덤 효과 모형에서의 ICC\n모던 ICC는 다음과 같은 one-way random effects 모형에서 정의된다. \\[\nY_{ij} = \\mu + \\alpha_j + \\varepsilon_{ij}\n\\]\n\n\\(Y_{ij}\\)는 \\(j\\)번째 그룹의 \\(i\\)번째 측정값\n\\(\\mu\\)는 모집단 전체의 평균\n\\(\\alpha_j\\)는 그룹 \\(j\\)에 해당하는 랜덤효과\n\\(\\varepsilon_{ij}\\)는 오차항"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-공식",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-공식",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 공식",
    "text": "모던 ICC의 공식\n모던 ICC는 다음과 같이 정의된다. \\[\nICC = \\frac{\\sigma^2_{\\alpha}}{\\sigma^2_{\\alpha} + \\sigma^2_{\\varepsilon}}\n\\]\n분자\\((\\sigma^2_{\\alpha})\\)는 그룹간 분산을 의미하고, 분모\\((\\sigma^2_{\\alpha}+\\sigma^2_{\\varepsilon})\\)은 전체 분산을 의미한다.\n즉 ICC는 전체 변동에서 그룹간 변동이 차지하는 비율이 된다. 따라서 ICC값이 클 수록 같은 그룹 내에서 값들이 더 유사하다는 것을 알 수 있다.\n증명\n\\(Y_{ij} = \\mu + \\alpha_j + \\varepsilon_{ij}\\)에서 \\(\\alpha_i \\sim N(0, \\sigma^2_{\\alpha})\\), \\(\\epsilon_{ij} \\sim N(0, \\sigma^2_{\\varepsilon})\\)이고 \\(\\alpha_i\\)와 \\(\\varepsilon_{ij}\\)은 서로 독립이다.\n\\(Var(Y_{ij}) = \\sigma^2_{\\varepsilon} + \\sigma^2_{\\alpha^2}\\)\n\\[\\begin{align}\nCov(Y_{ij}, Y_{ik}) &= Cov(\\mu + \\alpha_{i} + \\varepsilon_{ij}, \\mu + \\alpha_{i} + \\varepsilon_{ik})\\\\\n  &= Cov(\\alpha_i + \\varepsilon_{ij},\\alpha_i + \\varepsilon_{ik}) \\\\\n  &= Cov(\\alpha_i , \\alpha_i) + Cov(\\alpha_i, \\varepsilon_{ik}) + Cov(\\varepsilon_{ij}, \\alpha_i) + Cov(\\varepsilon_{ij}, \\varepsilon_{ik}) \\\\\n  &= Var(\\alpha_i) = \\sigma^2_\\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-장점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-장점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 장점",
    "text": "모던 ICC의 장점\n\n항상 0 이상이다\n\n초기 ICC는 표본에서 음수값이 나올 수 있었음\n\nANOVA와 결합 가능하다 \\(\\rightarrow\\) 샘플 개수가 달라도 적용 가능하다\n\n초기 ICC는 같은 크기의 그룹을 가정함\n하지만 ANOVA 기반 ICC는 데이터 개수가 달라도 계산 가능\n\n공변량을 포함할 수 있다\n\n공변량을 통제한 후에도 같은 그룹 내에서 얼마나 유사한지 평가할 수 있음\n\n복잡한 데이터 설계에 유리하다"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-한계점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-한계점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 한계점",
    "text": "모던 ICC의 한계점\n\n샘플 ICC가 실제 모집단 ICC보다 클 가능성이 높다\n\n초기 ICC는 편향되지 않은 추정량임\n모던 ICC는 항상 0 이상이므로, 모집단의 ICC가 정확히 0일 때에도 샘플에서 ICC가 0보다 크게 나올 가능성이 있음\n즉, 양의 편향을 가짐\n\n여러 종류의 ICC가 존재\\(\\rightarrow\\)어떤 ICC를 사용할지 논란이 된다\n\n연구자마다 다른 ICC 통계량을 사용하며, 각 방법이 서로 다른 결과를 낼 수 있음\n특정 연구 목적에 적합한 ICC를 신중하게 선택해야 함"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#특정-icc-방법을-실행하는-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#특정-icc-방법을-실행하는-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "특정 ICC 방법을 실행하는 R코드",
    "text": "특정 ICC 방법을 실행하는 R코드\nlibrary(irr)\n\nratings &lt;- data.frame(\n  Rater1 = c(4, 5, 3, 4, 2),\n  Rater2 = c(5, 5, 4, 4, 3),\n  Rater3 = c(4, 5, 3, 5, 2)\n)\n\nresult &lt;- icc(ratings, model = \"twoway\", type = \"agreement\", unit = \"single\")\n\nprint(result)\n출력\nSingle Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 5 \n     Raters = 3 \n   ICC(A,1) = 0.792\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n  F(4,9.09) = 15.1 , p = 0.000482 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.375 &lt; ICC &lt; 0.973"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss의-6개-방법을-모두-보여주는-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss의-6개-방법을-모두-보여주는-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Shrout & Fleiss의 6개 방법을 모두 보여주는 R코드",
    "text": "Shrout & Fleiss의 6개 방법을 모두 보여주는 R코드\nlibrary(psych)\nICC(ratings)\n출력\nCall: ICC(x = ratings)\n\nIntraclass correlation coefficients \n                         type  ICC  F df1 df2       p lower bound upper bound\nSingle_raters_absolute   ICC1 0.79 12   4  10 0.00072        0.37        0.97\nSingle_random_raters     ICC2 0.79 15   4   8 0.00085        0.37        0.97\nSingle_fixed_raters      ICC3 0.82 15   4   8 0.00085        0.40        0.98\nAverage_raters_absolute ICC1k 0.92 12   4  10 0.00072        0.64        0.99\nAverage_random_raters   ICC2k 0.92 15   4   8 0.00085        0.64        0.99\nAverage_fixed_raters    ICC3k 0.93 15   4   8 0.00085        0.66        0.99\n\n Number of subjects = 5     Number of Judges =  3\nSee the help file for a discussion of the other 4 McGraw and Wong estimates,"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#lmer-방식의-모던-icc-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#lmer-방식의-모던-icc-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "lmer 방식의 모던 ICC R코드",
    "text": "lmer 방식의 모던 ICC R코드\ndf &lt;- data.frame(\n  subject = rep(1:5, each = 3),\n  rater = rep(1:3, times = 5),\n  score = c(80, 82, 81,\n            75, 76, 74,\n            90, 89, 91,\n            70, 72, 71,\n            85, 86, 84)\n)\n\nlibrary(lme4)\nlibrary(performance)\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = df)\n\nicc(model)\n출력\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.985\n  Unadjusted ICC: 0.985"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc에서-ci-구하기",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc에서-ci-구하기",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC에서 CI 구하기",
    "text": "모던 ICC에서 CI 구하기\nlmer을 이용한 모던 ICC에서는 CI를 직접 제공하지 않는 것을 알 수 있다. 이는 ICC는 비선형 함수이고, \\(\\sigma^2\\)자체는 정규분포를 따르지 않기 때문에 정규 근사(CI ≈ estimate ± 1.96 × SE)가 잘 맞지 않기 때문이다.\n그러나 bootstrapping 방식을 통해서 충분히 CI를 추정할 수 있다. bootstrap은 데이터를 반복적으로 다시 샘플링해서, 어떤 통계량의 표본 분포를 직접 만들어내는 기법이다. 원래 데이터를 기준으로 같은 크기의 샘플을 복원추출로 다시 만든 후, 새롭게 만들어진 데이터로 lmer을 적합한다. 그 모델에서 ICC를 계산하고, 이 과정을 계속 반복하여 ICC의 분포를 만든다. 그 값들의 분포에서 CI을 추출할 수 있다.\n아래는 그 과정을 실행하는 코드이다.\nlibrary(lme4)\nlibrary(performance)\n\nset.seed(123)\n\nn_subjects &lt;- 30\nn_raters &lt;- 6\nratings_per_subject &lt;- 3\n\nsubjects &lt;- 1:n_subjects\nraters &lt;- 1:n_raters\n\n# 데이터프레임 생성\ndf &lt;- do.call(rbind, lapply(subjects, function(s) {\n  r &lt;- sample(raters, ratings_per_subject)\n  mu &lt;- rnorm(1, mean = 75, sd = 5)  # subject 고유의 평균 실력\n  data.frame(\n    subject = factor(s),\n    rater = factor(r),\n    score = rnorm(ratings_per_subject, mean = mu, sd = 2)\n  )\n}))\n\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = df, REML = TRUE)\n\nperformance::icc(model, ci =TRUE, iterations = 1000)\n출력\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.794 [0.472, 0.934]\n  Unadjusted ICC: 0.794 [0.472, 0.934]"
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html",
    "href": "source_code/Custom_Search_Zarathu.html",
    "title": "Depedency",
    "section": "",
    "text": "!pip install simplejson\n\nRequirement already satisfied: simplejson in c:\\users\\zarathu09\\anaconda3\\envs\\zarathu\\lib\\site-packages (3.18.3)\nfrom datetime import datetime\nimport os\nimport sys\nimport urllib.request\nimport pandas as pd \nimport json\nimport re \nimport requests\nimport simplejson"
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "href": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "title": "Depedency",
    "section": "API KEY 발급받는 방법은 링크를 참조해주세요",
    "text": "API KEY 발급받는 방법은 링크를 참조해주세요\n\n네이버 https://zerosecu.tistory.com/18\n\n일 허용 한도 25000건\n\n카카오 https://kadosholy.tistory.com/25\n구글 https://gomgomi.tistory.com/3\n\n일일 검색어 제한 10,000개\n\n# Naver_client_id = \n# Naver_client_secret = \n# Kakao_API_key= \n# Google_SEARCH_ENGINE_ID = \n# Google_API_KEY ="
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "href": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "title": "Depedency",
    "section": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!",
    "text": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!\n추가하실 도메인을 넣어주세요\n\nTrash_Link = [\"tistory\", \"kin\", \"youtube\", \"blog\", \"book\", \"news\", \"dcinside\", \"fmkorea\", \"ruliweb\", \"theqoo\", \"clien\", \"mlbpark\", \"instiz\", \"todayhumor\"]"
  }
]