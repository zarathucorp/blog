[
  {
    "objectID": "index.html#latest-insights",
    "href": "index.html#latest-insights",
    "title": "차라투 블로그",
    "section": "Latest Insights",
    "text": "Latest Insights"
  },
  {
    "objectID": "posts/2025-04-18-FDA_RCT/index.html",
    "href": "posts/2025-04-18-FDA_RCT/index.html",
    "title": "FDA의 RCT Guidance for industry",
    "section": "",
    "text": "이 문서의 경우 Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products Guidance for Industry를 기반으로 작성되었습니다.\n\nRCT의 경우 대부분 Internal validity의 측면에선 완전하기 때문에, 별다른 공변량의 보정 없이 보고 싶은 변수(ex. 치료 여부 와 결과 여부)만 분석 모형에 포함한 이후에 분석을 하게 됩니다. 다만, 이런 RCT의 경우에도 통계적으로 완전하지 않을 수 있습니다.\n\nRCT에 참여하게 된 환자를 완전하게 랜덤으로 나누게 되더라도, 통계적으로 두 집단의 특성이 완전히 일치하지 않을 수 있습니다(ex. 한 그룹의 나이가 다른 그룹보다 많을 수 있습니다). 이를 Random imbalance라고 부르는데, 집단의 특성이 여러개인 경우(ex. 나이 성별 당뇨병 유무 등등) 그 중 하나의 특성이라도 차이날 확률을 아래와 같이 계산해볼 수 있습니다 .\n\nlibrary(ggplot2)\nset.seed(123)\nn &lt;- 4000\nreps &lt;- 500\ncov_counts &lt;- c(1, 5, 10, 20, 40)\nprobs &lt;- sapply(cov_counts, function(p) {\n  mean(replicate(reps, {\n    covs &lt;- matrix(rnorm(n * p), ncol = p)\n    treat &lt;- rbinom(n, 1, 0.5)\n    any(sapply(seq_len(p), function(j) t.test(covs[treat == 1, j], covs[treat == 0, j])$p.value &lt; 0.05))\n  }))\n})\ndf &lt;- data.frame(covariates = cov_counts, prob = probs)\n\nggplot(df, aes(x = covariates, y = prob)) +\n  geom_smooth(method = \"loess\", span = 1.0, se = FALSE) +\n  scale_x_continuous(breaks = cov_counts) +\n  labs(\n    x = \"Number of Covariates\",\n    y = \"Probability of ≥1 p-value &lt; 0.05\",\n    title = \"Chance Imbalance vs Number of Covariates (n = 4000)\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n결과에서 알 수 있듯이, 사람의 수가 4000명이라고 가정하였을 때 보고 싶은 공변량의 갯수가 20개 이상만 되어도 하나 이상의 특성이 두 집단에서 차이가 날 확률이 50프로가 넘게 됩니다. 따라서 RCT에서도 공변량이 많은 경우 두 그룹의 모든 특성이 항상 완전히 일치한다고 말하기는 힘듭니다.\n\n잔차분산(residual variance)은 회귀모델이 설명하지 못한 남은 변동량을 나타냅니다.잔차는\\(\\hat\\varepsilon\\_i = y_i - \\hat y_i\\) (관측값 (y_i)에서 모델 예측값 (\\(\\hat y_i\\))를 뺀 값)으로 정의되고, 잔차분산 추정치는아래와 같이 계산됩니다.\n\\[\n\\hat\\sigma^2_{\\rm res}\n= \\frac{1}{n - k} \\sum_{i=1}^n \\hat\\varepsilon_i^2,\n\\]\n\n(n): 관측치 수\n\n(k): 회귀계수의 개수(절편 포함)\n\n\n“치료효과만” 조정한 모형(unadjusted)과 “치료효과 + baseline”을 함께 조정한 모형(adjusted)이 잔차분산에서 어떻게 달라지는지 살펴봅니다.\n모델 가정\\[\ny_i = \\alpha + \\tau,T_i + \\beta,X_i +\n\\varepsilon\\_i,\\quad \\varepsilon\\_i\\sim N(0,\\sigma^2)\n\\] \\[\nT_i: treatment 지시자, X_i: baseline 공변량 (예: 기저혈압), \\varepsilon\\_i: 순수 오차항\n\\] 이 실제 모델이라고 가정하겠습니다. 즉, baseline에 있는 공변량이 치료 결과에 영향을 미친다고 가정을 하겠습니다(실제 임상에서도 age, DM 유무와 같은 baseline covariate의 경우 치료 결과에 영향을 미치기 때문에 보다 현실적인 가정이라 할 수 있습니다.)\nUnadjusted 모형의 경우 아래와 같이 설정할 수 있습니다. \\[\n     y_i = \\alpha' + \\tau',T_i + e_i, \\quad e_i = \\beta,X_i +\n    \\varepsilon\\_unadjusted\n\\] \\[\n  잔차분산:  \\mathrm{Var}(e_i) =\n    {\\text{baseline이 설명하는 분산}} +\n    {\\text{실제 분산}}.\n\\] 으로 계산이 됨을 알 수 있습니다. Adjusted의 경우 모델이 아래와 같습니다. \\[\n   y_i = \\alpha + \\tau,T_i + \\beta,X_i + \\varepsilon\\_adjusted\n\\] 따라서 baseline을 포함하는 경우 실제 잔차에 더 가까운 추정을 할 수 있으며, 이는 잔차분산이 줄어들어 잔차의 밀도곡선이 더 좁아지고,결과적으로 treatment 효과의 표준오차(SE)가 감소하여 추론이 더 정밀해진다는 것을 알 수 있습니다다. 아래의 R code로 한번 테스트를 해보겠습니다\n\nlibrary(ggplot2)\nlibrary(reshape2)\nset.seed(123)\nn &lt;- 200\n\nbaseline &lt;- rnorm(n, mean = 150, sd = 20)\ntreat    &lt;- rbinom(n, 1, 0.5)\ny        &lt;- baseline - 20 * treat + rnorm(n, sd = 10)\ndf       &lt;- data.frame(y, treat = factor(treat), baseline)\n\n# Unadjusted 모형\nm0        &lt;- lm(y ~ treat, data = df)\ndf$res0   &lt;- resid(m0)\n\n# Adjusted 모형(baseline을 포함)\nm1        &lt;- lm(y ~ treat + baseline, data = df)\ndf$res1   &lt;- resid(m1)\n\n\ndf_long   &lt;- melt(df,\n                   measure.vars = c(\"res0\",\"res1\"),\n                   variable.name = \"model\",\n                   value.name    = \"residual\")\n\nggplot(df_long, aes(x = residual, fill = model)) +\n  geom_density(alpha = 0.4) +\n  coord_cartesian(ylim = c(0, 0.05)) +\n  labs(\n    title = \"Unadjusted vs Adjusted 모형에서 잔차분산 비교\",\n    x     = \"잔차(residual)\",\n    y     = \"밀도\"\n  ) +\n  scale_fill_manual(\n    name   = \"모형\",\n    values = c(\"res0\" = \"salmon\", \"res1\" = \"skyblue\"),\n    labels = c(\"Unadjusted\", \"Adjusted\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n결과에서 살펴보실 수 있듯이, 치료군을 랜덤으로 배정한 경우에도 결과에 영향을 미치는 covariate 변수를 보정하였을 때 표준오차가 감소하여 더 정밀한 추론이 가능해집니다.\n이제 문서의 내용을 본격적으로 살펴보도록 하겠습니다."
  },
  {
    "objectID": "posts/2025-04-18-FDA_RCT/index.html#intro-is-rct-always-perfect",
    "href": "posts/2025-04-18-FDA_RCT/index.html#intro-is-rct-always-perfect",
    "title": "FDA의 RCT Guidance for industry",
    "section": "",
    "text": "RCT의 경우 대부분 Internal validity의 측면에선 완전하기 때문에, 별다른 공변량의 보정 없이 보고 싶은 변수(ex. 치료 여부 와 결과 여부)만 분석 모형에 포함한 이후에 분석을 하게 됩니다. 다만, 이런 RCT의 경우에도 통계적으로 완전하지 않을 수 있습니다.\n\nRCT에 참여하게 된 환자를 완전하게 랜덤으로 나누게 되더라도, 통계적으로 두 집단의 특성이 완전히 일치하지 않을 수 있습니다(ex. 한 그룹의 나이가 다른 그룹보다 많을 수 있습니다). 이를 Random imbalance라고 부르는데, 집단의 특성이 여러개인 경우(ex. 나이 성별 당뇨병 유무 등등) 그 중 하나의 특성이라도 차이날 확률을 아래와 같이 계산해볼 수 있습니다 .\n\nlibrary(ggplot2)\nset.seed(123)\nn &lt;- 4000\nreps &lt;- 500\ncov_counts &lt;- c(1, 5, 10, 20, 40)\nprobs &lt;- sapply(cov_counts, function(p) {\n  mean(replicate(reps, {\n    covs &lt;- matrix(rnorm(n * p), ncol = p)\n    treat &lt;- rbinom(n, 1, 0.5)\n    any(sapply(seq_len(p), function(j) t.test(covs[treat == 1, j], covs[treat == 0, j])$p.value &lt; 0.05))\n  }))\n})\ndf &lt;- data.frame(covariates = cov_counts, prob = probs)\n\nggplot(df, aes(x = covariates, y = prob)) +\n  geom_smooth(method = \"loess\", span = 1.0, se = FALSE) +\n  scale_x_continuous(breaks = cov_counts) +\n  labs(\n    x = \"Number of Covariates\",\n    y = \"Probability of ≥1 p-value &lt; 0.05\",\n    title = \"Chance Imbalance vs Number of Covariates (n = 4000)\"\n  ) +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n결과에서 알 수 있듯이, 사람의 수가 4000명이라고 가정하였을 때 보고 싶은 공변량의 갯수가 20개 이상만 되어도 하나 이상의 특성이 두 집단에서 차이가 날 확률이 50프로가 넘게 됩니다. 따라서 RCT에서도 공변량이 많은 경우 두 그룹의 모든 특성이 항상 완전히 일치한다고 말하기는 힘듭니다.\n\n잔차분산(residual variance)은 회귀모델이 설명하지 못한 남은 변동량을 나타냅니다.잔차는\\(\\hat\\varepsilon\\_i = y_i - \\hat y_i\\) (관측값 (y_i)에서 모델 예측값 (\\(\\hat y_i\\))를 뺀 값)으로 정의되고, 잔차분산 추정치는아래와 같이 계산됩니다.\n\\[\n\\hat\\sigma^2_{\\rm res}\n= \\frac{1}{n - k} \\sum_{i=1}^n \\hat\\varepsilon_i^2,\n\\]\n\n(n): 관측치 수\n\n(k): 회귀계수의 개수(절편 포함)\n\n\n“치료효과만” 조정한 모형(unadjusted)과 “치료효과 + baseline”을 함께 조정한 모형(adjusted)이 잔차분산에서 어떻게 달라지는지 살펴봅니다.\n모델 가정\\[\ny_i = \\alpha + \\tau,T_i + \\beta,X_i +\n\\varepsilon\\_i,\\quad \\varepsilon\\_i\\sim N(0,\\sigma^2)\n\\] \\[\nT_i: treatment 지시자, X_i: baseline 공변량 (예: 기저혈압), \\varepsilon\\_i: 순수 오차항\n\\] 이 실제 모델이라고 가정하겠습니다. 즉, baseline에 있는 공변량이 치료 결과에 영향을 미친다고 가정을 하겠습니다(실제 임상에서도 age, DM 유무와 같은 baseline covariate의 경우 치료 결과에 영향을 미치기 때문에 보다 현실적인 가정이라 할 수 있습니다.)\nUnadjusted 모형의 경우 아래와 같이 설정할 수 있습니다. \\[\n     y_i = \\alpha' + \\tau',T_i + e_i, \\quad e_i = \\beta,X_i +\n    \\varepsilon\\_unadjusted\n\\] \\[\n  잔차분산:  \\mathrm{Var}(e_i) =\n    {\\text{baseline이 설명하는 분산}} +\n    {\\text{실제 분산}}.\n\\] 으로 계산이 됨을 알 수 있습니다. Adjusted의 경우 모델이 아래와 같습니다. \\[\n   y_i = \\alpha + \\tau,T_i + \\beta,X_i + \\varepsilon\\_adjusted\n\\] 따라서 baseline을 포함하는 경우 실제 잔차에 더 가까운 추정을 할 수 있으며, 이는 잔차분산이 줄어들어 잔차의 밀도곡선이 더 좁아지고,결과적으로 treatment 효과의 표준오차(SE)가 감소하여 추론이 더 정밀해진다는 것을 알 수 있습니다다. 아래의 R code로 한번 테스트를 해보겠습니다\n\nlibrary(ggplot2)\nlibrary(reshape2)\nset.seed(123)\nn &lt;- 200\n\nbaseline &lt;- rnorm(n, mean = 150, sd = 20)\ntreat    &lt;- rbinom(n, 1, 0.5)\ny        &lt;- baseline - 20 * treat + rnorm(n, sd = 10)\ndf       &lt;- data.frame(y, treat = factor(treat), baseline)\n\n# Unadjusted 모형\nm0        &lt;- lm(y ~ treat, data = df)\ndf$res0   &lt;- resid(m0)\n\n# Adjusted 모형(baseline을 포함)\nm1        &lt;- lm(y ~ treat + baseline, data = df)\ndf$res1   &lt;- resid(m1)\n\n\ndf_long   &lt;- melt(df,\n                   measure.vars = c(\"res0\",\"res1\"),\n                   variable.name = \"model\",\n                   value.name    = \"residual\")\n\nggplot(df_long, aes(x = residual, fill = model)) +\n  geom_density(alpha = 0.4) +\n  coord_cartesian(ylim = c(0, 0.05)) +\n  labs(\n    title = \"Unadjusted vs Adjusted 모형에서 잔차분산 비교\",\n    x     = \"잔차(residual)\",\n    y     = \"밀도\"\n  ) +\n  scale_fill_manual(\n    name   = \"모형\",\n    values = c(\"res0\" = \"salmon\", \"res1\" = \"skyblue\"),\n    labels = c(\"Unadjusted\", \"Adjusted\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n결과에서 살펴보실 수 있듯이, 치료군을 랜덤으로 배정한 경우에도 결과에 영향을 미치는 covariate 변수를 보정하였을 때 표준오차가 감소하여 더 정밀한 추론이 가능해집니다.\n이제 문서의 내용을 본격적으로 살펴보도록 하겠습니다."
  },
  {
    "objectID": "posts/2025-04-18-FDA_RCT/index.html#linear-models",
    "href": "posts/2025-04-18-FDA_RCT/index.html#linear-models",
    "title": "FDA의 RCT Guidance for industry",
    "section": "Linear Models",
    "text": "Linear Models\nATE(평균 효과) 추정\nRCT의 경우 선형 모델이 정확하지 않더라고 전체 집단의 평균 효과를 추정하는데는 편향이 없습니다. 따라서 회귀 계수가 전체 모집단 수준에서 추정이 가능하나, 모형이 현실을 더 잘 포착하면 분산이 줄어들어 검정력은 올라가게 됩니다. 아래의 예시를 통해 살펴보겠습니다.\n\nset.seed(123)\nn    &lt;- 500\nreps &lt;- 100\n\nests_unadj   &lt;- se_unadj   &lt;- numeric(reps)\nests_adj_lin &lt;- se_adj_lin &lt;- numeric(reps)\nests_adj_cub &lt;- se_adj_cub &lt;- numeric(reps)\n\nfor(i in seq_len(reps)) {\n  x   &lt;- rnorm(n)\n  tr  &lt;- rbinom(n,1,0.5)\n  # 실제로 y는 tr과 x의 세제곱에 의해서 결정된다고 가정\n  y   &lt;- 2*tr + 3*x^3 + rnorm(n,0,2)\n  \n  # y와 tr만 살펴봅니다\n  m0  &lt;- lm(y ~ tr)\n  sm0 &lt;- summary(m0)$coefficients\n  # y와 tr, 그리고 x의 선형관계만 살펴봅니다\n  m1  &lt;- lm(y ~ tr + x)\n  sm1 &lt;- summary(m1)$coefficients\n  # y와 tr, 그리고 x의 비선형관계를 3차항까지 함께 살펴봅니다\n  m2  &lt;- lm(y ~ tr + x + I(x^2) + I(x^3))\n  sm2 &lt;- summary(m2)$coefficients\n  \n  ests_unadj[i]   &lt;- sm0[\"tr\",\"Estimate\"]\n  se_unadj[i]     &lt;- sm0[\"tr\",\"Std. Error\"]\n  ests_adj_lin[i] &lt;- sm1[\"tr\",\"Estimate\"]\n  se_adj_lin[i]   &lt;- sm1[\"tr\",\"Std. Error\"]\n  ests_adj_cub[i] &lt;- sm2[\"tr\",\"Estimate\"]\n  se_adj_cub[i]   &lt;- sm2[\"tr\",\"Std. Error\"]\n}\n\nlibrary(dplyr)\ntibble(\n  Model         = c(\"Unadjusted\",\"Linear adj.\",\"Cubic adj.\"),\n  Mean_Est      = c(mean(ests_unadj),   mean(ests_adj_lin),   mean(ests_adj_cub)),\n  Empirical_SE  = c(sd(ests_unadj),     sd(ests_adj_lin),     sd(ests_adj_cub)),\n  Avg_Model_SE  = c(mean(se_unadj),     mean(se_adj_lin),     mean(se_adj_cub))\n)\n\n# A tibble: 3 × 4\n  Model       Mean_Est Empirical_SE Avg_Model_SE\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Unadjusted      2.09        0.954        1.04 \n2 Linear adj.     2.00        0.708        0.663\n3 Cubic adj.      2.02        0.166        0.180\n\n\n결과를 살펴보게 되면 세 모델 모두 tr의 효과인 2는 잘 포착하였지만, 표준오차의 경우 x의 3차항까지 정확하게 포착한 모델이 가장 정밀하게 포착함을 알 수 있습니다 (Empirical SE와 Avg_SE의 차이가 적을 수록 좋은 모델)\nCluster와 이분산성의 보정\n표준 오차의 경우 default를 1대1 배정 RCT에서는 모델이 misspecify되어도 허용하지만 그렇지 않은 경우엔 sandwich, bootstrap, cluster-robust를 권장한다고 되어있습니다. 아래의 예시를 통해 살펴보겠습니다\n\nlibrary(sandwich)  \nlibrary(lmtest)     \nlibrary(boot)      \n\nset.seed(123)\nn_cl    &lt;- 10    # 병원 수\nn_per   &lt;- 200   # 병원당 환자 수\ncluster &lt;- factor(rep(1:n_cl, each = n_per))  # 병원 클러스터\n\n# 병원별로 치료군과 비치료군을 반반씩 배정\ntreatment &lt;- factor(\n  rep(rep(c(0,1), each = n_per/2), times = n_cl),\n  levels = c(\"0\",\"1\")\n)\n\nx &lt;- rnorm(n_cl * n_per)\n# 병원별 랜덤 효과\ncluster_eff &lt;- rnorm(n_cl, sd = 2)[as.numeric(cluster)]\n# 사람별로 이분산성이 존재한다고 가정\nsigma &lt;- ifelse(x &gt; 0, 3, 1)\neps   &lt;- rnorm(n_cl * n_per, mean = 0, sd = sigma)\n\n# 2) 결과 변수 y 생성\n# y = 1.5*treatment + 0.8*x + 병원효과 + 이분산 오차\ny &lt;- 1.5 * as.numeric(as.character(treatment)) +\n     0.8 * x +\n     cluster_eff +\n     eps\n\ndf &lt;- data.frame(y, treatment, x, cluster)\n\n# 3) OLS 모형 적합\nm &lt;- lm(y ~ treatment + x, data = df)\n\n# 4.1 Nominal (기본) SE\nest_nominal &lt;- coef(m)[\"treatment1\"]  # treatment1 계수\nse_nominal  &lt;- coef(summary(m))[\"treatment1\",\"Std. Error\"]\n\n# 4.2 Huber-White sandwich SE\nse_sandwich &lt;- sqrt(diag(vcovHC(m, type = \"HC0\")))[\"treatment1\"]\n\n# 4.3 병원별 클러스터-로버스트 SE\nse_cluster &lt;- sqrt(diag(vcovCL(m,\n                                cluster = df$cluster,\n                                type    = \"HC0\")))[\"treatment1\"]\n\n# 4.4 부트스트랩 (개별 환자 단위 리샘플링)\nboot_ind &lt;- function(data, indices) {\n  coef(lm(y ~ treatment + x, data = data[indices, ]))[\"treatment1\"]\n}\nb1 &lt;- boot(df, statistic = boot_ind, R = 200)\nest_boot_ind &lt;- mean(b1$t) \nse_boot_ind  &lt;- sd(b1$t)    \n\n\nboot_clust &lt;- function(data, clusters) {\n  sel &lt;- sample(unique(clusters),\n                length(unique(clusters)),\n                replace = TRUE)\n  d2  &lt;- do.call(rbind,\n                lapply(sel, function(cl) data[clusters == cl, ]))\n  coef(lm(y ~ treatment + x, data = d2))[\"treatment1\"]\n}\nboot_coefs      &lt;- replicate(200, boot_clust(df, df$cluster))\nest_boot_clust &lt;- mean(boot_coefs)\nse_boot_clust  &lt;- sd(boot_coefs)\n\nres &lt;- data.frame(\n  Method       = c(\"Nominal\", \"Sandwich\", \"Cluster-robust\",\n                   \"Bootstrap(indiv.)\", \"Bootstrap(cluster)\"),\n  Estimate     = c(est_nominal,  est_nominal,  est_nominal,\n                   est_boot_ind, est_boot_clust),\n  StdError     = c(se_nominal,   se_sandwich,   se_cluster,\n                   se_boot_ind,  se_boot_clust)\n)\nprint(res)\n\n              Method Estimate   StdError\n1            Nominal 1.476932 0.13445377\n2           Sandwich 1.476932 0.13428108\n3     Cluster-robust 1.476932 0.06100908\n4  Bootstrap(indiv.) 1.492976 0.14092968\n5 Bootstrap(cluster) 1.480728 0.05974858\n\n\n결과를 살펴보면 모든 모델이 실제 치료효과인 1.5는 정확하게 포착한 것을 알 수 있습니다. 다만, cluster와 이분산성이 존재하는 경우 sandwich estimator의 경우 이분산을 보정하여 nominal보다 조금 더 나은 오차를 그리고 cluster-robust와 cluster를 반영한 bootstrapping의 경우 더 정밀한 표준오차를 제공하는 것을 알 수 있습니다. 따라서 cluster와 이분산성이 존재할 수 있는 데이터의 경우 이를 고려한 통계분석을 하는 것이 결과가 보다 정확하다는 것을 알 수 있습니니다.\n치료효과와 공변량 사이의 상호작용\n선형모델에서 treatment x covariate를 넣을 수 있지만 ATE의 경우 평범한 선형모델의 추정량으로도 사용이 가능합니니다.\n\nset.seed(123)\nn &lt;- 10000\nbiomarker &lt;- runif(n, 0, 100)\ntreatment &lt;- rbinom(n, 1, 0.5)\n# 치료결과는 치료변수와 biomarker와의 상호작용에도 영향을 받는다고 가정\ny &lt;- 5 * treatment + 0.2 * biomarker + 0.05 * treatment * biomarker + rnorm(n, 0, 10)\ndf &lt;- data.frame(y, treatment=factor(treatment), biomarker)\n\nm &lt;- lm(y ~ treatment * biomarker, data = df)\n\n# 조건부 효과(conditional effect) 계산\nbeta1 &lt;- coef(m)[\"treatment1\"]\nbeta3 &lt;- coef(m)[\"treatment1:biomarker\"]\neffects_subgroups &lt;- data.frame(\n  biomarker = c(10, 50, 90),\n  effect     = beta1 + beta3 * c(10, 50, 90)\n)\n\n# 평균처리효과(ATE) 계산\nmean_bio &lt;- mean(df$biomarker)\nate_hat  &lt;- beta1 + beta3 * mean_bio\nvc       &lt;- vcov(m)\nse_ate   &lt;- sqrt(\n  vc[\"treatment1\",\"treatment1\"] +\n  mean_bio^2 * vc[\"treatment1:biomarker\",\"treatment1:biomarker\"] +\n  2 * mean_bio * vc[\"treatment1\",\"treatment1:biomarker\"]\n)\n\nlist(\n  conditional_effects = effects_subgroups,\n  ATE                 = ate_hat,\n  SE_ATE              = se_ate\n)\n\n$conditional_effects\n  biomarker   effect\n1        10 5.808792\n2        50 7.693326\n3        90 9.577859\n\n$ATE\ntreatment1 \n   7.68178 \n\n$SE_ATE\n[1] 0.2003858\n\n\n결과를 살펴보면 biomarker의 값에 따라 치료효과가 달라진다는 것을 알 수 있습니다. biomarker(0-100사이의 정규분포)가 50일때의 치료효과가 기본적인 모델로 했을때의 값인 7.6과 가장 유사함을 알 수 있습니다. 따라서 두 모델을 모두 보여주는 것이 이상적이나 전체 치료효과를 보여주는데는 기본적인 모델을 써도 기초적인 기술은 가능하다는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2025-04-18-FDA_RCT/index.html#nonlinear-model",
    "href": "posts/2025-04-18-FDA_RCT/index.html#nonlinear-model",
    "title": "FDA의 RCT Guidance for industry",
    "section": "Nonlinear model",
    "text": "Nonlinear model\n모든 치료 결과 변수들이 연속변수일 수 없기 때문에 비선형 모델을 적합해야하는 경우가 많습니다.(생존분석, 결과가 이진변수) 기본적인 권고사항은 선형모델을 적합할때와 유사합니다. Subgroup 별로 treatment effect가 다를 수 있는데, OR HR 은 아까 얘기한 non-collapsibility가 존재하기 때문에 collapsible한 변수를 권고한다는 내용이 있습니다. 또한, 추정량을 사전에 정의할떄 conditional인지 unconditional인지 분명히 해야합니다.\n층화된 모델에서 binary 결과의 분석\nCochran-Mantel-Hasenszel(CMH) 방법은 binary 결과이고 covariate로 층화된 상태에서 조건부 치료 효과를 추정할떄 사용됩니다. 각 층마다 OR이 동일하다는 가정하고 층화된 카이제곱 검정 같은 분석이라고 보시면 됩니다. 예시 결과는 아래와 같습니다.\n\nset.seed(1)\nn_strata &lt;- 4\nn_per    &lt;- 200\nOR_true  &lt;- 2\n\nstratum    &lt;- rep(1:n_strata, each = n_per)\ntreatment  &lt;- rep(rep(c(0,1), each = n_per/2), times = n_strata)\nbase_p     &lt;- rep(c(0.1,0.2,0.3,0.4), each = n_per)\nlogit      &lt;- function(p) log(p/(1-p))\nexpit      &lt;- function(x) 1/(1+exp(-x))\n\np_control  &lt;- base_p\np_treated  &lt;- expit(logit(base_p) + log(OR_true))\n\ny &lt;- rbinom(length(stratum), 1,\n            ifelse(treatment==1, p_treated, p_control))\n\ndf &lt;- data.frame(stratum,\n                 treatment = factor(treatment, levels=c(0,1)),\n                 y)\n\ntab &lt;- with(df, table(stratum, treatment, y))\nres &lt;- mantelhaen.test(tab)\n\nres\n\n\n    Cochran-Mantel-Haenszel test\n\ndata:  tab\nCochran-Mantel-Haenszel M^2 = 3.2922, df = 3, p-value = 0.3487\n\n\n결과를 살펴보시면 p-value가 0.05이상으로 치료여부에 따른 결과의 차이가 없다는 것을 알 수 있습니다. 다만 CMH 방법의 경우 범주형 변수의 경우 사용이 가능하고 연속형 변수의 경우 로지스틱 회귀 분석을 사용할 수 있습니다. 비선형 모형을 쓸 때는misspecify 되지 않게 분석상의 함의를 담당 심사부서와 논의해야 한다는 내용또한 포함되어 있습니다. 즉, 선형모델에서 설명드린 바와 같이 공변량을 조정할때는 타당한 통계가정과 적합한 통계방법을 통해 표준오차를 토출해야합니다. 문서에서 제시한 예시방법(g-computation)과 reliable하다고 언급한 방법인 IPTW 분석을 살펴보겠습니다.\n문서에서 제시한 예시방법(g-computation)\n\n치료(treatment)와 사전 지정된 기저 공변량을 설명변수로 하는 로지스틱 회귀모형을 최우추정법(MLE)으로 적합한다.\n각 피험자마다, 치료군으로 가정했을 때의 반응 확률을 기저 공변량으로 예측한다.\n2단계에서 계산된 모든 피험자의 예측 확률을 평균내어 “치료 시 평균 반응 확률”을 구한다.\n각 피험자마다, 대조군으로 가정했을 때의 반응 확률을 기저 공변량으로 예측한다.\n4단계 예측 확률의 평균을 내어 “대조 시 평균 반응 확률”을 구한다.\n이 두 평균 반응률을 바탕으로 위험차(risk difference), 상대위험(relative risk), 오즈비(odds ratio) 등 원하는 무조건적 처리 효과(unconditional treatment effect)를 계산한다.\n예시\n\n고혈압 환자 100명을 대상으로, 1년 후 뇌졸중 발생 여부를 평가 RCT\n표준 vs 신약\nCovariate: age, baseline_SBP\nResult: Stroke\n신약의 효과(RR, RD, OR)을 g-computation으로 추정\ng computation\ng computation의 경우 로지스틱 회귀분석으로 개인별로 치료군과 혈압에 따라 뇌졸중이 발생할 확률을 계산합니다. 이후 모든 환자를 신약군이라 가정하여 기저 혈압으로 부터 뇌졸중이 발생할 확률을 평균화하여 구하고, 이후 모든 환자가 대조군일떄를 가정하여 기저 혈압으로부터 뇌졸중이 발생할 확률을 구하고 차이를 계산합니다.\nIPTW\nIPTW의 경우 각 환자가 혈압에 따라 신약군에 포함될 확률을 계산한 이후, 그 확률의 역수를 가중치로 주어 공변량을 보정하는 방법입니다.\n\nlibrary(dplyr)\n\nset.seed(123)\nn &lt;- 100                             # RCT 대상자 수\n\nbaseline_SBP &lt;- rnorm(n, 150, 20)    # 기저 혈압\ntreat_num    &lt;- rbinom(n, 1, 0.5)    # 무작위배정\ntreat        &lt;- factor(treat_num, levels=c(0,1))\nlp0          &lt;- -2 + 0.1*(baseline_SBP-150)\nlp1          &lt;- lp0 - 1.5 \np0           &lt;- plogis(lp0)\np1           &lt;- plogis(lp1)\ny            &lt;- rbinom(n,1,ifelse(treat_num==1, p1, p0))\ndf &lt;- data.frame(y, treat, baseline_SBP)\n\n\ncalc_measures &lt;- function(d) {\n  # Unadjusted\n  r1_un  &lt;- mean(d$y[d$treat==\"1\"])\n  r0_un  &lt;- mean(d$y[d$treat==\"0\"])\n  rd_un  &lt;- r1_un - r0_un\n  \n  # g-computation\n  m_gc   &lt;- glm(y ~ treat + baseline_SBP, family=binomial, data=d)\n  d1     &lt;- transform(d, treat=factor(1,levels=c(0,1)))\n  d0     &lt;- transform(d, treat=factor(0,levels=c(0,1)))\n  r1_gc  &lt;- mean(predict(m_gc,newdata=d1,type=\"response\"))\n  r0_gc  &lt;- mean(predict(m_gc,newdata=d0,type=\"response\"))\n  rd_gc  &lt;- r1_gc - r0_gc\n  rr_gc  &lt;- r1_gc / r0_gc\n  or_gc  &lt;- (r1_gc/(1-r1_gc)) / (r0_gc/(1-r0_gc))\n  \n  # IPTW\n  ps_mod &lt;- glm(as.numeric(as.character(treat)) ~ baseline_SBP,\n                family=binomial, data=d)\n  ps     &lt;- predict(ps_mod,type=\"response\")\n  w      &lt;- ifelse(d$treat==\"1\", 1/ps, 1/(1-ps))\n  r1_ip  &lt;- sum(w * d$y * (d$treat==\"1\")) / sum(w * (d$treat==\"1\"))\n  r0_ip  &lt;- sum(w * d$y * (d$treat==\"0\")) / sum(w * (d$treat==\"0\"))\n  rd_ip  &lt;- r1_ip - r0_ip\n  rr_ip  &lt;- r1_ip / r0_ip\n  or_ip  &lt;- (r1_ip/(1-r1_ip)) / (r0_ip/(1-r0_ip))\n  \n  c(rd_un, r1_gc, r1_ip,\n    rd_gc, rr_gc, or_gc,\n    rd_ip, rr_ip, or_ip)\n}\n\n# 3) 원본 지표 계산\norig &lt;- calc_measures(df)\n\n# 4) 부트스트랩으로 SE 계산 (B=200)\nB &lt;- 200\nboot_vals &lt;- replicate(B, {\n  idx &lt;- sample(seq_len(n), n, replace=TRUE)\n  calc_measures(df[idx,])\n})\nse_vals &lt;- apply(boot_vals, 1, sd)\n\n# 5) 결과 정리\nres &lt;- data.frame(\n  Method      = c(\"Unadjusted\",\"G-computation\",\"IPTW\"),\n  RiskDiff    = c(orig[1],   orig[4],   orig[7]),\n  SE_RiskDiff = c(se_vals[1],se_vals[4],se_vals[7]),\n  RR          = c(NA,       orig[5],   orig[8]),\n  SE_RR       = c(NA,       se_vals[5],se_vals[8]),\n  OR          = c(NA,       orig[6],   orig[9]),\n  SE_OR       = c(NA,       se_vals[6],se_vals[9])\n)\nprint(res)\n\n         Method   RiskDiff SE_RiskDiff        RR     SE_RR        OR     SE_OR\n1    Unadjusted -0.1041667  0.07519916        NA        NA        NA        NA\n2 G-computation -0.1567208  0.05039919 0.4074673 0.1410154 0.3358952 0.1375932\n3          IPTW -0.1386437  0.06140822 0.4791533 0.1800494 0.4030100 0.1910830\n\n\n결과를 살펴보면 표준오차가 줄어 정밀도가 개선되었다는 것을 알 수 있습니다.(실제 RD는 -0.136) 이는 G-computation과 IPTW 모두 공변량이 random imbalance가 발생하는 경우 이를 가중치 혹은 확률 계산을 통해 조정할 뿐만 아니라 이 과정에서 공변량이 모델에 반영되어 잔차분산도 줄여주기 때문입니다. 특히 n수가 적어 random imbalance가 발생하기 쉬운 경우 더 값을 정확하게 추정할 수 있습니다."
  },
  {
    "objectID": "posts/2025-04-02-model_compare_index/index.html",
    "href": "posts/2025-04-02-model_compare_index/index.html",
    "title": "Comparison of Models for Competing Risk Analysis",
    "section": "",
    "text": "[Theoretical background]\nCompeting Risk 분석에서 모델의 예측 성능을 평가하기 위해 일반적으로 사용되는 지표는 다음과 같다.\n1. Harrell’s C-index\n개념:\n관찰된 사건 발생 순서와 모델의 예측 위험 순서가 얼마나 잘 일치하는지를 나타낸다.\n범위는 0부터 1이고, 0.5는 무작위 예측과 동일한 성능이며 1에 가까울수록 예측 성능이 좋다.\n계산 방법:\nFine-Gray 모델을 이용하여 Competing Risk 데이터를 일반적인 생존 분석 형태로 변환한 후\nCox 비례위험 모델로 분석한 결과를 바탕으로 계산한다.\nR에서는 survival::finegray()로 데이터를 변환하고, survival::coxph()와 survival::concordance()로 C-index를 구한다.\n2. Wolbers’ C-index\n개념:\nHarrell’s C-index 개념을 확장하여 Competing Risk 상황에서 특정 원인의 사건 발생을 예측할 때 사용한다.\n계산 방법:\nFine-Gray 모델(FGR)을 직접 적용하여 사건 특화된 C-index를 구한다.\nBootstrapping을 이용하여 반복 표본추출로 신뢰구간을 얻는다.\nR에서는 riskRegression::FGR()과 pec::cindex()를 사용한다.\n3. AUC (Area Under Curve)\n개념:\n특정 시점에서 사건 발생 여부를 이진 분류로 간주하고, 예측의 민감도와 특이도를 종합적으로 평가한다.\n1에 가까울수록 예측 성능이 뛰어나고, 0.5에 가까울수록 무작위와 유사하다.\n계산 방법:riskRegression::Score() 함수를 이용해 특정 시점(예: 5년, 10년)의 AUC와 신뢰구간을 계산한다.\n4. Brier Score\n개념:\n예측 확률과 실제 관찰된 사건의 발생 여부 간의 차이를 측정하는 지표이다.\n낮을수록 성능이 우수하고, 0에 가까울수록 완벽한 예측이다.\n계산 방법:\nAUC와 같은 방식으로 riskRegression::Score() 함수로 특정 시점의 Brier Score와 신뢰구간을 구한다.\n[Preprocess]\nmelanoma 데이터에서 필요한 변수(sex, age, thickness, ulcer)를 추출하고, status 변수는 censor=0인 Competing Risk 형태로 재구성한다. 시간을 연 단위로 변환하여 분석을 위한 데이터(melanoma_dt)를 준비한다.status: 1=melanoma 사망, 2=생존, 3=melanoma 외 사망status_competing: 0=생존, 1=melanoma 사망, 2=melanoma 외 사망\n\nsuppressPackageStartupMessages({\n  library(data.table);library(survival);library(boot);library(magrittr);library(riskRegression);library(prodlim);library(pec);library(rmarkdown)\n})\n\nmelanoma_dt &lt;- as.data.table(melanoma) %&gt;% .[,c(\"sex\",\"ulcer\"):=lapply(.SD,as.factor),.SDcols=c(\"sex\",\"ulcer\")] %&gt;% \n  .[,status_competing:=factor(ifelse(status==1,1,ifelse(status==2,0,2)), levels=0:2)] %&gt;% \n  .[,time_Y:=time/365.25] %&gt;% .[,-c(\"year\",\"status\",\"time\")]\n\nmodel_var_list &lt;- list(c(\"sex\",\"age\"), c(\"sex\",\"age\",\"thickness\"), c(\"sex\",\"age\",\"ulcer\"), c(\"sex\",\"age\",\"thickness\",\"ulcer\"))\n\nHist_formula_list &lt;- list( Hist(time_Y, status_competing) ~ sex+age,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness,\n                           Hist(time_Y, status_competing) ~ sex+age+ulcer,\n                           Hist(time_Y, status_competing) ~ sex+age+thickness+ulcer )\n\n[Each model]\n4가지 변수 조합의 모델을 설정하고 각각의 Harrell C-index, Wolbers C-index, AUC, Brier score를 계산한다. Wolbers’ C-index의 신뢰구간은 bootstrap 방법을 이용해 반복 표본추출로 계산한다.\n\ninvisible(capture.output({\n  table_1 &lt;- lapply (1:length(model_var_list), function(i) { \n  # Harrell_C_index_info\n  fgr_data_for_Harrell_C_index &lt;- survival::finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n  fgr_model_for_Harrell_C_index &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index)\n  Harrell_C_index_info &lt;- survival::concordance(fgr_model_for_Harrell_C_index)\n  \n  # Wolbers_C_index_info\n  fgr_model_for_Wolbers_C_index &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  Wolbers_C_index_info &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index, formula=Hist_formula_list[[i]], data=melanoma_dt, cause=1, confInt=T, verbose=F)\n  \n  cindex_values &lt;- numeric()\n  for(b in 1:20){\n    indices &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n    boot_data &lt;- melanoma_dt[indices]\n    fgr_model_for_Wolbers_C_index_boot &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=boot_data, cause=1)\n    Wolbers_C_index_info_boot &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot, formula=Hist_formula_list[[i]], data=boot_data, cause=1, verbose=F)\n    cindex_values[[b]] &lt;- Wolbers_C_index_info_boot$AppCindex$FGR\n  }\n  Wolbers_C_index_lower &lt;- quantile(cindex_values, probs=0.05/2)\n  Wolbers_C_index_upper &lt;- quantile(cindex_values, probs=(1-(0.05/2)))\n\n  # AUC_and_Brier_info\n  fgr_model_for_AUC_and_Brier &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  AUC_and_Brier_info &lt;- riskRegression::Score(list(fgr_model_for_AUC_and_Brier), formula=Hist_formula_list[[i]] , data=melanoma_dt, times=c(5,10), null.model=F)\n\n  # return\n  data.table( \"Model\"=LETTERS[i],\n              \"Harrell_C_index\"=paste0(sprintf(\"%.04f\",Harrell_C_index_info$concordance),\"(\",\n                                       sprintf(\"%.04f\",Harrell_C_index_info$concordance-qnorm(0.975)*sqrt(Harrell_C_index_info$var)),\"-\",\n                                       sprintf(\"%.04f\",Harrell_C_index_info$concordance+qnorm(0.975)*sqrt(Harrell_C_index_info$var)),\")\"),\n              \"Wolbers_C_index\"= paste0(sprintf(\"%.04f\",Wolbers_C_index_info$AppCindex$FGR),\"(\",\n                                        sprintf(\"%.04f\",Wolbers_C_index_lower),\"-\",\n                                        sprintf(\"%.04f\",Wolbers_C_index_upper),\")\"),\n              \"AUC_t=5\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$AUC),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[1]$upper),\")\"),\n              \"AUC_t=10\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$AUC),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$AUC$score[2]$upper),\")\"),\n              \"Brier_t=5\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$Brier),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[1]$upper),\")\"),\n              \"Brier_t=10\"=paste0(sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$Brier),\" (\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$lower),\"-\",\n                               sprintf(\"%.04f\",AUC_and_Brier_info$Brier$score[2]$upper),\")\")\n  ) }) %&gt;% \n    do.call(rbind,.)\n}))\n\n[Compare two model]\n1. Compare Harrell_C_index\nHarrell’s C-index는 각 모델 간의 concordance 함수를 통해 비교하며, 차이의 유의성을 Z 검정으로 평가한다\n\ntable_2_Harrell_C_index &lt;- lapply(1:length(model_var_list), function(i) {\n  fgr_data_for_Harrell_C_index &lt;- survival::finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n  fgr_model_for_Harrell_C_index &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[i]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index)\n \n  lapply(1:length(model_var_list), function(j) {\n    fgr_data_for_Harrell_C_index_new &lt;- finegray(as.formula(paste0(\"Surv(time_Y, status_competing) ~ \",paste(model_var_list[[j]],collapse=\"+\"))), data=melanoma_dt, etype=1)\n    fgr_model_for_Harrell_C_index_new &lt;- coxph(as.formula(paste0(\"Surv(fgstart, fgstop, fgstatus) ~ \",paste(model_var_list[[j]],collapse=\"+\"))), data=fgr_data_for_Harrell_C_index_new)\n  \n    ctest &lt;- concordance(fgr_model_for_Harrell_C_index, fgr_model_for_Harrell_C_index_new)\n    contr &lt;- c(-1, 1)\n    dtest &lt;- contr %*% coef(ctest)\n    dvar &lt;- contr %*% vcov(ctest) %*% contr\n    z &lt;- dtest/sqrt(dvar)\n    p_value_temp &lt;- 2 * (1 - pnorm(abs(z))) \n    p_value &lt;- ifelse(p_value_temp&lt;0.001,\"&lt;0.001\",sprintf(\"%.03f\",p_value_temp))\n    return(p_value)\n  }) %&gt;% unlist()\n}) %&gt;% do.call(rbind,.) %&gt;% as.data.table() %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Harrell_C_index=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\nrmarkdown::paged_table(table_2_Harrell_C_index)\n\n\n  \n\n\n\n2. Compare Wolbers_C_index\nWolbers’ C-index는 bootstrap을 이용한 paired-sample t-test로 두 모델 간 차이를 평가한다. bootstrapping이므로 pair(model1, model2)에 대해 1번만 코드 실행한다.\n\nset.seed(10)\ninvisible(capture.output({\n   table_2_Wolbers_C_index &lt;- lapply(1:(length(model_var_list)-1), function(i) {\n  cindex_values &lt;- numeric()\n  for(b in 1:20){\n    indices &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n    boot_data &lt;- melanoma_dt[indices]\n    fgr_model_for_Wolbers_C_index_boot &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=boot_data, cause=1)\n    Wolbers_C_index_info_boot &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot, formula=Hist_formula_list[[i]], data=boot_data, cause=1, verbose=F)\n    cindex_values[[b]] &lt;- Wolbers_C_index_info_boot$AppCindex$FGR\n  }\n\n  lapply ((i+1):length(model_var_list), function(j) { # A와 B,C,D 비교, B와 C,D 비교, C와 D 비교 \n    cindex_values_new &lt;- numeric()\n    for(b in 1:20){\n      indices_new &lt;- sample(seq_len(nrow(melanoma_dt)), size=nrow(melanoma_dt), replace=T)\n      boot_data_new &lt;- melanoma_dt[indices_new]\n      fgr_model_for_Wolbers_C_index_boot_new &lt;- riskRegression::FGR(Hist_formula_list[[j]], data=boot_data_new, cause=1)\n      Wolbers_C_index_info_boot_new &lt;- pec::cindex(object=fgr_model_for_Wolbers_C_index_boot_new, formula=Hist_formula_list[[j]], data=boot_data_new, cause=1, verbose=F)\n      cindex_values_new[[b]] &lt;- Wolbers_C_index_info_boot_new$AppCindex$FGR\n    }\n      \n    p_value_temp &lt;- t.test(cindex_values-cindex_values_new, mu=0)$p.value\n    return( ifelse(p_value_temp&lt;0.001,\"&lt;0.001\",sprintf(\"%.03f\",p_value_temp)) )\n  }) %&gt;% unlist() %&gt;% c(rep(\"-\",(i-1)),.)\n}) %&gt;% do.call(rbind,.) %&gt;% as.data.table() %&gt;% setnames(c(\"B\",\"C\",\"D\")) %&gt;% \n  cbind(\"A\"=rep(\"-\",3),.) %&gt;% rbind(\"D\"=data.table(\"-\",\"-\",\"-\",\"-\"), use.names=F) %&gt;% cbind(Wolbers_C_index=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\n}))\nrmarkdown::paged_table(table_2_Wolbers_C_index)\n\n\n  \n\n\n\n3. Compare AUC_and_Brier\nAUC와 Brier Score는 riskRegression::Score 함수를 이용하여 각 모델 간의 성능 차이를 비교한다.\n\ninvisible(capture.output({\n  table_2_AUC_and_Brier &lt;- lapply(1:length(model_var_list), function(i) {\n  fgr_model_AUC_and_Brier &lt;- riskRegression::FGR(Hist_formula_list[[i]], data=melanoma_dt, cause=1)\n  \n  lapply (1:length(model_var_list), function(j) {\n    fgr_model_AUC_and_Brier_new &lt;- riskRegression::FGR(Hist_formula_list[[j]], data=melanoma_dt, cause=1)\n    \n    AUC_and_Brier &lt;- riskRegression::Score(list(fgr_model_AUC_and_Brier,fgr_model_AUC_and_Brier_new),\n                                           formula=Hist(time_Y, status_competing) ~ 1, data=melanoma_dt, times=c(5,10), null.model=F)\n    \n    data.table(\"V1\"=c(sprintf(\"%.03f\",AUC_and_Brier$AUC$contrasts$p), sprintf(\"%.03f\",AUC_and_Brier$Brier$contrasts$p))) %&gt;% setnames(as.character(j))\n  }) %&gt;% do.call(cbind,.)\n  })\n}))\n\n\ntable_2_AUC_t_5 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][1]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(AUC_t_5=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_AUC_t_5[i,(i+1):=\"-\"]\n}\nrmarkdown::paged_table(table_2_AUC_t_5)\n\n\n  \n\n\n\n\ntable_2_AUC_t_10 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][2]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(AUC_t_10=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_AUC_t_10[i,(i+1):=\"-\"]\n}\nrmarkdown::paged_table(table_2_AUC_t_10)\n\n\n  \n\n\n\n\ntable_2_Brier_t_5 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][3]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Brier_t_5=c(\"A\",\"B\",\"C\",\"D\"),.)\nfor (i in seq(length(model_var_list))) {\n  table_2_Brier_t_5[i,(i+1):=\"-\"]\n}\nrmarkdown::paged_table(table_2_Brier_t_5)\n\n\n  \n\n\n\n\ntable_2_Brier_t_10 &lt;- lapply(1:length(model_var_list), function(k){\n  table_2_AUC_and_Brier[[k]][4]\n}) %&gt;% do.call(rbind,.) %&gt;% setnames(c(\"A\",\"B\",\"C\",\"D\")) %&gt;% cbind(Brier_t_10=c(\"A\",\"B\",\"C\",\"D\"),.) %&gt;% replace(is.na(.),\"-\")\nfor (i in seq(length(model_var_list))) {\n  table_2_Brier_t_10[i,(i+1):=\"-\"]\n}\nrmarkdown::paged_table(table_2_Brier_t_10)\n\n\n  \n\n\n\n각 지표는 성능이 좋을수록 C-index와 AUC는 높게, Brier Score는 낮게 나타난다.\n모델 간 비교 시 p-value가 작을수록 두 모델 간 성능의 유의한 차이가 있음을 의미한다.p-value가 0.05 미만이면 통계적으로 유의하게 두 모델의 성능이 다름을 나타낸다.\n이러한 과정을 통해 Competing Risk 모델의 성능을 객관적으로 평가하고 비교할 수 있으며, 실무적 활용도가 높은 모델을 선택하는 데 중요한 기준을 제공한다.\n\n\nCitationBibTeX citation:@online{han2025,\n  author = {Han, Suhyun},\n  title = {Comparison of {Models} for {Competing} {Risk} {Analysis}},\n  date = {2025-04-07},\n  url = {https://blog.zarathu.com/posts/2025-04-02-model_compare_index/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHan, Suhyun. 2025. “Comparison of Models for Competing Risk\nAnalysis.” April 7, 2025. https://blog.zarathu.com/posts/2025-04-02-model_compare_index/."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "Intraclass Correlation Coefficient (ICC)는 집단으로 구성된 단위들에 대해 수치 측정값이 주어졌을 때, 동일한 집단 내 단위들이 서로 얼마나 유사한지를 나타내는 기술 통계량이다. 주로 측정 신뢰도를 평가할 때 사용되며, 특히 동일한 대상에 대해 여러 관측자(또는 반복 측정)가 수행한 측정의 일관성을 평가하는 데 유용하다.\nICC는 일반적인 상관계수와 달리 짝지어진 관측값이 아니라, 그룹으로 구성된 데이터에 대해 계산된다는 점에서 차이가 있다.\n\n\nICC와 Cohen’s Kappa는 모두 측정자 간 일치도를 측정하는 도구지만, 적용되는 데이터의 특성과 해석 방식에서 차이가 있다.\n자료의 유형\n\nICC는 연속형 변수에 사용됨\nCohen’s Kappa는 범주형 변수에 사용됨\n\n즉 ICC는 “얼마나 수치들이 서로 비슷한가?”를 보는 것이고, Cohen’s kappa는 “서로 같은 범주로 분류했는가?”를 보는 것이다.\n평가방식\n\nICC는 분산을 기반으로 측정값 간의 상관 관계(유사성)를 분석함.\nCohen’s Kappa는 기대 일치도를 고려하여, 관측된 일치도와 우연히 일어날 수 있는 일치도의 차이를 보정함.\n\n즉, ICC는 총 변동성 중에서 집단 내 일관성에 기인하는 부분의 비율을 나타내는 것이고, Cohen’s Kappa는 실제 일치가 단순히 우연에 의한 것이 아님을 보장해 주는 것이다.\n측정자 수\n\nICC는 둘 이상의 평가자에게 적용할 수 있음. 측정 모델에 따라 다양한 변형등이 존재함.\nCohen’s Kappa는 전통적으로 두 평가자에 대한 일치도 측정에 사용되며, 다수 평가자일 경우에는 Fleiss’ kappa 등을 사용함.\n\n\n\n\n\n심리학, 의학, 교육학 등에서 테스트 재현성 또는 평가자 간 신뢰도 분석\n머신러닝/데이터 분석에서는 데이터 라벨링 일관성 확인\n반복 측정 설계에서 측정의 안정성 검증"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-vs.-cohens-kappa",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-vs.-cohens-kappa",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "ICC와 Cohen’s Kappa는 모두 측정자 간 일치도를 측정하는 도구지만, 적용되는 데이터의 특성과 해석 방식에서 차이가 있다.\n자료의 유형\n\nICC는 연속형 변수에 사용됨\nCohen’s Kappa는 범주형 변수에 사용됨\n\n즉 ICC는 “얼마나 수치들이 서로 비슷한가?”를 보는 것이고, Cohen’s kappa는 “서로 같은 범주로 분류했는가?”를 보는 것이다.\n평가방식\n\nICC는 분산을 기반으로 측정값 간의 상관 관계(유사성)를 분석함.\nCohen’s Kappa는 기대 일치도를 고려하여, 관측된 일치도와 우연히 일어날 수 있는 일치도의 차이를 보정함.\n\n즉, ICC는 총 변동성 중에서 집단 내 일관성에 기인하는 부분의 비율을 나타내는 것이고, Cohen’s Kappa는 실제 일치가 단순히 우연에 의한 것이 아님을 보장해 주는 것이다.\n측정자 수\n\nICC는 둘 이상의 평가자에게 적용할 수 있음. 측정 모델에 따라 다양한 변형등이 존재함.\nCohen’s Kappa는 전통적으로 두 평가자에 대한 일치도 측정에 사용되며, 다수 평가자일 경우에는 Fleiss’ kappa 등을 사용함."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-사용분야",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-사용분야",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "",
    "text": "심리학, 의학, 교육학 등에서 테스트 재현성 또는 평가자 간 신뢰도 분석\n머신러닝/데이터 분석에서는 데이터 라벨링 일관성 확인\n반복 측정 설계에서 측정의 안정성 검증"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#초기-icc의-정의",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#초기-icc의-정의",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "초기 ICC의 정의",
    "text": "초기 ICC의 정의\nRonald Fisher가 제안한 초기 ICC는 다음과 같다.\n\\[\nr = \\sum_{n=1}^{N} \\frac{(x_{n,1} - \\bar{x})(x_{n,2} - \\bar{x})}{Ns^2}\n\\]\n\n데이터셋은 \\(N\\)개의 짝으로 구성됨\n각 개체 \\(n\\)에 대해 두 개의 측정값 \\((x_{n,1} , x_{n,2})\\)가 존재함\n\\(\\bar{x}\\)는 전체 데이터의 평균 \\[\n\\bar{x} = \\frac{1}{2N} \\sum_{n=1}^{N} (x_{n,1} + x_{n,2})\n\\]\n\\(s^2\\)은 전체 데이터의 분산 \\[\ns^2 = \\frac{1}{2N} \\left\\{ \\sum_{n=1}^{N} (x_{n,1} - \\bar{x})^2 + \\sum_{n=1}^{N} (x_{n,2} - \\bar{x})^2 \\right\\}\n\\]\n\n\n\n\nr 값 범위\n신뢰도의 정도\n\n\n\n\nr ≤ 0.00\nPoor\n\n\n0.00 &lt; r ≤ 0.20\nSlight\n\n\n0.20 &lt; r ≤ 0.40\nFair\n\n\n0.40 &lt; r ≤ 0.60\nModerate\n\n\n0.60 &lt; r ≤ 0.80\nSubstantial\n\n\n0.80 &lt; r ≤ 1.00\nAlmost Perfect"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#pearson-상관계수와-icc의-차이점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#pearson-상관계수와-icc의-차이점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Pearson 상관계수와 ICC의 차이점",
    "text": "Pearson 상관계수와 ICC의 차이점\nPearson 상관계수와 ICC는 모두 연속형 데이터를 비교할 때 사용하는 지표이지만, 그들이 측정하는 대상과 의미는 전혀 다르다.\n먼저 Pearson 상관계수는 두 변수 간의 선형 관계(일관된 증가/감소)를 측정한다. 예를 들어, 키와 몸무게처럼, 한 변수가 커질수록 다른 변수도 함께 커지는 경향이 있는지를 파악할 때 유용하다. 이때 값은 -1에서 +1 사이이며, 1에 가까울수록 완벽한 양의 선형 관계를 의미한다. 하지만 이 지표는 값 자체의 차이는 고려하지 않는다.\n반면 ICC는 동일한 대상을 여러 번 측정했을 때 그 값이 얼마나 비슷하게 나오는지를 측정하는 지표아다. 즉 서로 다른 평가자나 시간에 따라 측정값이 달라졌을 때, 그 변화가 개인의 특성 차이 때문인지, 아니면 측정자의 오차 때문인지를 분리할 수 있다. ICC 값은 일반적으로 0에서 1 사이에 있으며, 1에 가까울수록 일관되고 신뢰할 수 있는 측정이라고 해석할 수 있다.\n중요한 차이는 해석의 포인트에 있다:\nPearson 상관계수는 측정값 간의 선형 관계만을 보기 때문에, 두 평가자가 항상 10점 차이를 주더라도 여전히 상관계수는 1이 될 수 있다.\n하지만 ICC는 ’값 자체가 얼마나 유사한지’를 보며, 위와 같은 상황에서는 일치도가 낮다고 판단한다. 즉 Pearson은 “패턴의 일관성”, ICC는 “값의 일치도”를 본다고 이해할 수 있다.\n또한, Pearson 상관계수는 서로 다른 단위를 가진 두 변수(예: 키와 체중)에도 적용 가능하지만, ICC는 동일한 측정 단위 내에서만 적절하게 해석할 수 있다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#개-이상의-값을-가진-그룹에서의-icc",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#개-이상의-값을-가진-그룹에서의-icc",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "3개 이상의 값을 가진 그룹에서의 ICC",
    "text": "3개 이상의 값을 가진 그룹에서의 ICC\n데이터셋이 각 그룹당 3개의 측정값을 가지는 경우, ICC는 다음과 같이 확장된다. \\[\nr = \\frac{1}{3N s^2} \\sum_{n=1}^{N} \\left\\{\n(x_{n,1} - \\bar{x})(x_{n,2} - \\bar{x}) +\n(x_{n,1} - \\bar{x})(x_{n,3} - \\bar{x}) +\n(x_{n,2} - \\bar{x})(x_{n,3} - \\bar{x})\n\\right\\}\n\\]\n\n\\(\\bar{x}\\)는 전체 평균 \\[\n\\bar{x} = \\frac{1}{3N} \\sum_{n=1}^{N} (x_{n,1} + x_{n,2} + x_{n,3})\n\\]\n\\(s^2\\)는 전체 분산 \\[\ns^2 = \\frac{1}{3N} \\left\\{\n\\sum_{n=1}^{N} (x_{n,1} - \\bar{x})^2 +\n\\sum_{n=1}^{N} (x_{n,2} - \\bar{x})^2 +\n\\sum_{n=1}^{N} (x_{n,3} - \\bar{x})^2\n\\right\\}\n\\] 여기서, 그룹의 크기(K)가 커질수록, 계산 과정에서 고려해야 할 교차항의 수도 증가한다.\n\n위 공식을 일반화하면 다음과 같아진다. \\[\nr = \\frac{K}{K-1} \\cdot \\frac{1}{Ns^2} \\sum_{n=1}^{N} (\\bar{x}_n - \\bar{x})^2 - \\frac{1}{K-1}\n\\]\n\n\\(K\\)는 그룹당 데이터 개수\n\\(\\bar{x_n}\\)는 \\(n\\)번째 그룹의 평균\n\n\\(K=3\\)을 대입하면 위 공식과 완벽히 같아진다. 위 공식에 따르면, ICC값은 항상 \\(\\frac{-1}{K-1}\\)이상의 값을 가진다는 것을 알 수 있다. 따라서 ICC는 항상 \\(-1 \\leq r \\leq 1\\)의 범위 안에서 존재하지만, 데이터 개수(\\(K\\))가 많아질 수록 음수로 나올 가능성이 줄어든다.\n또한 충분히 큰 \\(K\\)에 대해서, 다음과 같이 근사할 수도 있다. \\[\nr = \\frac{K}{K-1} \\cdot \\frac{1}{Ns^2} \\sum_{n=1}^{N} (\\bar{x}_n - \\bar{x})^2\n\\]"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-해석-및-한계점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc의-해석-및-한계점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC의 해석 및 한계점",
    "text": "ICC의 해석 및 한계점\nICC는 총 분산 중에서 그룹 간 변동이 차지하는 비율로 해석할 수 있다.\n이상적인 데이터에서는 ICC 값이 0~1 사이에 있어야 하지만, 실제 샘플 데이터에서는 음수 값이 나올 수도 있다. 이는 Ronald Fisher가 ICC를 편향되지 않은 추정량으로 설계했기 때문이다. 따라서 모집단에서 ICC가 0일 경우, 표본 데이터에서는 음수 값이 나올 수 있다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss-1979",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss-1979",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Shrout & Fleiss (1979)",
    "text": "Shrout & Fleiss (1979)\nShrout & Fleiss는 초기의 ICC를 체계적으로 분류하였다. 3가지 모델 유형(Model 1, 2, 3), 2가지 측정 방식(1: single, k: 평균)으로 분류하여 총 6가지 유형의 ICC를 정의하였다.\n모델 종류\n\none-way random\n\n피험자 간의 변동만을 고려하는 모형\n피험자 간의 차이에 대한 평가지의 일치도를 평가할 때 사용함\n평가자의 효과는 고려하지 않고, 단순히 피험자 간의 일관성을 평가할 때 사용함\n\ntwo-way random\n\n피험자 간 변동뿐만이 아니라, 평가자 간의 변동도 고려하는 모형\n동일한 피험자에 대해 평가한 결과가 얼마나 일치하는지, 평가자들 간의 평가 결과가 얼마나 일관성 있는 지를 평가할 때 사용함\n\ntwo-way mixed\n\n피험자 간의 변동과 평가자 간의 변동을 고려하는 모형\n특정 평가자들이 고정되어 있을 때, 피험자 간의 일치도를 평가하는 데 사용함\n\n\n측정방식\n\n단일 측도(single)\n\n평가자 간에 얼마나 차이가 있는지 확인\n각 평가자에 의해 한 번의 측정이 일어난 경우\n\n평균 측도(average)\n\n평균값과 얼마나 차이가 있는지 확인\n각 평가자에 의해 여러 번 측정이 일어난 경우"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#mcgraw-wong-1996",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#mcgraw-wong-1996",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "McGraw & Wong (1996)",
    "text": "McGraw & Wong (1996)\nMcGraw & Wong은 Shrout & Fleiss의 체계를 확장하여 총 10가지 ICC 형태를 정의하였다. 다음과 같은 총 3가지의 분류 기준을 제시하였다.\n모델 종류\n\none-way random\ntwo-way random\ntwo-way mixed\n\n측정방식\n\n단일 측도(single)\n평균 측도(average)\n\n정의(Definition Agreement)\n\n일치도(consistency)\n\n상대적 순위/경향이 일치하는지를 의미\n평가자 간의 체계적인 차이는 무시하고, 변동성만 분석\n포함하는 오차 : 우연한 변동\n사용 상황 : 평가자가 고정되어 있고, 상대적 순위가 중요한 경우\n\n절대합의도(absolute agreement)\n\n두 평가자의 결과가 완전히 같은지를 의미\n평가자 간의 체계적인 차이까지도 고려\n포함하는 오차 : 우연한 변동, 평가자 간 편향\n사용 상황 : 정량적 측정이 실제 절대값의 일치도를 요구하는 경우"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-분류",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-분류",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC 분류",
    "text": "ICC 분류\n아래의 표는 Shrout & Fleiss와 McGraw & Wong의 기준에 따라 ICC를 정리한 것이다.\n\n\n\n\n\n\n\n\nMcGraw and Wong\nShrout and Fleiss\nFormulas for Calculating ICC\n\n\n\n\nOne-way random effects, absolute agreement, single rater/measurment\nICC(1,1)\n\\(\\frac{MS_R - MS_W}{MS_R +(k+1)MS_W}\\)\n\n\nTwo-way random effects, consistency, single rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E}\\)\n\n\nTwo-way random effects, absolute agreement, single rater/measurment\nICC(2,1)\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E + \\frac{k}{n} (MS_C - MS_E )}\\)\n\n\nTwo-way mixed effects, consistency , single rater/measurment\nICC(3,1)\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E}\\)\n\n\nTwo-way mixed effects, absolute agreement, single rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R +(k-1)MS_E + \\frac{k}{n} (MS_C - MS_E )}\\)\n\n\nOne-way random effects, absolute agreement, multiple rater/measurment\nICC(1,k)\n\\(\\frac{MS_R - MS_W}{MS_R}\\)\n\n\nTwo-way random effects, consistency, multiple rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R}\\)\n\n\nTwo-way random effects, absolute agreement, multiple rater/measurment\nICC(2,k)\n\\(\\frac{MS_R - MS_E}{MS_R + \\frac{MS_C -MS_E}{n}}\\)\n\n\nTwo-way mixed effects, consistency, multiple rater/measurment\nICC(3,k)\n\\(\\frac{MS_R - MS_E}{MS_R}\\)\n\n\nTwo-way mixed effects, absolute agreement, multiple rater/measurment\n-\n\\(\\frac{MS_R - MS_E}{MS_R + \\frac{MS_C -MS_E}{n}}\\)\n\n\n\n\n\\(k\\) : 평가자 수\n\\(n\\) : 피험자 수\n\\(MS_R\\) : mean square for rows\n\\(MS_W\\) : mean square for residual sources of variance\n\\(MS_E\\) : mean square for error\n\\(MS_C\\) : mean square for columns"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#one-way-모델에서-consistency가-정의-되지-않는-이유",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#one-way-모델에서-consistency가-정의-되지-않는-이유",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "One-way 모델에서 Consistency가 정의 되지 않는 이유",
    "text": "One-way 모델에서 Consistency가 정의 되지 않는 이유\nOne-way random model은 평가자 효과를 모델에 포함하지 않기 때문이다. One-way 모델에서는 오직 피험자 간 차이만 고려하기 때문에 평가자 간 차이가 분산 구조에서 빠져있다. 따라서 평가자 간 일관성을 측정할 수 없다. 결과적으로 One-way 모델은 “Agreement”는 가능하지만 “Consistency”는 정의할 수 없다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-방식을-정하는-법",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#icc-방식을-정하는-법",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "ICC 방식을 정하는 법",
    "text": "ICC 방식을 정하는 법\n\n\n\nICC 방식을 정하는 과정\n\n\n위 그림은 ICC 유형을 선택하는 의사결정 흐름도를 시각적으로 잘 정리한 자료이다. ICC는 McGraw & Wong이 제안한 모델을 기준으로 총 10가지 유형이 존재한다. 각각의 선택은 연구 설계에 따라 달라지며, 다음과 같은 과정을 거치면 적절한 유형을 결정할 수 있다.\n첫 번째로, 연구 유형이 Test-Retest / Intra-rater Reliability인지, Inter-rater Reliability인지 알아본다. 만일 Test-Retest / Intra-rater Reliability라면, 같은 평가자가 같은 대상을 반복 측정하는 경우이기 때문에 Two-way Mixed Effects 모델을 사용한다. 반면에 Inter-rater Reliability라면 여러 평가자가 각 대상을 평가하는 경우이기 때문에, 다음 과정으로 넘어간다.\n연구 유형이 Inter-rater Reliability일 때 모든 피평가자가 같은 평가자에게 평가되지 않았다면, 일부 평가자만 일부 피평가자를 평가하므로 One-way Random Effects 모델을 사용한다. 만일 모든 피평가자가 같은 평가자에게 평가되었다면, 평가자가 무작위 집단이라면 Two-way Random Effects 모델을 사용하고, 특정 평가자에 한정된다면 Two-way Mixed Effects 모델을 사용한다. 위 과정을 통해 ICC의 모델 중 어떤 것을 사용할 지 선택할 수 있다.\n두 번째로 측정 프로토콜이 어떤 지 확인한다. 만일 한 번만 측정한다면 single rater/measurment을 사용하고, 여러 번 측정 후 평균을 사용한다면 multiple rater/measurment을 사용한다.\n마지막으로 연구에서 중요한 것이 절대적 일치인지, 경향성 일치인지 판단한다. 진단 일치나 채점 점수 등 수치 자체가 같아야 하는 경우라면 Absolute Agreement를 사용하고, 순위 유지가 중요한 경우라면 Consistency를 사용하면 된다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#대중적으로-사용되는-icc-방식",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#대중적으로-사용되는-icc-방식",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "대중적으로 사용되는 ICC 방식",
    "text": "대중적으로 사용되는 ICC 방식\n연구 현장에서 많이 쓰이는 ICC는 다음과 같다.\n\nICC(2,1): Two-way random effects, single rater/measurment, absolute agreement\nICC(2,k): Two-way random effects, multiple rater/measurment, absolute agreement\n\nTwo-way random effects 모델은 평가자와 피평가자 모두 무작위 샘플링된 것으로 간주한다. 따라서 평가자도 연구의 일반적인 모집단에서 랜덤하게 선택된 경우를 반영할 수 있어 일반화 가능성이 높다.\nAbsolute agreement는 단순히 측정값 간의 일관성만이 아니라, 측정값 자체가 정확히 일치하는지를 따진다. 이는 consistency보다 더 보수적이고 엄격한 방식이다. 또한 consistency는 평가자의 bias를 왜곡하는 반면, Absolute agreement는 systematic bias까지 감지할 수 있다.\n따라서 Two-way random effects, Absolute agreement를 사용하는 ICC(2,1), ICC(2,k)를 실제 연구에서 많이 사용한다."
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#랜덤-효과-모형에서의-icc",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#랜덤-효과-모형에서의-icc",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "랜덤 효과 모형에서의 ICC",
    "text": "랜덤 효과 모형에서의 ICC\n모던 ICC는 다음과 같은 one-way random effects 모형에서 정의된다. \\[\nY_{ij} = \\mu + \\alpha_j + \\varepsilon_{ij}\n\\]\n\n\\(Y_{ij}\\)는 \\(j\\)번째 그룹의 \\(i\\)번째 측정값\n\\(\\mu\\)는 모집단 전체의 평균\n\\(\\alpha_j\\)는 그룹 \\(j\\)에 해당하는 랜덤효과\n\\(\\varepsilon_{ij}\\)는 오차항"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-공식",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-공식",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 공식",
    "text": "모던 ICC의 공식\n모던 ICC는 다음과 같이 정의된다. \\[\nICC = \\frac{\\sigma^2_{\\alpha}}{\\sigma^2_{\\alpha} + \\sigma^2_{\\varepsilon}}\n\\]\n분자\\((\\sigma^2_{\\alpha})\\)는 그룹간 분산을 의미하고, 분모\\((\\sigma^2_{\\alpha}+\\sigma^2_{\\varepsilon})\\)은 전체 분산을 의미한다.\n즉 ICC는 전체 변동에서 그룹간 변동이 차지하는 비율이 된다. 따라서 ICC값이 클 수록 같은 그룹 내에서 값들이 더 유사하다는 것을 알 수 있다.\n증명\n\\(Y_{ij} = \\mu + \\alpha_j + \\varepsilon_{ij}\\)에서 \\(\\alpha_i \\sim N(0, \\sigma^2_{\\alpha})\\), \\(\\epsilon_{ij} \\sim N(0, \\sigma^2_{\\varepsilon})\\)이고 \\(\\alpha_i\\)와 \\(\\varepsilon_{ij}\\)은 서로 독립이다.\n\\(Var(Y_{ij}) = \\sigma^2_{\\varepsilon} + \\sigma^2_{\\alpha^2}\\)\n\\[\\begin{align}\nCov(Y_{ij}, Y_{ik}) &= Cov(\\mu + \\alpha_{i} + \\varepsilon_{ij}, \\mu + \\alpha_{i} + \\varepsilon_{ik})\\\\\n  &= Cov(\\alpha_i + \\varepsilon_{ij},\\alpha_i + \\varepsilon_{ik}) \\\\\n  &= Cov(\\alpha_i , \\alpha_i) + Cov(\\alpha_i, \\varepsilon_{ik}) + Cov(\\varepsilon_{ij}, \\alpha_i) + Cov(\\varepsilon_{ij}, \\varepsilon_{ik}) \\\\\n  &= Var(\\alpha_i) = \\sigma^2_\\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-장점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-장점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 장점",
    "text": "모던 ICC의 장점\n\n항상 0 이상이다\n\n초기 ICC는 표본에서 음수값이 나올 수 있었음\n\nANOVA와 결합 가능하다 \\(\\rightarrow\\) 샘플 개수가 달라도 적용 가능하다\n\n초기 ICC는 같은 크기의 그룹을 가정함\n하지만 ANOVA 기반 ICC는 데이터 개수가 달라도 계산 가능\n\n공변량을 포함할 수 있다\n\n공변량을 통제한 후에도 같은 그룹 내에서 얼마나 유사한지 평가할 수 있음\n\n복잡한 데이터 설계에 유리하다"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-한계점",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc의-한계점",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC의 한계점",
    "text": "모던 ICC의 한계점\n\n샘플 ICC가 실제 모집단 ICC보다 클 가능성이 높다\n\n초기 ICC는 편향되지 않은 추정량임\n모던 ICC는 항상 0 이상이므로, 모집단의 ICC가 정확히 0일 때에도 샘플에서 ICC가 0보다 크게 나올 가능성이 있음\n즉, 양의 편향을 가짐\n\n여러 종류의 ICC가 존재\\(\\rightarrow\\)어떤 ICC를 사용할지 논란이 된다\n\n연구자마다 다른 ICC 통계량을 사용하며, 각 방법이 서로 다른 결과를 낼 수 있음\n특정 연구 목적에 적합한 ICC를 신중하게 선택해야 함"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#특정-icc-방법을-실행하는-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#특정-icc-방법을-실행하는-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "특정 ICC 방법을 실행하는 R코드",
    "text": "특정 ICC 방법을 실행하는 R코드\nlibrary(irr)\n\nratings &lt;- data.frame(\n  Rater1 = c(4, 5, 3, 4, 2),\n  Rater2 = c(5, 5, 4, 4, 3),\n  Rater3 = c(4, 5, 3, 5, 2)\n)\n\nresult &lt;- icc(ratings, model = \"twoway\", type = \"agreement\", unit = \"single\")\n\nprint(result)\n출력\nSingle Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 5 \n     Raters = 3 \n   ICC(A,1) = 0.792\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n  F(4,9.09) = 15.1 , p = 0.000482 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.375 &lt; ICC &lt; 0.973"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss의-6개-방법을-모두-보여주는-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#shrout-fleiss의-6개-방법을-모두-보여주는-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "Shrout & Fleiss의 6개 방법을 모두 보여주는 R코드",
    "text": "Shrout & Fleiss의 6개 방법을 모두 보여주는 R코드\nlibrary(psych)\nICC(ratings)\n출력\nCall: ICC(x = ratings)\n\nIntraclass correlation coefficients \n                         type  ICC  F df1 df2       p lower bound upper bound\nSingle_raters_absolute   ICC1 0.79 12   4  10 0.00072        0.37        0.97\nSingle_random_raters     ICC2 0.79 15   4   8 0.00085        0.37        0.97\nSingle_fixed_raters      ICC3 0.82 15   4   8 0.00085        0.40        0.98\nAverage_raters_absolute ICC1k 0.92 12   4  10 0.00072        0.64        0.99\nAverage_random_raters   ICC2k 0.92 15   4   8 0.00085        0.64        0.99\nAverage_fixed_raters    ICC3k 0.93 15   4   8 0.00085        0.66        0.99\n\n Number of subjects = 5     Number of Judges =  3\nSee the help file for a discussion of the other 4 McGraw and Wong estimates,"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#lmer-방식의-모던-icc-r코드",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#lmer-방식의-모던-icc-r코드",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "lmer 방식의 모던 ICC R코드",
    "text": "lmer 방식의 모던 ICC R코드\ndf &lt;- data.frame(\n  subject = rep(1:5, each = 3),\n  rater = rep(1:3, times = 5),\n  score = c(80, 82, 81,\n            75, 76, 74,\n            90, 89, 91,\n            70, 72, 71,\n            85, 86, 84)\n)\n\nlibrary(lme4)\nlibrary(performance)\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = df)\n\nicc(model)\n출력\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.985\n  Unadjusted ICC: 0.985"
  },
  {
    "objectID": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc에서-ci-구하기",
    "href": "posts/2025-03-24-IntraclassCorrelationCoefficient/index.html#모던-icc에서-ci-구하기",
    "title": "Intraclass Correlation Coefficient 공부하기",
    "section": "모던 ICC에서 CI 구하기",
    "text": "모던 ICC에서 CI 구하기\nlmer을 이용한 모던 ICC에서는 CI를 직접 제공하지 않는 것을 알 수 있다. 이는 ICC는 비선형 함수이고, \\(\\sigma^2\\)자체는 정규분포를 따르지 않기 때문에 정규 근사(CI ≈ estimate ± 1.96 × SE)가 잘 맞지 않기 때문이다.\n그러나 bootstrapping 방식을 통해서 충분히 CI를 추정할 수 있다. bootstrap은 데이터를 반복적으로 다시 샘플링해서, 어떤 통계량의 표본 분포를 직접 만들어내는 기법이다. 원래 데이터를 기준으로 같은 크기의 샘플을 복원추출로 다시 만든 후, 새롭게 만들어진 데이터로 lmer을 적합한다. 그 모델에서 ICC를 계산하고, 이 과정을 계속 반복하여 ICC의 분포를 만든다. 그 값들의 분포에서 CI을 추출할 수 있다.\n아래는 그 과정을 실행하는 코드이다.\nlibrary(lme4)\nlibrary(performance)\n\nset.seed(123)\n\nn_subjects &lt;- 30\nn_raters &lt;- 6\nratings_per_subject &lt;- 3\n\nsubjects &lt;- 1:n_subjects\nraters &lt;- 1:n_raters\n\n# 데이터프레임 생성\ndf &lt;- do.call(rbind, lapply(subjects, function(s) {\n  r &lt;- sample(raters, ratings_per_subject)\n  mu &lt;- rnorm(1, mean = 75, sd = 5)  # subject 고유의 평균 실력\n  data.frame(\n    subject = factor(s),\n    rater = factor(r),\n    score = rnorm(ratings_per_subject, mean = mu, sd = 2)\n  )\n}))\n\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = df, REML = TRUE)\n\nperformance::icc(model, ci =TRUE, iterations = 1000)\n출력\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.794 [0.472, 0.934]\n  Unadjusted ICC: 0.794 [0.472, 0.934]"
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html",
    "href": "posts/2025-03-13-Quarto/index.html",
    "title": "quarto 의 기초",
    "section": "",
    "text": "Quarto를 이용해 R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 강의할 예정입니다. 강의 내용을 미리 공유합니다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#시작하기-전에",
    "href": "posts/2025-03-13-Quarto/index.html#시작하기-전에",
    "title": "quarto 의 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n1\nQuarto는 Markdown을 기반으로 한 문서 작성 도구로, Python, R , Julia, and Observable 등 다양한 언어로 코드실행, 분석, 시각화를 포함한 컨텐츠를 만드는 툴이며 크게 3가지 활용법이 있다.\n\n문서(pdf, html, docx): 글쓰기, 분석 결과, 참고문헌 등 논문의 모든 작업을 Quarto으로 수행한다.\n프리젠테이션(pdf, html, pptx): R 코드나 분석결과가 포함된 프리젠테이션을 만든다. 기본 템플릿2 외에 xaringan3 패키지가 최근 인기를 끌고 있다.\n웹(html): 웹사이트나 블로그를 만든다. blogdown4 이나 distill5 패키지가 대표적이다. 이 글의 블로그도 distill로 만들었으며, 과거 차라투 홈페이지는 blogdown을 이용하였다.\n\n본 강의는 1의 가장 기초에 해당하는 강의로 간단한 문서를 작성하는 것을 목표로 한다. pdf 문서를 만들기 위해서는 추가로 LaTeX 문서작성 프로그램인 Tex Live를 설치해야 하며 본 강의에서는 생략한다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#qmd-문서-시작하기",
    "href": "posts/2025-03-13-Quarto/index.html#qmd-문서-시작하기",
    "title": "quarto 의 기초",
    "section": ".qmd 문서 시작하기",
    "text": ".qmd 문서 시작하기\nQuarto는 qmd 파일로 작성되며 Quarto6 패키지를 설치한 후, Rstudio에서 File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document… 의 순서로 클릭하여 시작할 수 있다.\n Rstudio File 메뉴7\n Quarto 시작 메뉴8\n문서의 제목과 저자 이름을 적은 후 파일 형태를 아무거나 고르면(나중에도 쉽게 수정 가능)확장자가 qmd인 문서가 생성될 것이다.\n다음은 각각 html, pdf, docx로 생성된 문서이다.\n\n\n\n\nhtml 문서\n\n\n\n\n\n\n\npdf 문서\n\n\n\n\n\n\n\nword 문서\n\n\n\n생각보다 간단하지 않은가? 이제 본격적으로 qmd 파일의 내용을 살펴보면서 어떻게 글과 코드를 작성하는지 알아보자. qmd는 크게 제목을 적는 YAML Header, 글을 쓰는 Markdown Text와 코드를 적는 Code Chunk로 나눌 수 있다.\n\n\n\n\nqmd 구성 예시"
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#yaml-header",
    "href": "posts/2025-03-13-Quarto/index.html#yaml-header",
    "title": "quarto 의 기초",
    "section": "YAML Header",
    "text": "YAML Header\nYAML은 YAML Ain’t Markup Language의 재귀형식의 이름을 갖고 있는 언어로 가독성에 초점을 두고 개발되었다. Quarto은 qmd의 시작 부분에 문서 형식을 설정하는 용도로 이 포맷을 이용한다. 다음은 기초 정보만 포함된 YAML이다.\n---\ntitle: \"My Document\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n    css: styles.css\n---\nYAML 에 기본적으로 title, author, date 등을 작성할 수 있고, format을 통해 출력 형식 (e.g html, pdf, docx…) 과 각 형식에 맞는 다양한 옵션을 설정하여 문서를 꾸밀 수 있다. Table of Contents, Layout, Fonts 등 다양한 옵션의 설정이 가능하고, 여기서는 Toc 옵션에 대해 살펴볼 것이다. 자세한 format option은 Quarto reference를 참고하기 바란다.\ntoc\nQuarto 문서(.qmd)에서 ##을 사용해 제목을 작성하면, 자동으로 목차(TOC)에 포함된다. 하위 목차를 추가하려면 ###, ####처럼 #의 개수를 늘려 계층 구조를 만들 수 있다.\ntoc 옵션에는 toc-depth, toc-location, toc-title, toc-expand 가 있다. 이 문서의 yaml 부분의 toc 옵션을 살펴보면 다음과 같이 설정되어있다.\n---\ntitle: \"quarto 의 기초\"\nauthor: .\neditor: visual\nformat: \n  html: \n    toc-depth: 3\n    toc-expand: true\n    toc-location: left\n    toc-title: \"Quarto 알아보기\"\n---"
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#markdown-글쓰기",
    "href": "posts/2025-03-13-Quarto/index.html#markdown-글쓰기",
    "title": "quarto 의 기초",
    "section": "Markdown 글쓰기",
    "text": "Markdown 글쓰기\nMarkdown 은 이름에서 알 수 있듯이 마크다운(Markdown) 을 기반으로 만들어졌다. 마크다운은 문법이 매우 간단한 것이 특징으로 깃허브의 README.md가 대표적인 마크다운 문서이다. Quarto 는 Pandoc markdown 을 바탕으로 하며, quarto guide9에 흔히 쓰는 문법이 정리되어 있다.\n2 가지만 따로 살펴보겠다.\nInline R code\n문장 안에 분석 결과값을 적을 때, 분석이 바뀔 때마다 바뀐 숫자를 직접 수정해야 한다. 그러나 숫자 대신 `r &lt;코드&gt;` 꼴로 R 코드를 넣는다면 재분석시 그 숫자를 자동으로 업데이트 시킬 수 있다.\nThere were  `r nrow(cars)` cars studied\n\nThere were 50 cars studied\n\n수식\nLaTeX 문법을 사용하며 hwp 문서의 수식 편집과 비슷하다. inline 삽입은 $...$, 새로운 줄은 $$...$$ 안에 식을 적으면 된다.\nThis summation expression $\\sum_{i=1}^n X_i$ appears inline.\n\nThis summation expression \\(\\sum_{i=1}^n X_i\\) appears inline.\n\n$$\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n$$\n\\[\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\]\n수식 전반은 LaTeX math and equations10을 참고하기 바란다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#r-chunk",
    "href": "posts/2025-03-13-Quarto/index.html#r-chunk",
    "title": "quarto 의 기초",
    "section": "R chunk",
    "text": "R chunk\nQuarto 에서는 내장된 knitr 패키지을 이용하여 R에서 작성한 코드를 실행하고 그 결과를 실시간으로 출력하여 문서에 삽입할 수 있다.\nR chunk 생성하기\nR chunk 를 생성하는 방법은 위의 단추를 통해 생성하거나 \n혹은 다음과 같이 직접 타이핑하여 생성도 가능하다. \nR chunk 옵션\n.qmd 문서에서 R 코드가 들어가는 방식은 4가지이다.\n\n몰래 실행. 코드와 결과는 다 숨긴다\n실행. 코드와 결과를 모두 보여준다. - default\n실행. 코드는 숨기고 결과만 보여준다.\n실행하지 않음. 코드 보여주기만 한다.\n\ninclude, echo, eval 3가지 옵션으로 지정한다. - eval=F : 코드를 실행하지 않는다. - echo=F : 코드를 보여주지 않는다. - include=F : 실행 결과를 보여주지 않는다.\n코드 청크의 옵션은 YAML 에서 지정하여 문서 전체에 적용되게 할 수 있고, 각각 R 청크마다 #| 을 쳐서 각각 옵션을 변경할 수도 있다.\n최초 설정\n문서를 처음 생성 시 옵션을 따로 지정하지 않으면 다음의 값으로 실행된다.\ninclude = TRUE \necho = TRUE \neval = TRUE \n코드를 실행하고, 코드와 결과물 모두 문서에 보여준다.\n\nprint(\"Hello world\")\n\n[1] \"Hello world\"\n\n\n이를 잘 활용하여 R내에서 문서를 완성할 수 있다.\n생존곡선을 그릴 때를 생각해보자, 생존 곡선을 그릴 때는 먼저 survfit 함수를 통해 생존확률을 구해야 한다.\n이 때 survfit 함수는 결과를 보이지 않아도 되므로\n실행하고 결과를 보이지 않기\n다음은 이 html을 생성할 때 쓴 quarto 문서의 캡처본으로 \n현재 강의 화면인 html 에는 코드 및 결과를 보이지 않는다.\n실행하고 결과를 보여주기\n그러나 실행은 되었기 때문에 다음의 코드에서 fit3 에 대한 ggsurvplot 함수를 적용할 수 있었고 코드 및 실행 후 결과는 다음과 같다.\n\nggsurv &lt;- ggsurvplot(fit3, data = colon,\n  fun = \"cumhaz\", conf.int = F,\n  risk.table = F, risk.table.col=\"strata\",\n  ggtheme = theme_bw())\nggsurv\n\n\n\n\n\n\n\n이렇게 시행하여 echo = False 옵션까지 적용하면 quarto 내에서 분석을 시행하고 그에 대한 문서작성을 한번에 할 수 있다.\n이외에도 코드 청크에 다음과 같은 옵션을 적용 가능하다.\n\n\nmessage=F - 실행 때 나오는 메세지를 보여주지 않는다.\n\nwarning=F - 실행 때 나오는 경고를 보여주지 않는다.\n\nerror=T - 에러가 있어도 실행하고 에러코드를 보여준다.\n\nfig.height = 7 - 그림 높이, R로 그린 그림에만 해당한다.\n\nfig.width = 7 - 그림 너비, R로 그린 그림에만 해당한다.\n\nfig.align = 'center' - 그림 위치, R로 그린 그림에만 해당한다.\n\nr chunk 에 적용할 수 있는 전체 옵션은 knitr::opts_chunk$get 함수로 확인할 수 있다. `\n\nknitr::opts_chunk$get()\n\n$eval\n[1] TRUE\n\n$echo\n[1] TRUE\n\n$results\n[1] \"markup\"\n\n$tidy\n[1] FALSE\n\n$tidy.opts\nNULL\n\n$collapse\n[1] FALSE\n\n$prompt\n[1] FALSE\n\n$comment\n[1] NA\n\n$highlight\n[1] TRUE\n\n$size\n[1] \"normalsize\"\n\n$background\n[1] \"#F7F7F7\"\n\n$strip.white\n[1] TRUE\n\n$cache\n[1] FALSE\n\n$cache.path\n[1] \"index_cache/html/\"\n\n$cache.vars\nNULL\n\n$cache.lazy\n[1] TRUE\n\n$dependson\nNULL\n\n$autodep\n[1] FALSE\n\n$cache.rebuild\n[1] FALSE\n\n$fig.keep\n[1] \"high\"\n\n$fig.show\n[1] \"asis\"\n\n$fig.align\n[1] \"default\"\n\n$fig.path\n[1] \"index_files/figure-html/\"\n\n$dev\n[1] \"png\"\n\n$dev.args\nNULL\n\n$dpi\n[1] 96\n\n$fig.ext\nNULL\n\n$fig.width\n[1] 7\n\n$fig.height\n[1] 5\n\n$fig.env\n[1] \"figure\"\n\n$fig.cap\nNULL\n\n$fig.scap\nNULL\n\n$fig.lp\n[1] \"fig:\"\n\n$fig.subcap\nNULL\n\n$fig.pos\n[1] \"\"\n\n$out.width\nNULL\n\n$out.height\nNULL\n\n$out.extra\nNULL\n\n$fig.retina\n[1] 2\n\n$external\n[1] TRUE\n\n$sanitize\n[1] FALSE\n\n$interval\n[1] 1\n\n$aniopts\n[1] \"controls,loop\"\n\n$warning\n[1] FALSE\n\n$error\n[1] FALSE\n\n$message\n[1] FALSE\n\n$render\nNULL\n\n$ref.label\nNULL\n\n$child\nNULL\n\n$engine\n[1] \"R\"\n\n$split\n[1] FALSE\n\n$include\n[1] TRUE\n\n$purl\n[1] TRUE\n\n$fig.asp\nNULL\n\n$fenced.echo\n[1] FALSE\n\n$ft.shadow\n[1] FALSE\n\n\n다음은 필자가 논문을 quarto로 쓸 때 흔히 쓰는 디폴트 옵션이다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#figures",
    "href": "posts/2025-03-13-Quarto/index.html#figures",
    "title": "quarto 의 기초",
    "section": "Figures",
    "text": "Figures\n.qmd 문서에 그림이 들어가는 방법은 2가지가 있다.\n\nR 코드로 생성 : plot 함수, ggplot2 패키지 등\n외부 그림 삽입\n\n앞서도 언급했듯이 주의할 점은 그림이 만들어지는 방법에 따라 서로 다른 옵션이 적용된다는 것이다. 일단 전자부터 살펴보자.\nFigures with R\n\nR 코드에서 자체적으로 만든 그림은 전부 chunk 옵션의 지배를 받아 간단하다.\n\n#|fig-cap: \"scatterplot: cars\"\n#|fig-width: 8\n#|fig-height: 6\n\nplot(cars, pch = 18)\n\n\n\n\n\n\n\nExternal figures\n외부 그림은 R 코드로도 삽입할 수 있고 마크다운 문법을 쓸 수도 있는데, 어떤 방법을 쓰느냐에 따라 다른 옵션을 적용받는다는 것을 주의해야 한다. R에서는 knitr::include_graphics 함수를 이용하여 그림을 넣을 수 있고 이 때는 chunk 내부의 옵션이 적용된다.\n\nlibrary(knitr)\ninclude_graphics(\"https://www.tidyverse.org/images/tidyverse-default.png\")\n\n\n\ntidyverse logo\n\n\n\n같은 그림을 chunk없이 바로 마크다운에서 삽입할 수도 있다. 이 때는 YAML의 옵션이 적용된다.\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n\n\ntidyverse logo\n\n{ width=50% } 는 그림의 크기를 조절하는 옵션이며 R chunk에서도 같은 옵션 out.width=\"50%\"이 있다. 위치를 가운데로 조절하려면 &lt;center&gt;...&lt;/center&gt; 를 포함시키자.\n&lt;center&gt;\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n&lt;/center&gt;\n\n\n\ntidyverse logo\n\n\n개인적으로는 외부 이미지도 chunk 내부에서 읽는 것을 추천한다. chunk 내부의 옵션들이 마크다운의 그것보다 훨씬 체계적이고 쉬운 느낌이다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#tables",
    "href": "posts/2025-03-13-Quarto/index.html#tables",
    "title": "quarto 의 기초",
    "section": "Tables",
    "text": "Tables\n논문을 쓸 때 가장 귀찮은 부분 중 하나가 분석 결과를 테이블로 만드는 것으로, knitr::kable() 함수를 쓰면 문서 형태에 상관없이 Rmd에서 바로 테이블을 만들 수 있다. 아래는 데이터를 살펴보는 가장 간단한 예시이다.\n\n#|label: \"tables-mtcars\"\nknitr::kable(iris[1:5, ], caption = 'A caption', row.names = T)\n\n\nA caption\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\nepiDisplay 패키지의 regress.display, logistic.display 함수를 활용하면 회귀분석의 결과를 바로 테이블로 나타낼 수 있다.\n\n#|label: \"regtable\"\nmtcars$vs &lt;- as.factor(mtcars$vs)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\nmodel &lt;- glm(mpg ~ disp + vs + cyl, data = mtcars)\nmodel.display &lt;- epiDisplay::regress.display(model, crude = T, crude.p.value = T)\nmodel.table &lt;- model.display$table[rownames(model.display$table)!=\"\", ]\nkable(model.table, caption = model.display$first.line)\n\n\nLinear regression predicting mpg\n\n\n\n\n\n\n\n\n\n\ncrude coeff.(95%CI)\ncrude P value\nadj. coeff.(95%CI)\nP(t-test)\nP(F-test)\n\n\n\ndisp (cont. var.)\n-0.04 (-0.05,-0.03)\n&lt; 0.001\n-0.03 (-0.05,0)\n0.019\n&lt; 0.001\n\n\nvs: 1 vs 0\n7.94 (4.6,11.28)\n&lt; 0.001\n0.04 (-3.81,3.89)\n0.984\n0.334\n\n\ncyl: ref.=4\n\n\n\n\n0.041\n\n\n6\n-6.92 (-10.11,-3.73)\n&lt; 0.001\n-4.77 (-8.56,-0.98)\n0.016\n\n\n\n8\n-11.56 (-14.22,-8.91)\n&lt; 0.001\n-4.75 (-12.19,2.7)\n0.202\n\n\n\n\n\n\n테이블을 좀 더 다듬으려면 kableExtra 패키지가 필요하며, 자세한 내용은 cran 설명서11를 참고하기 바란다. html 문서의 경우 kable()외에도 다양한 함수들을 이용할 수 있는데, 대표적인 것이 rmarkdown::paged_table() 함수와 DT 패키지이다. 전자는 아래와 같이 YAML에서 테이블 보기의 기본 옵션으로 설정할 수도 있다.\n---\ntitle: \"Motor Trend Car Road Tests\"\noutput:\n  html_document:\n    df_print: paged\n---\nDT 패키지에 대한 설명은 Rstudio DT 홈페이지12를 참고하기 바란다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#마치며",
    "href": "posts/2025-03-13-Quarto/index.html#마치며",
    "title": "quarto 의 기초",
    "section": "마치며",
    "text": "마치며\n본 강의를 통해 Quarto으로 기본적인 문서를 만드는 법을 알아보았다. 본 강의에서는 시간 관계상 참고문헌 다는 법을 언급하지 않았는데 궁금하다면 Bibliographies and Citations13을 참고하자.\n이 내용까지 숙지한다면 Quarto으로 논문을 쓸 준비가 된 것이다. Quarto에 대한 전반적인 내용은 아래의 Quarto Cheet Sheet14에 잘 요약되어 있으니 그때그떄 확인하면 좋다."
  },
  {
    "objectID": "posts/2025-03-13-Quarto/index.html#footnotes",
    "href": "posts/2025-03-13-Quarto/index.html#footnotes",
    "title": "quarto 의 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://bioinformatics.ccr.cancer.gov/docs/btep-coding-club/CC2024/Quarto/GettingStarted_with_Quarto_orig.html↩︎\nhttps://rmarkdown.rstudio.com/lesson-11.html↩︎\nhttps://github.com/yihui/xaringan↩︎\nhttps://github.com/rstudio/blogdown↩︎\nhttps://rstudio.github.io/distill/↩︎\nhttps://github.com/quarto-dev/quarto-r↩︎\nhttps://github.com/rstudio/rstudio/issues/10966↩︎\nhttps://github.com/quarto-dev/quarto-r↩︎\nhttps://quarto.org/docs/authoring/markdown-basics.html↩︎\nhttps://www.latex-tutorial.com/tutorials/amsmath/↩︎\nhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html↩︎\nhttps://rstudio.github.io/DT/↩︎\nhttps://pandoc.org/MANUAL.html#citations↩︎\nhttps://rstudio.github.io/cheatsheets/quarto.pdf↩︎"
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html",
    "href": "posts/2025-02-28-reg2/index.html",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "",
    "text": "2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#들어가며",
    "href": "posts/2025-02-28-reg2/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "",
    "text": "2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#generalized-linear-models-glms",
    "href": "posts/2025-02-28-reg2/index.html#generalized-linear-models-glms",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "1. Generalized Linear Models (GLMs)",
    "text": "1. Generalized Linear Models (GLMs)\n1.1. Linear Model 한계\n\n1장에서 본 Linear Regression Model은 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (3) 오차항의 독립성(Independence) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정에서 비롯된 모델이었고, Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)를 통해 오차항의 등분산성(Homoscedasticity) 가정이 깨진 data에 대해서도 Linear model로부터 얻은 모델 parameter의 분산을 robust하게 추정할 수 있었습니다. 그러나, 위에서 언급하였듯 outcome of single yes/no, outcome of single K-way, count 등 많은 data는 반응 변수 Y가 정규분포를 따르지 않거나 등분산성 가정, 선형성에 위배됩니다. 각각에 대해서 좀 더 설명하자면, 어떤 사건이나 행동이 일어나거나 그렇지 않은 경우를 고려하는 이진 데이터(binary data, outcome of single yes/no)의 경우, \\(Y \\in \\{0, 1\\}\\)로 제한되며 이를 \\(Y \\in \\mathbb{R}\\)인 정규분포로 가정하는 것은 옳지 않습니다. 특정 기간 동안 발생하는 사건의 횟수 등, 이진 분류처럼 discrete한 종속변수 값을 가지는 카운트 데이터(count data) 또한 discrete(정수) 값만 갖으며, 이 두 경우는 종종 분산이 평균(모델의 예측)에 비례하는 형태를 갖을 수 있고, 이는 당연하게도 등분산성 가정을 위배합니다.\n이러한 데이터의 경우 단순히 독립변수의 선형결합 형태, 또는 기하학적으로는 Hyper plane 형태로 모델을 만들면, 비선형적인 (이진 데이터 등) 위 같은 경우에 대해서는 올바르게 고려하지 못할 것입니다. 이러한 기존의 Linear Regression 모델의 한계를 극복하고, (종속변수의) 다양한 형태의 데이터를 모델링하기 위해 여러 함수를 설계함으로써 유연성을 확장한 Generalized Linear Models이 개발되었습니다. Generalized Linear Models(GLMs)의 중요 구성 요소들과 원리를 간략히 설명해보자면, 선형 결합으로 바로 종속변수를 예측하는 대신, non-linear한 Link Function에 넣어 최종적으로 예측함으로써 non-linear한 종속변수에도 fit 할 수 있고, 이에 따라 종속변수의 분포가 정규분포가 아닌 다른 분포(Exponential Family)도 포함할 수 있도록 하였으며, 이 Exponential Family와 Variance function구성은 종속변수의 분산이 모델의 예측값(종속변수의 mean)마다 다를 수 있도록 합니다. 이를 통해 Generalized Linear Models는 위 네 개의 Linear Regression 가정 중 (1) 선형성(Linearity) 가정, (2) 오차항의 정규성(Normality) 가정, (4) 오차항의 등분산성(Homoscedasticity) 가정을 깼으며, 위에서 Linear Model의 한계로 언급한 데이터들을 고려할 수 있는 모델입니다.\n1.2. GLM 정의 및 수학적 표현\n\nGLM은 세 가지 구성 요소 (Random component, Systematic component, Link function)으로 정의 되며, 이때 Random component는 Y를 Exponential Family로, Systematic component는 Linear predictor와 Link function으로 구성됩니다. 어떻게 Generalized Linear Models가 설계되었는지 component들을 하나하나 자세히 다뤄보겠습니다.\nLinear predictor\n\nLinear predictor \\(\\eta\\)는 말그대로 Linear Model처럼 모델 parameter \\(\\boldsymbol{\\beta}\\)와 독립변수 \\(\\mathbf{X}\\)의 선형 결합으로, 기존에는 \\(\\eta\\)로 바로 \\(\\mathbf{y}\\)를 추정하여 non-linear한 종속변수를 고려하지 못하였었다면, GLM은 \\(\\eta\\)를 계산한 후, 이 값을 non-linear한 Link function에 input하여 최종적으로 종속변수를 예측합니다. 중요한 점은, 이는 단순히 종속변수를 transform한 뒤(로그 등) 이전처럼 선형적으로 추정하는 Transformation (with LM)과 다르다는 점입니다. 가장 큰 차이점은 Transformation을 함으로써 종속변수의 sample space에서 boundaries에 있는 값들은 정의가 되지 않고(로그는 0에서 정의되지 않음.), 이후 바로 Linear Model을 사용하기 위해선 종속변수가 변형 이후 반드시 linearity와 variance의 homogeneity가 거의 보장되어야 합니다.(기존 LM을 사용하기 때문에 이때 사용한 가정 또한 필요하게 됩니다.) GLM의 Linear predictor (선형 예측자) \\(\\eta\\)의 식은 다음과 같습니다: \\[\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_p x_{pi}\\]\nLink function (링크 함수)\n\nLink function \\(g(\\mu_i)\\)는 non-linear하고 미분 및 inverse(역)가 가능한 함수로 정의되며, 종속변수의 평균 \\(E(Y_i) = \\mu_i\\)를 선형 예측자 \\(\\eta_i\\)와 연결하여 간접적으로 독립변수 및 모델 parameter의 선형결합과 종속변수를 mapping하는 역할을 합니다.\\[g(\\mu_i) = \\eta_i\\]\nVariance function (분산 함수)\n\n분산 함수는 평균 \\(\\mu_i\\)에 따라 종속변수의 분산이 어떻게 변하는지를 나타냅니다. 이를 통해 간접적으로 독립변수에 따라 분산이 다르게 나오는 것을 반영할 수 있으며 식은 아래와 같고,\\[\\mathrm{Var}(Y_i) = \\phi V(\\mu_i)\\]\n여기서 \\(\\phi\\)는 dispersion parameter로, 일반적으로 특정 분포에 따라 다르게 정의됩니다. (예: Poisson 분포에서는 \\(\\phi = 1\\)).\nExponential Family\n\nGLM은 종속변수의 분포로 Gaussian(또는 정규분포)를 포함한, 더욱 general한 Exponential Family을 고려하며, 이 분포는 다음과 같은 일반 형태를 가집니다.\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\right\\}\n\\]\n즉 GLM은 LM과 다르게 linear predictor, link function, variance function을 설계함으로써 종속변수가 더욱 general한 분포인 exponential family distribution인 경우에도 잘 mapping할 수 있도록 하는 모델이라고 볼 수 있습니다. 위 식에서 의미론적으로 각 parameters를 해석하면 \\(\\theta\\)는 canonical parameter로 분포의 위치를 나타내는 파라미터, \\(\\phi\\)는 dispersion parameter로 분산과 관련된 파라미터, \\(b(\\theta)\\)는 평균과 분산 관계를 정의하는 함수입니다. 이 분포에 대해 \\(E(Y) = b'(\\theta) = \\mu\\), \\(\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)\\)이라는 특성이 증명 가능하고, 이는 “2. GLMs 추정”에서 모델 \\(\\beta\\)를 추정하는 과정에 필요하기 때문에 아래에서 증명할 것입니다. 이보다 더욱 general한 분포로 (dispersion parameter 관련) exponential dispersion family가 있습니다.\n다음으로 넘어가기 전에 간단하게 잘 알려져있는 Exponential Family의 예시인 정규분포, 이항분포, 포아송분포, 감마분포가 이에 포함됨을 확인해보겠습니다.\n(1) 정규분포 (Normal Distribution)\n정규분포의 확률밀도함수는 다음과 같습니다: \\[\n    f(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(y - \\mu)^2}{2\\sigma^2} \\right\\}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; \\mu, \\sigma^2) = \\exp\\left\\{ \\frac{y\\mu - \\frac{\\mu^2}{2}}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\mu\\),\nDispersion parameter: \\(\\phi = \\sigma^2\\),\n\n\\(b(\\theta) = \\frac{\\theta^2}{2}\\),\n\n\\(b'(\\theta) = \\theta = \\mu\\),\n\n\\(b''(\\theta) = 1\\),\n\n\\(c(y, \\phi) = -\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log(2\\pi\\phi)\\).\n\n(2) 이항분포 (Binomial Distribution)\n이항분포의 확률질량함수는 다음과 같습니다: \\[\n    f(y; n, p) = \\binom{n}{y} p^y (1-p)^{n-y}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; n, p) = \\exp\\left\\{ y \\log\\left(\\frac{p}{1-p}\\right) + n \\log(1-p) + \\log\\binom{n}{y} \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\log\\left(\\frac{p}{1-p}\\right)\\),\nDispersion parameter: \\(\\phi = 1\\),\n\n\\(b(\\theta) = n \\log(1 + e^\\theta)\\),\n\n\\(b'(\\theta) = e^\\theta = \\lambda\\),\n\n\\(b''(\\theta) = e^\\theta = \\lambda\\),\n\n\\(c(y, \\phi) = \\log\\binom{n}{y}\\).\n\n(3) 포아송분포 (Poisson Distribution)\n포아송분포의 확률질량함수는 다음과 같습니다: \\[\n    f(y; \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; \\lambda) = \\exp\\left\\{ y \\log\\lambda - \\lambda - \\log(y!) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = \\log\\lambda\\),\nDispersion parameter: \\(\\phi = 1\\),\n\n\\(b(\\theta) = e^\\theta\\),\n\n\\(b'(\\theta) = \\frac{n e^\\theta}{1 + e^\\theta} = np\\),\n\n\\(b''(\\theta) = \\frac{n e^\\theta}{(1 + e^\\theta)^2} = np(1-p)\\),\n\n\\(c(y, \\phi) = -\\log(y!)\\).\n\n(4) 감마분포 (Gamma Distribution)\n감마분포의 확률밀도함수는 다음과 같습니다: \\[\n    f(y; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} y^{k-1} e^{-\\frac{y}{\\theta}}.\n    \\] 이를 Exponential Family 형태로 변환하면: \\[\n    f(y; k, \\theta) = \\exp\\left\\{ -\\frac{y}{\\theta} + (k-1)\\log y - k\\log\\theta - \\log\\Gamma(k) \\right\\}.\n    \\]\n\nCanonical parameter: \\(\\theta = -\\frac{1}{\\theta}\\),\nDispersion parameter: \\(\\phi = \\frac{1}{k}\\),\n\n\\(b(\\theta) = -\\log(-\\theta)\\),\n\n\\(b'(\\theta) = \\frac{1}{\\theta} = \\mu\\),\n\n\\(b''(\\theta) = \\frac{1}{\\theta^2} = \\mu^2\\),\n\n\\(c(y, \\phi) = (k-1)\\log y - \\log\\Gamma(k)\\).\n\n위에서 \\(b(\\theta)\\)는 한 번 미분하면 mean, 두 번 미분하면 variance의 term과 관련됨을 언급하였고 위 4개의 분포에서 원래 알고 계신 mean, variance와 \\(b'(\\theta)\\), \\(b''(\\theta)\\)가 dispersion parameter를 고려하면 일치한 것을 확인하실 수 있습니다. 이는 cumulant generating function의 일부이기 때문이며, 따라서 평균과 분산 관계를 정의하는 항이라고 언급하였던 것입니다.\nCanonical Link\n\nCanonical link는 GLM에서 통계적 성질을 최적화하기 위해 사용되는 링크 함수(link function)로, 다음과 같이 정의됩니다:\n\\[\ng(\\mu_i) = g(b'(\\theta_i)) = \\theta_i = \\eta_i\n\\] 이 식의 의미는 결국 아래 식과 같습니다. \\[\ng = (b')^{-1}\n\\]\n아래에서 확인하겠지만, Binomial 분포의 경우 canonical link는 logit 함수이고, Poisson 분포의 경우 canonical link는 log 함수이며, Canonical link를 사용하면 MLE(Maximum Likelihood Estimation) 과정이 단순화되고, efficient한 추정치를 얻을 수 있기 때문에 link function은 거의 항상 Canonical link로 정의합니다. 또한, canonical하지 않은 link function의 경우에도 위 Exponential Family distribution에서 식 조작을 통해 canonical link 형태를 만들 수 있습니다.\n1.3. GLM 예시\n\n위 철학에 따라, data가 따르는 Exponential Family 중 특정 분포가 정해지면, 이에 해당하는 보통 사용하는 Link function(Canonical link), Variance function가 정해져 있고 결국 모델이 특정되며, GLM은 이렇게 특정될 수 있는 모든 모델에서 공통적으로 parameter와 그 variance를 추정해내는 general한 모델이라고 생각할 수 있습니다. 여기에서 다루지는 않겠지만, 사실 특정한 형태의 data에서 가능한 link function은 여러 개이며, 이에 따라 variance function도 여러 가지가 가능할 수 있습니다. 그러나 효율성과 computation cost를 고려하여 보통 사용되는 function forms는 정해져 있다고 알아두시면 좋을 것 같습니다. 아래 예시 중 대표적으로 Binomial 예시에서는 link function이 0 이상 1 이하의 정의역에서 실수 전체(for linear predictor)를 map할 수 있는 미분 및 역이 가능한 함수이면 되지만, 보통 logit function이 사용됩니다. 헷갈릴 수 있지만 아래 Exponential Family 중 친숙한 분포의 예시를 직관적인 관점에서 고려하여 위에서 얻은 Exponential Family의 form과 같은 결과가 나옴을 보시면 좋을 것 같습니다.\n아래의 예시에서 linear predictor는 공통이므로 미리 정의하고 각각의 link function, variance function은 어떻게 특정되는지를 보겠습니다.\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}\n\\]\nwhere \\(\\eta_i\\)는 linear predictor, \\(\\beta_0, \\beta_1, ..., \\beta_p\\)는 regression coefficients(parameters), \\(x_{1i}, ..., x_{pi}\\)는 predictor variables(독립변수) 입니다.\nBinomial Case\n\nBinomial Data, 즉 종속변수가 \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\)인 data의 경우, 종속변수의 기댓값의 sample space 또한 0~1이며, 우리가 모델링하고 싶은 값이 \\(Y_i/n_i\\)인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, \\(E(Y_i / n_i) = p_i\\)이고, 분산은 \\(\\frac{1}{n_i} p_i (1 -p_i)\\) 입니다. \\(Y_i/n_i\\)의 variance 식에 \\(Y_i/n_i\\)의 mean이 들어감을 알 수 있고, 기존 LM에서는 이렇게 관측치에 따라 다르게 variance를 고려할 수 없었지만, GLM에서는 이 관계를 variance function을 통해 고려할 수 있으며, 식은 다음과 같습니다: \\[\nV(\\mu_i) = \\mu_i (1 - \\mu_i)\n\\] 또한, \\(Y_i / n_i\\)와 linear predictor를 matching 해줄 수 있는 미분가능한 function을 link로 고려해야 하고, Binomial에서는 non-linear link function으로 대부분 logit 함수를 사용합니다. (이때, logit function의 inverse는 sigmoid function입니다.)\n\\[g(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1 - \\mu_i} \\right)\\]\n이 식은 위에서 확인한 이항분포의 canonical parameter와 같은 형태임을 알 수 있습니다.\nPoisson Case\n\nPoisson Data, 즉 종속변수가 \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\)인 data이고 우리가 모델링하고 싶은 값이 종속변수 \\(Y_i\\)인 경우를 상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지 생각해보면, \\(E(Y_i) = \\lambda_i\\)이고 분산은 \\(\\lambda_i\\) 이므로, 마찬가지로 \\(Y_i\\)의 variance 식에 \\(Y_i\\)의 mean이 들어감을 알 수 있고, variance function은 다음과 같습니다:\n\\[\nV(\\mu_i) = \\mu_i\n\\] 또한, \\(Y_i\\)의 sample space는 0 이상의 실수로, 이와 linear predictor를 matching 해줄 수 있는 미분가능한 non-linear function을 link로 고려해야 하고, Binomial에서는 link function으로 대부분 log 함수를 사용합니다. (inverse는 지수 함수.) \\[\ng(\\mu_i) = \\log(\\mu_i)\n\\]이 식은 위에서 확인한 포아송분포의 canonical parameter와 같은 형태임을 알 수 있습니다. 위 두 예시에서는 어떻게 GLMs의 구성 요소들이 선택되는지 직관적으로 보았고, 이는 이해를 돕기 위한 해석이었으며 이미 위에서 Exponential Family에 포함됨을 보일 때 같은 결과가 나왔다는 것을 보시면 됩니다. 위 Binomial Data의 GLM은 Logistic Regression, Poisson Data의 GLM은 Poisson Regression으로도 불립니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#glms-추정",
    "href": "posts/2025-02-28-reg2/index.html#glms-추정",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "2. GLMs 추정",
    "text": "2. GLMs 추정\n위 내용들을 통해서 GLMs가 어떻게 비정규분포를 갖는 종속변수를 고려해서 잘 작동하며, 어떠한 함수(Link function, Variance function, Exponential Family, Canonical Link)가 어떠한 수식과 철학으로 GLM을 구성하고 있는지 확인할 수 있었습니다. GLMs은 거의 대부분의 고려 가능한 data 분포가 Exponential Family를 따르며, 이에 대해 일관적인 form과 parameter estimation이 가능하기 때문에 아주 powerful한 Regression Model입니다. 그러나 어떻게 Exponential Family를 따르는 data를 다룰 수 있는지는 확인할 수 있었지만, 어떻게 Regression Model의 parameter와 그 분산을 추정할 수 있는지는 다루지 않았습니다. Linear Model에서는 closed-form solution을 쉽게 찾을 수 있었지만, GLM은 대부분의 경우(있는 경우도 있습니다.) 이러한 closed-form이 없어 컴퓨터 프로그램으로 여러 번에 걸쳐 추정할 수 있도록 알고리즘을 구현하여 이를 추정합니다. 실제로 이 estimation의 수식과 실제 구현 과정을 다루기 위해서는 긴 증명 과정을 거치는데, 최대한 중요한 부분은 빠지지 않으면서 증명해보겠습니다. 우선, MLE로 모델을 추정하는 과정을 증명하기 위해 필요한 두 가지 유용한 성질을 살펴보겠습니다. (Derivatives of Log Likelihood’s, Exponential Family 성질)\n2.1. Derivatives of Log Likelihood’s 성질\n\n확률변수 \\(Y\\)의 밀도 함수 \\(f(y; \\theta)\\)가 주어지며, 이때 \\(\\theta\\)는 스칼라 매개변수라고 가정하겠습니다. 또, \\(\\ell\\)이 \\(\\theta\\)에 대해 최소 두 번 미분 가능하다고 가정하면, 단일 관측치 \\(Y\\)에 대한 로그 가능도(log likelihood) 함수 \\(\\ell(\\theta; Y)\\)에 대해서 함수의 첫 번째 및 두 번째 도함수는 다음과 같습니다.\n\n\n첫 번째 도함수: \\[ \\ell' = \\frac{d\\ell}{d\\theta} \\]\n\n\n두 번째 도함수: \\[ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} \\]\n\n\n이때, 이 두 함수들은 다음 관계식이 성립합니다.\n\\[\nE \\{ \\ell'(\\theta; Y) \\} = 0\n\\]\n\\[\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n\\]\n이를 증명해보겠습니다. 의지를 잃지 않기 위해 위 식들의 의미를 스포하자면, MLE를 통해 모델을 추정할 때 보통 log likelihood를 모델의 parameter로 미분한 식이 0(또는 영벡터)이 되도록 하는 parameter를 찾음으로써 이를 수행하는데, 첫 번째 식은 이 미분한 식(score function)의 기댓값(평균)이 0이라는 의미이고, 두 번째 식은 첫 번째 식에서 mean이 0이었으므로 왼쪽항의 제곱 안에 -0을 넣어주면 \\[\nE\\left[ \\{ \\ell'(\\theta; Y) - E [ \\ell'(\\theta; Y) ] \\}^2 \\right]\n\\]\n가 되어 분산 term이 되고, 따라서 분산은 이차 도함수의 기댓값의 음수와 같다는 의미입니다. 이러한 성질들을 이용해서 앞으로 Likelihood 기반의 다양한 모델 추정을 수행할 수 있게 됩니다.\n(1) Prove \\(E \\{ \\ell'(\\theta; Y) \\} = 0\\).\n우선, 확률 분포는 모든 범위에서의 적분 또는 누적합이 1이므로,\n\\[\n1 = \\int f(y; \\theta) dy\n\\]\n입니다. 이제 양변을 \\(\\theta\\)에 대해 미분한 후 미분과 적분의 순서를 바꾸면,\n\\[\n0 = \\frac{d}{d\\theta} \\int f(y; \\theta) dy \\\\\n=\\int \\frac{d}{d\\theta} f(y; \\theta) dy\n\\]\n입니다. 여기서 수학적 증명 과정에서 굉장히 자주 사용되는 skill \\(\\frac{d}{d\\theta} f(y; \\theta) = \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta)\\)를 사용하면\n\\[\n0 = \\int \\frac{d}{d\\theta} f(y; \\theta) dy  \\\\\n= \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy \\\\\n= \\int \\ell'(\\theta; Y) f(y; \\theta) dy \\\\\n= E \\{ \\ell'(\\theta; Y) \\}\n\\] 입니다. 의미를 다시 해석해보면, 어떠한 분포를 따르는 \\(Y\\)와 이의 매개변수 \\(\\theta\\)에 대해서, 우리는 MLE를 통해 log likelihood 함수 \\(\\ell(\\theta; Y)\\)를 \\(\\theta\\)로 미분하였을 때 0이 나오도록 하는 \\(\\hat{\\theta}\\)를 찾음으로써 parameter를 estimate합니다. 위 (1)은 이러한 \\(\\ell'(\\theta; Y)\\)의 기댓값은 \\(Y\\)의 분포가 이계도함수가 존재한다면 어떤 분포이건 관계 없이 0임을 보인 것입니다.\n(2) Prove \\(\\ell'' = \\frac{d^2\\ell}{d\\theta^2}\\).\n동일한 논리를 따라 위 식을 한 번 더 미분하면,\n\\[\n0 = \\frac{d}{d\\theta} \\left[ \\int \\frac{d}{d\\theta} \\{ \\log f(Y; \\theta) \\} f(y; \\theta) dy \\right]\n\\]\n입니다. 두 함수의 곱 형태의 미분이며 둘 다 \\(\\theta\\)를 포함하므로 이를 전개하고 마찬가지로 기댓값으로 표기하면,\n\\[\n0 = \\int \\frac{d^2}{d\\theta^2} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy + \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} \\frac{d}{d\\theta} f(y; \\theta) dy \\\\\n= E \\{ \\ell''(\\theta; Y) \\} + E \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right]\n\\]\n이고, 따라서 아래 식이 증명되었습니다.\n\\[\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n\\]\n위 증명 과정에서 미분과 적분 연산자의 교환을 정당화하는 과정의 설명이 생략되었지만 exponential family에선 문제가 없고, Y가 discrete한 경우는 적분을 누적합으로 바꿔주면 된다고 얘기해두며 마무리 하겠습니다. 또한, 위 증명에서는 \\(\\theta\\)가 1차원 스칼라 변수라고 가정했지만, 다차원 매개변수에 대해서도 동일한 결과가 성립됩니다. 식의 의미를 마지막으로 되짚어보면, log likelihood의 일차 도함수는 기대값이 0이고, 이 일차 도함수의 공분산 행렬은 이차 도함수 행렬의 기대값의 음수에 해당합니다. 이 값은 피셔 정보 행렬(Fisher Information Matrix)이라고도 불립니다. (함수의 기댓값이라는 말이 어색하게 들릴 수도 있는데, 애초에 모든 랜덤(확률)변수는 어떠한 관측치에 대해서 실수를 output하는 함수임을 되새기면 좋을 것 같습니다.)\n2.2 Exponential Family 성질\n\n이번에는 위에서 증명한 수식을 통해서 Exponential Family를 소개할 때 언급한 \\(E(Y) = b'(\\theta) = \\mu\\), \\(\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)\\)을 증명할 것입니다. Exponential Family distribution은 다음과 같은 일반적인 형식으로 정의됩니다.\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\}\n\\] 때문에 log likelihood는 단순하게 아래와 같이 도출됩니다.\n\\[\n\\ell(y; \\theta, \\phi) = \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\n\\] 이제 이 log likelihood의 derivatives를 계산하면 다음과 같습니다:\n\n첫 번째 도함수 (Score Function): \\[\n\\ell' (y; \\theta, \\phi) = \\frac{y - b'(\\theta)}{a(\\phi)}\n\\]\n두 번째 도함수 (Observed Information): \\[\n\\ell'' (y; \\theta, \\phi) = \\frac{-b''(\\theta)}{a(\\phi)}\n\\]\n\n이 두 함수를 통해서 위에서 유도한 두 공식을 활용하면 다음 두 수식을 얻을 수 있습니다.\n\\[\nE \\left\\{ \\frac{Y - b'(\\theta)}{a(\\phi)} \\right\\} = 0\n\\]\n\\[\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{b''(\\theta)}{a(\\phi)}\n\\] 이때, 식을 잘 보면 첫 번째 식은 결국 \\[E[Y - b'(\\theta)] = E[Y] - b'(\\theta) = 0\\] 이 되어 \\(E\\{ Y \\} = b'(\\theta)\\)을 얻을 수 있고, 두 번째 식에서 \\[\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{E[(Y - b'(\\theta))^2]}{E[a(\\phi)^2]} = \\frac{\\operatorname{Var}(Y)}{a(\\phi)^2} = \\frac{b''(\\theta)}{a(\\phi)}\n\\] 이므로, \\(\\text{Var}(Y) = b''(\\theta) a(\\phi)\\)임을 보일 수 있습니다.\n증명한 수식을 다시 한 번 확인하면,\n\\[\nE(Y) = b'(\\theta) = \\mu\n\\]\n\\[\nVar(Y) = a(\\phi) V(\\mu_i) = a(\\phi)b''(\\theta)\n\\]\n2.3. GLMs’ parameter 추정식 유도\n\n이제 필요한 식이 준비되었으니, 위에서 계속 다루고 있는 log likelihood을 이용해서 MLE estimation으로 GLMs’ parameter을 추정하는 과정을 살펴볼 것입니다. 이때, 추정 과정은 계속 언급한대로 Exponential Family distribution을 따르는 종속변수에 대해서 log likelihood의 model parameter에 대해 미분한 식이 (parameter가 벡터이므로, 좀더 엄밀하게 정의해야 하지만, 의미는 같으니 이렇게 얘기하겠습니다.) 0이 되게 하는 parameter를 찾음으로써 수행되며, 이 때의 함수 (log likelihood의 1차 도함수)를 앞으로는 score function이라고 부르겠습니다.\n우리는 MLE estimation을 통해 여러 Exponential Family distributions에 대해 통일된 estimation algorithm으로 parameter를 추정할 수 있습니다. (이러한 분포 가정 마저 없다면, 3장 GEE에서 보겠지만 분포에 대한 직접적 가정없이 cumulative generating function 등 몇 함수 만으로 Likelihood를 고려하는 Quasi-likelihood Estimation의 개념으로 이어집니다.)\n주어진 data가 \\((y_1, ... , y_n)\\)일 때, 위에서부터 계속 사용해왔던 log-likelihood function은 다음과 같습니다.\n\\[\nl = \\sum_{i=1}^{n} \\left( \\frac{y_i \\theta_i - b(\\theta_i)}{\\phi_i} + c(y_i, \\phi_i) \\right)\n\\]\n지금까지 우리는 \\(\\theta\\)로 log likelihood를 다뤘지만, 추정해야 하는 parameter는 \\(\\boldsymbol{\\beta}\\)입니다. \\(\\boldsymbol{\\beta}\\)는 벡터이기 때문에 이 중 하나의 파라미터 \\(\\beta_j\\)에 대해 먼저 log likelihood를 미분해보면 아래와 같은 식이 나옵니다. (\\(\\text{Var}(y) = \\phi_i V(\\mu_i)\\), \\(\\frac{\\partial \\mu}{\\partial \\eta} = \\frac{1}{g'(\\mu_i)}\\))임은 위에서 보았습니다. g는 link function이었습니다.)\n\\[\n\\frac{\\partial l}{\\partial \\beta_j} = s(\\beta_j)\n\\]\n\\[\n= \\left(\\frac{\\partial l}{\\partial \\theta} \\right) \\left(\\frac{\\partial \\theta}{\\partial \\mu} \\right) \\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right) \\left(\\frac{\\partial \\eta}{\\partial \\beta_j} \\right)\n\\]\n\\[\n= \\frac{y_i - \\mu_i}{\\text{Var}(y_i)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) x_{ij}, \\quad \\text{or}\n\\]\n\\[\n= \\sum_{i=1}^{n} \\frac{y_i - \\mu_i}{\\phi_i V(\\mu_i)} \\times \\frac{x_{ij}}{g'(\\mu_i)} = 0\n\\]\n식이 혼란스러울 수 있는데, 이는 단지 chain rule을 이용해서 \\(\\beta\\)에 대한 \\(\\ell\\)의 기울기를 구하는 과정이며, 위에서 GLM을 구성하는 과정을 차근차근 복기하면 각각의 변화율은 다음과 같이 구할 수 있기 때문에 최종 식을 얻을 수 있었음을 알 수 있습니다.\n\\[\n\\frac{\\partial l}{\\partial \\theta} =\n\\frac{y - b'(\\theta)}{a(\\phi)} =\n\\frac{y - \\mu}{a(\\phi)}\n\\]\n\\[\n\\frac{\\partial \\theta}{\\partial \\mu} =\n\\frac{1}{b''(\\theta)} =\n\\frac{1}{V(\\mu)} =\n\\frac{a(\\phi)}{\\text{Var}(y)}\n\\]\n\\[\n\\frac{\\partial \\eta}{\\partial \\beta_j} = x_{ij}\n\\]\n이제 이 score function의 음의 미분(또는 분산)의 기댓값을 전개하면 다음과 같습니다.\n\\[\n- E \\left( \\frac{\\partial^2 l}{\\partial \\beta_j \\partial \\beta_k}\n\\right) = E \\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right)\n\\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right]\n\\]\n\\[\n= E \\left[ \\left( \\frac{y - \\mu}{\\text{Var}(y)} \\right)^2\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right]\n\\]\n\\[\n= E \\left[ \\frac{\\text{Var}(y)}{\\text{Var}(y)^2}\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right]\n\\]\n\\[\n= \\frac{1}{\\text{Var}(y)} \\left(\\frac{\\partial \\mu}{\\partial \\eta}\n\\right)^2 x_{ij} x_{ik}\n\\]\n위 term은 Fisher Information matrix라고도 부르며, 이 term이 분산의 기댓값인 이유를 생각해보면, 이전에 구한 derivatives of log likelihood의 성질들에 의해 \\(\\ell\\)의 negative 2차 도함수의 기댓값은 1차 도함수의 기댓값의 square와 같고, 이 1차 도함수의 기댓값이 0이므로 이는 분산과 같습니다. 정리하자면, score function의 미분식이 score function의 분산과 기댓값이 같으므로, 미분을 직접하는 대신 분산으로 근사 후 식을 전개한 것이며, 이 때 근사한 이 Matrix를 Fisher Information matrix라고 부릅니다.\n이들을 한 번에 벡터와 행렬 연산으로 표현하면 다음과 같습니다:\n\\[\n\\frac{\\partial l}{\\partial \\beta} = X^\\top A (y - \\mu)\n\\]\n\\[\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) =\n-X^T W X\n\\]\n\\[\nwhere, W \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right)^2,\n\\]\n\\[\n\\text{and }A \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right).\n\\]\n이제 우리는 score function와 그 미분, 또는 log likelihood의 1차 도함수와 (negative) 2차 도함수의 추정식을 얻었습니다. 사실, \\(X^\\top A (y - \\mu)\\)가 0이 되도록 하는 parameter \\(\\beta\\)만 찾으면 parameter 추정이 끝나지만, 이는 closed-form solution이 존재하지 않기 때문에 그 미분(2차 도함수)식을 이용해 근사적으로 구할 수 있는 알고리즘을 최종적으로 다룰 것입니다. (물론 후에 보겠지만 이 Fisher Information matrix는 분산과도 관련이 있습니다.) 즉, 우리는 추정식을 유도하는 것은 완성했지만, 실제로 알고리즘을 설계하여 어떻게 이를 추정할 지에 대해서는 모르는 상태이고, 때문에 최종적으로 GLM을 제안한 학자가 소개하였으며 대부분의 패키지에서 이 GLM을 estimate하기 위해 사용하고 있는 method인 IRLS(Iteratively Reweighted Least Squares) Algorithm을 살펴볼 것입니다. (negative) 2차 도함수의 추정식(or Fisher Matrix) \\(X^T W X\\)을 유도한 이유는, 이 알고리즘에서 필요로 하기 때문이고, 위 식에서 \\(W, A\\)는 식이 복잡해보이지만, 그저 observations(data) 하나 당 GLM 모델의 구성요소를 통해 determinant하게 미리 계산되어 대각성분으로 각각 들어가는 term임을 명심하시면 좋을 것 같습니다. (이전 설명에서, 종속변수의 분포로 Exponential Family 중 특정 분포가 정해지면, 이에 따라 Link function(Canonical link), Variance function가 정해져 모델이 특정된다고 설명드린 적이 있고, 위 \\(W, A\\)모두 이 두 함수로 이루어진 식이기 때문에 관측치마다 각각 넣으면 determinant하게 하나의 값이 나오는 식인 것입니다.)\n2.4. GLMs’ parameter 추정 (IRLS)\n\n위에서 언급하였듯, GLM의 MLE estimation은 비선형 최적화 문제이기 때문에 공통된 framework에서 사용할 수 있는 closed-form solution이 존재하지 않으며, 대신 여러 최적화 방법을 사용할 수 있습니다. 뉴턴-랩슨 방법(Newton-Raphson Method)은 2차 도함수(Hessian Matrix)를 사용하여 score function을 수렴시키지만, Hessian Matrix \\(H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}\\)를 직접 구해야 하고, 이는 계산이 복잡하여 computation cost가 큽니다. Fisher Scoring은 Newton Method에서 Hessian Matrix 대신 이를 근사하는 Fisher Information Matrix를 사용합니다. 이는 이전에 Derivatives of log likelihood 의 성질이랑 추정식 유도에서 모두 보았던대로 2차 도함수가 1차 도함수의 분산(또는 제곱)와 기댓값이 같다는 수학적 성질을 토대로 \\[\nE \\left(\n\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right]\n= X^T W X\\\n\\] 가 만족함을 확인하였기 때문에, MLE추정에서 Newton-Raphson Method보다 computation cost가 합리적인 method라고 생각해 볼 수 있습니다. 이외에도 경사 하강법(Gradient Descent) 알고리즘은 1차 도함수(Gradient)만 사용하여 특정 값만큼 조금씩 점진적으로 parameter를 움직여 최적점을 찾는 방법입니다. 모델이 매우 복잡해서 2차 도함수를 계산하기 힘든 딥러닝에서는 많은 경우 이를 발전시킨 여러 methods로 iterativaly하게 parameter를 추정합니다. (이렇게 iterative하게 model’s parameter를 움직이면서 추정하는 과정이 AI에서 얘기하는 learning입니다.)\n위에서 얘기한 IRLS(Iteratively Reweighted Least Squares) Algorithm는 이 Fisher Scoring의 알고리즘적 변형으로, Fisher Scoring의 식을 가중 최소제곱(Weighted Least Squares) 문제로 치환하여, 이 문제에서 사용하는 IRLS 알고리즘으로 GLM의 parameter 해를 구하는 method입니다. 우선 Newton-Raphson Method, Fisher Scoring에 대한 이야기를 간단하게 하고, 자세하게 어떻게 IRLS가 GLM의 parameter를 추정하는지 보겠습니다.\nNewton-Raphson Method & Fisher Scoring\n\nGLM (Generalized Linear Model)의 파라미터 추정을 위한 최적화 과정은 우선 log likelihood function의 최대화를 목적으로 합니다. 이때, Newton-Raphson Method와 그 변형인 Fisher Scoring은 모두 이를 위한 알고리즘입니다.\nNewton-Raphson 방법은 다음과 같은 일반적인 업데이트 식을 갖습니다:\n\\[\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]^{-1} \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}},\n\\] 이후의 method들은 모두 이 method에서 기인하므로, 위 식의 의미를 이해하는 것은 매우 중요합니다. 위 식이 어떻게 안정적으로 \\(\\boldsymbol{\\beta}\\)를 수렴시킬 수 있는지 2차 테일러 전개를 통해 수식적으로 좀 더 명확히 볼 수 있지만, 여기서는 좀더 직관적으로 가볍게 이해해보겠습니다.\nNewton-Raphson 방법은 어떠한 함수 \\(f(x)\\)에 대해서 함수의 해, 즉 \\(f(x) = 0\\)을 만족하는 \\(x\\)를 찾기 위한 반복적인 근사 방법입니다.(informal한 증명이므로 1-dimension case로 보겠습니다.) 이 방법은 현재 점에서 함수의 접선을 그려 \\(x\\)축과 만나는 점을 다음 근사해로 사용합니다. 예를 들어, 초기값 \\(x_0\\)에서 접선을 그리면 그 접선의 방정식은 \\(y = f'(x_0)(x - x_0) + f(x_0)\\)입니다. 이 접선이 \\(x\\) 축과 만나는 점은 \\(y = 0\\)일 때이므로, 이를 대입하여 접선이 0이 되는 점을 구해보면 \\(x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\\)가 됩니다. 그러나 접선은 함수의 선형 approximation이므로 이 접선이 0이 되는 점 \\(x_1\\)이 실제 함수에서도 곧바로 0이 되지는 않습니다. 따라서 이 과정을 반복하여 특정 단계에서 \\(f(x_n)\\)이 0에 충분히 가까워지면, \\(x_n\\)을 근사해로 채택합니다.\n이 방법이 작동하는 이유는 접선의 기울기 \\(f'(x_n)\\)이 함수의 곡률을 반영하기 때문입니다. 곡률이 클수록(기울기가 가파를수록) 업데이트의 크기가 작아지고, 곡률이 작을수록 업데이트의 크기가 커집니다. 또한, Newton-Raphson 방법은 2차 수렴(Quadratic Convergence) 속도를 가집니다. 이는 오차가 반복마다 제곱으로 줄어들기 때문에 매우 빠르게 해에 수렴한다는 의미입니다.\n이정도로 간단하게 Newton-Raphson method를 이해할 수 있고, 다시 돌아와서 위 식에서는 multi-dimention 상황에서 해를 찾고 싶은 함수가 score function, 즉 \\(\\frac{\\partial  \\ell}{\\partial \\beta}\\)인 경우이기 때문에 위처럼 식이 구성되었다는 것을 알 수 있습니다.(f의 미분이 분모로 들어간 term은 행렬에서 역행렬 -1과 같은 의미라고 보시면 됩니다.) 첨언하자면, 이 경우 Newton-Raphson method는 두 가지 중요 조건이 붙는데, 언급만 하자면 Hessian matrix가 Positive-definite (볼록) 해야 하며, 초기값이 최적점에 충분히 가까워야 합니다.\nFisher Scoring은 Hessian 행렬 대신 Fisher Information 행렬 \\(\\mathcal{I}(\\boldsymbol{\\beta})\\)를 사용하여 다음과 같이 업데이트합니다:\n\\[\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\left( \\mathcal{I}(\\boldsymbol{\\beta}^{(t)}) \\right)^{-1} \\mathbf{S}(\\boldsymbol{\\beta}^{(t)}),\n\\]\n여기서 \\[\n\\mathbf{S}(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}}\n\\] 는 Score function으로 구한 (Fisher) score 벡터이고, \\[\n\\mathcal{I}(\\boldsymbol{\\beta}) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right] = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]\n\\] 는 Fisher information matrix입니다.\n즉, Fisher Scoring 방법은 뉴턴-랩슨 방법(Newton-Raphson Method)에서 Hessian Matrix \\(H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}\\) 를 사용하는 대신, Fisher information matrix를 사용해서 업데이트를 수행함으로써 parameter를 estimation하는 매커니즘입니다. IRLS는 GLM에서 이 Fisher Scoring와 거의 일치하다 봐도 무방하며, 단순히 위에서 추정한 \\(\\mathbf{S}(\\boldsymbol{\\beta})\\)와 \\(\\mathcal{I}(\\boldsymbol{\\beta})\\)를 Fisher Scoring 공식에 넣으면 weighted least squares problme(가중 최소제곱 문제)와 완전히 유사해지기 때문에, 이 문제를 해결하는 방식으로 parameter를 추정한다는 의미라고 생각하시면 될 것 같습니다. 좀 더 수식과 같이 자세하게 설명드리겠습니다.\nIRLS (Iteratively Reweighted Least Squares) Algorithm\n\n앞서, 2.3.에서 log likelihood의 gradient(1차 도함수, Score function)와 expected Hessian(Fisher Information matrix)가 각각\n\\[\n\\frac{\\partial l}{\\partial \\beta} = X^T A (y - \\mu)\n\\]\n\\[\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = X^T W X\n\\]\n\\[\nwhere, \\quad A = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) \\; and \\quad W = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\n\\]\n임을 보았습니다. 이 결과들을 Fisher Scoring method의 식에 대입하면,\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n\\]\n이고, 이 equation은 가중 최소제곱 문제(Weighted Least Squares, WLS)의 parameter 추정식과 일치하기 때문에 비선형 GLM의 parameter 추정을 WLS problem으로 치환할 수 있음을 알 수 있습니다.\n위의 업데이트 식은 아래 working response \\(z\\)를 정의하면\n\\[\nz = X \\beta^{(t)} + W^{-1} A (y - \\mu),\n\\]\n아래와 같이 표현할 수 있습니다.\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n\\] \\[\n= \\left( X^T W X \\right)^{-1}\\left( X^T W X \\right)\\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T W W^{-1} A (y - \\mu)\n\\] \\[\n= \\left( X^T W X \\right)^{-1}X^T W \\left( X \\beta^{(t)} + W^{-1} A (y - \\mu) \\right)\n\\]\n\\[\n= \\left( X^T W X \\right)^{-1} X^T W z\n\\]\n또 강조하지만, 이는 가중치 행렬 \\(W\\)에 따라 각 관측치의 기여도를 달리하는 선형 회귀 문제(가중 최소제곱 문제)의 정규방정식과 동일합니다:\n\\[\n(X^T W X) \\beta = X^T W z.\n\\]\n따라서 해당 정규방정식의 \\(\\beta\\)를 가중 최소제곱 문제 방식으로 풀어냄으로써 추정치 \\(\\beta^{(t+1)}\\)를 구할 수 있으며, 이는 현재 단계의 추정치 \\(\\beta^{(t)}\\)에서의 예측값과 오차를 반영한 새로운 업데이트가 됩니다.\nIRLS 구체적 절차\n\n정리하자면, IRLS(Iteratively Reweighted Least Squares) 알고리즘은 위의 아이디어를 바탕으로 GLM의 최대우도추정 문제를 반복적으로 가중 최소제곱 문제로 전환하여 해결합니다. 구체적인 단계는 다음과 같습니다:\n(1) 초기화: 초기 파라미터 \\(\\beta^{(0)}\\)를 설정합니다.\n(2) 현재 단계 계산:\n(2.1) 예측값 계산: 현재 추정치 \\(\\beta^{(t)}\\)을 이용하여 선형 예측치 \\(\\eta^{(t)} = X \\beta^{(t)}\\)를 구하고, link 함수의 역함수를 통해 \\(\\mu^{(t)} = g^{-1}(\\eta^{(t)})\\)를 계산합니다.\n(2.2) 가중치 및 보조 행렬 계산: 정의한 식에 따라\n\\[\nA^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t)}\n\\]\n\\[\nW^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t) 2}\n\\]\n을 계산합니다.\n(2.3) Working Response 구성:\n\\[\nz^{(t)} = X \\beta^{(t)} + \\left( W^{(t)} \\right)^{-1} A^{(t)} (y - \\mu^{(t)}).\n\\]\n(3) 가중 최소제곱 문제 해결:\n위의 working response와 가중치 행렬을 사용하여 정규방정식\n\\[\n(X^T W^{(t)} X) \\beta^{(t+1)} = X^T W^{(t)} z^{(t)}\n\\] 을 풀어 새로운 추정치 \\(\\beta^{(t+1)}\\)를 구합니다.\n(4) 수렴 판단 및 반복:\n\\(||\\beta^{(t+1)} - \\beta^{(t)}||\\)(L1 norm, 쉽게는 절댓값)가 미리 설정한 임계값 이하가 될 때까지 2번과 3번의 단계를 반복하고, 수렴이 되었다면 이 \\(\\beta^{(t+1)}\\) 값이 GLM의 parameter에 대한 IRLS의 최종 Estimation 결과입니다. 결국, Fisher Scoring에서 대입한 수식이 결국 가중 최소제곱 문제로 귀착됨을 통해, IRLS 알고리즘은 각 반복마다 선형 회귀 문제와 유사한 방식으로 parameter를 업데이트합니다.\n이 IRLS의 소프트웨어 구현에 대한 첨언을 하자면, 보통 IRLS에서는 각 단계마다 위 3단계와 같이 아래 정규방정식을 풉니다:\n\\[\n(X^T W X) \\beta = X^T W z.\n\\]\n이때 직접 \\((X^T W X)^{-1}\\)를 구해 업데이트하는 방법은 계산적으로 불안정할 수 있습니다. 특히, 데이터의 규모가 크거나 \\(X\\) 행렬이 ill-conditioned(조건수가 열악한)인 경우에는 직접 역행렬을 계산하는 과정에서 수치적인 문제가 발생할 위험이 큽니다. 따라서, 구현의 영역이기 때문에 더이상 나열하지는 않겠지만 실제는 역행렬을 direct하게 구하는 대신, QR 분해나 Cholesky 분해 같은 선형대수 기법을 활용하여 안정적으로 선형 시스템을 풀 수 있습니다.\n2.5. GLMs’ parameter Variance\n\n앞서 IRLS(Iteratively Reweighted Least Squares) 알고리즘으로 GLM의 파라미터를 추정하는 과정을 살펴보았습니다. 이때 우리는 MLE(최대우도추정)을 IRLS(반복적으로 가중 최소제곱)문제로 전환하는 과정을 거쳤는데, 최종적으로 구해지는 추정치 \\(\\hat{\\boldsymbol{\\beta}}\\)의 분산에 대한 추정까지 마쳐야 유의성 검정 등의 분석을 수행할 수 있을 것입니다. GLM에서 최대우도추정(MLE)을 사용해 얻은 \\(\\hat{\\boldsymbol{\\beta}}\\)는, 이론적으로 위에서 언급한 Fisher 정보 행렬(Fisher information matrix)의 역행렬로써 구할 수 있습니다:\n\\[ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = \\bigl(\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})\\bigr)^{-1}, \\]\n여기서 \\(\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})\\)는 \\(\\hat{\\boldsymbol{\\beta}}\\)에서의 (observed 혹은 expected) Fisher 정보 행렬입니다.(3장에서 이 이유에 대해 살펴볼 것입니다.) 모델의 parameter를 추정하는 과정에서, 각 관측치 \\(i\\)에 대해 \\(\\mathrm{Var}(y_i)\\)와 \\(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\)가 포함된 특정 가중치 \\(W^{(t)}\\)가 등장하였었고, 반복(step)마다 업데이트되는 정규방정식을 풀어감으로써 추정치 \\(\\beta^{(t+1)}\\)를 얻었습니다. 이후 최종 수렴 시점(\\(t \\to \\infty\\))에서, 우리는 \\(\\hat{\\boldsymbol{\\beta}} = \\beta^{(\\infty)}\\)에 도달하게 되고, 이 시점에서 계산된 Hessian(또는 Fisher information) matrix \\(\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X}\\)에 대해서, 이 행렬의 역수가 분산이 되는 것입니다.\n즉, 실제 계산 시에는 아래와 같은 모양이 됩니다.\n\\[ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, \\]\n왜 최종적으로 추정된 모델의 분산이 수렴된 time step에서의 Fisher information matrix로 추정할 수 있는지에 대해서는 3장의 M-estimation에서 증명할 것입니다. 여기서 미리 강조할 것은, 위에서 얻는 분산의 추정값은 GLM에서의 기본 가정들이었던 독립성, 분산 함수의 형태 등이 성립한다는 조건 위에서 도출된 것이므로, 이러한 기본 가정이 어느 정도 엄격히 맞아떨어지는 상황(정확한 포아송 분포를 따르는 count data, 명시적으로 독립적인 개별 관측치 등)이라면 괜찮지만, 현실의 data에서는 이질분산, 클러스터 내 상관, 과산포(overdispersion) 등으로 인해 이 기본 가정들이 깨질 수 있습니다. 다행히도, GLM에서도 모델이 consist할 때(GLM의 위로부터 추정된 parameter 자체는 consist합니다.) HC(Heteroskedasticity-Consistent) se와, 아래에서 clustered data에서 고려할 수 있는 버전인 Cluster-robust standard errors를 사용하여 더욱 robust하게 분산을 추정할 수 있습니다. 때문에 아래에서는 Cluster-robust se를 OLS 버전으로 소개드린 후, GLM에서 사용하기 위해 \\(\\hat{\\mathbf{W}}\\) 행렬 \\(\\hat{\\mathbf{A}}\\) 행렬 등의 구조가 어떻게 수식적으로 첨가되어 LM(Linear Model)에서의 식과는 살짝 다른 모습을 취하게 되는지 보고 마치겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#cluster-robust-standard-errors",
    "href": "posts/2025-02-28-reg2/index.html#cluster-robust-standard-errors",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "3. Cluster-Robust Standard Errors",
    "text": "3. Cluster-Robust Standard Errors\n3.1. Clustered Data 정의\n\nClustered data란 데이터 내에서 동일 그룹에 속하는 관측치들이 상관관계를 가지는 경우를 의미합니다. 예를 들어, 한 환자의 여러 진료 기록이 서로 상관되어 있을 수 있습니다. 이 때, cluster간에는 상관관계가 없고 cluster 내의 데이터들은 상관관계가 있다는 가정하에 Cluster-robust standard errors나 3장의 GEE, GLMM model이 개발되었습니다. 의료 분석 상황의 예시로는 대표적으로 데이터의 각 환자 당 여러 시간 또는 주기에 걸쳐 측정한 데이터, 여러 학교나 병원과 같은 단체에서 얻은 데이터들을 한 번에 고려하는 경우가 있을 것입니다. 또한, cluster간에 상관관계가 있거나 cluster 안에 cluster가 있는 hierarchical의 경우도 있지만, 이에 대한 공식들은 위에서 고려하는 1차적인 상황을 이해하면 쉽게 이해할 수 있으며, 의료 분석에서 고려하는 피험자 내 관측치 간 상관관계, 병원 내 관측치 간 상관관계 등을 고려해야 하는 상황은 이번 블로그에서 이야기 할 1차적인 clustered 상황임을 알아두시면 좋을 것 같습니다. 이 observations간의 상관관계에 대한 이야기와, 이때 사용해야 하는 Regression Models에 대한 설명은 3장에서 GEE, GLMM과 함께 더욱 자세하게 다뤄볼 예정입니다.\n확실한 것은, 비선형 분포를 추정할 수 있는 GLM이나, OLS에서 안정적인 parameter 분산 추정 method였던 HC(Heteroskedasticity-Consistent) 표준오차는 관측치 간의 독립성을 가정하였었고, 이는 위와 같은 data를 다룰 때에는 깨져야 하는 가정이라는 것입니다. 이제 설명드릴 Cluster-robust standard errors는 HC se와 형태가 매우 비슷하며, 같은 철학으로 clustered data에서 robust한 모델 분산 추정 method입니다. 이때 기억하셔야 할 부분은, 1장에서는 HC se의 안정성을 Linear Regression에 대해서 고려하였고 R의 sandwich 패키지를 통해 구현할 수 있음을 보았는데, 이번 장에서 다루고 있는 GLM에서도 이 HC se, Cluster-robust se를 모두 사용할 수 있다는 것입니다. 이때 식이 LM과 GLM에서 살짝 다른데, 우선 Linear Model에서의 Cluster-robust se에 대해서 설명드리고, GLM에서는 무엇이 다른지 보겠습니다.\n실제 소프트웨어의 구현에 대해서 첨언하자면, R의 sandwich 패키지나 대부분의 패캐지에서는 이 두 robust 분산 추정의 계산 및 검정을 LM, GLM 모두에 사용 가능하고, 이 패키지들은 들어오는 모델의 객체가 LM, GLM임을 분류한 뒤 각각에 맞는 살짝 변형된 식으로 추정한다고 생각하시면 될 것 같습니다.\n3.2. Cluster-robust standard errors 정의 및 수학적 표현\n\nCluster-robust standard errors는 클러스터 내 상관관계를 고려하여 분산을 추정합니다. 이를 통해 클러스터 간 독립성은 유지하되, 클러스터 내 관측치 간 상관관계가 존재할 때도 일관된 추정치를 제공합니다. LM에서 Cluster-robust standard error를 구하는 식은 다음과 같습니다:\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n위 식에서 \\(g\\)는 클러스터 인덱스, \\(\\mathbf{\\hat{u}}_g\\)는 클러스터 \\(g\\)의 잔차 벡터입니다.이 식은 1장에서의 HC0과 아주 유사하며, 가운데 meat항 (두 \\((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\) 사이에 있는 항)만 달라졌음을 알 수 있습니다. 이는 실제로 HC0에서, 위에서 설명드린 가정인 1차적 clustered 구조(클러스터 간 독립성을 가정하지만, 클러스터 내 관측치들 간 상관관계는 허용)를 고려해서 \\(\\Phi\\) 항만 바뀌었음을 짐작해볼 수 있습니다. 이제 이 Cluster-robust standard errors의 철학에 대해서 구체적으로 살펴보겠습니다.\n3.3. Cluster-robust standard errors 수학적 표현\n\nLM에서는 이전에 봤던대로 parameter의 분산을 유도하면 다음과 같습니다:\n\\[\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n여기서 \\(\\Phi\\)는 오차항의 공분산 행렬을 나타냈었습니다. HC0 (Heteroskedasticity-Consistent 0)에서는 모든 관측치가 서로 독립임을 가정하였기 때문에 (Heteroskedasticity를 고려하였지 dependent case를 고려하지는 않았었습니다.) 이에 따라 \\(\\Phi\\)는 대각행렬로 표현되며,\n\\[\n\\Phi_{\\text{HC0}} = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)\n\\]\n결과적으로 분산 추정량은 개별 관측치에 대해아래와 같이 계산하였었습니다.\n\\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\] 위 식은 1장의 HC0 식과 같은 식입니다. 표현이 어색하다고 느끼시는 분을 위해 이전에 사용한 식을 가져오면 \\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n이며, 위에서부터 얘기하고 있는 \\(\\hat{u}\\)는 이 error term \\(e\\)와 비슷한(잔차이기 때문에 사실 의미는 다릅니다) 의미입니다. HC0에서는 각 관측치만을 고려하기 때문에 \\(\\hat{u}_i\\)는 \\(e_i\\)와 같고, 길이가 1인 벡터, 즉 scalar이기 때문에 제곱을 사용하였지만 위 cluster-robust 식에서 사용한 \\(\\hat{u}_g\\)는 클러스터 g에 해당하는 모든 관측치를 한 줄로 나열한 임의의 길이의 벡터이기 때문에 제곱이 아니라 \\(\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top\\) term을 사용한 것입니다.\n이에 대한 이해를 바탕으로 Cluster-robust se의 meat term을 생각해보면, Cluster-robust에서는 cluster간은 독립적이고, cluster안의 관측치들은 상관관계를 가질 수 있다고 가정하기 때문에 각 cluser에 대해서 \\(\\Phi_i\\)를 \\(\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top\\)로 각각 구한 후, 전체 \\(\\Phi\\)는 아래와 같이 block diagonal 구조로 넣어준다고 이해할 수 있습니다. (block 행렬은 행렬을 특정한 block으로 나누었을 때 대각선 이외의 모든 행렬 블록이 영행렬인 행렬을 의미하며, cluster간의 독립을 block diagonal 구조로 고려하였다고 이해하면 됩니다.)\n\\[\n\\Phi_{\\text{cluster}} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & \\cdots & 0 \\\\\n0 & \\Phi_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\Phi_G\n\\end{pmatrix}\n\\]\n즉, 여기서 각 \\(\\Phi_g = E[\\mathbf{u}_g \\mathbf{u}_g^\\top]\\)는 클러스터 \\(g\\) 내의 오차의 공분산 행렬이고, 각 cluster에 대해 잔차\\(\\hat{\\mathbf{u}}_g\\)를 사용하여\n\\[\n\\widehat{\\text{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\] 와 같이 추정합니다.\n위 Cluster-robust의 meat항에 대한 이해는 비슷하게 3장에서도 필요하기 때문에 예시를 통해 좀더 직관적으로 보여드리겠습니다. 우선 이 중앙항은 다음과 같고,\n\\[\n\\mathbf{B} = \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g\n\\]\n이는 행렬로 보면 위에서 보신 \\(\\Phi\\)항과 같이 block diagonal 형태를 갖습니다. 3개의 cluster가 있고, 각 cluster 내 관측치 수가 2, 3, 1개라고 가정하면 각각의 \\(\\Phi\\)는 다음과 같고,\n\\[\n\\Phi_1 = \\mathbf{X}_1^\\top \\hat{\\mathbf{u}}_1 \\hat{\\mathbf{u}}_1^\\top \\mathbf{X}_1 =\n\\begin{pmatrix}\n\\sigma_{11}^2 & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}^2\n\\end{pmatrix}\n\\]\n\\[\n\\Phi_2 = \\mathbf{X}_2^\\top \\hat{\\mathbf{u}}_2 \\hat{\\mathbf{u}}_2^\\top \\mathbf{X}_2 =\n\\begin{pmatrix}\n\\sigma_{33}^2 & \\sigma_{34} & \\sigma_{35} \\\\\n\\sigma_{34} & \\sigma_{44}^2 & \\sigma_{45} \\\\\n\\sigma_{35} & \\sigma_{45} & \\sigma_{55}^2\n\\end{pmatrix}\n\\]\n\\[\n\\Phi_3 = \\mathbf{x}_6 \\hat{u}_6^2 \\mathbf{x}_6^\\top\n\\]\n로 표현될 수 있으며, 결국 Cluster-robust의 중앙 term은\n\\[\n\\mathbf{\\Phi} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & 0 \\\\\n0 & \\Phi_2 & 0 \\\\\n0 & 0 & \\Phi_3\n\\end{pmatrix}\n\\] 가 될 것입니다.\n정리하자면, HC0는 \\(\\Phi\\)가 대각행렬인 경우로, 개별 관측치의 Heteroskedasticity 만을 고려하며\n\\[\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nCluster-Robust 분산 추정량은 clusr별 \\(\\Phi\\)가 block diagonal 구조로, Heteroskedasticity와 cluster 내의 상관관계를 반영합니다.\n\\[\nE\\left[\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}})\\right] = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n3.4. In GLMs..\n\n이미 위(3장에서) Cluster-robust standard errors가 어떤 원리로부터 유도되며, OLS 환경(Linear Model)에서의 공식이 어떻게 생겼는지 살펴보았습니다. 또한 HC(Heteroskedasticity-Consistent) se 역시 기본 가정(등분산, 독립성 등)이 약화되었을 때도 일관된 추정을 제공하기 위해 Robust(샌드위치) 분산 추정량을 쓰게 된다는 것을 보았습니다. GLM에서도 LM과 같이 위 두 robust한 분산 추정치 식을 사용하여 Fisher information matrix의 역행렬로 분산을 추정하는 대신, 더욱 안정적으로 분산을 추정할 수 있습니다. GLM의 경우, 단순 OLS와 달리 \\(\\hat{\\mathbf{W}}, \\hat{\\mathbf{A}}\\) 등 추가적인 항이 존재하고, 이 행렬들이 실제 분산 추정 과정에 반영됩니다. 이 때문에 “bread”(양쪽에 곱해지는 행렬)와 “meat”(중간에 오는 분산·잔차 구조) 부분이 LM에서의 표기와는 형태가 조금 달라집니다. 즉, 원리는 동일하되, link & variance function으로 부터 비롯된 미분 항(\\(\\mathbf{A}\\))과 가중치 항(\\(\\mathbf{W}\\))이 반영되어야 한다는 점만 다릅니다. 1장에서 소개했던 HC0를 떠올리면, LM의 경우\n\\[ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}, \\]\n로 식이 구성되었습니다. GLM의 경우에는 \\(\\hat{\\mathbf{W}}\\)가 \\(\\mathbf{X}\\)와 상호작용하여 분산 추정에 들어가므로, 실제로는 다음과 같은 형태를 가집니다. (식은 패키지나 저자별 표기 차이에 따라 다소 달라질 수 있습니다). \\[ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n \\mathbf{x}_i \\Bigl(\\hat{u}_i^2 \\Bigr) \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}. \\]\n여기서 \\(\\hat{u}_i\\)는 단순 잔차가 아니라, 펄슨(pearson) 잔차 등 비선형적인 GLM 설정에 맞춰 적절히 조정된 형태일 수 있습니다. 구현별로 이탈도(deviance) 잔차를 사용할 수도 있고, 핵심은 “관측치별 잔차의 크기”를 통해 이질분산성을 추정하는 것입니다. 여기서는 앞뒤의 \\((\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}\\)이 bread(빵)이고, 가운데 잔차 \\(\\hat{u}_i\\hat{u}_i^\\top\\)가 meat(고기) 역할을 한다고 보면 됩니다. 철학적으로 해석하면, GLM에서도 LM에서와 동일하게 HC se는 “각 관측치별 오차분산”이 서로 다르더라도 일관된 추정을 제공하기 위하는 목적이며, 식은 (1) 잔차(오차항) 부분은 그대로 meat로 넣고, (2) 정보를 제공하는 bread에는 \\(\\mathbf{X}\\)에 가중치의 의미를 가진 \\(\\hat{\\mathbf{W}}\\) 항을 추가하여 구성한 위 형태로 구성됩니다.\n클러스터링이 있는 데이터에 대하여, LM과 마찬가지로 GLM에서도 Cluster-robust se가 적용될 수 있습니다. 이미 섹션 3.2~3.3에서 보았듯, 클러스터 간에는 독립이지만 클러스터 내 관측치들 간에는 상관관계가 존재할 수 있으므로, \\(\\Phi\\) 행렬(오차의 공분산 구조)을 block diagonal 형태로 가정하고, 이를 샌드위치 가운데(meat)에 반영합니다.\nLM에서의 일반적 식은 다음과 같았습니다.\n\\[ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\]\nGLM에서는 동일한 철학으로, 단순히 \\(\\mathbf{X}\\) 대신 가중치를 고려해 \\(\\hat{\\mathbf{W}}^{1/2}\\mathbf{X}\\)와 같은 형태(혹은 관련 도함수 항)가 곱해지게 됩니다. 즉,\n\\[ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G (\\mathbf{X}_g^\\top \\hat{\\mathbf{W}}_g^{1/2}) \\, \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\, (\\hat{\\mathbf{W}}_g^{1/2} \\mathbf{X}_g) \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, \\]\n와 같은 꼴이 됩니다(마찬가지로 패키지마다 표기 방식이나 구현 세부가 약간씩 다를 수 있습니다).각 항들 또한 한 번 더 설명하자면, \\(\\hat{\\mathbf{u}}_g\\)는 클러스터 \\(g\\) 내 잔차 벡터(pearson 또는 deviance 잔차 등).\\(\\mathbf{X}_g\\)는 클러스터 \\(g\\)에 해당하는 행만 추출한 \\(\\mathbf{X}\\)의 서브 행렬. \\(\\hat{\\mathbf{W}}\\)는 클러스터 (\\(g\\))에 해당하는 마찬가지로 가중치 서브 행렬이고, 이때 1/2승을 한다는 의미는 이가 diagonal matrix이므로 이 경우에는 단순히 diagonal 성분들 각각을 루트 씌운 값입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#r-예시-glm-cluster-robust-se",
    "href": "posts/2025-02-28-reg2/index.html#r-예시-glm-cluster-robust-se",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "4. R 예시: GLM, Cluster-robust SE",
    "text": "4. R 예시: GLM, Cluster-robust SE\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. GLM모델의 분산과 cluster-robust 분산을 비교하시면서 해석하면 됩니다.\n\n## 필요한 패키지 설치 (필요시)\n## install.packages(\"sandwich\")\n## install.packages(\"lmtest\")\n## install.packages(\"nlme\")\n#\n## 데이터 불러오기\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환\n#\n## 기본 GLM (로지스틱 회귀)\n#glm_fit &lt;- glm(binary ~ age + Sex, \n#               data = Orthodont, \n#               family = binomial)\n#summary(glm_fit) \n## 클러스터-로버스트 표준오차 (Subject 기준)\n#library(sandwich)\n#library(lmtest)\n#cluster_se &lt;- vcovCL(glm_fit, cluster = ~ Subject)\n#coeftest(glm_fit, vcov = cluster_se)  # 결과 출력"
  },
  {
    "objectID": "posts/2025-02-28-reg2/index.html#마무리하며",
    "href": "posts/2025-02-28-reg2/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error",
    "section": "마무리하며",
    "text": "마무리하며\n이번 2장에서는 1장에서 다룬 Linear Model을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수에서도 분석할 수 있도록 확장한 Generalized linear model의 기본 개념과, 실제로 패키지에서 이 GLM의 parameter를 estimate할 때 사용하는 대표적인 알고리즘인 IRLS(Fisher scoring)을 수학적으로 상당히 깊게 살펴보았습니다. 이후, HC standard errors의 clustered data 버전인 Cluster-robust standard error를 보고, GLMs에서도 이 둘을 사용할 수 있다는 것을 밝힌 뒤 그 변형된 수식을 보았습니다. 다음 3장에서는 아직 깨지 못한 가정이었던 오차항의 독립, 즉 data(observations)간의 correlation이 존재하는 경우 자체를 모델에 반영하기 위해 개발된 모델들인 GEE, GLMM에 대하여 어느 정도 살펴보고 (GLMM의 내용은 너무 길어지기 때문에 얕게 다룰 것입니다.), 모델의 분산을 robust하게 추정하기 위한 가장 general한 형태의 Sandwich estimator를 M-estimation의 개념과 함께 공부할 것입니다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서 새로운 영상 장비의 도입이 기존 방법보다 우수한지 평가하는 것은 매우 중요한 과정이다. 이를 위해 흔히 사용되는 연구 디자인이 MRMC 연구이며, 이는 여러 명의 판독자가 여러 증례를 평가하는 방식으로 진행된다. 이러한 MRMC 연구에서의 가장 큰 고민 중 하나는 “몇 명의 판독자와 몇 개의 증례가 필요한가?” 하는 문제이다.\n즉, 연구의 신뢰성을 확보하면서도 불필요한 비용과 시간을 줄이기 위해 표본 크기를 적절하게 설정하는 것이 매우 중요하다.\n\nMRMC 연구에서는 판독자와 증례를 두 가지 주요 요소로 고려해야 한다.\n일반적으로 판독자가 많아질수록 데이터의 변동성이 줄어들고 연구의 신뢰성이 높아지지만, 판독자의 시간과 연구 비용을 고려해야 한다. 마찬가지로 증례 수가 많을수록 통계적 검정력이 향상되지만, 데이터 수집 및 분석 비용이 증가하게 된다.\n또한, MRMC 연구에서는 다음과 같은 요소들이 표본 크기 결정에 영향을 미친다.\n\n\n효과 크기(Effect Size, \\(d\\))\n\n비교하고자 하는 두 판독 조건 간의 차이\n연구 설계에서 “임상적으로 의미 있는 차이”를 미리 정의하는 것이 중요\n\n\n\n검정력(Statistical Power, \\(1 - \\beta\\))\n\n실제로 차이가 있을 때 이를 검출할 확률\n일반적으로 80% 이상을 목표로 설정\n\n\n\n유의수준(Significance Level, \\(\\alpha\\))\n\nType I 오류의 허용 범위\n보통 5%로 설정\n\n\n\n분산 성분(Variance Components)\n\n판독자 간 변동성, 증례 간 변동성, 판독자와 증례 간 상호작용 등의 변동 요인을 반영\n\n\n\n의료 영상 분석에서는 병변의 유무를 파악하는 것이 중요하다.\n그 성능을 평가하는 대표적인 방법으로는 ROC(Receiver Operating Characteristic) 분석이 있는데, 이는 병변의 단순 유무를 파악하는 데 사용된다.\n그러나 영상 판독 연구에서는 단순히 병변의 유무뿐만 아니라 병변의 위치(Localization)까지 고려하는 것이 중요하다.\n이때 FROC(Free-Response ROC) 분석이 사용된다.\nFROC 분석을 활용한 MRMC 연구에서는 판독자가 병변의 위치까지도 설정하도록 하여,병변 탐지(Detection)뿐만 아니라 위치 정확도(Localization Accuracy)도 평가된다.\n권장되는 평가 지표로는 가중 AFROC(Weighted AFROC, wAFROC) 가 있으며,\n모든 병변은 동일한 가중치를 갖는 것이 일반적이다(특정 임상적인 이유가 있다면 병변마다 다른 가중치를 부여할 수 있음)."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#why-is-sample-size-estimation-important",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#why-is-sample-size-estimation-important",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서 새로운 영상 장비의 도입이 기존 방법보다 우수한지 평가하는 것은 매우 중요한 과정이다. 이를 위해 흔히 사용되는 연구 디자인이 MRMC 연구이며, 이는 여러 명의 판독자가 여러 증례를 평가하는 방식으로 진행된다. 이러한 MRMC 연구에서의 가장 큰 고민 중 하나는 “몇 명의 판독자와 몇 개의 증례가 필요한가?” 하는 문제이다.\n즉, 연구의 신뢰성을 확보하면서도 불필요한 비용과 시간을 줄이기 위해 표본 크기를 적절하게 설정하는 것이 매우 중요하다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서-표본-크기-추정의-핵심-개념",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서-표본-크기-추정의-핵심-개념",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "MRMC 연구에서는 판독자와 증례를 두 가지 주요 요소로 고려해야 한다.\n일반적으로 판독자가 많아질수록 데이터의 변동성이 줄어들고 연구의 신뢰성이 높아지지만, 판독자의 시간과 연구 비용을 고려해야 한다. 마찬가지로 증례 수가 많을수록 통계적 검정력이 향상되지만, 데이터 수집 및 분석 비용이 증가하게 된다.\n또한, MRMC 연구에서는 다음과 같은 요소들이 표본 크기 결정에 영향을 미친다.\n\n\n효과 크기(Effect Size, \\(d\\))\n\n비교하고자 하는 두 판독 조건 간의 차이\n연구 설계에서 “임상적으로 의미 있는 차이”를 미리 정의하는 것이 중요\n\n\n\n검정력(Statistical Power, \\(1 - \\beta\\))\n\n실제로 차이가 있을 때 이를 검출할 확률\n일반적으로 80% 이상을 목표로 설정\n\n\n\n유의수준(Significance Level, \\(\\alpha\\))\n\nType I 오류의 허용 범위\n보통 5%로 설정\n\n\n\n분산 성분(Variance Components)\n\n판독자 간 변동성, 증례 간 변동성, 판독자와 증례 간 상호작용 등의 변동 요인을 반영"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서의-성능-평가",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#mrmc-연구에서의-성능-평가",
    "title": "Sample Size Estimation in MRMC",
    "section": "",
    "text": "의료 영상 분석에서는 병변의 유무를 파악하는 것이 중요하다.\n그 성능을 평가하는 대표적인 방법으로는 ROC(Receiver Operating Characteristic) 분석이 있는데, 이는 병변의 단순 유무를 파악하는 데 사용된다.\n그러나 영상 판독 연구에서는 단순히 병변의 유무뿐만 아니라 병변의 위치(Localization)까지 고려하는 것이 중요하다.\n이때 FROC(Free-Response ROC) 분석이 사용된다.\nFROC 분석을 활용한 MRMC 연구에서는 판독자가 병변의 위치까지도 설정하도록 하여,병변 탐지(Detection)뿐만 아니라 위치 정확도(Localization Accuracy)도 평가된다.\n권장되는 평가 지표로는 가중 AFROC(Weighted AFROC, wAFROC) 가 있으며,\n모든 병변은 동일한 가중치를 갖는 것이 일반적이다(특정 임상적인 이유가 있다면 병변마다 다른 가중치를 부여할 수 있음)."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#hypothesis-testing-in-mrmc-studies",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#hypothesis-testing-in-mrmc-studies",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. Hypothesis Testing in MRMC Studies",
    "text": "1. Hypothesis Testing in MRMC Studies\nMRMC 연구에서 새로운 영상 기술(또는 판독 조건)이 기존 방법보다 우수한지 검증하려면 통계적 검정을 수행해야 한다.\n이를 위해 귀무가설(\\(H_0\\))과 대립가설(\\(H_1\\))을 설정하고, 효과 크기(Effect Size)를 기반으로 표본 크기를 결정한다.\n\n\n귀무가설(\\(H_0\\)): 두 판독 조건(예: 기존 방법 vs. 새로운 방법)의 성능 차이가 없다.\n\n즉, \\[\\theta_1 = \\theta_2\\] (ROC-AUC 또는 wAFROC-AUC 값이 동일)\n\n\n\n대립가설(\\(H_1\\)): 두 판독 조건의 성능 차이가 있다.\n\n즉, \\[\\theta_1 \\neq \\theta_2\\]"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#observed-vs.-anticipated-effect-size",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#observed-vs.-anticipated-effect-size",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. Observed vs. Anticipated Effect-Size",
    "text": "2. Observed vs. Anticipated Effect-Size\n표본 크기를 결정하려면 두 판독 조건 간의 차이를 수량화한 효과 크기(Effect Size, \\(d\\))를 추정해야 한다.\n이때 관측된 효과 크기(Observed Effect-Size)와 예상 효과 크기(Anticipated Effect-Size)를 구분해야 한다.\n✔ 관측된 효과 크기(\\(d_{\\text{obs}}\\)):\n- 파일럿 연구(Pilot Study)에서 얻은 효과 크기의 추정값\n- 작은 표본 크기로 인해 오차가 포함될 가능성이 있음\n✔ 예상 효과 크기(\\(d_{\\text{ant}}\\)):\n- 본 연구(Pivotal Study)의 표본 크기를 결정하기 위해 설정하는 값\n- 신뢰구간(Confidence Interval, CI)을 고려하여 설정해야 함\n예상 효과 크기를 설정할 때 다음과 같은 접근법이 가능하다.\n\n\n신뢰구간의 하한을 사용 (보수적 접근법)\n\nType II 오류를 줄여 검정력을 확보 가능\n그러나 불필요하게 많은 판독자 및 증례가 필요할 수 있음\n\n\n\n신뢰구간의 중앙값을 사용 (균형적 접근법)\n\n너무 보수적이지 않으면서도 무리한 연구 설계를 방지 가능\n\n\n\n관측된 효과 크기 그대로 사용 (위험 부담 있음)\n\n파일럿 연구 표본이 작으면 변동성이 크므로 추천되지 않음"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#variance-components-and-their-impact-on-power",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#variance-components-and-their-impact-on-power",
    "title": "Sample Size Estimation in MRMC",
    "section": "3. Variance Components and Their Impact on Power",
    "text": "3. Variance Components and Their Impact on Power\nMRMC 연구에서 표본 크기를 결정할 때, 효과 크기(Effect Size)뿐만 아니라 분산 성분(Variance Components)이 통계적 검정력(Statistical Power)에 미치는 영향을 고려해야 한다.\n분산 성분은 판독자 간 변동성(Reader Variability), 증례 간 변동성(Case Variability), 판독자-처치 간 변동성(Treatment-Reader Interaction Variability) 등을 반영하며, 연구 설계에서 중요한 요소이다.\n3.1. 통계적 검정력(Statistical Power)과 분산 성분의 관계\nMRMC 연구에서 검정력은 효과 크기(\\(d\\))와 분산 성분의 함수로 결정된다. 통계적 검정력은 비중심 모수(Non-centrality Parameter, \\(\\lambda\\)) 에 의해 결정되며, 다음과 같은 조건에서 검정력이 증가한다.\n검정력은 다음과 같은 확률로 정의된다.\n\\[\n\\text{Power} = P\\left(F &gt; F_{\\alpha, \\text{ndf}, \\text{ddf}} \\mid \\lambda \\right)\n\\]\n여기서,\n- \\(F\\) : 검정 통계량 (F-statistic)\n- \\(F_{\\alpha, \\text{ndf}, \\text{ddf}}\\) : 유의수준 \\(\\alpha\\)에서의 F-분포 임계값 (분자 자유도 \\(\\text{ndf}\\), 분모 자유도 \\(\\text{ddf}\\))\n- \\(\\lambda\\) : 비중심 모수(Non-centrality Parameter)\n비중심 모수 \\(\\lambda\\) 는 다음과 같이 정의된다.\n\\[\n\\lambda = \\frac{J K d^2}{\\sigma^2_{\\text{total}}}\n\\]\n여기서,\n- \\(J\\) : 판독자 수(Readers)\n- \\(K\\) : 증례 수(Cases)\n- \\(d\\) : 효과 크기(Effect Size)\n- \\(\\sigma^2_{\\text{total}}\\) : 총 분산 성분 (전체 변동성)\n3.2. 분산 성분이 검정력에 미치는 영향\n(1) 분자가 클 경우 → 검정력 증가\n\n\n예상 효과 크기(Anticipated Effect-Size, \\(d_{\\text{ant}}\\))가 클 경우\n\n\n효과 크기가 클수록 귀무가설(\\(H_0\\))을 기각할 가능성이 높아진다.\n\n\n판독자 수(\\(J\\)) 또는 증례 수(\\(K\\))가 클 경우\n\n판독자(\\(J\\)) 또는 증례(\\(K\\))가 많을수록 추정값의 변동성이 감소하여 통계적 검정력이 증가한다.\n이는 큰 표본 크기가 귀무가설 기각 확률을 증가시키는 일반적인 원리와 동일하다.\n\n\n(2) 분모가 작을 경우 → 검정력 증가\n통계적 검정력의 분모는 다음과 같이 구성된다.\n\\[\n\\sigma^2_{\\epsilon} + \\sigma^2_{\\tau RC} + K\\sigma^2_{\\tau R} + J\\sigma^2_{\\tau C}\n\\]\n이때, 각 분산 성분이 작아질수록 검정력이 증가한다.\n\n\n잔차 변동성(\\(\\sigma^2_{\\epsilon} + \\sigma^2_{\\tau RC}\\)) 감소 → 검정력 증가\n\n이 두 항은 Jackknife Pseudovalues의 잔차 변동성을 나타낸다.\n잔차 변동성이 작을수록 비중심 모수(Non-centrality Parameter, \\(\\lambda\\))가 증가하여 검정력이 증가한다.\n\n\n\n처치-판독자 변동성(\\(\\sigma^2_{\\tau R}\\)) 감소 → 검정력 증가\n※ 처치 : 비교하고자하는 판독 조건을 의미;의료 영상 분석의 예시에서는 기존 방식과 AI 보조 영상 판독가 될 수 있음\n\n판독자 자체의 변동성(\\(\\sigma^2_R\\))은 모든 처치 조건에서 동일한 영향을 미치므로 검정력에 영향을 주지 않는다.\n처치-판독자(Treatment-Reader) 분산 성분(\\(\\sigma^2_{\\tau R}\\))은 노이즈로 작용하여 효과 크기 추정을 방해하고 검정력을 감소시킨다.\n따라서 이 값이 작을수록 표본 크기 추정의 정확도가 높아진다.\n\n\n\n\n처치-증례 변동성(\\(\\sigma^2_{\\tau C}\\)) 감소 → 검정력 증가\n\n증례 자체의 변동성(\\(\\sigma^2_C\\))은 모든 처치 조건에서 동일한 영향을 미치므로 검정력에 영향을 주지 않는다.\n그러나 처치-증례 변동성(\\(\\sigma^2_{\\tau C}\\))은 효과 크기 추정치에 노이즈를 추가하여 검정력을 감소시킨다.\n이 항은 판독자 수(\\(J\\))와 곱해지므로, 보통의 \\(J \\ll K\\)인 연구에서는 그 영향이 크지 않다.\n\n\n\n📌 **즉, MRMC 연구 설계에서 표본 크기를 결정할 때, 단순히 숫자를 늘리는 것이 아니라, 이러한 분산 성분을 고려해야 한다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#using-pilot-studies",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#using-pilot-studies",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. Using Pilot Studies",
    "text": "1. Using Pilot Studies\n파일럿 연구(Pilot Study)는 본 연구에서 필요한 효과 크기(\\(d_{\\text{obs}}\\)) 및 분산 성분을 추정하는 역할을 한다. - 파일럿 연구에서 얻은 정보는 RJafroc 패키지의 SsSampleSizeKGiven() 함수를 활용해 표본 크기를 결정하는 데 사용된다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#sample-size-estimation-using-rjafroc",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#sample-size-estimation-using-rjafroc",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. Sample Size Estimation Using RJafroc",
    "text": "2. Sample Size Estimation Using RJafroc\n2.1. 함수 개요\n\n# SsSampleSizeKGivenJ()\n# SsSampleSizeKGivenJ(dataset, FOM, J, effectSize, method, analysisOption)\n\n\ndataset : Pilot Data\nFOM : 평가 지표 (“ROC-AUC” in ROC Studies)\nJ : 판독자 수\nK : 증례 수\neffectSize : 효과 크기 (파일럿 연구에서 추정)\nmethod : 분석 방법 (“DBM” 또는 “OR”)\nanalysisOption : 분산 분석 Option (“RRRC”, “FRRC”, “RRFC”)\n2.2. 실제 예제\n일반적인 연구 설계에선 판독자수는 제한된 자원으로 인해 고정되거나, 대략적인 좁은 범위에서 결정되는 경우가 많다.\n이에 특정 판독자 수에서 검정력을 만족하는 최소한의 증례 수를 찾는 것이 일반적이고, RJafroc의 SsSampleSizeKGivenJ 함수도 그러하다.\n본 예시 코드에서는 판독자수가 6~13명 정도의 범위일때, 검정력이 80% 이상이 되는 최소한의 \\(K\\) 값을 찾는다.\nlibrary(RJafroc)\n\npilot_data &lt;- dataset02  # 예제 데이터셋 사용\n\n\ntarget_power &lt;- 0.8 # 목표 검정력 \nJ_vals &lt;- 6:13 # 판독자 수 범위 지정 \noptimal_K_results &lt;- data.frame(J = integer(), K = integer(), Power = numeric())  \n\n# 목표 검정력을 만족하는 최소 K(증례수) 도출 \nfor (J in J_vals) {\n  ret &lt;- SsSampleSizeKGivenJ(\n    dataset = pilot_data,\n    FOM = \"Wilcoxon\",\n    J = J,\n    analysisOption = \"RRRC\",\n    alpha = 0.05,\n    desiredPower = target_power\n  )\n  optimal_K_results &lt;- rbind(optimal_K_results, \n                             data.frame(J = J, K = ret$K, Power = signif(ret$powerRRRC, 3)))\n}\n\nrmarkdown::paged_table(optimal_K_results)\n\n  \n\n\n위 코드를 실행하면 판독자가 많아짐에 따라 같은 효과 크기에서도 더 작은 증례수로 목표 검정력을 확보할 수 있음을 확인 가능하다.\nSsSampleSizeKGivenJ() 함수를 이용하면 파일럿 연구에서 추정된 효과 크기와 목표 검정력을 바탕으로 MRMC 연구의 판독자수에 따른 적절한 증례 수를 합리적으로 결정할 수 있게 된다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#froc와-roc-연구의-차이점",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#froc와-roc-연구의-차이점",
    "title": "Sample Size Estimation in MRMC",
    "section": "1. FROC와 ROC 연구의 차이점",
    "text": "1. FROC와 ROC 연구의 차이점\nFROC 연구에서 효과 크기 변환이 필요한 이유를 이해하려면, ROC와 wAFROC의 차이를 먼저 살펴보아야 한다.\nROC 연구의 특징\n\n\n판독자가 병변이 있는지(Yes/No)만 결정 → 위치 정보는 고려되지 않음\n\n성능 평가 지표: ROC-AUC (0.5 ~ 1)\n\n무작위 성능 ≈ 0.5 (랜덤으로 판별할 경우)\n완벽한 성능 ≈ 1\n\n\n\nFROC 연구의 특징\n\n\n판독자가 병변이 있는 위치까지 특정해야 함 → 탐지뿐만 아니라 위치 정확도(Localization Accuracy)도 고려됨\n\n성능 평가 지표: wAFROC-AUC (0 ~ 1)\n\n무작위 성능 ≈ 0 (무작위로 위치를 선택하면 병변을 정확히 찾을 확률이 0에 가까움)\n완벽한 성능 ≈ 1\n\n\n\n\n✔ ROC-AUC는 0.5 ~ 1 범위에서 변화하는 반면, wAFROC-AUC는 0 ~ 1 범위에서 변화한다.\n✔ 따라서 판독 능력의 차이가 비슷하더라도, 수치적인 AUC 차이는 서로 다르게 나타날 수 있다.\n✔ 따라서 ROC 연구에서 정의한 효과 크기를 FROC 연구에서 적용하려면 변환 과정이 필요하다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환을-위한-rsmradiological-search-model-적용",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환을-위한-rsmradiological-search-model-적용",
    "title": "Sample Size Estimation in MRMC",
    "section": "2. 변환을 위한 RSM(Radiological Search Model) 적용",
    "text": "2. 변환을 위한 RSM(Radiological Search Model) 적용\nFROC 연구에서 ROC 효과 크기를 wAFROC 효과 크기로 변환하려면Radiological Search Model (RSM)을 활용해야 한다.\nRSM 모델의 핵심 개념\nRSM 모델은 판독자의 탐색(Search)과 의사결정(Decision Making) 과정을 모델링하며,\n이를 위해 3가지 주요 매개변수를 사용한다.\n\n\nμ (mu): 병변과 비병변 간의 신호 대비 (Signal-to-Noise Ratio, SNR)\n\nROC에서 병변을 감지하는 능력(Detection Ability)을 나타냄\n값이 클수록 병변과 비병변을 더 잘 구별할 수 있음\n\n\n\nλ (lambda): 비병변을 오탐(False Positive)할 가능성\n\n값이 클수록 판독자가 비병변(non-lesion)을 병변으로 잘못 판단할 확률이 증가\n\n\n\n\nν (nu): 실제 병변을 정확히 찾는 확률 (Localization Accuracy)\n\n값이 클수록 판독자가 병변을 정확한 위치에 마킹할 가능성이 증가"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-과정",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-과정",
    "title": "Sample Size Estimation in MRMC",
    "section": "3. 변환 과정",
    "text": "3. 변환 과정\n변환 계수(Scaling Factor) 계산\n변환 계수는 ROC 효과 크기(\\(\\Delta\\)ROC-AUC)를 FROC 효과 크기(\\(\\Delta\\)wAFROC-AUC)로 변환하기 위한 비율이며, 다음 절차를 통해 도출된다.\n\n1. 파일럿 데이터에서 RSM 모델 적합(fitting) 및 AUC 계산\n✔ 파일럿 데이터에서 RSM 모델을 적합(fit)하여 ROC-AUC 및 wAFROC-AUC 값을 도출한다.\n✔ 이를 통해 현재의 \\(\\mu\\), \\(\\lambda\\), \\(\\nu\\) 값을 얻는다.\n2. \\(\\mu\\)를 단계적으로 증가시키면서 ROC-AUC와 wAFROC-AUC 변화를 측정\n✔ 모델의 탐지 능력이 향상된다고 가정하고 \\(\\mu\\) 값을 일정한 간격(예: \\(\\Delta \\mu = 0.01\\))으로 증가시킨다.\n✔ \\(\\mu\\)가 증가함에 따라 ROC-AUC와 wAFROC-AUC도 증가하게 된다.\n✔ 그러나 \\(\\lambda\\)와 \\(\\nu\\)는 \\(\\mu\\)와 상관성이 있기 때문에, \\(\\mu\\)만 단순 증가시키면 변환이 정확하지 않다.\n✔ 이를 해결하기 위해, 내재적 파라미터 변환을 적용하여 \\(\\lambda\\)와 \\(\\nu\\)도 함께 변화하도록 조정한다.\n3. 내재적 파라미터 변환 (Util2Intrinsic() 사용)\n✔ \\(\\mu\\) 값이 변화하면 \\(\\lambda\\)와 \\(\\nu\\)도 함께 조정되어야 하는데, 직접적으로 조정하기 어렵다.\n✔ 따라서 물리적(Physical) 파라미터 (\\(\\mu, \\lambda, \\nu\\))를 내재적(Intrinsic) 파라미터 (\\(\\lambda_i, \\nu_i\\))로 변환한다.\n\\[\n\\lambda_i = \\lambda \\times \\mu\n\\]\n\\[\n\\nu_i = -\\log(1 - \\nu) / \\mu\n\\]\n✔ 변환 후, \\(\\mu\\)를 증가시키면서도 \\(\\lambda\\)와 \\(\\nu\\)를 올바르게 유지할 수 있는 상태가 된다.\n4. 각 단계에서 ROC-AUC 변화량 (\\(\\Delta\\)ROC-AUC)과 wAFROC-AUC 변화량 (\\(\\Delta\\)wAFROC-AUC) 비교\n✔ \\(\\mu\\) 값을 증가시킨 후, 해당하는 새로운 ROC-AUC와 wAFROC-AUC를 계산한다.\n✔ 그러나 이 상태에서는 여전히 기존 \\(\\lambda\\)와 \\(\\nu\\)를 사용하고 있으므로, 이를 새로운 \\(\\mu\\) 값에 맞게 변환해야 한다.\n내재적 → 물리적 변환 (Util2Physical() 사용)\n✔ 증가된 \\(\\mu\\) 값(\\(\\mu_{\\text{new}}\\))에 대해 내재적 파라미터를 다시 물리적 파라미터로 변환하여 \\(\\lambda\\)와 \\(\\nu\\)를 조정한다.\n\\[\n\\lambda = \\frac{\\lambda_i}{\\mu_{\\text{new}}}\n\\]\n\\[\n\\nu = 1 - \\exp(-\\nu_i \\times \\mu_{\\text{new}})\n\\]\n✔ 즉, 새로운 \\(\\mu\\) 값에서 RSM 모델을 다시 적합하여, 해당하는 ROC-AUC 및 wAFROC-AUC를 구한다.\n✔ 이를 반복 수행하여 \\(\\Delta\\)ROC-AUC과 \\(\\Delta\\)wAFROC-AUC을 측정한다.\n5. 선형 회귀를 통해 변환 계수(Scaling Factor) 결정\n✔ 여러 번의 \\(\\mu\\) 증가 단계에 대해 \\(\\Delta\\)ROC-AUC(독립 변수)와 \\(\\Delta\\)wAFROC-AUC(종속 변수) 사이의 관계를 분석한다.\n✔ 이를 선형 회귀(linear regression)를 사용하여 분석하면 변환 계수(Scaling Factor)를 구할 수 있다.\n✔ 선형 회귀 분석에서 \\(\\Delta\\)ROC-AUC를 독립 변수, \\(\\Delta\\)wAFROC-AUC를 종속 변수로 설정하면 기울기(Slope)가 변환 계수가 된다.\n✔ 위 과정을 통해 ROC 연구에서의 효과 크기를 FROC 연구에서 사용할 수 있도록 변환할 수 있다."
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-후-표본-크기-추정",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#변환-후-표본-크기-추정",
    "title": "Sample Size Estimation in MRMC",
    "section": "4. 변환 후 표본 크기 추정",
    "text": "4. 변환 후 표본 크기 추정\n변환 계수를 적용하고 나면, ROC 연구와 동일한 방법으로 FROC 연구에서 필요한 표본 크기를 계산할 수 있다.\n\nROC 연구에서 정의한 효과 크기(ΔROC-AUC)를 변환하여 wAFROC 효과 크기(ΔwAFROC-AUC)로 변환\n변환된 wAFROC 효과 크기를 기반으로, SsSampleSizeKGivenJ() 함수를 이용하여 샘플 크기 계산"
  },
  {
    "objectID": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#reference",
    "href": "posts/2025-02-14-Sample Size Estimation in MRMC/index.html#reference",
    "title": "Sample Size Estimation in MRMC",
    "section": "Reference",
    "text": "Reference\nhttps://cran.r-project.org/web/packages/RJafroc/RJafroc.pdf\nhttps://dpc10ster.github.io/RJafrocBook/"
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html",
    "href": "posts/2025-01-03-competingrisk/index.html",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "경쟁 위험(Competing risk)은 한 종류의 이벤트가 발생하면 다른 이벤트가 발생할 수 없는 상황으로 정의됩니다. 예를 들어, 암 재발이 event of interest 인 경우 사망이 competing risk로 작용할 수 있습니다 (사망한 암 환자에게는 암 재발이라는 event가 발생 할 수 없기 때문). 경쟁 위험을 무시하면 모집단의 생존 함수나 이벤트 발생률을 과대 혹은 과소평가 할 위험이 생기기에 이를 고려한 분석이 필요한 경우들이 있습니다. 여러 상황들을 살펴보고, 어떤 지표들을 이용했을 때 어떤 해석이 가능한지 알아보도록 하겠습니다.\n\n\\[\nS(t) = \\prod_{i: t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n\\]\n\nS(t): 생존 함수\n\\(t_i\\): 사건 발생 시점\n\\(d_i\\): 시점 \\(t_i\\)에서 발생한 사건 수\n\\(n_i\\): 시점 \\(t_i\\)에서 위험에 노출된 대상 수\n결국 생존 함수란 각 사건 발생 시점에서의 생존 확률(어떤 이벤트도 발생하지 않을 확률)을 누적 곱하여 계산한 시간 \\(t\\)까지 생존할 확률을 뜻합니다.\n\n\\[\nh_k(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t, \\epsilon = k \\mid T \\geq t)}{\\Delta t}\n\\]\n\n\\(\\lambda_k(t)\\): 이벤트 유형 \\(k\\)의 원인별 위험 함수\nT: 이벤트 발생 시간\n\\(\\epsilon\\): 이벤트 유형 지표(Ex. \\(\\epsilon\\) = 1: event of interest, \\(\\epsilon\\) = 2: competing risk )\n결국 위험 함수란 각 시점에서 event k 가 발생할 확률을 뜻합니다.\n\n\\[\nCIF_k(t) = \\int_{0}^{t} h_k(u) S(u) \\, du\n\\]\n이벤트 k에 대한 누적발생률 함수란 결국 시점 마다 생존해 있을 확률 \\(S(t)\\) 와 그 시점 에서 k가 발생할 위험 \\(h_k(t)\\)를 곱하여 합산한 값으로, 시점 t에 이번트 k의 누적 발생률을 뜻하게 됩니다.\n\n경쟁 위험이 없는 생존 분석에서는 대게 Kaplan-Meier 추정치를 사용하지만, 이는 앞서 말했던 예시와 같이 이벤트 발생률을 과대 혹은 과소 평가할 위험이 있기 때문에 경쟁 위험이 있는 경우 적합하지 않은 경우가 많습니다. 암 재발과 사망의 예시를 생각해보겠습니다. Kaplan-Meier 추정치의 경우 사망한 환자의 경우 censoring 된 것으로 처리가 되어 “이 환자의 경우 이후 정보는 알 수 없다”고 간주를 하고 계산하는 추정치입니다. 하지만, 현실에서는 사망한 환자의 경우 이후 암 재발이 일어날 가능성이 0이라는 것을 알 수 있습니다. 반면 CIF의 경우 수식에서도 볼 수 있듯이 사망이 발생해버린 사람은 이미 사건이 발생한 상태로 처리가 되며, 경쟁 이벤트가 발생했다는 \\(CIF_2(t)\\)라는 누적발생률 함수에 따로 기록되고 있는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#생존-함수",
    "href": "posts/2025-01-03-competingrisk/index.html#생존-함수",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nS(t) = \\prod_{i: t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n\\]\n\nS(t): 생존 함수\n\\(t_i\\): 사건 발생 시점\n\\(d_i\\): 시점 \\(t_i\\)에서 발생한 사건 수\n\\(n_i\\): 시점 \\(t_i\\)에서 위험에 노출된 대상 수\n결국 생존 함수란 각 사건 발생 시점에서의 생존 확률(어떤 이벤트도 발생하지 않을 확률)을 누적 곱하여 계산한 시간 \\(t\\)까지 생존할 확률을 뜻합니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#위험-함수",
    "href": "posts/2025-01-03-competingrisk/index.html#위험-함수",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nh_k(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t, \\epsilon = k \\mid T \\geq t)}{\\Delta t}\n\\]\n\n\\(\\lambda_k(t)\\): 이벤트 유형 \\(k\\)의 원인별 위험 함수\nT: 이벤트 발생 시간\n\\(\\epsilon\\): 이벤트 유형 지표(Ex. \\(\\epsilon\\) = 1: event of interest, \\(\\epsilon\\) = 2: competing risk )\n결국 위험 함수란 각 시점에서 event k 가 발생할 확률을 뜻합니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#누적발생률-함수cif",
    "href": "posts/2025-01-03-competingrisk/index.html#누적발생률-함수cif",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "\\[\nCIF_k(t) = \\int_{0}^{t} h_k(u) S(u) \\, du\n\\]\n이벤트 k에 대한 누적발생률 함수란 결국 시점 마다 생존해 있을 확률 \\(S(t)\\) 와 그 시점 에서 k가 발생할 위험 \\(h_k(t)\\)를 곱하여 합산한 값으로, 시점 t에 이번트 k의 누적 발생률을 뜻하게 됩니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#왜-경쟁-위험-분석에서-cif를-사용할까",
    "href": "posts/2025-01-03-competingrisk/index.html#왜-경쟁-위험-분석에서-cif를-사용할까",
    "title": "Competing Risk Analysis",
    "section": "",
    "text": "경쟁 위험이 없는 생존 분석에서는 대게 Kaplan-Meier 추정치를 사용하지만, 이는 앞서 말했던 예시와 같이 이벤트 발생률을 과대 혹은 과소 평가할 위험이 있기 때문에 경쟁 위험이 있는 경우 적합하지 않은 경우가 많습니다. 암 재발과 사망의 예시를 생각해보겠습니다. Kaplan-Meier 추정치의 경우 사망한 환자의 경우 censoring 된 것으로 처리가 되어 “이 환자의 경우 이후 정보는 알 수 없다”고 간주를 하고 계산하는 추정치입니다. 하지만, 현실에서는 사망한 환자의 경우 이후 암 재발이 일어날 가능성이 0이라는 것을 알 수 있습니다. 반면 CIF의 경우 수식에서도 볼 수 있듯이 사망이 발생해버린 사람은 이미 사건이 발생한 상태로 처리가 되며, 경쟁 이벤트가 발생했다는 \\(CIF_2(t)\\)라는 누적발생률 함수에 따로 기록되고 있는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#fine-gray-method",
    "href": "posts/2025-01-03-competingrisk/index.html#fine-gray-method",
    "title": "Competing Risk Analysis",
    "section": "Fine-Gray Method",
    "text": "Fine-Gray Method\n경쟁위험이 존재하는 경우 생존 함수인 \\(S(t)\\)가 \\(1-CIF(t)\\)로 치환되지 않기 때문에 전통적 위험함수로는 \\(CIF(t)\\)를 추정할 수 없다는 한계가 있습니다. (\\(S(t)+CIF_1(t)+CIF_2(t)+....+CIF_k(t)=1\\)이기 때문) 따라서, Fine-Gray Method 에서는 \\(1-CIF(t)\\)의 시간에 따른 변화량을 추적하는 함수인 Subdistribution hazard function을 새로 추정합니다.\n하위 분포 위험의 경우 아래의 식과 같이 계산할 수 있습니다. \\[\nh_k^{sd}(t)\n\\;=\\;\n-\\,\\frac{d}{dt} \\,\\ln\\!\\bigl\\{1 - CIF_k(t)\\bigr\\}\n\\;=\\;\n\\frac{h_k^{cs}(t) S(t)}{\\,1 - CIF_k(t)\\,}.\n\\] 하위 분포 위험의 경우 \\(h_k^{sd}(t)\\)로 표기를 하고 이는 \\(CIF_k(t)\\)의 변화량을 추정할 수 있는 위험 함수입니다. 또한 여기서 \\(h_k^{cs}(t)\\)의 경우 cause-specific hazard function으로, 앞서 정의한 위험함수와 같은 의미를 가집니다.(subdistrivution hazard function과 구분하기 위해 이렇게 지칭하도록 하겠습니다). 그렇다면 \\(h_k^{sd}(t)\\)는 어떻게 해석을 할 수 있을지 알아보겠습니다. 우선 \\(S(t)\\)가 \\(1-CIF(t)\\)로 치환이 되는 경쟁위험이 없는 상황에서는 cause-specific hazard function과 완전히 동일하다는 것을 알 수 있습니다. 하지만 그렇지 않은 경우 cause-specific hazard function에 \\(\\frac {S(t)}{\\,1 - CIF_k(t)\\,}\\)을 곱한 값이 된다는 것을 알 수 있고, 이는 관심 event가 발생하지 않은 모든 사람이 분모 즉, risk set에 포함된다는 것을 알 수 있습니다. 이는 곧 아래의 수식으로 표현될 수 있습니다.\n\\[\nh_k^{sd}(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t+\\Delta t, \\text{Cause} = k \\mid T \\geq t \\cup \\text{(Cause } \\neq k \\text{ 발생)})}{\\Delta t}\n\\] 예를 들어, event of interest가 암의 재발이고 comepting risk가 사망인 경우 전통적 생존 분석에서는 시간 t에서 살아 있는 환자들이 risk set이 된다면 subdistribution hazard function 에서는 시간 t에서 살아있는 사람들과 시간 t전에 competing risk를 경험한(사망한)사람들 또한 risk set에 포함된다는 차이가 있습니다. Fine-Gray Method에서는 이 subdistribution hazard function을 이용하여 \\(\\beta\\)값을 추정하여 \\(CIF(t)\\)에 대한 보다 직접적인 추정을 가능하게 하는 방법입니다. Fine-Gray Method 분석법을 실행하는 방법과 전통적 생존 분석과 어떻게 비교하여 어떻게 수치들을 해석할 수 있는지를 알아보겠습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#fine-gray-method-와-전통적-생존함수-비교",
    "href": "posts/2025-01-03-competingrisk/index.html#fine-gray-method-와-전통적-생존함수-비교",
    "title": "Competing Risk Analysis",
    "section": "Fine-Gray Method 와 전통적 생존함수 비교",
    "text": "Fine-Gray Method 와 전통적 생존함수 비교\n다음과 같은 상황을 한번 생각해보겠습니다. 심부전 환자에서 중증 우울증 진단 여부에 따른 심장질환 관련 사망여부가 달라지는지 관심이 있는 상황을 가정해보겠습니다. 이 경우 CVD death가 Event of Interest가 되고 non-CVD death가 Competing risk가 될 것입니다. (non-CVD death로 사망하는 경우 심장 질환 관련 사망으로 사망하는지 관찰할 수 없기 때문). 우울증 진단 여부가 심장 질환 관련 사망에 관련이 없지만, 자살율을 크게 증가시켜 non-CVD death를 많이 발생시킨다고 한번 가정해보도록 하겠습니다. 가정에 따라 데이터를 한번 만들어보겠습니다.\n\nN &lt;- 10000\n# 우울증 여부를 배정\nDepression_status &lt;- c(rep(0, N/2), rep(1, N/2))\n#임의로 성별 배정\nSex &lt;- sample(c(0,1), size = N, replace = TRUE)\n\n# hazard를 가정에 맞게 정의\nlambda_cvd &lt;- 0.2  # CVD death 여부는 우울증과 무관하게 매년 0.2\nlambda_non_cvd_no_depression &lt;- 0.1  # Non-CVD death의 경우 비우울증 인구에서는 0.1\nlambda_non_cvd_depression &lt;- 0.5      # Non-CVD death rate가 자살율의 증가로 인해 우울증 인구에서 0.5\n\nCVD_time &lt;- numeric(N)\nnon_CVD_time &lt;- numeric(N)\nTime &lt;- numeric(N)\nEvent &lt;- numeric(N)\n\n# Simulate times\nfor (i in 1:N) {\n\n  CVD_time[i] &lt;- rexp(1, rate = lambda_cvd)\n  \n  if (Depression_status[i] == 0) {\n    non_CVD_time[i] &lt;- rexp(1, rate = lambda_non_cvd_no_depression)\n  } else {\n    non_CVD_time[i] &lt;- rexp(1, rate = lambda_non_cvd_depression)\n  }\n  if (CVD_time[i] &lt; non_CVD_time[i]) {\n    Time[i] &lt;- CVD_time[i]\n    Event[i] &lt;- 1  \n  } else {\n    Time[i] &lt;- non_CVD_time[i]\n    Event[i] &lt;- 2  \n  }\n}\n\nsample_data &lt;- data.frame(\n  ID = 1:N,\n  Time = round(Time, 2),\n  Event = Event,\n  Sex = ifelse(Sex == 1, \"Male\", \"Female\"),\n  Depression = ifelse(Depression_status == 1, \"Yes\", \"No\"),\n  CVD_time = round(CVD_time, 2)  \n)\n\nhead(sample_data, n= 15)\n\n   ID Time Event    Sex Depression CVD_time\n1   1 3.73     2 Female         No     9.03\n2   2 5.74     2 Female         No     9.50\n3   3 3.47     1 Female         No     3.47\n4   4 5.96     1   Male         No     5.96\n5   5 0.79     2   Male         No     2.80\n6   6 6.27     1 Female         No     6.27\n7   7 0.06     1 Female         No     0.06\n8   8 2.74     2 Female         No     3.49\n9   9 4.01     1   Male         No     4.01\n10 10 1.43     2 Female         No     5.07\n11 11 0.08     1   Male         No     0.08\n12 12 2.65     1 Female         No     2.65\n13 13 5.22     1 Female         No     5.22\n14 14 1.42     1 Female         No     1.42\n15 15 2.00     1 Female         No     2.00\n\n\n데이터에서 Time은 Death(CVD, non-CVD 포함)가 발생한 시점입니다. R의 cmprsk 패키지를 통해 fine gray method로 분석을 진행하고, 전통적 생존 분석또한 함께 진행해보겠습니다.\n\nlibrary(dplyr);library(survival);library(jstable)\nsample_data &lt;- sample_data %&gt;%\n  mutate(\n    Sex_num = ifelse(Sex == \"Male\", 1, 0),\n    Depression_num = ifelse(Depression == \"Yes\", 1, 0)\n  )\nsurv_object_csh &lt;- Surv(time = sample_data$Time, event = sample_data$Event == 1)\ncox_csh &lt;- coxph(surv_object_csh ~ Depression_num, data = sample_data)\ncox2.display(cox_csh)\n\n$table\n               HR(95%CI)         P value\nDepression_num \"1.03 (0.97,1.1)\" \"0.351\"\n\n$metric\n                        [,1] [,2]\n&lt;NA&gt;                      NA   NA\nNo. of observations 10000.00   NA\nNo. of events        4750.00   NA\nAIC                 76672.56   NA\n\n$caption\n[1] \"Cox model on time ('NA, surv_object_csh, character(0)') to event ('surv_object_csh')\"\n\n\nexp(coef= beta)즉 Hazard ratio가 1이고 p-value가 0.936으로, 우울증 여부가 CVD-death에 영향을 미치지 않는 다는 것을 알 수 있습니다. 이는 데이터를 만들때의 가정과 동일하기 때문에 전통적 생존분석은 이를 잘 반영한다는 것을 알 수 있습니다. 이번엔 competing risk를 고려한 fine gray method로 분석을 해보겠습니다.\n\nlibrary(cmprsk)\nftime &lt;- sample_data$Time\nfstatus &lt;- sample_data$Event\ncovariates &lt;- sample_data %&gt;% select(Depression_num)\nfg_model &lt;- crr(ftime = ftime, fstatus = fstatus, cov1 = covariates)\nsummary(fg_model)\n\nCompeting Risks Regression\n\nCall:\ncrr(ftime = ftime, fstatus = fstatus, cov1 = covariates)\n\n                coef exp(coef) se(coef)     z p-value\nDepression_num -1.05     0.352   0.0333 -31.4       0\n\n               exp(coef) exp(-coef)  2.5% 97.5%\nDepression_num     0.352       2.84 0.329 0.375\n\nNum. cases = 10000\nPseudo Log-likelihood = -41790 \nPseudo likelihood ratio test = 1190  on 1 df,\n\n\nexp(coef) 즉 subdistribution hazard ratio가 0.363이고, p value 가 &lt;0.01임으로 우울증이 있는 사람에게는 CVD-death가 덜 발생한다고 해석할 수 있습니다. 우울증 여부가 CVD-death에 영향을 미치지 않게 데이터를 만들었는데 왜 이런 결과가 나왔는지 그리고 서로 다르게 나온 두 결과를 각각 어떻게 해석해야 되는지를 알아보겠습니다. 우울증 환자와 비 우울증 환자에서 CVD-death에 대한 CIF를 한번 그려보도록 하겠습니다.\n\ngroup &lt;- sample_data$Depression_num\ncif_cvd &lt;- cuminc(ftime, fstatus, group = group, cencode = 0)\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"purple\", lty = 1, lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\"),\n       col = c(\"blue\", \"purple\"),\n       lty = 1,\n       lwd = 2)\n\n\n\n\n\n\n\n우울증 여부가 CVD-death에 영향을 미치지 않는다 하더라고, 우울증 환자에게서 CVD-death가 덜 발생한다는 것을 관찰할 수 있습니다. 이는 우울증 환자에서 이미 non-CVD death를 경험하여 CVD-death를 경험할 수 없기 때문이라는 것을 CVD, non-CVD death에 대한 CIF로 알아볼 수 있습니다.\n\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"red\", lty = 1, lwd = 2)\nlines(cif_cvd[[3]]$time, cif_cvd[[3]]$est, col = \"skyblue\", lty = 1, lwd = 2)\nlines(cif_cvd[[4]]$time, cif_cvd[[4]]$est, col = \"pink\", lty = 1, lwd = 2)\n\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\", \n                  \"Non-Depression - Non-CVD Death\", \"Depression - Non-CVD Death\"),\n       col = c(\"blue\", \"red\", \"skyblue\", \"pink\"),\n       lty = 1, lwd = 2, bty = \"n\", cex = 0.8)\n\n\n\n\n\n\n\n실제로 Depression 군에서는 non-CVD death가 급격하게 많이 발생하여, CVD-death의 CIF가 낮게 나오고 있다는 것을 확인 할 수 있습니다. non-CVD death가 발생하지 않은 경우를 가정하여 만들어 놓은 CVD_time변수로 Depression군과 non-Depression 군의 CIF를 비교해보도록 하겠습니다.\n\nlibrary(cmprsk)\nftime &lt;- sample_data$CVD_time\nfstatus &lt;- 1\ngroup &lt;- sample_data$Depression_num\ncif_cvd &lt;- cuminc(ftime, fstatus, group = group, cencode = 0)\nplot(1, type = \"n\", xlab = \"Time (years)\", ylab = \"Cumulative Incidence\",\n     xlim = c(0, 15), ylim = c(0, 1),\n     main = \"Cumulative Incidence of CVD Death by Depression Status\")\nlines(cif_cvd[[1]]$time, cif_cvd[[1]]$est, col = \"blue\", lty = 1, lwd = 2)\nlines(cif_cvd[[2]]$time, cif_cvd[[2]]$est, col = \"purple\", lty = 1, lwd = 2)\n\nlegend(\"topright\", \n       legend = c(\"Non-Depression - CVD Death\", \"Depression - CVD Death\"),\n       col = c(\"blue\", \"purple\"),\n       lty = 1, lwd = 2, bty = \"n\", cex = 0.8)\n\n\n\n\n\n\n\n두개의 그래프가 거의 일치하는 것을 확인할 수 있습니다. 즉 Competing risk의 존재는, 실제로 우울증이 CVD death에 영향을 미치지 않더라고, CIF에는 변화를 줄 수 있다는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2025-01-03-competingrisk/index.html#how-to-interpret",
    "href": "posts/2025-01-03-competingrisk/index.html#how-to-interpret",
    "title": "Competing Risk Analysis",
    "section": "How to Interpret",
    "text": "How to Interpret\n지금까지 얻은 정보를 종합하여, cause-specific hazard와 subdistribution hazard 함수가 각각 어떤 질문에 적합한지, 어떤 내용을 설명하는지 정리해보겠습니다. Cause-specific hazard function은 “A라는 변수가 실제로 B에 영향을 미치는가?”와 같은 원인적(etiologic) 질문에 더 적합한 함수입니다. 예를 들어, “우울증이 CVD 사망을 유발하는가?”라는 질문에 답하기 위해서는 cause-specific hazard function을 사용하는 것이 더 적절할 수 있습니다. 반면, subdistribution hazard ratio는 “우울증이 CVD 사망의 발생률에 어떤 영향을 미치는가?”와 같은 예후(prognostic) 질문에 더 적합한 함수입니다. 이는 subdistribution hazard ratio가 Cumulative Incidence Function (CIF)을 직접적으로 추정하기 때문에, CIF 함수와 긴밀하게 연관되어 있습니다. 따라서 예후를 예측하는 측면에서는 Fine-Gray 방법을 이용한 분석이, 원인을 분석하는 측면에서는 전통적인 생존 분석이 더 적절합니다. 두 분석 결과를 함께 고려함으로써, 경쟁 위험이 존재하는 상황에서 관심 있는 사건에 영향을 미치는 요인들에 대한 포괄적인 분석을 수행할 수 있습니다.\nPS. Censoring이 존재하는 경우\nSubdistribution hazard function을 살펴볼 때, censoring이 존재하지 않고 환자를 이벤트가 발생할 때까지 관찰할 수 있다는 가정(complete data로 통칭)하에 risk set이 정의가 되었습니다. Censoring이 존재하는 경우 risk set이 어떻게 존재하는지 알아보겠습니다. Complete data의 경우 competing risk가 발생한 환자도, 발생한 시점 외에도 계속 한명의 존재로 risk set에 남아있다는 것을, 앞선 subdistribution hazard function의 유도과정에서 살펴보았습니다. Censoring이 생기는 경우 IPCW(Inverse probability censor weighting)이라는 방법을 통해, censoring이 된 환자의 예후를 반영하는 방법을 fine-gray method에서 사용하고 있습니다. 시점 t에서 100명이 생존한 와중에 60명이 censoring되고 40명에게 event가 발생하거나, 관측되거나 했다고 가정해봅시다. Censoring이라는 과정이 환자의 event 발생여부나 baseline에 관계없이 발생한다는 가정이 있다면 남은 40명에게 각각 2.5배의 가중치를 준다면, censoring이 발생하지 않고 100명을 계속 관측하는 것과 동일한 결과를 얻을 수 있다는 것이 IPCW의 방법론입니다. 따라서 시점 t이전에 competing event가 발생한 사람의 경우 여전히 한명으로 risk set에 기여를 하게 되고, 시점 t에서 censoring된 사람들은 2.5배의 가중치를 받아 risk set과 event set에 반영이 됩니다. 즉 competing risk가 발생한 환자들의 경우 여전히 한명으로 risk set에 기여하지만, 실질적으로 기여하는 가중치는 적어진다는 것을 알 수 있습니다. 이를 반영한 것이 survival package의 finegray함수이며, 이를 이용하여 finegray method를 통한 분석 또한 가능합니다.\n\nlibrary(survival)\ndata &lt;- mgus2\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))\npdata &lt;- finegray(Surv(etime, event) ~ ., data=data)\nhead(pdata)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death fgstart fgstop\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1       0     35\n2  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      35     44\n3  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      44     47\n4  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      48     52\n5  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      53     56\n6  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      56     57\n  fgstatus      fgwt\n1        0 1.0000000\n2        0 0.9990449\n3        0 0.9980368\n4        0 0.9959629\n5        0 0.9905896\n6        0 0.9873022\n\n\n데이터에서 볼 수 있듯이 competing risk가 발생한 환자의 경우 추적 종료까지 risk set에 기여도는 낮아지지만, 끝까지 포함되어 있다는 것을 알 수 있습니다. 또한 얻은 pdata로 기존의 생존분석과 같은 함수를 사용하면, 가중치가 반영되어 fine gray method로 구한 subdistribution hazard function에 대한 coefficient값을 얻을 수 있다는 것을 알 수 있습니다.\n\nfgfit &lt;- coxph(Surv(fgstart, fgstop, fgstatus) ~ age+sex,\n               weight=fgwt, data=pdata, model = T)\nsummary(fgfit)\n\nCall:\ncoxph(formula = Surv(fgstart, fgstop, fgstatus) ~ age + sex, \n    data = pdata, weights = fgwt, model = T)\n\n  n= 41775, number of events= 115 \n\n          coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)   \nage  -0.017302  0.982847  0.007022  0.005528 -3.13  0.00175 **\nsexM -0.259757  0.771239  0.187049  0.181707 -1.43  0.15285   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     0.9828      1.017    0.9723    0.9936\nsexM    0.7712      1.297    0.5402    1.1012\n\nConcordance= 0.548  (se = 0.026 )\nLikelihood ratio test= 7.28  on 2 df,   p=0.03\nWald test            = 11.19  on 2 df,   p=0.004\nScore (logrank) test = 7.58  on 2 df,   p=0.02,   Robust = 9.28  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html",
    "href": "posts/2024-12-13-rollmerge/index.html",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "data.table은 대용량의 데이터를 처리하는 데 있어 빠른 속도와 메모리 효율을 보여주는 패키지이다. 또한 dplyr와 비교하여, dplyr에서 지원하지 않는 기능도 가지고 있는데, 그 중 하나인 rolling join을 소개한다.\n\n일반적으로 join이라 함은 원하는 재료집합이 2개 이상일때, 이를 인결하여 새로운 집합을 만드는 연산을 의미한다. 당연히 join 연산을 할 때마다 집합이 확장되며 컬럼의 수가 증가하게 된다. join 연산의 경우 일반적으로는 equality condition을 사용한다. 즉 사용되는 집합이 일치하는 경우에만 연산이 이루어진다. 보통 이러한 조건 때문에 inner join, outter join 등을 사용하게 된다.\n\nrolling join은 inequality condition을 사용한다. 병합의 기준이 되는 컬럼 내에서 값을 탐색할 때, 다음과 같은 단계를 따를 수 있다.\n① 일단 일치하는 값이 있는지 확인하고, 없으면 선택한 방향을 따라 탐색한다. ② 탐색 범위에 기준 값에 가장 가까운 값이 있으면 그 값이 존재하는 행과 merge를 실행한다. ③ 탐색 범위 내에 값이 존재하지 않으면 병합을 실행하지 않는다.\n일반적으로 두 data.table object를 병합할 때에는 다음과 같이 실행할 수 있다. 여기서 on = 을 활용해서 기준이 되는 컬럼을 지정할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2)]\n\n그러나 rolling join을 실행하고 싶은 경우, 다음과 같이 작성할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf] # roll option = c(Inf, -Inf, number, \"nearest\")"
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#join",
    "href": "posts/2024-12-13-rollmerge/index.html#join",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "일반적으로 join이라 함은 원하는 재료집합이 2개 이상일때, 이를 인결하여 새로운 집합을 만드는 연산을 의미한다. 당연히 join 연산을 할 때마다 집합이 확장되며 컬럼의 수가 증가하게 된다. join 연산의 경우 일반적으로는 equality condition을 사용한다. 즉 사용되는 집합이 일치하는 경우에만 연산이 이루어진다. 보통 이러한 조건 때문에 inner join, outter join 등을 사용하게 된다."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#rolling-join",
    "href": "posts/2024-12-13-rollmerge/index.html#rolling-join",
    "title": "data.table의 rolling join",
    "section": "",
    "text": "rolling join은 inequality condition을 사용한다. 병합의 기준이 되는 컬럼 내에서 값을 탐색할 때, 다음과 같은 단계를 따를 수 있다.\n① 일단 일치하는 값이 있는지 확인하고, 없으면 선택한 방향을 따라 탐색한다. ② 탐색 범위에 기준 값에 가장 가까운 값이 있으면 그 값이 존재하는 행과 merge를 실행한다. ③ 탐색 범위 내에 값이 존재하지 않으면 병합을 실행하지 않는다.\n일반적으로 두 data.table object를 병합할 때에는 다음과 같이 실행할 수 있다. 여기서 on = 을 활용해서 기준이 되는 컬럼을 지정할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2)]\n\n그러나 rolling join을 실행하고 싶은 경우, 다음과 같이 작성할 수 있다.\n\nresult_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf] # roll option = c(Inf, -Inf, number, \"nearest\")"
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#roll-옵션",
    "href": "posts/2024-12-13-rollmerge/index.html#roll-옵션",
    "title": "data.table의 rolling join",
    "section": "1. roll = 옵션",
    "text": "1. roll = 옵션\n아래는 실습에 사용할 데이터를 전처리하는 코드이다. 실습 데이터는 다음 링크에서 다운받을 수 있으며, 4개 파일을 사용하여 실습을 진행할 것이다.\n\nlibrary(data.table);library(magrittr)\n\nbnc &lt;- fread(\"data/nsc2_bnc_1000.csv\") \nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\")[, Deathdate := (lubridate::ym(DTH_YYYYMM) %&gt;% lubridate::ceiling_date(unit = \"month\") - 1)][]\nm20 &lt;- fread(\"data/nsc2_m20_1000.csv\") \nm40 &lt;- fread(\"data/nsc2_m40_1000.csv\")[SICK_CLSF_TYPE %in% c(1, 2, NA)] \n\ncode.HTN &lt;- paste(paste0(\"I\", 10:15), collapse = \"|\")\ndata.start &lt;- m20[like(SICK_SYM1, code.HTN) & (MDCARE_STRT_DT &gt;= 20060101), .(Indexdate = min(MDCARE_STRT_DT)), keyby = \"RN_INDI\"]\n\n## Previous disease: Among all sick code\nexcl &lt;- m40[(MCEX_SICK_SYM %like% code.HTN) & (MDCARE_STRT_DT &lt; 20060101), .SD[1], .SDcols = c(\"MDCARE_STRT_DT\"), keyby = \"RN_INDI\"]\n\n## Merge: left anti join\ndata.incl &lt;- data.start[!excl, on = \"RN_INDI\"][, Indexdate := as.Date(as.character(Indexdate), format = \"%Y%m%d\")][]\ndata.asd &lt;- merge(bnd, bnc[, .(SEX = SEX[1]), keyby = \"RN_INDI\"], by = \"RN_INDI\") %&gt;% \n  merge(data.incl, by = \"RN_INDI\") %&gt;% \n  .[, `:=`(Age = year(Indexdate) - as.integer(substr(BTH_YYYY, 1, 4)),\n           Death = as.integer(!is.na(DTH_YYYYMM)),\n           Day_FU = as.integer(pmin(as.Date(\"2015-12-31\"), Deathdate, na.rm =T) - Indexdate))] %&gt;% .[, -c(\"BTH_YYYY\", \"DTH_YYYYMM\", \"Deathdate\")] \n\n\ncode.cci &lt;- list(\n  MI = c(\"I21\", \"I22\", \"I252\"),\n  CHF = c(paste0(\"I\", c(\"099\", 110, 130, 132, 255, 420, 425:429, 43, 50)), \"P290\"),\n  Peripheral_VD = c(paste0(\"I\", 70, 71, 731, 738, 739, 771, 790, 792), paste0(\"K\", c(551, 558, 559)), \"Z958\", \"Z959\"),\n  Cerebro_VD = c(\"G45\", \"G46\", \"H340\", paste0(\"I\", 60:69)),\n  Dementia = c(paste0(\"F0\", c(0:3, 51)), \"G30\", \"G311\"),\n  Chronic_pulmonary_dz = c(\"I278\", \"I279\", paste0(\"J\", c(40:47, 60:67, 684, 701, 703))),\n  Rheumatologic_dz = paste0(\"M\", c(\"05\", \"06\", 315, 32:34, 351, 353, 360)),\n  Peptic_ulcer_dz = paste0(\"K\", 25:28),\n  Mild_liver_dz = c(\"B18\", paste0(\"K\", c(700:703, 709, 713:715, 717, 73, 74, 760, 762:764, 768, 769)), \"Z944\"),\n  DM_no_complication = paste0(\"E\", c(100, 101, 106, 108:111, 116, 118:121, 126, 128:131, 136, 138:141, 146, 148, 149)),\n  DM_complication = paste0(\"E\", c(102:105, 107, 112:115, 117, 122:125, 127, 132:135, 137, 142:145, 147)),\n  Hemi_paraplegia = paste0(\"G\", c(\"041\", 114, 801, 802, 81, 82, 830:834, 839)),\n  Renal_dz = c(\"I120\", \"I131\", paste0(\"N\", c(\"032\", \"033\", \"034\", \"035\", \"036\", \"037\", \"052\", \"053\", \"054\", \"055\", \"056\", \"057\",\n                                             18, 19, 250)), paste0(\"Z\", c(490:492, 940, 992))),\n  Malig_with_Leuk_lymphoma = paste0(\"C\", c(paste0(\"0\", 0:9), 10:26, 30:34, 37:41, 43, 45:58, 60:76, 81:85, 88, 90, 97)),\n  Moderate_severe_liver_dz = c(paste0(\"I\", c(85, 859, 864, 982)), paste0(\"K\", c(704, 711, 721, 729, 765:767))),\n  Metastatic_solid_tumor = paste0(\"C\", 77:80),\n  AIDS_HIV = paste0(\"B\", c(20:22, 24))\n)\ncciscore &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 6, 6, 2)\nnames(cciscore) &lt;- names(code.cci)\n\n지금부터는 실제 의료 데이터를 활용하여 roll merge가 사용될 수 있는 상황에 대해 알아보고, 이를 코드로 적용해 보겠다. 데이터는 성균관대학교 바이오헬스규제학과 강의에 사용된 건강보험공단 데이터를 사용하였다. 일단 roll merge를 시행할 데이터에 대해 알아보자.\nroll = Inf\n\n\n[1] \"병력 진단 기준 날짜(Indexdate)가 있는 data.asd 데이터\"\n\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI   COD1   COD2   SEX  Indexdate   Age Death Day_FU\n     &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1:   13546                   1 2006-08-08    45     0   3432\n2:   23682                   1 2008-09-22    53     0   2656\n3:   36714                   2 2010-01-19    64     0   2172\n4:   39217                   1 2013-04-02    56     0   1003\n5:   46621                   1 2011-03-24    51     0   1743\n6:   51049                   1 2006-11-23    21     0   3325\n\n\n[1] \"병력 진단 날짜(MDCARE_STRT_DT)가 있는 m40 데이터\"\n\n\n   RN_INDI        RN_KEY MDCARE_STRT_DT FORM_CD MCEX_SICK_SYM\n     &lt;int&gt;         &lt;i64&gt;          &lt;int&gt;   &lt;int&gt;        &lt;char&gt;\n1:  596535 2002120187152       20021202       3          J209\n2:  615374 2002121012274       20021202       3          J209\n3: 1005547 2002120808216       20021202       3          J209\n4:  226594 2002120381612       20021202       3          J209\n5:  204930 2002120790182       20021202       3          J209\n6:  798943 2002040446183       20020401       3          J209\n   DETAIL_TMSG_SUBJ_CD SICK_CLSF_TYPE STD_YYYY\n                &lt;char&gt;          &lt;int&gt;    &lt;int&gt;\n1:                                 NA     2002\n2:                                 NA     2002\n3:                                 NA     2002\n4:                                 NA     2002\n5:                                 NA     2002\n6:                                 NA     2002\n\n\n첫 번째 데이터는 환자별로 병력 진단 기준일이 되는 날짜가 적혀 있다. 그리고 두 번째 데이터는 환자가 병력 진단을 받았을 경우 해당 날짜가 적혀 있다. 우리는 이 데이터를 가지고, 나름의 기준을 세워서 두 데이터를 병합하는 것이 목적이다. 만약 ’첫 번째 데이터의 Indexdate를 기준으로 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면 병력이 존재하는 것으로 간주’하려면 어떻게 해야 할까?\n병합을 하는 기준이 비교하는 두 날짜가 완벽히 일치하는 것이기 아니기 때문에 일반적인 join method를 사용할 수 없다. 이러한 경우에 roll merge를 사용할 수 있다.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  data.asd[, MDCARE_STRT_DT := Indexdate]\n  dt &lt;- m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][, MDCARE_STRT_DT := as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\")][, .(RN_INDI, MDCARE_STRT_DT, Incidate = MDCARE_STRT_DT)]  \n  dt[, .SD[1], keyby = c(\"RN_INDI\", \"MDCARE_STRT_DT\")][data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = Inf]})\n\nprint(head(info.cci[[10]], n = 5))\n\n   RN_INDI MDCARE_STRT_DT   Incidate   COD1   COD2   SEX  Indexdate   Age Death\n     &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;\n1:   13546     2006-08-08       &lt;NA&gt;                   1 2006-08-08    45     0\n2:   23682     2008-09-22       &lt;NA&gt;                   1 2008-09-22    53     0\n3:   36714     2010-01-19 2009-12-26                   2 2010-01-19    64     0\n4:   39217     2013-04-02 2013-04-02                   1 2013-04-02    56     0\n5:   46621     2011-03-24 2011-02-16                   1 2011-03-24    51     0\n   Day_FU\n    &lt;int&gt;\n1:   3432\n2:   2656\n3:   2172\n4:   1003\n5:   1743\n\n\n주의할 점은 data.table의 on=으로 병합을 시도할 경우에는 대상 컬럼이 이름이 같아야 한다는 것이다. 위의 코드에서는 Indexdate, incidate와 값이 같은 MDCARE_STRT_DT 컬럼을 대상으로 roll merge를 시도하였다.\nroll = Inf로 옵션을 주었기 때문에 dt의 날짜를 기준으로 data.asd를 연결할 때 정확히 일치하는 날짜가 없다면 data.asd의 날짜를 뒤로 밀어서 일치하는 날짜를 찾는다. 결과적으로 data.asd의 Indexdate 기준으로 앞 날짜에 m40의 Incidate가 존재한다면 병합이 되는 로직이라고 할 수 있다.\nroll merge를 사용하지 않는다면 cartesian = T 옵션을 사용하여 모든 가능한 조합을 허용하여 merge하고 그후에 조건에 맞게 필터링하는 과정을 거쳐야 한다. 다음은 rolling join을 사용하지 않은 코드의 예시이다.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  result &lt;- merge(data.asd[, .(RN_INDI, Indexdate)],\n                  m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][order(MDCARE_STRT_DT), .SD[1], keyby = \"RN_INDI\"][, .(RN_INDI, Incidate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))],\n                  by = \"RN_INDI\", all.x = T)\n  result[Indexdate &lt; Incidate, Incidate := NA]\n  return(result)\n})\nprint(head(info.cci[[10]], n = 5))\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI  Indexdate   Incidate\n     &lt;int&gt;     &lt;Date&gt;     &lt;Date&gt;\n1:   13546 2006-08-08       &lt;NA&gt;\n2:   23682 2008-09-22       &lt;NA&gt;\n3:   36714 2010-01-19 2005-12-09\n4:   39217 2013-04-02 2007-11-08\n5:   46621 2011-03-24 2005-09-20\n\n\n일단 merge를 수행한 이후, Incidate가 가장 빠른 첫 번째 날짜를 채택하여, 그 날짜가 Indexdate보다 뒤에 있을 경우 NA로 바꾸는 방식으로 필터링하였다. 이렇게 작업하여도 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면이라는 조건을 만족하는 행을 필터링하는 데에는 문제가 없지만 로직의 차이로 인해 Incidate에 적힌 날짜가 다른 것을 알 수 있다. 또한 필터링 과정 때문에 코드가 길어져 가독성이 좋지 않다.\nroll = -Inf\n\n\n[1] \"병력 진단 기준 날짜(Indexdate)가 있는 data.asd 데이터\"\n\n\nKey: &lt;RN_INDI&gt;\n   RN_INDI   COD1   COD2   SEX  Indexdate   Age Death Day_FU MDCARE_STRT_DT\n     &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;         &lt;Date&gt;\n1:   13546                   1 2006-08-08    45     0   3432     2006-08-08\n2:   23682                   1 2008-09-22    53     0   2656     2008-09-22\n3:   36714                   2 2010-01-19    64     0   2172     2010-01-19\n4:   39217                   1 2013-04-02    56     0   1003     2013-04-02\n5:   46621                   1 2011-03-24    51     0   1743     2011-03-24\n6:   51049                   1 2006-11-23    21     0   3325     2006-11-23\n\n\n[1] \"병력 진단 날짜(MDCARE_STRT_DT)가 있는 m40 데이터\"\n\n\n   RN_INDI        RN_KEY MDCARE_STRT_DT FORM_CD MCEX_SICK_SYM\n     &lt;int&gt;         &lt;i64&gt;          &lt;int&gt;   &lt;int&gt;        &lt;char&gt;\n1:  596535 2002120187152       20021202       3          J209\n2:  615374 2002121012274       20021202       3          J209\n3: 1005547 2002120808216       20021202       3          J209\n4:  226594 2002120381612       20021202       3          J209\n5:  204930 2002120790182       20021202       3          J209\n6:  798943 2002040446183       20020401       3          J209\n   DETAIL_TMSG_SUBJ_CD SICK_CLSF_TYPE STD_YYYY\n                &lt;char&gt;          &lt;int&gt;    &lt;int&gt;\n1:                                 NA     2002\n2:                                 NA     2002\n3:                                 NA     2002\n4:                                 NA     2002\n5:                                 NA     2002\n6:                                 NA     2002\n\n\n두 번째 예시는 동일한 데이터를 활용할 것이지만, 이번에는 ’첫 번째 데이터의 Indexdate를 기준으로 이후의 모든 날짜에서 발병 기록이 한 번이라도 있으면 발병한 것으로 간주’하려면 어떻게 해야 할까?\n\ndata.asd[, MDCARE_STRT_DT := Indexdate]\ninfo.MI &lt;- m40 %&gt;% \n  .[like(MCEX_SICK_SYM, paste(code.cci[[\"MI\"]], collapse = \"|\")), .(RN_INDI, MDCARE_STRT_DT = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"), MIdate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))] %&gt;%\n  .[data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = -Inf] \n\nprint(info.MI[40:50])\n\n    RN_INDI MDCARE_STRT_DT     MIdate   COD1   COD2   SEX  Indexdate   Age\n      &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt;\n 1:  484978     2006-02-04 2006-02-04                   2 2006-02-04    63\n 2:  505228     2013-04-02       &lt;NA&gt;                   2 2013-04-02    45\n 3:  517447     2011-10-05       &lt;NA&gt;                   2 2011-10-05    61\n 4:  518792     2011-01-09       &lt;NA&gt;                   1 2011-01-09    50\n 5:  529690     2013-01-09       &lt;NA&gt;                   2 2013-01-09    58\n 6:  530990     2015-11-25       &lt;NA&gt;                   1 2015-11-25    63\n 7:  540586     2008-06-03       &lt;NA&gt;                   1 2008-06-03    59\n 8:  546772     2006-07-24       &lt;NA&gt;                   2 2006-07-24    19\n 9:  551252     2008-05-20       &lt;NA&gt;                   2 2008-05-20    73\n10:  559420     2014-12-22       &lt;NA&gt;                   2 2014-12-22    52\n11:  562142     2013-03-27 2013-03-27                   1 2013-03-27    53\n    Death Day_FU\n    &lt;int&gt;  &lt;int&gt;\n 1:     0   3617\n 2:     0   1003\n 3:     0   1548\n 4:     0   1817\n 5:     0   1086\n 6:     0     36\n 7:     0   2767\n 8:     0   3447\n 9:     0   2781\n10:     0    374\n11:     0   1009\n\n\nroll = -Inf로 옵션을 주었기 때문에 m40의 날짜를 기준으로 data.asd를 연결할 때 정확히 일치하는 날짜가 없다면 data.asd의 날짜를 앞으로 당겨서 일치하는 날짜를 찾는다. 결과적으로 data.asd의 Indexdate 기준으로 뒤 날짜에 m40의 Incidate가 존재한다면 병합이 되는 로직이라고 할 수 있다. (여기서는 시작 날짜도 포함하였다.)\nroll merge를 사용하지 않는다면 cartesian = T 옵션을 사용하여 모든 가능한 조합을 허용하여 merge하고 그후에 조건에 맞게 필터링하는 과정을 거쳐야 한다. 다음은 rolling join을 사용하지 않은 코드의 예시이다.\n\ninfo.MI &lt;- merge(data.asd[, .(RN_INDI, Indexdate)],\n                 m40[like(MCEX_SICK_SYM, paste(code.cci[[\"MI\"]], collapse = \"|\"))][order(MDCARE_STRT_DT), .SD[1], keyby = \"RN_INDI\"][, .(RN_INDI, MIdate = as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\"))],\n                 by = \"RN_INDI\", all.x = T) %&gt;%\n  .[Indexdate &lt; MIdate, MIdate := NA]\n\nprint(info.MI[40:50])\n\nKey: &lt;RN_INDI&gt;\n    RN_INDI  Indexdate     MIdate\n      &lt;int&gt;     &lt;Date&gt;     &lt;Date&gt;\n 1:  484978 2006-02-04 2006-02-04\n 2:  505228 2013-04-02       &lt;NA&gt;\n 3:  517447 2011-10-05       &lt;NA&gt;\n 4:  518792 2011-01-09       &lt;NA&gt;\n 5:  529690 2013-01-09       &lt;NA&gt;\n 6:  530990 2015-11-25       &lt;NA&gt;\n 7:  540586 2008-06-03       &lt;NA&gt;\n 8:  546772 2006-07-24       &lt;NA&gt;\n 9:  551252 2008-05-20       &lt;NA&gt;\n10:  559420 2014-12-22       &lt;NA&gt;\n11:  562142 2013-03-27 2013-03-27\n\n\n일단 merge를 수행한 이후, MIdate의 첫 번째 날짜를 채택하여, 그 날짜가 Indexdate보다 뒤에 있을 경우 NA로 바꾸는 방식으로 필터링하였다. 이렇게 작업하여도 그 이전의 모든 날짜에서 진단일이 한 번이라도 있으면이라는 조건을 만족하는 행을 필터링하는 데에는 문제가 없지만 로직의 차이로 인해 MIdate에 적힌 날짜가 다른 것을 알 수 있다. 또한 필터링 과정 때문에 코드가 길어져 가독성이 좋지 않다.\n정리하면 result_dt &lt;- dt1[dt2, on = .(key_column1, key_column2), roll = Inf]에서 roll의 방향은 Inf 옵션일 때에는 dt1을 기준으로 dt2의 컬럼 값을 더 큰값으로 바꾸며 탐색하며, -Inf 옵션일 때에는 dt1을 기준으로 dt2의 컬럼 값을 더 작은 값으로 바꾸며 탐색한다. roll = 옵션에는 숫자도 줄 수 있는데, 이 경우 정해준 컬럼 값 기준 숫자 범위 내에서만 탐색한다. roll = nearest 옵션에서는 양방향 탐색을 진행하되, 가장 가까운 값을 찾아서 merge를 시도한다."
  },
  {
    "objectID": "posts/2024-12-13-rollmerge/index.html#rollends-옵션",
    "href": "posts/2024-12-13-rollmerge/index.html#rollends-옵션",
    "title": "data.table의 rolling join",
    "section": "2. rollends = 옵션",
    "text": "2. rollends = 옵션\nrollends = 옵션을 활용하여 rolling을 시작한 경계와 끝 경계에서 어떤 동작을 취할 지 지정할 수 있다. 아래 코드 예시와 같이, rollends 옵션은 두 개의 boolean 값을 가진다. 첫 번째 index는 rolling 시작 경계값에 대한 처리이며, 두 번째 index는 rolling 끝 경계값에 대한 처리이다.\n\nresult_dt &lt;- dt1[dt2, on = .(id), roll = Inf, rollends = c(TRUE, TRUE)]\n\n즉 양 끝의 값을 포함할 것인지, 버리고 NA를 취할 것인지에 대한 조정이라고 보면 된다. rollends =옵션이 Inf일 경우 (T, F), -Inf일 경우 (F, T), 숫자일 경우 (F, F)이다. 이전에 사용했던 건강보험공단 데이터로 실행해보면서 알아보자.\n\ninfo.cci &lt;- lapply(names(code.cci), function(x){\n  data.asd[, MDCARE_STRT_DT := Indexdate]\n  dt &lt;- m40[like(MCEX_SICK_SYM, paste(code.cci[[x]], collapse = \"|\"))][, MDCARE_STRT_DT := as.Date(as.character(MDCARE_STRT_DT), format = \"%Y%m%d\")][, .(RN_INDI, MDCARE_STRT_DT, Incidate = MDCARE_STRT_DT)]  \n  dt[, .SD[1], keyby = c(\"RN_INDI\", \"MDCARE_STRT_DT\")][data.asd, on = c(\"RN_INDI\", \"MDCARE_STRT_DT\"), roll = Inf, rollends = c(T, F)]})\n\nprint(head(info.cci[[10]], n = 5))\n\n   RN_INDI MDCARE_STRT_DT   Incidate   COD1   COD2   SEX  Indexdate   Age Death\n     &lt;int&gt;         &lt;Date&gt;     &lt;Date&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;Date&gt; &lt;int&gt; &lt;int&gt;\n1:   13546     2006-08-08 2007-03-12                   1 2006-08-08    45     0\n2:   23682     2008-09-22       &lt;NA&gt;                   1 2008-09-22    53     0\n3:   36714     2010-01-19 2009-12-26                   2 2010-01-19    64     0\n4:   39217     2013-04-02 2013-04-02                   1 2013-04-02    56     0\n5:   46621     2011-03-24 2011-02-16                   1 2011-03-24    51     0\n   Day_FU\n    &lt;int&gt;\n1:   3432\n2:   2656\n3:   2172\n4:   1003\n5:   1743\n\n\nroll = Inf이므로 Incidate 기준으로 Indexdate는 Incidate보다 큰 값이 탐지되어야 merge를 할 수 있다. 하지만 rollends = (T, F) 로 되어 있으므로 첫 번째 행에서 시작 바운더리인 기준값보다 작아도 roll merge가 일어난 모습을 볼 수 있다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html",
    "href": "posts/2024-11-20-DeLongsMethod/index.html",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "우리는 ROC 곡선에서 AUC 값을 구할 수 있다. AUC 값이 1에 가까우면 모델의 성능이 좋다라고 하며, 0.5에 가까워질 수록 성능이 나쁘다- 라고 한다. 그렇다면, ROC에서 신뢰구간은 어떻게 구할까? 특정 AUC값과 비교하여 p value를 구하려면 어떻게 해야 할까?\n이를 위해 DeLong의 AUC 표준 오차 구하는 방법을 소개하려 한다. (E. DeLong, 1988)\n\n\n\nROC 곡선은 이진 분류 문제에서 모델의 성능을 평가하는데 사용되는 시각적 도구이다.\n다양한 임계값에서의 민감도와 False Positive Rate의 관계를 시각화한 것으로,\n\n민감도(Sensitivity)를 Y축으로, 1 - 특이도(False Positive Rate)을 X축으로 하여 관계를 그린 그래프이다.\n\n민감도(Sensitivity, True Positive Rate)\n\n실제 양성 사례를 얼마나 잘 분류하는지/ 실제 양성인 샘플을 양성으로 올바르게 분류한 비율 \\[\nSens = \\frac{True Positives(TP)}{True Positives(TP) + False Negatives(FN)}\n\\]\n\n\n특이도(Specificity, True Negative Rate)\n\n실제 음성 사례를 얼마나 잘 분류하는지/ 실제 음성인 샘플을 음성으로 올바르게 분류한 비율\n\nFalse Positive Rate(FPR) : 1-Spec, 실제 음성인 샘플을 잘못 양성으로 분류한 비율\n\n\\[\nSpec = \\frac{True Negatives(TN)}{True\\ Negatives(TN) + False \\ Positives(FP)}\n\\]\n이상적인 ROC 곡선\n\n이상적인 분류는 ROC가 (0,1)을 지날 때, 즉 FPR이 0이고, TPR이 1인 경우.\n무작위 분류는 ROC가 y=x일 때, TPR = FPR인 경우.\n\nAUC(Area Under the Curve)\n\n\nAUC는 ROC 곡선 아래 면적을 의미한다.\n\nAUC가 1에 가까울수록 성능이 좋은 모델. 0.5에 가까울수록 성능이 안좋은 모델.\n\n\n\n\n\n\nFigure 1: ROC\n\n\n\n\n아래와 같이 ROC Curve를 추정할 수 있다.\ntest를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자.\n\n\\[\n\\text{for any real number z,} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\]\n\\[\nSens(z) = \\frac{1}{m}\\sum_{i=1}^{m}{I(X_i \\geq z)},\n\\]\n\\[\nSpec(z) = \\frac{1}{n}\\sum_{j=1}^{n}{I(Y_i &lt; z)},\n\\]\n\\[\nI(A) =\n\\begin{cases}\n1, &  A \\text{ is true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n이때 실수 z가 variable 내 가능한 값들 내에서 움직인다면, ROC curve는 [1 - spec(z)]를 X로, Sens(z)를 Y로 갖는 plot이라 할 수 있다. 만약 z가 가능한 최대값보다 크다면 curve는 (0,0)을, 최솟값보다 작다면(1,1)을 지날 것이다. Sens(z) = 1 - Spec(z)라면 y=x 위, 45도 선 위에 놓일 것이다.\n\n\n\n\n\n\nFigure 2: AUC\n\n\n\n\n\nROC curve는 위와 같이 구할 수 있다. 그럼 이의 넓이: AUC 값은 어떻게 구할까? 보통, 곡선 아래 넓이는 trapezoidal rule을 통해 구한다. 고등학교 때 배운 적분을 떠올리면 된다. 수많은 사다리꼴로 쪼개어 넓이를 근사하던 기억을 되살려 보자.\n\n\n\n\n\n\nFigure 3: Trapezes\n\n\n여기서, Mann-Whitney two sample statistic에 따르면, ROC curve 아래 넓이를 구할때, trapezodial rule로 구한 넓이는 Mann-Whitney two sample statistic으로 구한 넓이로 대체할 수 있다.\n\n\nMann-Whitney statistic는 확률\\(\\theta\\)를 예측한다. \\(C_2\\)에서 무작위 추출한 값이 \\(C_1\\)의 값보다 같거나 작을 확률을 추정한다. (test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자)\n\n\n\\[\n\\hat{\\theta} = \\frac{1}{mn}\\sum_{j=1}^{n}\\sum_{i=1}^{m}{\\psi(X_i, Y_j)}\n\\]\n\\[\n\\psi(X,Y) = \\begin{cases}\n1 &  \\ Y &lt; X \\\\\n1/2 & \\  Y=X \\\\\n0 & \\ Y&gt;X\n\\end{cases}\n\\]\n\n모든 (X, Y) 쌍에 대해 X &gt; Y이면 1을, X = Y이면 \\(\\frac{1}{2}\\), X &lt; Y 이면 0을 부여하여 확률을 구한다.\n직관적으로, ROC curve와 AUC 값은 곧 모델의 추정이 옳을 확률이며, 이의 성능의 최고값은 1, 최저값은 1/2이라는 점을 고민하면 위의 추정은 그럴듯 하다.\n\n\\[\nE(\\hat{\\theta}) = Pr(X&gt;Y) + \\frac{1}{2}Pr(X= Y)\n\\]\n\n그럼, 확률(AUC)은 위와 같이 정리할 수 있다.\n\n이제, AUC값의 신뢰성을 측정하기 위해서는 SE(standard Error)를 계산하는 것이 필요하다. 이는 DeLong(1988)이 제시한 방법을 참고하자.\nξ를 각각의 집단 간 공분산이라 하자.\nξ₁₀은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j, Y_k\\)간 공분산,\nξ₀₁은 \\(C_2\\)의 \\(Y_j\\)와 \\(C_1\\)의 \\(X_i, Y_k\\)간 공분산,\nξ₁₁은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j\\)간 자기공분산이다.\nξ₁₀, ξ₀₁, ξ₁₁의 기대값은 아래와 같다.\n\\[\nξ_{10} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_k)]-\\theta^2,\n\\]\n\\[\nξ_{01} = E[\\psi(X_i, Y_j) \\psi(X_k,Y_j)]-\\theta^2,\n\\]\n\\[\nξ_{11} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_j)]-\\theta^2,\n\\]\n기댓값들을 이용하여 AUC 추정치의 분산을 계산할 수 있다.\n\\[\nvar(\\hat{\\theta}) = \\frac{(n-1)ξ_{10}+(m-1)ξ_{01}}{mn} + \\frac{ξ_{11}}{mn}\n\\]\n이와 같은 방법으로 단일 표본 집합에서의 AUC의 표준 오차를 구할 수 있다.\n\n이제, 단일 표본 집합이 아닌 다른 표본집합 r과 s에 대해 다뤄보자. 여러 표본 집합이 있을 경우, 각 표본 간의 상호 공분산 또한 고려되어야한다. 두 표본 집합 r과 s에 대해 AUC의 공분산 계산은 아래와 같다.\n\n\\[\nξ_{10}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{01}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{11}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n이제, 아래 식을 통해 표본 집합의 AUC 값 간의 공분산을 계산할 수 있다.\n\\[\ncov(\\hat{\\theta^r},\\hat{\\theta^s}) = \\frac{(n-1)ξ_{10}^{rs}+(m-1)ξ_{01}^{rs}}{mn} + \\frac{ξ_{11}^{rs}}{mn}\n\\] 이 수식을 통해 여러 표본 집합 간의 공분산을 반영하여 AUC의 표준 오차를 더 정확하게 추정할 수 있다.\n\n이를 바탕으로 우리가 궁금한 값 “표준 오차”에 접근해 보자. (Hoeffding_1948, Bamber_1975, Sen_1960)과 같은 분들 덕분에, 우리는 AUC 표준 오차를 보다 정확히 추정할 수 있다.\n\n\\[\nV^r_{10}(X_i) = \\frac{1}{n} \\sum_{j=1}^n\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (i= 1,2,...,m)\n\\]\n\\[\nV^r_{01}(Y_j) = \\frac{1}{m} \\sum_{i=1}^m\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (j= 1,2,...,m)\n\\]\n\n\n\\(V^r_{10}(X_i)\\)은 집합r에서 값을 기반으로 한 분산이다.\n\n\\(\\psi(X_i^r,Y_j^r)\\)는 \\(X_i^r,Y_j^r\\)의 관계를 나타내며, 이를 통해 분산을 구한다. \\(V^r_{01}(Y_j)\\)은 집합 \\(Y^r_j\\)에서의 분산이다.\n두 값을 통해 각 표본 집합에 대한 분산을 따로 계산한 후, 이를 결합하여 최종적으로 표준 오차를 추정할 수 있다. 각 분산 값이 표본 크기에 따라 가중평균되며, AUC 표준 오차 S는 아래와 같다.\n\\[\n\\\\\nS = \\frac{1}{m}S_{10} + \\frac{1}{n}S_{01}\n\\\\\n\\]\n\n표준 오차를 통해 우리는 AUC 값이 얼마나 신뢰할 수 있는지 평가할 수 있으며, 이는 모델의 성능을 명확히 이해하는데 중요한 역할을 한다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#roc-curve-recevier-operating-characteristic-curve",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#roc-curve-recevier-operating-characteristic-curve",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "ROC 곡선은 이진 분류 문제에서 모델의 성능을 평가하는데 사용되는 시각적 도구이다.\n다양한 임계값에서의 민감도와 False Positive Rate의 관계를 시각화한 것으로,\n\n민감도(Sensitivity)를 Y축으로, 1 - 특이도(False Positive Rate)을 X축으로 하여 관계를 그린 그래프이다.\n\n민감도(Sensitivity, True Positive Rate)\n\n실제 양성 사례를 얼마나 잘 분류하는지/ 실제 양성인 샘플을 양성으로 올바르게 분류한 비율 \\[\nSens = \\frac{True Positives(TP)}{True Positives(TP) + False Negatives(FN)}\n\\]\n\n\n특이도(Specificity, True Negative Rate)\n\n실제 음성 사례를 얼마나 잘 분류하는지/ 실제 음성인 샘플을 음성으로 올바르게 분류한 비율\n\nFalse Positive Rate(FPR) : 1-Spec, 실제 음성인 샘플을 잘못 양성으로 분류한 비율\n\n\\[\nSpec = \\frac{True Negatives(TN)}{True\\ Negatives(TN) + False \\ Positives(FP)}\n\\]\n이상적인 ROC 곡선\n\n이상적인 분류는 ROC가 (0,1)을 지날 때, 즉 FPR이 0이고, TPR이 1인 경우.\n무작위 분류는 ROC가 y=x일 때, TPR = FPR인 경우.\n\nAUC(Area Under the Curve)\n\n\nAUC는 ROC 곡선 아래 면적을 의미한다.\n\nAUC가 1에 가까울수록 성능이 좋은 모델. 0.5에 가까울수록 성능이 안좋은 모델.\n\n\n\n\n\n\nFigure 1: ROC"
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#empirical-roc-curve",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#empirical-roc-curve",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "아래와 같이 ROC Curve를 추정할 수 있다.\ntest를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자.\n\n\\[\n\\text{for any real number z,} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\]\n\\[\nSens(z) = \\frac{1}{m}\\sum_{i=1}^{m}{I(X_i \\geq z)},\n\\]\n\\[\nSpec(z) = \\frac{1}{n}\\sum_{j=1}^{n}{I(Y_i &lt; z)},\n\\]\n\\[\nI(A) =\n\\begin{cases}\n1, &  A \\text{ is true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]\n\n이때 실수 z가 variable 내 가능한 값들 내에서 움직인다면, ROC curve는 [1 - spec(z)]를 X로, Sens(z)를 Y로 갖는 plot이라 할 수 있다. 만약 z가 가능한 최대값보다 크다면 curve는 (0,0)을, 최솟값보다 작다면(1,1)을 지날 것이다. Sens(z) = 1 - Spec(z)라면 y=x 위, 45도 선 위에 놓일 것이다.\n\n\n\n\n\n\nFigure 2: AUC"
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#auc",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#auc",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "ROC curve는 위와 같이 구할 수 있다. 그럼 이의 넓이: AUC 값은 어떻게 구할까? 보통, 곡선 아래 넓이는 trapezoidal rule을 통해 구한다. 고등학교 때 배운 적분을 떠올리면 된다. 수많은 사다리꼴로 쪼개어 넓이를 근사하던 기억을 되살려 보자.\n\n\n\n\n\n\nFigure 3: Trapezes\n\n\n여기서, Mann-Whitney two sample statistic에 따르면, ROC curve 아래 넓이를 구할때, trapezodial rule로 구한 넓이는 Mann-Whitney two sample statistic으로 구한 넓이로 대체할 수 있다.\n\n\nMann-Whitney statistic는 확률\\(\\theta\\)를 예측한다. \\(C_2\\)에서 무작위 추출한 값이 \\(C_1\\)의 값보다 같거나 작을 확률을 추정한다. (test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 \\(C_1\\)(\\(X_i\\),n=m), 발생하지 않은 집단을 \\(C2\\)(\\(Y_i\\),n=n)이라 하자)\n\n\n\\[\n\\hat{\\theta} = \\frac{1}{mn}\\sum_{j=1}^{n}\\sum_{i=1}^{m}{\\psi(X_i, Y_j)}\n\\]\n\\[\n\\psi(X,Y) = \\begin{cases}\n1 &  \\ Y &lt; X \\\\\n1/2 & \\  Y=X \\\\\n0 & \\ Y&gt;X\n\\end{cases}\n\\]\n\n모든 (X, Y) 쌍에 대해 X &gt; Y이면 1을, X = Y이면 \\(\\frac{1}{2}\\), X &lt; Y 이면 0을 부여하여 확률을 구한다.\n직관적으로, ROC curve와 AUC 값은 곧 모델의 추정이 옳을 확률이며, 이의 성능의 최고값은 1, 최저값은 1/2이라는 점을 고민하면 위의 추정은 그럴듯 하다.\n\n\\[\nE(\\hat{\\theta}) = Pr(X&gt;Y) + \\frac{1}{2}Pr(X= Y)\n\\]\n\n그럼, 확률(AUC)은 위와 같이 정리할 수 있다."
  },
  {
    "objectID": "posts/2024-11-20-DeLongsMethod/index.html#standard-error",
    "href": "posts/2024-11-20-DeLongsMethod/index.html#standard-error",
    "title": "DeLong’s Method; for ROC AUC",
    "section": "",
    "text": "이제, AUC값의 신뢰성을 측정하기 위해서는 SE(standard Error)를 계산하는 것이 필요하다. 이는 DeLong(1988)이 제시한 방법을 참고하자.\nξ를 각각의 집단 간 공분산이라 하자.\nξ₁₀은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j, Y_k\\)간 공분산,\nξ₀₁은 \\(C_2\\)의 \\(Y_j\\)와 \\(C_1\\)의 \\(X_i, Y_k\\)간 공분산,\nξ₁₁은 \\(C_1\\)의 \\(X_i\\)와 \\(C_2\\)의 \\(Y_j\\)간 자기공분산이다.\nξ₁₀, ξ₀₁, ξ₁₁의 기대값은 아래와 같다.\n\\[\nξ_{10} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_k)]-\\theta^2,\n\\]\n\\[\nξ_{01} = E[\\psi(X_i, Y_j) \\psi(X_k,Y_j)]-\\theta^2,\n\\]\n\\[\nξ_{11} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_j)]-\\theta^2,\n\\]\n기댓값들을 이용하여 AUC 추정치의 분산을 계산할 수 있다.\n\\[\nvar(\\hat{\\theta}) = \\frac{(n-1)ξ_{10}+(m-1)ξ_{01}}{mn} + \\frac{ξ_{11}}{mn}\n\\]\n이와 같은 방법으로 단일 표본 집합에서의 AUC의 표준 오차를 구할 수 있다.\n\n이제, 단일 표본 집합이 아닌 다른 표본집합 r과 s에 대해 다뤄보자. 여러 표본 집합이 있을 경우, 각 표본 간의 상호 공분산 또한 고려되어야한다. 두 표본 집합 r과 s에 대해 AUC의 공분산 계산은 아래와 같다.\n\n\\[\nξ_{10}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{01}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n\\[\nξ_{11}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n\\]\n이제, 아래 식을 통해 표본 집합의 AUC 값 간의 공분산을 계산할 수 있다.\n\\[\ncov(\\hat{\\theta^r},\\hat{\\theta^s}) = \\frac{(n-1)ξ_{10}^{rs}+(m-1)ξ_{01}^{rs}}{mn} + \\frac{ξ_{11}^{rs}}{mn}\n\\] 이 수식을 통해 여러 표본 집합 간의 공분산을 반영하여 AUC의 표준 오차를 더 정확하게 추정할 수 있다.\n\n이를 바탕으로 우리가 궁금한 값 “표준 오차”에 접근해 보자. (Hoeffding_1948, Bamber_1975, Sen_1960)과 같은 분들 덕분에, 우리는 AUC 표준 오차를 보다 정확히 추정할 수 있다.\n\n\\[\nV^r_{10}(X_i) = \\frac{1}{n} \\sum_{j=1}^n\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (i= 1,2,...,m)\n\\]\n\\[\nV^r_{01}(Y_j) = \\frac{1}{m} \\sum_{i=1}^m\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (j= 1,2,...,m)\n\\]\n\n\n\\(V^r_{10}(X_i)\\)은 집합r에서 값을 기반으로 한 분산이다.\n\n\\(\\psi(X_i^r,Y_j^r)\\)는 \\(X_i^r,Y_j^r\\)의 관계를 나타내며, 이를 통해 분산을 구한다. \\(V^r_{01}(Y_j)\\)은 집합 \\(Y^r_j\\)에서의 분산이다.\n두 값을 통해 각 표본 집합에 대한 분산을 따로 계산한 후, 이를 결합하여 최종적으로 표준 오차를 추정할 수 있다. 각 분산 값이 표본 크기에 따라 가중평균되며, AUC 표준 오차 S는 아래와 같다.\n\\[\n\\\\\nS = \\frac{1}{m}S_{10} + \\frac{1}{n}S_{01}\n\\\\\n\\]\n\n표준 오차를 통해 우리는 AUC 값이 얼마나 신뢰할 수 있는지 평가할 수 있으며, 이는 모델의 성능을 명확히 이해하는데 중요한 역할을 한다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html",
    "href": "posts/2024-10-17-TTE/index.html",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "",
    "text": "임상연구에서 exposure와 outcome 간의 인과관계를 밝히는 gold standard는 randomized controlled trial(RCT)입니다. 그러나 임상현장 및 임상연구자의 현실적인 제약들로 인해 모든 임상연구를 RCT로 시행할 수는 없으며, 대부분의 연구는 real world data(RWD)를 기반으로 이루어집니다. Target trial emulation이란, 목표로 하는 가상의 RCT(target trial)를 설정한 후, RWD를 대상으로 이 RCT를 모사하여 인과성을 추론하는 연구기법입니다.  이 글에서는 Clone-Censor-Weight method를 소개하고, 이를 이용하여 Target trial emulation을 R에서 구현하는 과정을 살펴봅니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#introduction",
    "href": "posts/2024-10-17-TTE/index.html#introduction",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "",
    "text": "임상연구에서 exposure와 outcome 간의 인과관계를 밝히는 gold standard는 randomized controlled trial(RCT)입니다. 그러나 임상현장 및 임상연구자의 현실적인 제약들로 인해 모든 임상연구를 RCT로 시행할 수는 없으며, 대부분의 연구는 real world data(RWD)를 기반으로 이루어집니다. Target trial emulation이란, 목표로 하는 가상의 RCT(target trial)를 설정한 후, RWD를 대상으로 이 RCT를 모사하여 인과성을 추론하는 연구기법입니다.  이 글에서는 Clone-Censor-Weight method를 소개하고, 이를 이용하여 Target trial emulation을 R에서 구현하는 과정을 살펴봅니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method",
    "href": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Clone-Censor-Weight method",
    "text": "Clone-Censor-Weight method\nClone-censor-weight method에서는  1) eligibile한 모든 대상자를 clone하여 치료군(Treatment)과 비교군(Control)에 할당하고,  2) 실제 대상자가 치료군에 속한 경우라면 대조군에 있던 clone은 치료 시작 시점에 artificial censoring되며(치료군의 대상자 중 정의된 치료시작시점 이후에 치료가 시작된 경우도 artificial censoring),  3) 해당 baseline characteristics를 가진 대상자가 관찰시간에 따라 artificial censoring되지 않고 남아있을 확률의 역수를 가중치로 하여 분석 하게 됩니다.\nImmortal time bias\nimmortal time bias는 추적관찰 연구에서 Follow up start period와 Treatment start period가 다를 때 발생하는 bias입니다. 연구대상자가 치료군에 속하기 위해서는 Treatment start period까지 생존(event 미발생)해 있어야 합니다. 즉, 대상자가 치료군에 속해있다면 Follow up start period와 Treatment start period 사이의 시간은 event가 발생할 수 없는 immortal time이 되는 것입니다. 따라서 immortal time 구간은 치료를 받지 않은 시기임에도 time to event를 계산할 때 치료를 받은 시기로 산입되는 misclassification bias가 생기고, 치료군은 치료시작시점까지 생존한 환자들이 선택되어 selection bias가 발생합니다.\n\n\n\n\n\nFigure 1: Immortal time bias\n\n\nImmortal time bias는 약물 복용의 효과나 질환의 경과 중에 시행하는 시술의 효과를 평가하는 연구 등에서 치료의 효과를 overestimation할 수 있어 중요하게 고려해야 하는 bias입니다. \n지금까지는 immortal time bias를 보정하기 위해 Landmark analysis나 Time-dependent survival analysis 등을 사용했습니다. 하지만 Landmark method는 어느 시점을 landmark로 결정할지에 대한 문제가 있으며 실제 치료나 약물복용을 정확히 반영하지 못한다는 단점이 있습니다. Clone-censor-weight 과정을 거치면 실제로 치료를 받은 대상자의 clone이 immortal time 동안 비교군에 있다가 치료 시작시점에 censor되므로, bias를 보정할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#실습-데이터-구성",
    "href": "posts/2024-10-17-TTE/index.html#실습-데이터-구성",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "실습 데이터 구성",
    "text": "실습 데이터 구성\n데이터 소개\n실습에 사용할 데이터는 국민건강보험공단에서 제공하는 예시 데이터입니다. 실습에서는 개인별 기본정보(성별 등)를 담고 있는 bnc, 사망정보를 담고 있는 bnd, 진료명세서 데이터(상병명)인 m20, 약물처방 데이터인 m60 데이터를 사용할 것입니다. bnd 데이터에는 사망일자 변수(DTH_YYYYMM)가 연도와 월 까만 있고 날짜는 없어, 분석을 위해 모든 대상자의 사망일자를 사망한 월의 말일로 변환하여 데이터를 불러오겠습니다.\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(data.table))\nsuppressPackageStartupMessages(library(survival))\nsuppressPackageStartupMessages(library(boot))\nsuppressPackageStartupMessages(library(DT))\n\ninst &lt;- fread(\"data/nsc2_inst_1000.csv\")\nbnc &lt;- fread(\"data/nsc2_bnc_1000.csv\") \nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\") \nm20 &lt;- fread(\"data/nsc2_m20_1000.csv\") \nm30 &lt;- fread(\"data/nsc2_m30_1000.csv\") \nm40 &lt;- fread(\"data/nsc2_m40_1000.csv\") \nm60 &lt;- fread(\"data/nsc2_m60_1000.csv\") \ng1e_0915 &lt;- fread(\"data/nsc2_g1e_0915_1000.csv\") \n\nbnd &lt;- fread(\"data/nsc2_bnd_1000.csv\")[, Deathdate := (lubridate::ym(DTH_YYYYMM) %&gt;% lubridate::ceiling_date(unit = \"month\") - 1)][]\n\nTarget trial 설정\n이 실습의 Target trial에서는 2011년 1월 1일부터 2015년 12년 31까지 위염 및 십이지장염(공단 질병분류기호 K29.0~K29.9)을 진단받은 환자들 중, Proton pump inhibitor (PPI) 복용에 따라 위염 및 십이지장염의 재발 위험이 달라지는지를 보고자 합니다. 즉, time-zero는 위염의 진단 시점이고, 관심 event는 위염의 재발입니다. 본 실습은 Clone-censor-weight 과정을 소개하기 위한 글이므로, 임상적 타당성은 고려하지 않았음을 미리 밝힙니다.\nData set 구성\n먼저 처방정보(m60)데이터에서 PPI에 해당하는 처방코드(code.ppi)가 입력된 case들을 선택한 뒤, 가장 마지막으로 PPI를 처방받은 행을 선택하겠습니다(m60.drug).\n\ncode.ppi &lt;-  c(\"367201ACH\", \"367201ATB\", \"367201ATD\", \"367202ACH\", \"367202ATB\", \n               \"367202ATD\", \"498001ACH\", \"498002ACH\", \"509901ACH\", \"509902ACH\", \n               \"670700ATB\", \"204401ACE\", \"204401ATE\", \"204402ATE\", \"204403ATE\", \n               \"664500ATB\", \"640200ATB\", \"664500ATB\", \"208801ATE\", \"208802ATE\", \n               \"656701ATE\", \"519201ATE\", \"519202ATE\", \"656701ATE\", \"519203ATE\", \n               \"222201ATE\", \"222202ATE\", \"222203ATE\", \"181301ACE\", \"181301ATD\", \n               \"181302ACE\", \"181302ATD\", \"181302ATE\", \"621901ACR\", \"621902ACR\", \n               \"505501ATE\")\n\nm60.drug &lt;- m60[GNL_NM_CD %in% code.ppi][order(MDCARE_STRT_DT, TOT_MCNT), .SD[.N], keyby = \"RN_KEY\"] %&gt;%\n  .[order(-MDCARE_STRT_DT), .SD[1], by = \"RN_INDI\"] %&gt;% .[, .SD, .SDcols = c(\"RN_INDI\", \"MDCARE_STRT_DT\")]\n\n이후 진료명세서(m20) 데이터에서, 2011년 1월 1일 이후 주상병과 첫 번째 부상병에서 K29 코드가 포함된 행을 선택하겠습니다. 그리고 한 환자에서 처음으로 진단명이 입력된 시점을 first_date(위염의 처음 진단 ; time zero)로 가정하고, 마지막으로 진단명이 입력된 시점을 recurr_date(위염이 재발한 시점 ; event 발생 시점)로 가정하겠습니다. 데이터를 m60.drug와 합쳐서, first_date 이후 처음으로 PPI가 처방된 시점을 treat_date 변수로 코딩하겠습니다(데이터 kk).\n\nkk &lt;- m20[(grepl('K29', SICK_SYM1) | grepl('K29', SICK_SYM2)) & (MDCARE_STRT_DT &gt;= 20110000), \n          .SD, .SDcols = c(\"RN_INDI\", \"MDCARE_STRT_DT\")] %&gt;% \n  .[order(MDCARE_STRT_DT), .SD, keyby=\"RN_INDI\"] %&gt;%\n  .[, .(first_date = min(MDCARE_STRT_DT, na.rm = TRUE),\n        recurr_date = ifelse(.N &gt; 1, max(MDCARE_STRT_DT, na.rm = TRUE), NA_integer_)), \n    keyby = \"RN_INDI\"]  %&gt;% \n  m60.drug[, .(RN_INDI, MDCARE_STRT_DT)][., on = \"RN_INDI\"] %&gt;% \n  .[, treat_date := ifelse(MDCARE_STRT_DT &gt; first_date & (is.na(recurr_date) | MDCARE_STRT_DT &lt; recurr_date),\n                           MDCARE_STRT_DT,\n                           NA)]\nkk$MDCARE_STRT_DT &lt;- NULL\n\n이후 분석에서 공변량으로 사용할 성별 정보를 기본정보(bnc) 데이터에서 추가하고, 날짜 변수의 class를 변환하겠습니다.\n\nkk &lt;- merge(kk, bnc[, .(SEX = SEX[1]), keyby = \"RN_INDI\"], by = \"RN_INDI\")\n\nkk[, `:=`(recurr_date = as.Date(as.character(recurr_date), format = \"%Y%m%d\"),\n          treat_date = as.Date(as.character(treat_date), format = \"%Y%m%d\"),\n          first_date = as.Date(as.character(first_date), format = \"%Y%m%d\")\n                                )]\n\n이제 PPI 처방 여부(treatment ; 0,1)와 위염 재발 여부(recurr ; 0,1)를 새로운 변수로 만들고, 사망정보(bnd) 데이터에서 관찰기간 내 사망 여부와 사망 날짜를 확인하겠습니다. 사망했다면 사망일을, 그렇지 않다면 2015-12-31을 Obs_day로 코딩하고, 위염의 진단부터 재발까지(event가 발생한)의 시간을 gastritis_day로 코딩합니다. 마지막으로, Obs_day와 gastritis_day 중 먼저 온 것 ; event가 발생한 시점 또는 관찰이 종료된 시점을 FU_time으로 코딩합니다.\n\nkk[, `:=` (treatment = ifelse(is.na(treat_date), 0, 1),\n           recurr = ifelse(is.na(recurr_date), 0, 1))]\n\nkk.death &lt;- bnd[, .(RN_INDI, Deathdate, BTH_YYYY)][kk, on=\"RN_INDI\", ] %&gt;% \n  .[, `:=`(Age = year(first_date) - as.integer(substr(BTH_YYYY, 1, 4)),\n           death = as.integer(!is.na(Deathdate)),\n           Obs_day = as.integer(pmin(Deathdate, as.Date(\"2015-12-31\"), na.rm=T)-first_date),\n           treat_day = as.integer(treat_date-first_date))]\n\nkk.death[, `:=` (gastritis_day = as.integer(recurr_date - first_date))]\nkk.death[, FU_day := pmin(Obs_day, gastritis_day, na.rm = T)]\n\nori &lt;- kk.death[, .(RN_INDI, treatment, treat_day, recurr, Age, SEX, FU_day)]\n\n이제 분석을 위한 데이터(ori)가 구성되었습니다. 실제 연구라면 이전에 위염을 진단받은 환자를 배제하거나, treat_date로부터 특정 시간 이내에 PPI 처방 이력이 있는 환자를 제외하는 등의 다양한 조건을 설정할 수 있겠습니다.\n\ndatatable(ori, rownames = F, caption = \"Original data\", options = list(scrollX = T))"
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method-적용",
    "href": "posts/2024-10-17-TTE/index.html#clone-censor-weight-method-적용",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Clone-Censor-Weight method 적용",
    "text": "Clone-Censor-Weight method 적용\nClone-Censor-Weight method를 이해하기 위해 다음 그림을 설명하겠습니다.\n\n\n\n\n\nFigure 2: Clone-Censor-Weight model\n\n\n그림에서 세모 표시는 치료를, X 표시는 event 발생을, 괄호 표시는 censor(follow up loss 등)을 나타냅니다.  일반적으로 Clone-Censor-Weight design에서는 ‘grace time’을 설정하는데, 실제로 치료를 받은 대상자라고 하더라도 grace time(그림에서는 180일)이 경과된 시점에 치료를 받았다면 치료를 받지 않았다고 간주하는 것입니다. 즉, 치료군에 속하기 위해서는 추적관찰 시작 시점으로부터 grace time 이내에 치료를 받아야 합니다. 이 정의에 따르면 그림의 A~E는 치료를 받은 대상자이지만, F~H는 치료를 받지 않은 대상자가 됩니다. \n\nA~E는 Grace time 이전에 치료를 받았으므로 치료군이 됩니다. A~E의 clone은 비교군에 할당될 것이고, 비교군의 입장에서는 치료를 받은 시점에 protocol violation이 발생한 것이므로 artificial censoring됩니다.\nF~H는 Grace time 이후에 치료를 받았으므로 비교군이 됩니다. 이 때, 치료군에 할당된 F~H의 clone은 grace time인 180일에 artificial censoring됩니다.\nI, J, M은 치료 자체를 받지 않았으므로 비교군이 되고, 이들의 clone은 grace time인 180일에 artificial censoring 됩니다.\nK와 L은 치료를 받지 않았으면서 grace time 이전에 follow up이 종료되었습니다. 이런 경우에는 clone에서 artificial censoring이 일어나지 않습니다.\n\n대상자의 clone에서 protocol violation에 의해 발생한 censoring은 실제 RCT에서 일어나지 않는 Artificial censoring입니다(일반적인 생존분석에서 follow up period 내에 event가 발생하지 않는 censoring과 다릅니다). Artificial censoring은 관찰 시간에 따라 발생 확률이 달라지고, 대상자의 특성(성별, 나이, 과거력 등의 공변량)에 따라서도 발생 확률이 달라집니다. Artificial censoring은 Clone이라는 연구 디자인에 의해 발생한 것이고, 만일 특정 대상자의 clone이 어떤 구간에서 artificial censoring될 확률이 높다면 실제로는 censoring 이후에 event가 발생할 수 있음에도 불구하고 그 event를 관찰할 수 없게 됩니다. 따라서 시간에 따라 artificial censoring이 되지 않고 관찰대상으로 남아 있을 확률을 공변량을 보정하여 구하고, 그 역수를 가중치로 부여합니다(Inverse Probability of Censoring Weighting).  이렇게 하면 1) 동일한 대상자에서 시간구간 (5,6)까지 censor되지 않을 확률이 0.8이라면 가중치 1.25를 부여하고, (10,12)까지 censor되지 않을 확률이 0.5라면 가중치 2.0을 부여하여 시간에 따라 censor될 확률을 동일하게 맞추고, 2) 공변량(예를 들면 성별)이 다른 대상자에서 시간구간 (5,6)까지 censor되지 않을 확률이 0.7이라면 이 확률에 대한 성별의 영향을 보정합니다. 만일 real world에서 중년 여성이 남성에 비해 위염 치료를 위해 PPI를 많이 처방받는다고 하면, CCW design에서 중년여성은 남성에 비해 control arm의 artificial censoring이 많이 될 것입니다. RCT에서는 treatment group과 control group에서 성별의 분포가 동일하므로, 이를 모방하기 위해서는 weight를 주어야 합니다. 이후 분석에는 대상자별 가중치를 고려한 생존분석을 시행합니다.\nClone\n먼저 전체 대상자를 Clone 하여 Treatment arm(PPI arm)과 Control arm을 만들고, 각 arm에서 follow up time과 outcome(event)을 입력하겠습니다.\nControl arm_follow up time, outcome(event) 설정\n최종 데이터인 ori를 control arm으로 할당합니다(arm=Control). 치료를 받은 대상자(Case1)에서는 control group의 clone이 치료를 시작한 날(treat_day)에 artificial censoring되므로, treat_day를 follow-up time(fup)으로 설정하고 outcome(위염 재발)은 발생하지 않은 것으로 설정합니다. 치료를 받지 않은 대상자(grace time 이후에 받은 대상자 포함, Case2)에서는 기존의 outcome(recurr)과 추적기간(FU_day)을 그대로 입력합니다.\n\n#Arm \"Control\": no treatment within 180days\nori_control&lt;-ori  # We create a first copy of the dataset: \"clones\" assigned to the control (no treatment) arm\nori_control$arm&lt;-\"Control\"\n\n#Case 1: Patients receive PPI within 180days (scenarii A to E)\n#they are still alive and followed-up until treatment\nori_control$outcome[ori_control$treatment==1 & ori_control$treat_day &lt;= 180] &lt;- 0\n\nori_control$fup[ori_control$treatment==1 & ori_control$treat_day &lt;= 180]&lt;-ori_control$treat_day[ori_control$treatment==1 & ori_control$treat_day &lt;= 180]\n\n#Case 2: Patients do not receive PPI within 180days (either no treatment or treatment after 6 months): \n#we keep their observed outcomes and follow-up times (scenarii F to M)\nori_control$outcome[ori_control$treatment==0  | (ori_control$treatment==1 & ori_control$treat_day &gt; 180)] &lt;- ori_control$recurr[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt; 180)]\n\nori_control$fup[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt;180)] &lt;- ori_control$FU_day[ori_control$treatment==0  | (ori_control$treatment==1  & ori_control$treat_day &gt;180)]\n\n예를 들어 45461번 대상자는 39일째 PPI를 복용했으므로 Control arm에서 39일까지 follow up하였으며, outcome은 발생하지 않았습니다.\n\ndatatable(ori_control, rownames = F, caption = \"ori_control; fup & outcome\", options = list(scrollX = T))\n\n\n\n\n\nTreatment(PPI) arm_follow up time, outcome(event) 설정\n이번에는 ori를 treatment arm으로 할당합니다(arm=PPI). 치료군의 정의를 만족하는 대상자(Case1)에서는 기존 추적기간과 outcome을 유지합니다. Case2와 같이 치료를 받지 않고 grace time 이전에 추적이 종료된 대상자도 마찬가자입니다. Case3와 같이 180일 이후에 PPI를 복용했거나 치료 없이 180일 이상 추적관찰 된 대상자는, 대상자의 clone이 treatment arm에서 180일째 artificial censoring되므로 follow-up time(fup)을 180일로 설정하고 outcome은 발생하지 않은 것으로 설정합니다\n\n#Arm \"ppi\": Treatment within 180days\nori_ppi&lt;-ori # We create a second copy of the dataset: \"clones\" assigned to the PPI arm\nori_ppi$arm&lt;-\"PPI\"\n\n#Case 1: Patients receive PPI within 180 days : \n#we keep their observed outcomes and follow-up times\n\nori_ppi$outcome[ori_ppi$treatment==1\n                    & ori_ppi$treat_day &lt;=180]&lt;-ori_ppi$recurr[ori_ppi$treatment==1\n                                                                             & ori_ppi$treat_day &lt;=180]\n\nori_ppi$fup[ori_ppi$treatment==1\n                & ori_ppi$treat_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==1\n                                                                           & ori_ppi$treat_day &lt;=180]\n\n#Case 2: Patients die or are lost to follow-up before 180days without recieving treatment: \n#we keep their observed outcomes and follow-up times (scenarii K and L)\n\nori_ppi$outcome[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$recurr[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\nori_ppi$fup[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\n#Case 3: Patients do not receive PPI within 180days and are still alive or \n#at risk at 6 months (scenarii F-J and M)\n# they are considered alived and their follow-up time is 6 months\n\nori_ppi$outcome[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==0  & ori_ppi$treat_day &gt;180)|(ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-0\n\nori_ppi$fup[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==0 & ori_ppi$treat_day &gt;180)|(ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-180\n\n예를 들어 24053번 대상자는 PPI를 복용하지 않았고 297일간 추적관찰하였으므로, Treatment arm에 180일간 follow up 하였습니다.\n\ndatatable(ori_ppi, rownames = F, caption = \"ori_ppi; fup & outcome\", options = list(scrollX = T))\n\n\n\n\n\nControl arm_censoring status, follow-up time uncensored 설정\n이제 각 arm에 있는 대상자들이 artificial censor되는지 여부(censoring)와, censor 되기 전까지의 follow-up time(fup_uncensored)을 변수로 만들겠습니다. fup_uncensored는 최댓값을 grace time인 180일로 정합니다. Control arm의 경우 180일 이전에 PPI를 복용한 대상자만 censoring이 되고, 나머지 대상자는 censoring 되지 않습니다.\n\n#Arm \"Control\": no treatment within 6 months\n\n#Case 1: Patients receive treatment within 6 months: \n#they are censored in the control group at time of treatment (scenarii A to E)\nori_control$censoring[ori_control$treatment==1 & ori_control$treat_day &lt;=180]&lt;-1\n\nori_control$fup_uncensored[ori_control$treatment==1 & ori_control$treat_day &lt;=180]&lt;-(ori_control$treat_day[ori_control$treatment==1 & ori_control$treat_day &lt;=180])\n\n#Case 2: Patients die or are lost to follow-up before 180days : \n#we keep their follow-up time but they are uncensored (scenarii K and L)\nori_control$censoring[ori_control$treatment==0 & ori_control$FU_day &lt;=180]&lt;-0\n\nori_control$fup_uncensored[ori_control$treatment==0 & ori_control$FU_day &lt;=180]&lt;-ori_control$FU_day[ori_control$treatment==0 & ori_control$FU_day &lt;=180]\n\n#Case 3: Patients do not receive PPI within 6 months and are still alive or \n#at risk at 6 months : (scenarii F-J and M)\n# they are considered uncensored and their follow-up time is 180days\nori_control$censoring[(ori_control$treatment==0 & ori_control$FU_day &gt;180) | (ori_control$treatment==1 & ori_control$treat_day &gt;180)]&lt;-0\n\nori_control$fup_uncensored[(ori_control$treatment==0 & ori_control$FU_day &gt;180) | (ori_control$treatment==1 & ori_control$treat_day &gt;180)]&lt;- 180\n\n\ndatatable(ori_control, rownames = F, caption = \"ori_control\", options = list(scrollX = T))\n\n\n\n\n\nTreatment(PPI) arm_censoring status, follow-up time uncensored 설정\n다음으로, tretment arm에서도 동일하게 censoring status, follow-up time uncensored 변수를 만들겠습니다. Treatment arm에서는 PPI를 복용하지 않은 대상자와, grace time인 180일 이후 복용한 대상자에서 censoring이 발생합니다.\n\n#Arm \"PPI\": PPI within 6 months\n\n#Case 1: Patients receive treatment within 180days : \n# they are uncensored in the ppi arm and remain at risk of \n# censoring until time of treatment (scenarii A to E)\nori_ppi$censoring[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180]&lt;-0\n\nori_ppi$fup_uncensored[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180]&lt;-(ori_ppi$treat_day[ori_ppi$treatment==1 & ori_ppi$treat_day &lt;=180])\n\n#Case 2: Patients die or are lost to follow-up before 180days : \n#we keep their follow-up times but they are uncensored (scenarii K and L)\nori_ppi$censoring[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-0\n\nori_ppi$fup_uncensored[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]&lt;-ori_ppi$FU_day[ori_ppi$treatment==0 & ori_ppi$FU_day &lt;=180]\n\n#Case 3: Patients do not receive ppi within 180days and are still alive \n#or at risk at 6 months (scenarii F-J and M): \n# they are considered censored and their follow-up time is 180days\nori_ppi$censoring[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==1 & ori_ppi$treat_day &gt;180)]&lt;-1\n\nori_ppi$fup_uncensored[(ori_ppi$treatment==0 & ori_ppi$FU_day &gt;180) | (ori_ppi$treatment==1  & ori_ppi$treat_day &gt;180)]&lt;-180\n\n마지막으로, 분석을 위해 control arm과 treatment(PPI) arm을 하나의 데이터(dat)로 합치겠습니다. 합친 데이터에서는 모든 대상자가 두 명씩 존재할 것입니다.\n\ndat &lt;-rbind(ori_control, ori_ppi)\ndatatable(dat[dat$RN_INDI == 45461, ], rownames = F, caption = \"dat ; original + clone\", options = list(scrollX = T))\n\n\n\n\n\nTime-split data의 생성\n이제 데이터를 event가 발생한 시점마다 자르는(split) 작업을 하겠습니다. 혼란을 피하기 위해 데이터 이름을 tab으로 새로 생성하겠습니다. event가 발생하거나, artificial censor가 발생한 모든 시점은 fup 변수에 포함되어 있습니다. 따라서 fup 변수의 시간대별로 구간을 생성하기 위해 times 데이터 프레임을 만듭니다.\n\ntab &lt;- dat\n#Dataframe containing the time of events and an ID for the times of events\nt_events&lt;-sort(unique(tab$fup))\ntimes&lt;-data.frame(\"tevent\"=t_events,\"ID_t\"=seq(1:length(t_events)))\n\ntevent는 event가 발생한 시점이고, ID_t는 해당 시점이 전체 발생 시점들 중 몇 번째에 해당하는지를 나타냅니다.\n\ndatatable(times, rownames = F, caption = \"times ; dataframe for fup\", options = list(scrollX = T))\n\n\n\n\n\n먼저 treatment arm인 tab_s에 대해, Survival 패키지의 survSplit 함수를 이용하여 t_event에 해당하는 시점마다 구간을 나누고 시간 순으로 정렬합니다. 이렇게 만들어진 data.long은 구간별로 outcome, 즉 위염의 재발이 일어났는지를 보여줍니다. 다음으로 tab_s를 동일하게 t_event에 따라 split하되, 구간별로 outcome이 아니라 artificial censoring이 일어났는지를 보겠습니다(코드에서 event=“censoring”으로 입력합니다). data.long과 data.long.cens는 동일한 데이터(tab_s)를 동일한 시간구간(t_event)별로 나눈 것이므로, data.long.cens의 censoring status를 data.long과 합치면 구간별로 outcome 발생 여부와 censoring 여부를 확인할 수 있습니다.\n\ntab_s&lt;-tab[tab$arm==\"PPI\",]\n  \n#Creation of the entry variable (Tstart, 0 for everyone)\ntab_s$Tstart&lt;-0\n  \n#Splitting the dataset at each time of event until the event happens and sorting it\ndata.long&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"outcome\",id=\"ID\") \ndata.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  \n#Splitting the original dataset at each time of event and sorting it\n#until censoring happens. This is to have the censoring status at each time of event \ndata.long.cens&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"censoring\",id=\"ID\") \ndata.long.cens&lt;-data.long.cens[order(data.long.cens$ID,data.long.cens$fup),] \n  \n#Replacing the censoring variable in data.long by the censoring variable obtained\n# in the second split dataset\ndata.long$censoring&lt;-data.long.cens$censoring\n  \n#Creating Tstop (end of the interval) \ndata.long$Tstop&lt;-data.long$fup\n  \n#Merge and sort\ndata.long&lt;-merge(data.long,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\ndata.long&lt;-data.long[order(data.long$ID,data.long$fup),] \ndata.long$ID_t[is.na(data.long$ID_t)]&lt;-0\n\n각 구간의 끝은 fup 이므로, Tstart에 대응되는 Tstop 변수에 fup을 입력하고, 위에서 만든 times와 merge하게 되면 대상자별로 각 행이 몇 번째 구간인지를 알 수 있습니다(ID_t : 구간번호). 이 때, times는 1부터 시작하므로 tstart가 0, tstop이 1인 행은 ID_t가 NA로 출력되므로 해당 구간번호를 0으로 바꾸어줍니다. data.long을 보면, RN_INDI별로 [tstart, tstop]으로 나누어진 시간 구간이 있고, 해당 구간의 outcome과 censoring이 있습니다.\n\ndatatable(data.long, rownames = F, caption = \"data.long ; time-splitted data for control arm\", options = list(scrollX = T))\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n동일한 작업을 control arm인 tab_c에 대해서도 수행합니다.\n\ntab_c&lt;-tab[tab$arm==\"Control\",]\n  \n#Creation of the entry variable (Tstart, 0 for everyone)\ntab_c$Tstart&lt;-0\n  \n  \n#Splitting the dataset first at each time of event\n#until the event happens \ndata.long2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"outcome\",id=\"ID\") \ndata.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n  \n#Splitting the original dataset at each time of event\n#until censoring happens \ndata.long.cens2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", start=\"Tstart\", event=\"censoring\",id=\"ID\") \ndata.long.cens2&lt;-data.long.cens2[order(data.long.cens2$ID,data.long.cens2$fup),] \n  \n#Replacing the censoring variable in data.long by the censoring variable obtained\n# in the second split dataset\ndata.long2$censoring&lt;-data.long.cens2$censoring\n  \n#Creating Tstop (end of the interval)\ndata.long2$Tstop&lt;-data.long2$fup\n  \n#Merge and sort\ndata.long2&lt;-merge(data.long2,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\ndata.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \ndata.long2$ID_t[is.na(data.long2$ID_t)]&lt;-0\n\n예시로, RN_INDI가 45461인 대상자를 살펴보겠습니다. 이 대상자는 39일째 PPI를 처방받은 사람으로, 관찰 767일째에 위염의 재발이 발생했습니다.\n\ndatatable(dat[dat$RN_INDI == 45461, ], rownames = F, caption = \"RN_INDI=45461\", options = list(scrollX = T))\n\n\n\n\n\nTreatment arm에서 45461은 artificial censor 되지 않으며, 마지막 구간인 (766,767)에서 outcome이 발생합니다.\n\ndatatable(data.long[data.long$RN_INDI == 45461, ], rownames = F, caption = \"45461 in treatment arm\", options = list(scrollX = T))\n\n\n\n\n\nControl arm에서는 치료가 시작된 39일에 protocol violation에 의해 artificial censor되며, outcome은 발생합니다.\n\ndatatable(data.long2[data.long2$RN_INDI == 45461,], rownames = F, caption = \"45461 in control arm\", options = list(scrollX = T))\n\n\n\n\n\n이제 weight 계산을 위해 data.long과 data.long2를 하나로 합칩니다.\n\n#Final dataset\ndata&lt;-rbind(data.long,data.long2)\ndata_final&lt;-merge(data,times,by=\"ID_t\",all.x=T)\ndata_final&lt;-data_final[order(data_final$ID,data_final$fup),]\n\nCensoring weight 계산\n이제 Cox model을 이용하여 구간별로 대상자가 artificial censoring 되지 않고 관찰대상으로 남아 있을 확률을 구하겠습니다. 확인하고자 하는 것은 censoring이므로, Surv 함수 안에 들어갈 event는 outcome이 아니라 censoring입니다. 보정할 covariate는 대상자별 연령(Age)과 성별(SEX)입니다. 동일한 대상자가 treatment arm에 있을 때와 control arm에 있을 때 censoring되는 여부가 다르고, raw data(ori)에서 치료를 받은 군과 받지 않은 군의 분포가 다르므로 weight는 각각의 arm에서 따로 구해야 합니다.  Treatment arm부터 확률을 구해봅니다.\n\ndata.long&lt;-data_final[data_final$arm==\"PPI\",]\n# Cox model\nms_cens&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long)\n\n데이터에서 공변량인 Age와 SEX를 추출한 행렬(design_mat)을 만들고, Cox model인 ms_cens의 회귀계수를 beta에 저장합니다. design_mat과 beta를 곱하여 lin_pred을 구합니다.\n\n#Design matrix\ndesign_mat&lt;-as.matrix(data.long[,c(\"Age\",\"SEX\")])\n#Vector of regression coefficients\nbeta&lt;-coef(ms_cens)\n  \n#Calculation of XB (linear combineation of the covariates)\ndata.long$lin_pred&lt;-design_mat%*%beta\n  \n#Estimating the cumulative hazard (when covariates=0)\ndat.base&lt;-data.frame(basehaz(ms_cens,centered=F))\nnames(dat.base)&lt;-c(\"hazard\",\"t\")\ndat.base&lt;-unique(merge(dat.base,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n\ndat.base에는 covariate(Age, SEX)가 모두 0일 때, 시간에 따른 hazard가 저장됩니다.\n\ndatatable(dat.base, rownames = F, caption = \"baseline hazard\", options = list(scrollX = T))\n\n\n\n\n\ndata.long에 시간구간에 따른 hazard를 병합하고, 대상자의 covariate 정보가 포함된 lin_pred과의 연산을 통해 해당 구간에 artificial censor되지 않고 남아 있을 확률인 P_uncens를 구합니다. 이후 outcome에 대한 Cox regression을 할 때 사용될 weight는 P_uncens의 역수입니다.\n\n#Merging and reordering the dataset\ndata.long&lt;-merge(data.long,dat.base,by=\"ID_t\",all.x=T)\ndata.long&lt;-data.long[order(data.long$RN_INDI,data.long$fup),]\ndata.long$hazard&lt;-ifelse(is.na(data.long$hazard),0,data.long$hazard)\n  \n#Estimating the probability of remaining uncensored at each time of event\ndata.long$P_uncens&lt;-exp(-(data.long$hazard)*exp(data.long$lin_pred))  \n  \n#Weights are the inverse of the probability of remaining uncensored\ndata.long$weight_Cox&lt;-1/data.long$P_uncens\n\n동일한 과정을 통해 Control arm에 대해서도 weight를 구합니다.\n\ndata.long2&lt;-data_final[data_final$arm==\"Control\",]\n  \n#Cox model\nms_cens2&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long2)\n  \n#Design matrix\ndesign_mat2&lt;-as.matrix(data.long2[,c(\"Age\",\"SEX\")])\n#Vector of regression coefficients\nbeta2&lt;-coef(ms_cens2)\n  \n#Calculation of XB (linear combineation of the covariates)\ndata.long2$lin_pred&lt;-design_mat2%*%beta2\n  \n#Estimating the cumulative hazard (when covariates=0)\ndat.base2&lt;-data.frame(basehaz(ms_cens2,centered=F))\nnames(dat.base2)&lt;-c(\"hazard\",\"t\")\n  \ndat.base2&lt;-unique(merge(dat.base2,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n#Merging and reordering the dataset\ndata.long2&lt;-merge(data.long2,dat.base2,by=\"ID_t\",all.x=T)\ndata.long2&lt;-data.long2[order(data.long2$RN_INDI,data.long2$fup),]\ndata.long2$hazard&lt;-ifelse(is.na(data.long2$hazard),0,data.long2$hazard)\n  \n#Estimating the probability of remaining uncensored at each time of event\ndata.long2$P_uncens&lt;-exp(-(data.long2$hazard)*exp(data.long2$lin_pred))\n  \n#Weights are the inverse of the probability of remaining uncensored\ndata.long2$weight_Cox&lt;-1/data.long2$P_uncens\ndata.long2$weight_Cox[data.long2$ID_t==0]&lt;-1\n\n이제 data.long과 data.long2에 대상자별, 시간 구간별로 weight_Cox 변수가 생성되었습니다.\n예를 들어, 아까 살펴본 RN_INDI가 45461인 대상자를 보겠습니다. Control arm에 할당된 이 대상자의 clone의 시간에 따른 분석 가중치 ; 해당 구간에 censor지 않고 남아 있을 확률의 역수는 아래와 같습니다.\n\ndatatable(data.long2[data.long2$RN_INDI == 45461, c(\"RN_INDI\", \"Tstart\", \"Tstop\", \"ID_t\", \"weight_Cox\")], rownames = F, caption = \"Weight for Cox model\", options = list(scrollX = T))\n\n\n\n\n\n45461 대상자의 경우에는 control arm에서만 artificial censor가 되고, treatment arm에서는 censor가 되지 않습니다. 따라서 두 arm을 합쳐서 weight를 구하게 되면 별도로 구한 결과와 비교하여 다음과 같은 차이가 발생합니다.\n\n\n\n\n\n\n\n\nCox model을 이용한 분석\n이제 가중치가 계산된 최종 데이터(data.long.Cox)를 가지고, outcome(위염 재발)에 대한 생존분석을 시행하겠습니다. 따라서 Surv 함수에 들어갈 event는 censoring이 아니라 outcome이 됩니다. Cox model에 weights가 반영됩니다.  비례위험가정이 위배될 가능성이 있어, 먼저 Kaplan-Meier cuve의 RMST(Restricted Mean Survival Time)를 이용하여 두 arm 사이의 1년 발생율을 비교해보겠습니다.\n\ndata.long.Cox&lt;-rbind(data.long,data.long2)\n\nemul_Cox_s &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1, data=data.long.Cox[data.long.Cox$arm==\"PPI\",],weights = weight_Cox)\nS1 &lt;- summary(emul_Cox_s, times = 365)$surv\nfit.tableM &lt;- summary(emul_Cox_s, rmean=365)$table\nRMST1 &lt;- fit.tableM[\"rmean\"] # Estimated RMST in the treatment arm\n  \nemul_Cox_c &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1, data=data.long.Cox[data.long.Cox$arm==\"Control\",],weights = weight_Cox)\nS0 &lt;- summary(emul_Cox_c, times = 365)$surv\nfit.tableM2 &lt;- summary(emul_Cox_c, rmean=365)$table\nRMST0 &lt;- fit.tableM2[\"rmean\"] # Estimated RMST in the control arm\n\nDiff_surv&lt;-S1-S0 #Difference in 1 year survival\nDiff_RMST&lt;-RMST1-RMST0 #Difference in RMST\n\nDiff_surv\n\n[1] -0.03980216\n\nDiff_RMST\n\n    rmean \n-8.770445 \n\n\n마지막으로, Cox model을 이용한 Hazard ratio를 구해보겠습니다.\n\nCox_w &lt;- coxph(Surv(Tstart,Tstop, outcome) ~ arm, data=data.long.Cox, weights=weight_Cox)\nHR&lt;-exp(Cox_w$coefficients)\nsummary(Cox_w)\n\nCall:\ncoxph(formula = Surv(Tstart, Tstop, outcome) ~ arm, data = data.long.Cox, \n    weights = weight_Cox)\n\n  n= 283192, number of events= 669 \n\n          coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)\narmPPI 0.27392   1.31510  0.05942   0.20553 1.333    0.183\n\n       exp(coef) exp(-coef) lower .95 upper .95\narmPPI     1.315     0.7604     0.879     1.967\n\nConcordance= 0.54  (se = 0.029 )\nLikelihood ratio test= 21.19  on 1 df,   p=4e-06\nWald test            = 1.78  on 1 df,   p=0.2\nScore (logrank) test = 21.38  on 1 df,   p=4e-06,   Robust = 1.36  p=0.2\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not).\n\nHR\n\n  armPPI \n1.315104 \n\n\nHazard ratio가 1.31(95% CI 0.879-1.967, p-value 0.183)으로 계산됩니다.\nControl arm과 treatment arm은 서로 독립적인 데이터가 아니므로, 일반적인 confidence interval을 신뢰하기 어려워 censor-weight 과정을 다음과 같이 bootstrap으로 시행할 수 있습니다. 코드의 실행 속도를 빠르게 하기 위해 boot 함수에서 CPU의 개수를 지정하여 multicore 연산을 시행할 수 있습니다.\n\nfboot &lt;- function(dat, indices) {\n  t&lt;-dat[dat$arm==\"Control\",]\n  t1&lt;-dat[dat$arm==\"PPI\",]\n  tab0 &lt;- t[indices,] # allows boot to select sample\n  select&lt;-tab0$RN_INDI\n  tab1&lt;-t1[t1$RN_INDI %in% select,] \n  tab&lt;-rbind(tab0,tab1)\n  \n  t_events&lt;-sort(unique(tab$fup))\n  times&lt;-data.frame(\"tevent\"=t_events,\"ID_t\"=seq(1:length(t_events)))\n  \n  tab_s&lt;-tab[tab$arm==\"PPI\",]\n  tab_s$Tstart&lt;-0\n  \n  data.long&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", \n                       start=\"Tstart\", event=\"outcome\",id=\"ID\") \n  data.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  \n  data.long.cens&lt;-survSplit(tab_s, cut=t_events, end=\"fup\", \n                            start=\"Tstart\", event=\"censoring\",id=\"ID\") \n  data.long.cens&lt;-data.long.cens[order(data.long.cens$ID,data.long.cens$fup),] \n\n  data.long$censoring&lt;-data.long.cens$censoring\n  \n  data.long$Tstop&lt;-data.long$fup\n  \n  data.long&lt;-merge(data.long,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\n  data.long&lt;-data.long[order(data.long$ID,data.long$fup),] \n  data.long$ID_t[is.na(data.long$ID_t)]&lt;-0\n  \n  tab_c&lt;-tab[tab$arm==\"Control\",]\n  tab_c$Tstart&lt;-0\n  \n  data.long2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", \n                        start=\"Tstart\", event=\"outcome\",id=\"ID\") \n  data.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n\n  data.long.cens2&lt;-survSplit(tab_c, cut=t_events, end=\"fup\", \n                             start=\"Tstart\", event=\"censoring\",id=\"ID\") \n  data.long.cens2&lt;-data.long.cens2[order(data.long.cens2$ID,data.long.cens2$fup),] \n  \n  data.long2$censoring&lt;-data.long.cens2$censoring\n  data.long2$Tstop&lt;-data.long2$fup\n\n  data.long2&lt;-merge(data.long2,times,by.x=\"Tstart\",by.y=\"tevent\",all.x=T)\n  data.long2&lt;-data.long2[order(data.long2$ID,data.long2$fup),] \n  data.long2$ID_t[is.na(data.long2$ID_t)]&lt;-0\n  \n  data&lt;-rbind(data.long,data.long2)\n  data_final&lt;-merge(data,times,by=\"ID_t\",all.x=T)\n  data_final&lt;-data_final[order(data_final$ID,data_final$fup),]\n  \n  data.long&lt;-data_final[data_final$arm==\"PPI\",]\n  \n  ms_cens&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long) \n  design_mat&lt;-as.matrix(data.long[,c(\"Age\",\"SEX\")])\n\n  beta&lt;-coef(ms_cens)\n  \n  data.long$lin_pred&lt;-design_mat%*%beta\n  \n  dat.base&lt;-data.frame(basehaz(ms_cens,centered=F))\n  names(dat.base)&lt;-c(\"hazard\",\"t\")\n  dat.base&lt;-unique(merge(dat.base,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n  data.long&lt;-merge(data.long,dat.base,by=\"ID_t\",all.x=T)\n  data.long&lt;-data.long[order(data.long$RN_INDI,data.long$fup),]\n  data.long$hazard&lt;-ifelse(is.na(data.long$hazard),0,data.long$hazard)\n  \n  data.long$P_uncens&lt;-exp(-(data.long$hazard)*exp(data.long$lin_pred))  \n\n  data.long$weight_Cox&lt;-1/data.long$P_uncens\n\n  data.long2&lt;-data_final[data_final$arm==\"Control\",]\n  \n  ms_cens2&lt;-coxph(Surv(Tstart, Tstop, censoring)~Age+SEX, ties=\"efron\", data=data.long2)\n  summary(ms_cens2)\n  \n  design_mat2&lt;-as.matrix(data.long2[,c(\"Age\",\"SEX\")])\n\n  beta2&lt;-coef(ms_cens2)\n  \n  data.long2$lin_pred&lt;-design_mat2%*%beta2\n  \n  dat.base2&lt;-data.frame(basehaz(ms_cens2,centered=F))\n  names(dat.base2)&lt;-c(\"hazard\",\"t\")\n  \n  \n  dat.base2&lt;-unique(merge(dat.base2,times,by.x=\"t\",by.y=\"tevent\",all.x=T))\n  \n  data.long2&lt;-merge(data.long2,dat.base2,by=\"ID_t\",all.x=T)\n  data.long2&lt;-data.long2[order(data.long2$RN_INDI,data.long2$fup),]\n  data.long2$hazard&lt;-ifelse(is.na(data.long2$hazard),0,data.long2$hazard)\n  \n  data.long2$P_uncens&lt;-exp(-(data.long2$hazard)*exp(data.long2$lin_pred))\n  \n  data.long2$weight_Cox&lt;-1/data.long2$P_uncens\n  data.long2$weight_Cox[data.long2$ID_t==0]&lt;-1\n  \n  data.long.Cox&lt;-rbind(data.long,data.long2)\n  \n  emul_Cox_s &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1,\n                        data=data.long.Cox[data.long.Cox$arm==\"PPI\",],weights = weight_Cox)\n  S1&lt;- summary(emul_Cox_s, times = 365)$surv\n  fit.tableM &lt;- summary(emul_Cox_s, rmean=365)$table\n  RMST1 &lt;- fit.tableM[\"rmean\"]\n  \n  emul_Cox_c &lt;- survfit(Surv(Tstart, Tstop, outcome) ~ 1,\n                        data=data.long.Cox[data.long.Cox$arm==\"Control\",],weights = weight_Cox)\n  S0&lt;- summary(emul_Cox_c, times = 365)$surv\n  fit.tableM2 &lt;- summary(emul_Cox_c, rmean=365)$table\n  RMST0 &lt;- fit.tableM2[\"rmean\"] \n  \n  Diff_surv&lt;-S1-S0 \n  Diff_RMST&lt;-RMST1-RMST0 \n  \n  Cox_w &lt;- coxph(Surv(Tstart,Tstop, outcome) ~ arm,\n                 data=data.long.Cox, weights=weight_Cox)\n  HR&lt;-exp(Cox_w$coefficients) \n  \n  res&lt;-c(Diff_surv,Diff_RMST,HR)\n  \n  return(res)\n  \n}\n\nresults &lt;- boot(data=dat, statistic=fboot, R=100, parallel = \"multicore\", ncpus = 40)\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = dat, statistic = fboot, R = 100, parallel = \"multicore\", \n    ncpus = 40)\n\n\nBootstrap Statistics :\n       original       bias    std. error\nt1* -0.03980216 -0.003960884  0.05664037\nt2* -8.77044466 -0.527749924  8.65111431\nt3*  1.31510388  0.026720376  0.25511206\n\n# 95% confidence intervals for each measure\nboot.ci(results,type=\"norm\",index=1) #Difference in survival\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 1)\n\nIntervals : \nLevel      Normal        \n95%   (-0.1469,  0.0752 )  \nCalculations and Intervals on Original Scale\n\nboot.ci(results,type=\"norm\",index=2) #Difference in RMST\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 2)\n\nIntervals : \nLevel      Normal        \n95%   (-25.199,   8.713 )  \nCalculations and Intervals on Original Scale\n\nboot.ci(results,type=\"norm\",index=3) #HR\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"norm\", index = 3)\n\nIntervals : \nLevel      Normal        \n95%   ( 0.788,  1.788 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#conclusion",
    "href": "posts/2024-10-17-TTE/index.html#conclusion",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Conclusion",
    "text": "Conclusion\nClone-censor-weight method는 Eligibility가 있다고 판단된 모든 환자를 clone하여 치료군과 비교군에 할당하므로 두 군 간의 비교성을 극대화할 수 있다는 장점이 있습니다. 따라서 RCT를 시행하기 어려운 환자군 또는 질환군에 대한 연구에 있어 RWD 바탕으로 RCT를 모방할 수 있습니다.  또한 immortal time biase에 대한 보정이 가능하며, censoring weight를 계산할 때 여러 공변량을 고려할 수 있어 연구대상자가 특정 시점에 특정 치료를 시행할지 여부를 반영할 수 있다는 장점이 있습니다."
  },
  {
    "objectID": "posts/2024-10-17-TTE/index.html#reference",
    "href": "posts/2024-10-17-TTE/index.html#reference",
    "title": "RWD를 이용한 RCT 모방 : Target Trial Emulation by Clone-Censor-Weight method",
    "section": "Reference",
    "text": "Reference\nFigure 1 Jiannong Liu, Eric D. Weinhandl, David T. Gilbertson, Allan J. Collins, Wendy L. St Peter, Issues regarding ‘immortal time’ in the analysis of the treatment effects in observational studies, Kidney International, Volume 81, Issue 4, 2012, Pages 341-350.\nFigure 2 Maringe, C., Benitez Majano, S., Exarchakou, A., Smith, M., Rachet, B., Belot, A., & Leyrat, C. (2020). Reflection on modern methods: trial emulation in the presence of immortal-time bias. Assessing the benefit of major surgery for elderly lung cancer patients using observational data. Int J Epidemiol, 49(5), 1719-1729.\n[3] Chen, A., Ju, C., Mackenzie, I. S., MacDonald, T. M., Struthers, A. D., Wei, L., & Man, K. K. C. (2023). Impact of beta-blockers on mortality and cardiovascular disease outcomes in patients with obstructive sleep apnoea: a population-based cohort study in target trial emulation framework. Lancet Reg Health Eur, 33, 100715.\n\n실습데이터가 필요하신 분께서는 Reply 달아주시면 데이터를 보내드리겠습니다."
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html",
    "href": "posts/2024-10-08 Survey/index.html",
    "title": "Survey design 모델에서의 통계",
    "section": "",
    "text": "이 문서에서는 Survey 모델에 대한 개요와 이를 jstable 패키지, forestploter package를 사용하여 필요한 모형들을 제작하는 방법을 알아보겠습니다"
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#survey-model의-특징",
    "href": "posts/2024-10-08 Survey/index.html#survey-model의-특징",
    "title": "Survey design 모델에서의 통계",
    "section": "Survey model의 특징",
    "text": "Survey model의 특징\n층화\n국민건강영양조사의 경우 무작위로 표본을 추출하지 않고, 층화, 군집화, 가중치 등의 방법을 사용하여 표본을 추출합니다. 층화에 대해 먼저 알아보겠습니다. 인구 집단을 성별, 연령대, 지역 등으로 층화하여 각 층에서 독립적으로 표본을 추출합니다. 이렇게 하는 경우 특정 집단이 과소 또는 과대 대표되는 문제를 방지할 수 있습니다.\n군집화\n가구나 지역단위로 표본을 추출하는 군집화 방법또한 사용합니다. 군집화된 데이터는 같은 군집 내의 사람들 간에 상관성이 존재할 수 있기 때문에, 이를 반영할 수 있도록 데이터를 추출하여 사용합니다.\n가중치\n각 표본을 추출한 이후에는 그 표본이 전체 인구를 어느정도 반영하는지를 보는 가중치를 제공하여야합니다. 특정 연령대나 지역에서 표본이 과대 추출되는 경우 가중치를 낮추는 등의 방법으로 전체 인구를 보다 정확히 대변할 수 있습니다.\n유한 모집단 보정\n이는 국민건강영양조사와 같이 전체 인구집단과 표본의 차이가 큰 경우에는 크게 중요하지 않을 수도 있으나, 표본의 크기가 전체 모집단에 비해 클 때 모집단이 가지는 변동성을 조정할 필요하 있습니다. 따라서 이런 경우 Survey 모델에서는 보정이 필요합니다. FPC를 구하는 공식을 보시면 모집단이 커질 수록 변동성이 커진다는 것을 알 수 있습니다.\n\\[\nFPC = \\sqrt{\\frac{N - n}{N - 1}}\n\\]"
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#survey-모델을-가지고-모형-만들기",
    "href": "posts/2024-10-08 Survey/index.html#survey-모델을-가지고-모형-만들기",
    "title": "Survey design 모델에서의 통계",
    "section": "Survey 모델을 가지고 모형 만들기",
    "text": "Survey 모델을 가지고 모형 만들기\nSurvey 모델을 통한 선형회귀 분석\n간단하게 국민건강영양조사에서 공개된 자료로 선형회귀 분석을 직접 진행해보겠습니다. 이 포스트에서 다루는 통계는 코드 실행과 통계학적 모형 설명을 위해 사용하는 것임으로, 엄격한 inclusion, exclusion criteria 과정 등을 적용하지 않아 결과는 현실과 상이할 수 있음을 미리 밝힙니다.\n국민건강영양조사의 2012년 안검사 데이터를 사용해보겠습니다. 국민건강영양조사 사이트에서 데이터를 다운 받은 이후 haven, data.table 라이브러리를 사용하여 자료를 datatable로 바꾸겠습니다.\n\nlibrary(survey)\nlibrary(haven)\nsas_data &lt;- read_sas(\"hn12_eye.sas7bdat\")\nhn12&lt;-data.table(sas_data)\n\n이후 교육, 성별, 나이에 따른 백내장 진단 여부를 알아보도록 하겠습니다. Survey 모델의 경우, 앞서 말씀드린 특징들로 인해 데이터를 사용하기 이전에 Survey design모델이 필요합니다.\n\nhn12_cat&lt;- hn12[E_DH2_dg %in% c(1,2) &!is.na(edu)&!is.na(sex)&!is.na(age)]\nhn12_cat$E_DH2_dg[hn12_cat$E_DH2_dg == 2] &lt;- 0\nsurvey_hn12 &lt;- svydesign(id = ~psu, strata = ~kstrata, weights = ~wt_itvex, data = hn12_cat)\n\n분석을 위해 간단하게, 성별, 나이, 교육에서 결측치를 제거하였고 백내장 진단 여부에 대해서는 무응답과 같은 변수를 제거하고 진단 여부를 (1.예,2.아니오)만 사용하기로 하였고, 0,1을 가지는 이항변수로 만들기 위해 2.아니오는 0으로 코딩하는 과정을 거쳤습니다.\n모델 디자인의 경우, svydesign이라는 함수를 사용하게 되는데, 표본조사를 제공하는 곳에서 보통 id, strata, weights에 사용할 수 있는 변수들에 대한 설명이 같이 첨부되어있습니다. 국민건강영양조사에서 제공하는 안대에 따라 psu, kstrata, wt_itvex변수를 코딩하였습니다. 앞서 말씀드린바와 같이 국민건강영양조사에서는 전체 인구가 표본에 비해 크기 때문에 fpc 변수가 필요하지 않지만, 모델에 따라 fpc변수를 svydesign에 추가할 수 있습니다. 백내장 진단여부를 이항변수로 코딩했기 때문에 앞선 포스트에서 살펴본 glm모델을 사용하여 데이터를 살펴보도록 하겠습니다. 이후 jstable 패키지를 이용해 glm모델을 살펴볼 수 있는 테이블을 만들어보겠습니다\n\nds&lt;-svyglm(E_DH2_dg~age+as.factor(edu)+as.factor(sex), design=survey_hn12, family =quasibinomial())\nsvyregress.display(ds)\n\n$first.line\n[1] \"Logistic regression predicting E_DH2_dg- weighted data\\n\"\n\n$table\n                       crude OR.(95%CI)   crude P value adj. OR.(95%CI)   \nage                    \"1.15 (1.13,1.16)\" \"&lt; 0.001\"     \"1.15 (1.13,1.16)\"\nas.factor(edu): ref.=1 NA                 NA            NA                \nas.factor(edu)2        \"0.3 (0.22,0.41)\"  \"&lt; 0.001\"     \"0.86 (0.58,1.3)\" \nas.factor(edu)3        \"0.19 (0.14,0.27)\" \"&lt; 0.001\"     \"1 (0.68,1.46)\"   \nas.factor(edu)4        \"0.17 (0.11,0.24)\" \"&lt; 0.001\"     \"1.22 (0.78,1.91)\"\nas.factor(sex): 2 vs 1 \"1.71 (1.39,2.1)\"  \"&lt; 0.001\"     \"1.47 (1.12,1.93)\"\n                       adj. P value\nage                    \"&lt; 0.001\"   \nas.factor(edu): ref.=1 NA          \nas.factor(edu)2        \"0.479\"     \nas.factor(edu)3        \"0.997\"     \nas.factor(edu)4        \"0.392\"     \nas.factor(sex): 2 vs 1 \"0.006\"     \n\n$last.lines\n[1] \"No. of observations = 3891\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n테이블의 결과를 살펴 보았을 때 나이는 통계적으로 유의미하게 백내장의 발생과 관련이 있고, 교육수준은 거의 없는 것으로 알 수 있습니다. 성별에 따른 차이도 p-value가 0.05보다는 크지만 0.06으로 통계적으로 유의미하지는 않지만 조금의 관련성이 있다는 것을 알 수 있습니다. 이번에는 strata별로 볼 수 있는 table을 만들어보도록 하겠습니다.\n\nhn12_cat$E_DH2_dg&lt;-as.numeric(hn12_cat$E_DH2_dg)\nhn12_cat$age&lt;-as.numeric(hn12_cat$age)\nsvyCreateTableOne2(strata = 'sex', vars = c('E_DH2_dg','age'), data = survey_hn12)\n\n          level 1                      2                      p        test\nn         \"\"    \"10311810.38\"          \"11404537.11\"          \"\"       \"\"  \nE_DH2_dg  \"\"    \"       0.09 ± 0.29\"  \"       0.15 ± 0.36\"  \"&lt;0.001\" \"\"  \nage       \"\"    \"      54.92 ± 10.74\" \"      56.64 ± 11.52\" \"&lt;0.001\" \"\"  \n          sig \nn         NA  \nE_DH2_dg  \"**\"\nage       \"**\"\n\nsvyCreateTableOneJS(strata = 'sex', strata2 = 'edu', vars = c('E_DH2_dg','age'), data = survey_hn12)\n\n$table\n          level 1                    2                    3                   \nn         \"\"    \"2112878.74\"         \"1499859.85\"         \"3737738.11\"        \nE_DH2_dg  \"\"    \"      0.16 ± 0.37\" \"      0.10 ± 0.30\" \"      0.08 ± 0.27\"\nage       \"\"    \"     63.35 ± 9.70\" \"     57.76 ± 9.59\" \"     52.83 ± 9.84\"\n          4                    p        test sig  1                   \nn         \"2961333.68\"         \"\"       \"\"   NA   \"4381837.88\"        \nE_DH2_dg  \"      0.06 ± 0.23\" \"&lt;0.001\" \"\"   \"**\" \"      0.30 ± 0.46\"\nage       \"     50.09 ± 9.11\" \"&lt;0.001\" \"\"   \"**\" \"     66.23 ± 9.29\"\n          2                    3                    4                   \nn         \"1694071.75\"         \"3575372.10\"         \"1753255.37\"        \nE_DH2_dg  \"      0.09 ± 0.28\" \"      0.05 ± 0.21\" \"      0.05 ± 0.21\"\nage       \"     55.89 ± 7.87\" \"     49.75 ± 7.83\" \"     47.47 ± 7.20\"\n          p        test sig \nn         \"\"       \"\"   NA  \nE_DH2_dg  \"&lt;0.001\" \"\"   \"**\"\nage       \"&lt;0.001\" \"\"   \"**\"\n\n$caption\n[1] \"Stratified by sex() & edu- weighted data\"\n\n\nsvyCreateTableOne2같은 경우에는 보다 단순한 데이터에서 하나의 층으로 테이블을 만들고 싶을 때 사용할 수 있으며, svyCreateTableOneJS의 경우에는 층화를 두 층 이상, 그리고 조금 더 다양한 옵션들이 가능합니다. 유사한 방식으로 진단여부와 같은 이항분포뿐만 아니라 안압과 같은 연속변수에도 적용이 가능합니다. 녹내장 환자를 에시로 하면 아래와 같이 할 수 있습니다.\n\nhn12_glau&lt;-hn12[!(E_Gr_p%in% c(NA, 888, 999))&!is.na(E_Gl_p)&!is.na(edu)&!is.na(sex)&!is.na(age)]\nsurvey_hn12 &lt;- svydesign(id = ~psu, strata = ~kstrata, weights = ~wt_itvex, data = hn12_glau)\nds2&lt;-svyglm(E_Gr_p~age+as.factor(edu)+as.factor(sex), design=survey_hn12)\nsvyregress.display(ds2)\n\n$first.line\n[1] \"Linear regression predicting E_Gr_p- weighted data\\n\"\n\n$table\n                       crude coeff.(95%CI)   crude P value\nage                    \"0.01 (0,0.01)\"       \"0.142\"      \nas.factor(edu): ref.=1 NA                    NA           \nas.factor(edu)2        \"0.11 (-0.26,0.48)\"   \"0.552\"      \nas.factor(edu)3        \"0.06 (-0.27,0.38)\"   \"0.725\"      \nas.factor(edu)4        \"0 (-0.38,0.38)\"      \"0.995\"      \nas.factor(sex): 2 vs 1 \"-0.32 (-0.52,-0.11)\" \"0.002\"      \n                       adj. coeff.(95%CI)    adj. P value\nage                    \"0.01 (0,0.02)\"       \"0.059\"     \nas.factor(edu): ref.=1 NA                    NA          \nas.factor(edu)2        \"0.16 (-0.22,0.54)\"   \"0.399\"     \nas.factor(edu)3        \"0.22 (-0.18,0.62)\"   \"0.287\"     \nas.factor(edu)4        \"0.16 (-0.31,0.63)\"   \"0.496\"     \nas.factor(sex): 2 vs 1 \"-0.31 (-0.51,-0.12)\" \"0.002\"     \n\n$last.lines\n[1] \"No. of observations = 5339\\nAIC value = 27920.3506\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\nSurvey 모델을 통한 생존 분석\n이번에는 미국의 국민건강영양조사의 데이터를 가지고 생존분석을 진행해보도록 하겠습니다. 사이트에서 Mortality, 와 기본 조사 데이터를 다운 받은 이후, 이전과 유사한 방법으로 제공된 weight, id, strata를 통해 design모델을 만들어보겠습니다.\n\nlibrary(readr)\ndemo &lt;- read_xpt(\"DEMO_J.XPT\")\nmortality &lt;- read_fwf(\"NHANES_2017_2018_MORT_2019_PUBLIC.dat\",\n         col_types = \"iiiiiiii\",\n         fwf_cols(SEQN = c(1,6),\n                  ELIGSTAT = c(15,15),\n                  MORTSTAT = c(16,16),\n                  UCOD_LEADING = c(17,19),\n                  DIABETES = c(20,20),\n                  HYPERTEN = c(21,21),\n                  PERMTH_INT = c(43,45),\n                  PERMTH_EXM = c(46,48)),\n         na = c(\"\", \".\")\n)\nhead(mortality)\n\n# A tibble: 6 × 8\n   SEQN ELIGSTAT MORTSTAT UCOD_LEADING DIABETES HYPERTEN PERMTH_INT PERMTH_EXM\n  &lt;int&gt;    &lt;int&gt;    &lt;int&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n1 93703        2       NA           NA       NA       NA         NA         NA\n2 93704        2       NA           NA       NA       NA         NA         NA\n3 93705        1        0           NA       NA       NA         18         18\n4 93706        1        0           NA       NA       NA         35         34\n5 93707        2       NA           NA       NA       NA         NA         NA\n6 93708        1        0           NA       NA       NA         19         18\n\nmortality$PERMTH_EXM[mortality$PERMTH_EXM == \".\"] &lt;- NA\nmortality$PERMTH_EXM &lt;- as.numeric(mortality$PERMTH_EXM)\nmortality$MORTSTAT &lt;- as.numeric(mortality$MORTSTAT)\nmerged_data &lt;- merge(demo, mortality, by = \"SEQN\")\nmerged_data_clean &lt;- merged_data[complete.cases(merged_data[, c(\"MORTSTAT\", \"PERMTH_EXM\", \"RIDAGEYR\", \"DMDEDUC2\", \"RIAGENDR\",'INDFMIN2')]), ]\nsurvey_nhanes &lt;- svydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, data = merged_data_clean, nest = TRUE)\n\n이번에는 생존 분석을 진행할 예정이기 때문에 사망여부인 변수 MORTSTAT과 관찰기관 변수인 PERMTH_EXM을 사용하여 survey cox모델을 만들어보겠습니다. 기존의 Cox모델과 함수 작성방법이 유사하며 svycoxph함수를 사용하여, 사망이벤트와 나이(RIDAGEYR), 교육(DMDEDUC2), 성별(RIAGENDR), 가정소득수준(IDFMIN2)의 관련성을 알아보겠습니다.\n\nds3 &lt;- svycoxph(Surv(PERMTH_EXM, MORTSTAT) ~ RIDAGEYR + DMDEDUC2 + RIAGENDR+INDFMIN2, design = survey_nhanes)\nsvycox.display(ds3)\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (30) clusters.\nsvydesign(id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR, \n    data = merged_data_clean, nest = TRUE)\n\n\n$table\n         crude HR(95%CI)    crude P value adj. HR(95%CI)     adj. P value\nRIDAGEYR \"1.08 (1.06,1.1)\"  \"&lt; 0.001\"     \"1.08 (1.06,1.1)\"  \"&lt; 0.001\"   \nDMDEDUC2 \"0.74 (0.6,0.9)\"   \"0.003\"       \"0.79 (0.67,0.93)\" \"0.005\"     \nRIAGENDR \"0.68 (0.38,1.19)\" \"0.172\"       \"0.59 (0.35,1)\"    \"0.05\"      \nINDFMIN2 \"0.98 (0.92,1.03)\" \"0.404\"       \"0.98 (0.95,1.02)\" \"0.356\"     \n\n$metric\n                       [,1] [,2] [,3] [,4]\n&lt;NA&gt;                     NA   NA   NA   NA\nNo. of observations 4991.00   NA   NA   NA\nNo. of events        124.00   NA   NA   NA\nAIC                 1401.96   NA   NA   NA\n\n$caption\n[1] \"Survey cox model on time ('PERMTH_EXM') to event ('MORTSTAT')\"\n\n\n결과를 대략 살펴보면 조정 이후 나이와 성별 교육 수준이 모두 통게적으로 유의미하게 관련이 있다는 것을 확인할 수 있습니다. 이번에는 인종, 나이(50세 이상 이하), 미국 시민권(Naturalized여부) 등의 그룹에 대해 subgroup analysis를 진행해보도록 하겠습니다.\n\nmerged_data_mod&lt;- merged_data %&gt;% \n  mutate(gender = factor(RIAGENDR, labels = c(\"Male\", \"Female\")),\n         MORTSTAT = as.numeric(MORTSTAT == 1),\n         age_group = ifelse(RIDAGEYR &gt;= 50, \"Over 50\", \"Under 50\"),\n         race_ethnicity = factor(case_when(\n           RIDRETH1 == 1 | RIDRETH1 == 2 ~ \"Hispanic\",\n           RIDRETH1 == 3 ~ \"Non-Hispanic White\",\n           RIDRETH1 == 4 ~ \"Non-Hispanic Black\")),\n         citizenship_status = factor(case_when(\n           DMDCITZN == 1 ~ \"U.S. Citizen\",\n           DMDCITZN == 2 ~ \"Naturalized Citizen\"\n         )),\n  )\nmerged_data_mod&lt;-merged_data_mod[complete.cases(merged_data_mod[, c(\"MORTSTAT\", \"PERMTH_EXM\", \"RIDAGEYR\",  \"RIAGENDR\",\"RIDRETH1\", \"DMDCITZN\",\"age_group\", \"race_ethnicity\", \"citizenship_status\")]), ]\nTableSubgroupMultiCox(Surv(PERMTH_EXM, MORTSTAT) ~ gender, var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, time_eventrate = 365 , line = FALSE)\n\n                     Variable Count Percent Point Estimate Lower Upper\ngender                Overall  4413     100           0.65  0.45  0.94\n1                   age_group  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n2                     Over 50  2387    54.1           0.69  0.47     1\n3                    Under 50  2026    45.9           0.59   0.1  3.56\n4              race_ethnicity  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n5                    Hispanic  1250    28.3           1.79  0.54  5.94\n6          Non-Hispanic Black  1266    28.7           0.55  0.26  1.17\n7          Non-Hispanic White  1897      43           0.61  0.38  0.96\n8          citizenship_status  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n9         Naturalized Citizen   504    11.4           2.46  0.26 23.63\n10               U.S. Citizen  3909    88.6           0.63  0.43  0.92\n       gender=Male gender=Female P value P for interaction\ngender         4.8           2.7   0.022              &lt;NA&gt;\n1             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.869\n2              8.5           4.9    0.05              &lt;NA&gt;\n3              0.5           0.3   0.569              &lt;NA&gt;\n4             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.202\n5              0.7           1.3   0.342              &lt;NA&gt;\n6              4.1           2.3   0.122              &lt;NA&gt;\n7              8.2             4   0.035              &lt;NA&gt;\n8             &lt;NA&gt;          &lt;NA&gt;    &lt;NA&gt;             0.232\n9              0.4           1.2   0.436              &lt;NA&gt;\n10             5.5           2.9   0.016              &lt;NA&gt;\n\nTableSubgroupMultiGLM(MORTSTAT ~ gender,var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, family = \"binomial\")\n\n                           Variable Count Percent   OR Lower Upper P value\ngenderFemale                Overall  4413     100 0.64  0.44  0.93    0.02\n1                         age_group  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n2                           Over 50  2387    54.1 0.67  0.46  0.99   0.043\n3                          Under 50  2026    45.9 0.58   0.1   3.5   0.557\n4                    race_ethnicity  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n5                          Hispanic  1250    28.3  1.8  0.54     6    0.34\n6                Non-Hispanic Black  1266    28.7 0.54  0.25  1.16   0.114\n7                Non-Hispanic White  1897      43 0.59  0.37  0.95    0.03\n8                citizenship_status  &lt;NA&gt;    &lt;NA&gt; &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;\n9               Naturalized Citizen   504    11.4 2.49  0.26 24.15    0.43\n10                     U.S. Citizen  3909    88.6 0.62  0.42  0.91   0.014\n             P for interaction\ngenderFemale              &lt;NA&gt;\n1                        0.881\n2                         &lt;NA&gt;\n3                         &lt;NA&gt;\n4                        0.192\n5                         &lt;NA&gt;\n6                         &lt;NA&gt;\n7                         &lt;NA&gt;\n8                        0.235\n9                         &lt;NA&gt;\n10                        &lt;NA&gt;\n\n\n이번에는 TableSubgroupMultiCox 함수를 사용하면, 원하는 테이블을 얻을 수 있다는 것을 알 수 있습니다. Subgroup별로 남성과 여성의 사망률의 차이를 직관적이게 table로 확인할 수 있습니다. Subgroup analysis의 경우 하지만, forestplot도 필요한 경우들이 많기 때문에 forestploter 패키지를 이용하여 만들어보도록 하겠습니다."
  },
  {
    "objectID": "posts/2024-10-08 Survey/index.html#forestploter를-이용한-forestplot만들기",
    "href": "posts/2024-10-08 Survey/index.html#forestploter를-이용한-forestplot만들기",
    "title": "Survey design 모델에서의 통계",
    "section": "Forestploter를 이용한 forestplot만들기",
    "text": "Forestploter를 이용한 forestplot만들기\n우선, 필요한 경우 forestploter를 R에서 설치합니다(Link Text에서 더 상세한 사용법에 대해서 알 수 있습니다) 이후 TableSubgroupMultiCox 함수를 이용하여 데이터를 불러온 이후, 변수명을 제외한 나머지 행들을 numeric으로 바꿔줍니다.\n\nlibrary(grid)\nlibrary(forestploter)\ndf&lt;- TableSubgroupMultiCox(Surv(PERMTH_EXM, MORTSTAT) ~ gender, var_subgroups = c(\"age_group\", \"race_ethnicity\", \"citizenship_status\"), data = merged_data_mod, line = FALSE)\ndf&lt;-data.table(df)\nnum_cols&lt;- names(df)[-c(1)]\ndf[, (num_cols) := lapply(.SD, as.numeric), .SDcols = num_cols]\n\n이후 가독성을 위해 subgroup 변수들은 띄어쓰기를 진행하고, NA 값들은 빈칸으로 바꾸어줍니다. 또한 표준 오차값을 변수를 통해 지정하여 주고, plot의 line을 그리기 위한 공간을 테이블에 확보해줍니다. HR값 또한 변수내에서 계산을 통해 행을 추가합니다. 이후 원하는 테마를 지정한 이후, 위와 같이 코드를 실행한다면 forest plot을 얻을 수 있습니다.\n\ndf$Variable &lt;- ifelse(is.na(df$Count), \n                      df$Variable,\n                      paste0(\"   \", df$Variable))\ndf$Count &lt;- ifelse(is.na(df$Count), \"\", df$Count)\ndf$se &lt;- (log(df$Upper) - log(df$'Point Estimate'))/1.96\ndf$` ` &lt;- paste(rep(\" \", 20), collapse = \" \")\ndf$`HR (95% CI)` &lt;- ifelse(is.na(df$se), \"\",\n                           sprintf(\"%.2f (%.2f to %.2f)\",\n                                   df$'Point Estimate', df$Lower, df$Upper))\ntm &lt;- forest_theme(base_size = 10,\n                   refline_col = \"red\",\n                   arrow_type = \"closed\",\n                   footnote_gp = gpar(col = \"blue\", cex = 0.6))\n\nrefline_col will be deprecated, use refline_gp instead.\n\np &lt;- forest(df[,c(1:2, 12:13)],\n            est = df$'Point Estimate',\n            lower = df$Lower, \n            upper = df$Upper,\n            sizes = df$se,\n            ci_column = 3,\n            ref_line = 1,\n            arrow_lab = c(\"Female Better\", \"Male Better\"),\n            xlim = c(0, 4),\n            ticks_at = c(0.5, 1, 2, 3),\n            footnote = \"Example data using NHANES\",\n            theme = tm)\nplot(p)"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "",
    "text": "2x2 table에서 민감도와 특이도는 본래 McNemar’s test를 통해 \\(z = \\frac{(f_{21} - f_{12})}{\\sqrt{f_{12} + f_{21}}}\\)로 정의된 z값의 p-value를 확인하는 방법으로 진행되었습니다. 하지만 위 방식으로는 민감도와 특이도의 신뢰 구간에 대한 정보를 알 수 없기에, 신뢰 구간까지 정할 수 있는 방법인 Wald confidence interval을 적용함으로써 더 많은 정보를 얻을 수 있습니다."
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#분산의-기본-식",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#분산의-기본-식",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.1. 분산의 기본 식",
    "text": "2.1. 분산의 기본 식\n두 확률 변수 \\(p_1\\)과 \\(p_2\\)의 차이의 분산은 다음과 같이 계산됩니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\text{Var}(p_2) + \\text{Var}(p_1) - 2\\text{Cov}(p_2, p_1)\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#각-분산의-계산",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#각-분산의-계산",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.2. 각 분산의 계산",
    "text": "2.2. 각 분산의 계산\n우리는 \\(p_1\\)과 \\(p_2\\)를 비율로 간주할 수 있습니다. 이 비율들의 분산은 다음과 같이 계산됩니다:\n\\[\n\\text{Var}(p_1) = \\frac{p_1(1 - p_1)}{n} = \\frac{p_{12} + p_{11}}{n} \\cdot \\left(1 - \\frac{p_{12} + p_{11}}{n}\\right)\n\\]\n\\[\n\\text{Var}(p_2) = \\frac{p_2(1 - p_2)}{n} = \\frac{p_{21} + p_{11}}{n} \\cdot \\left(1 - \\frac{p_{21} + p_{11}}{n}\\right)\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#공분산-textcovp_2-p_1-의-계산",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#공분산-textcovp_2-p_1-의-계산",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.3. 공분산 \\(\\text{Cov}(p_2, p_1)\\) 의 계산",
    "text": "2.3. 공분산 \\(\\text{Cov}(p_2, p_1)\\) 의 계산\n\\(p_2\\) 와 \\(p_1\\) 의 공분산은 다음과 같이 정의됩니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\text{Cov}(p_{11} + p_{21}, p_{11} + p_{12})\n\\]\n이를 확장하면 다음과 같은 네 가지 항으로 분리할 수 있습니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\text{Cov}(p_{11}, p_{11}) + \\text{Cov}(p_{11}, p_{12}) + \\text{Cov}(p_{21}, p_{11}) + \\text{Cov}(p_{21}, p_{12})\n\\]\n공분산은 다음과 같이 정리할 수 있습니다.\n\\[\n\\text{Var}(p_{11}) = \\frac{p_{11}(1 - p_{11})}{n}\n\\]\n\\[\n\\text{Cov}(p_{11}, p_{12}) = -\\frac{p_{11}p_{12}}{n}\n\\]\n\\[\n\\text{Cov}(p_{11}, p_{21}) = -\\frac{p_{11}p_{21}}{n}\n\\]\n\\[\n\\text{Cov}(p_{21}, p_{12}) = -\\frac{p_{21}p_{12}}{n}\n\\]\n이를 바탕으로 \\(\\text{Cov}(p_2, p_1)\\)를 계산하면:\n\\[\n\\text{Cov}(p_2, p_1) = \\frac{p_{11}(1 - p_{11} - p_{12} - p_{21}) - p_{21}p_{12}}{n}\n\\]\n여기서 \\(p_{11} + p_{12} + p_{21} + p_{22} = 1\\)임을 이용하여, 공분산을 다음과 같이 단순화할 수 있습니다:\n\\[\n\\text{Cov}(p_2, p_1) = \\frac{p_{11}p_{22} - p_{21}p_{12}}{n}\n\\]"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#최종적으로-textvarp_2---p_1-의-유도",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#최종적으로-textvarp_2---p_1-의-유도",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "2.4. 최종적으로 \\(\\text{Var}(p_2 - p_1)\\) 의 유도",
    "text": "2.4. 최종적으로 \\(\\text{Var}(p_2 - p_1)\\) 의 유도\n이제 분산 공식을 대입하여 최종적으로 \\(p_2 - p_1\\)의 분산을 계산합니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\text{Var}(p_2) + \\text{Var}(p_1) - 2\\text{Cov}(p_2, p_1)\n\\]\n따라서 \\(p_2 - p_1\\)의 분산은 다음과 같습니다:\n\\[\n\\text{Var}(p_2 - p_1) = \\frac{(p_{12} + p_{21}) - (p_{21} - p_{12})^2/n}{n}\n\\]\n\\[\n\\text{Var}(p_2 - p_1) = \\frac{(b + c) - (b - c)^2/n}{n}\n\\]\n이를 적용한 민감도의 차이에 대한 신뢰 구간은 다음과 같이 계산됩니다. \\[\n\\ CI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm z_{1-\\alpha/2} \\cdot \\frac{1}{n} \\cdot \\sqrt{b + c - \\frac{(b - c)^2}{n}} \\right] \\\n\\]\n\n  if ( (ci.method == \"wald\") & (cont.corr == FALSE) ) {\n    # sensitivity\n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    sens.diff.se &lt;- sqrt((b+c) - ((b-c)**2) / n) / n\n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}\n\n만약 이항분포로 얻어진 값을 정규분포에 맞도록 조정하기 위해 continuity correction을 진행한다면 wald에서는 확률 \\(p\\) 하나당 \\(\\frac{1}{2n}\\)만큼 분산을 늘려, 아래와 같은 값을 갖게 된다.\n\\[\nCI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm \\left( z_{1-\\alpha/2} \\cdot \\frac{1}{n} \\sqrt{b + c - \\frac{(b - c)^2}{n} }+ \\frac{1}{n} \\right) \\right].\n\\]\n\n  if ( (ci.method == \"wald\") & (cont.corr == TRUE) ) {\n    # sensitivity\n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    sens.diff.se &lt;- (sqrt((b+c) - ((b-c)**2) / n) / n) + 1/n\n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#agresti-신뢰-구간agresti-ci",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#agresti-신뢰-구간agresti-ci",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "3.1. Agresti 신뢰 구간(Agresti CI)",
    "text": "3.1. Agresti 신뢰 구간(Agresti CI)\nWald 신뢰 구간을 수정한 방법 중 하나로, 샘플 크기에 특정 상수를 더한 후 위 wald와 같은 방식으로 전체 샘플 크기 𝑛에 1에서 4까지의 상수를 더한 후 이 방법들을 비교했습니다. 그 결과, n+2를 사용하는 것이 표준 Wald 신뢰 구간과 비교했을 때 표본 증가의 효과로 신뢰 구간의 성능(coverage probability)을 향상시킴을 확인했습니다. \\[\n\\ CI_{1-\\alpha/2}(\\hat{\\theta}) = \\left[ \\hat{\\theta} \\pm z_{1-\\alpha/2} \\cdot \\frac{1}{n+2} \\cdot \\sqrt{(b+0.5) + (c+0.5) - \\frac{(b - c)^2}{n+2}} \\right] \\\n\\]\n\n  if (ci.method == \"agresti-min\") {\n    k &lt;- 0.5\n    # sensitivity    \n    b &lt;- tab$diseased[1,2]+k; c &lt;- tab$diseased[2,1]+k; n &lt;- tab$diseased[3,3]+4*k\n    sens.diff.se &lt;- (sqrt((b+c) - ((b-c)**2) / n) / n) \n    sens.diff.cl &lt;- sens.diff + c(-1,1) * qnorm(1-alpha/2) * sens.diff.se}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#tango",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#tango",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "3.2. Tango",
    "text": "3.2. Tango\nTango는 귀무가설과 함수는 두 비율 간의 차이와, 주어진 정보 행렬(\\(b\\), \\(c\\), \\(n\\))을 기반으로 likelihood를 계산해 신뢰구간을 계산하는 방법입니다. R에서는 scoreci.mp라는 별도의 함수를 사용해 진행합니다.\n\n  if (ci.method == \"tango\") {\n    # sensitivity    \n    b &lt;- tab$diseased[1,2]; c &lt;- tab$diseased[2,1]; n &lt;- tab$diseased[3,3]\n    tango &lt;- scoreci.mp(b, c, n, conf.level=1-alpha)    \n    sens.diff.se &lt;- NA    \n    sens.diff.cl &lt;- sort(c(tango$conf.int[1], tango$conf.int[2]))\n    if ( (tango$conf.int[1] &gt; sens.diff) | (tango$conf.int[2] &lt; sens.diff))\n      sens.diff.cl &lt;- sort(-1*sens.diff.cl)}"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#일반적인-민감도-특이도-검정",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#일반적인-민감도-특이도-검정",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "4.1. 일반적인 민감도, 특이도 검정",
    "text": "4.1. 일반적인 민감도, 특이도 검정\n\nlibrary(DTComPair)\n\nLoading required package: PropCIs\n\nt1 &lt;- read.tab.paired(18, 14, 0, 18,\n                      18, 12, 2, 18)\nt1\n\nTwo binary diagnostic tests (paired design)\n\nTest1: 'Noname 1'\nTest2: 'Noname 2'\n\nDiseased:\n           Test1 pos. Test1 neg. Total\nTest2 pos.         18         14    32\nTest2 neg.          0         18    18\nTotal              18         32    50\n\nNon-diseased:\n           Test1 pos. Test1 neg. Total\nTest2 pos.         18         12    30\nTest2 neg.          2         18    20\nTotal              20         30    50\n\nsesp.diff.ci(t1, ci.method=\"wald\", cont.corr=FALSE)\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.06349803 0.15554615 0.40445385 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.06928203 -0.33579029 -0.06420971 \n\n$ci.method\n[1] \"wald\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\nsesp.diff.ci(t1, ci.method=\"wald\", cont.corr=TRUE)\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.08349803 0.11634687 0.44365313 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.08928203 -0.37498957 -0.02501043 \n\n$ci.method\n[1] \"wald\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] TRUE\n\nsesp.diff.ci(t1, ci.method=\"agresti-min\")\n\n$sensitivity\n     test1      test2       diff    diff.se   diff.lcl   diff.ucl \n0.36000000 0.64000000 0.28000000 0.06444681 0.15368658 0.40631342 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000  0.06954236 -0.33630053 -0.06369947 \n\n$ci.method\n[1] \"agresti-min\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\nsesp.diff.ci(t1, ci.method=\"tango\")\n\n$sensitivity\n    test1     test2      diff   diff.se  diff.lcl  diff.ucl \n0.3600000 0.6400000 0.2800000        NA 0.1747417 0.4166512 \n\n$specificity\n      test1       test2        diff     diff.se    diff.lcl    diff.ucl \n 0.60000000  0.40000000 -0.20000000          NA -0.34470882 -0.06111243 \n\n$ci.method\n[1] \"tango\"\n\n$alpha\n[1] 0.05\n\n$cont.corr\n[1] FALSE\n\n\n아래와 같은 방법으로 answer, test1, test2를 열로 가지고, 각 결과 데이터가 1,0(1은 diseased sample, 2는 non-diseased sample)로 표시되어 있는 경우에는 바로 paired table을 제작할 수 있습니다.\n\nlibrary(DTComPair)\ntb &lt;- tab.paired(answer, test1, test2, data = na.omit(sample_data))"
  },
  {
    "objectID": "posts/2024-08-26-sensspec-waldinterval/index.html#민감도-특이도의-비열등성-검정",
    "href": "posts/2024-08-26-sensspec-waldinterval/index.html#민감도-특이도의-비열등성-검정",
    "title": "Wald Confidence Interval for a Difference of Binomial Proportions Based on Paired Data",
    "section": "4.2. 민감도, 특이도의 비열등성 검정",
    "text": "4.2. 민감도, 특이도의 비열등성 검정\n어떤 테스트의 민감도와 특이도가 대조 테스트에 비해 떨어지지 않는다는 것을 확인하기 위해서는 비열등성 검정을 사용해야 합니다. 이 때는 sesp.diff.ci 함수를 통해 구한 standard error 값과 두 민감도/특이도의 차이를 통해 p value를 계산합니다. 아래 예시에서는 민감도에 대한 비열등성 마진(sens_margin)을 5%, 특이도에 대한 비열등성 마진(spec_margin)을 10%로 설정했습니다.\n민감도를 예로 들자면 귀무가설은 아래와 같고, 아래 수식에 따라 p value를 계산합니다.\n\\[H_0 \\colon \\text{test1 sensitivity} - \\text{test2 sensitivity} \\leq -\\text{ sensitivity margin}\\] \\[p-value = 1 - Φ\\left(\\frac{\\text{test1 sensitivity} - \\text{test2 sensitivity} + \\text{sensitivity margin}}{\\text{sensitivity diff.SE}}\\right)\\]\n\nlibrary(DTComPair)\nt1 &lt;- read.tab.paired(18, 14, 0, 18,\n                      18, 12, 2, 18)\nt1.wald &lt;- sesp.diff.ci(t1, ci.method=\"wald\", cont.corr=FALSE)\n\nsens_margin &lt;-  0.05\nspec_margin &lt;-  0.1\n\np_value_sensitivity &lt;- pnorm((t1.wald$sensitivity['diff'] + sens_margin) / t1.wald$sensitivity['diff.se'], lower.tail = FALSE)\np_value_specificity &lt;- pnorm((t1.wald$specificity['diff'] + spec_margin) / t1.wald$specificity['diff.se'], lower.tail = FALSE)\np_value_sensitivity\n\n        diff \n1.012589e-07 \n\np_value_specificity\n\n     diff \n0.9255427 \n\n\n따라서 민감도에 대해서는 위 귀무가설이 기각 되었음으로 test1의 민감도가 test2보다 열등하지 않다고 말할 수 있지만, 특이도에 대해서는 귀무가설이 기각되지 않았기 때문에 test1의 특이도가 test2에 비해 열등하지 않다는 결론을 내릴 수 없습니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html",
    "title": "the Extended DLNM 소개",
    "section": "",
    "text": "지연 효과와 비선형 관계를 모두 고려한 모델인 DLNM의 확장 버전, the Extended DLNM에 대해 소개하고, R의 dlnm 패키지를 이용하여 the Extended DLNM을 적합시키는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#data",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#data",
    "title": "the Extended DLNM 소개",
    "section": "3.1 Data",
    "text": "3.1 Data\nthe Extended DLNM의 R 실습에서 사용될 데이터는 dlnm 패키지에 포함되어 있는 drug와 nested 데이터셋입니다.\ndrug 데이터는 환자별 시간에 따른 약물 복용량의 효과에 대한 데이터입니다. 해당 실험은 200명의 무작위 추출된 환자들에 대해 진행되었으며, 각 환자들은 4주 중 무작위로 선정된 2주동안 투약을 받게 되고, 약물 복용량은 매주 달라집니다. 아래 데이터를 보면, 각 환자별로 주별 복용량(day1.7, day8.14, day15.21, day22.28)이 기록되어 있으며, out에 28일째에 측정된 outcome 값이 기록되어 있습니다.\n\nhead(drug, 5)\n\n  id out sex day1.7 day8.14 day15.21 day22.28\n1  1  46   M      0       0       40       37\n2  2  50   F      0      47       55        0\n3  3   7   F     56      22        0        0\n4  4  70   M     91       0        0       87\n5  5  -3   F      0      42       28        0\n\n\nnested 데이터는 시간에 따른 노출 요인(exposure)과 암 사이의 연관성에 대한 nested case-control study 데이터입니다. 300명의 case 집단과 300명의 control 집단에 대한 데이터가 포함되어 있습니다. 아래 데이터의 case에 case(1)/control(0) 여부가 기록되어 있으며, exp15-exp60은 15세부터 65세까지 5년 간격으로 평균 노출 요인(exposure)을 구한 값입니다. 만약, 환자의 나이(age)가 컬럼에 해당하는 나이보다 적다면 해당 컬럼에는 NA가 입력됩니다. 예를 들어, 아래 데이터의 4번 환자는 52세이기 때문에 exp55 이후의 컬럼에는 NA가 입력되어 있습니다.\n\nhead(nested, 5)\n\n  id case age riskset exp15 exp20 exp25 exp30 exp35 exp40 exp45 exp50 exp55\n1  1    1  81     240     5    84    34    45   128    81    14    52    11\n2  2    1  69     129    11     8    25     6     8    12    19    60    16\n3  3    1  73     180    14    15     7    69    10   143    18    19    44\n4  4    0  52      19    10    16     5    30    24    33    14   122    NA\n5  4    0  66      96    10    16     5    30    24    33    14   122     2\n  exp60\n1    16\n2    10\n3    23\n4    NA\n5    11\n\n\nThe matrix of exposure histories\nDLNM을 적합시키기 전, 데이터의 형태를 matrix of exposure histories 형태로 바꿔주어야 합니다.\ndrug 데이터의 matrix of exposure histories에서 lag0에 해당하는 값은 28일째의 약물 복용량입니다. 즉, lag0-lag6은 마지막 주의 약물 복용량, lag7-lag13은 셋째 주의 약물 복용량입니다. drug 데이터의 주별 약물 복용량 값을 7번씩 반복하여 다음과 같은 matrix of exposure histories를 만들어줍니다.\n\nQdrug &lt;- as.matrix(drug[,rep(7:4, each=7)])\ncolnames(Qdrug) &lt;- paste(\"lag\", 0:27, sep=\"\")\nQdrug[1:3,1:14]\n\n  lag0 lag1 lag2 lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1   37   37   37   37   37   37   37   40   40   40    40    40    40    40\n2    0    0    0    0    0    0    0   55   55   55    55    55    55    55\n3    0    0    0    0    0    0    0    0    0    0     0     0     0     0\n\n\nnested 데이터의 matrix of exposure histories에서 lag0에 해당하는 시점은 환자의 나이마다 다릅니다. 예를 들어, 52세 환자의 lag0 시점은 52년의 값이고, 65세 환자의 lag0 시점은 65년의 값입니다. 이런 경우, matrix of exposure histories를 만들기 까다로운데, dlnm 패키지에서는 이런 경우의 matrix of exposure histories를 만들어주는 exphist()라는 함수를 제공합니다. exphist() 함수는 다음과 같이 이용합니다.\n\nexphist(exp, times, lag, fill=0)\n\nexp에는 exposure profile(관측 첫 시점부터 매 시점의 exposure 값)을 입력합니다. times에는 lag0에 해당하는 시점을 입력합니다. times에 입력된 시점부터 시점을 거슬러가며 각 시차별 exposure 값이 구해집니다. lag에는 maximum lag 또는 lag range를 입력합니다. fill에는 어떤 시차에 해당하는 exposure 값이 존재하지 않을 때 채울 값을 입력합니다.\nexphist() 함수를 이용하여 다음과 같은 matrix of exposure histories를 만들어줍니다. nested 데이터의 평균 노출 요인 값에 0, 0, 0을 추가하고 각 데이터를 5번씩 반복하여 1년부터 65년까지에 해당하는 exposure profile을 만들고 이 값을 exp에 넣습니다. times에는 각 환자별 나이(age)를 넣습니다. nested 데이터를 이용한 분석에서는 lag3부터 lag40까지의 데이터만 사용할 예정이기 때문에 lag에는 다음과 같이 lag range를 벡터로 넣어줍니다. nested 데이터의 matrix of exposure histories는 다음과 같습니다.\n\nQnest &lt;- t(apply(nested, 1, function(sub) exphist(rep(c(0,0,0,sub[5:14]),\n                                                      each=5), sub[\"age\"], lag=c(3,40))))\ncolnames(Qnest) &lt;- paste(\"lag\", 3:40, sep=\"\")\nQnest[1:3,1:11]\n\n  lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1    0    0    0    0    0    0    0     0     0     0     0\n2    0   10   10   10   10   10   16    16    16    16    16\n3    0    0    0    0    0   23   23    23    23    23    44"
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#a-simple-dlm",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#a-simple-dlm",
    "title": "the Extended DLNM 소개",
    "section": "3.2 A simple DLM",
    "text": "3.2 A simple DLM\ndrug 데이터와 dlnm 패키지의 함수를 이용하여 약물 복용량과 outcome 사이의 관계에 대해 분석해보고자 합니다.\n먼저, 약물 복용량에 대한 cross-basis matrix를 만들어야 합니다. crossbasis() 함수를 이용합니다. crossbasis()의 첫 번째 인수에는 matrix of exposure histories를 넣어줍니다. lag에는 lag-response 차원에서 시차를 얼마나 고려할지 lag period를 입력합니다. maximum lag 또는 lag range를 입력해야하며, minimum lag는 기본값이 0으로 지정되어 있습니다. 이때, lag period는 matrix of exposure histories의 시차 범위와 일치해야 합니다. argvar에는 exposure-response 차원에서 사용할 basis function, arglag에는 lag-response 차원에서 사용할 basis function을 입력합니다.\ndrug 데이터의 cross-basis는, exposure-response 차원에서 simple linear function을 이용하고, lag-response 차원에서 natural cubic spline을 이용합니다. 이 때, natural cubic spline에서 lag9, 18을 knots로 이용하는데, 이는 lag9, 18을 기준으로 구간을 나눠 각 구간에서 cubic regression model을 적합시킨다는 의미입니다.\nsummary() 함수를 통해 cross-basis matrix의 세부 사항을 확인할 수 있습니다.\n\ncbdrug &lt;- crossbasis(Qdrug, lag=27, argvar=list(\"lin\"),\n                     arglag=list(fun=\"ns\",knots=c(9,18)))\nsummary(cbdrug)\n\nCROSSBASIS FUNCTIONS\nobservations: 200 \nrange: 0 to 100 \nlag period: 0 27 \ntotal df:  4 \n\nBASIS FOR VAR:\nfun: lin \nintercept: FALSE \n\nBASIS FOR LAG:\nfun: ns \nknots: 9 18 \nintercept: TRUE \nBoundary.knots: 0 27 \n\n\n위에서 생성된 cross-basis 객체인 cbdrug를 회귀식에 포함시키고, 성별에 대한 효과를 보정하여 단순 선형 회귀 분석을 진행합니다. 모형을 통해 추정된 약물 복용량 및 그 시차에 대한 효과를 crosspred() 함수를 통해 확인할 수 있습니다. crosspred()의 첫 번째 인수에는 cross-basis 객체를, 두 번째 인수에는 cross-basis 객체를 사용한 모델을 입력합니다. 다음 코드에서, crosspred()의 at은 약물 복용량이 0:20*5(즉, 0, 5, 10, 15, …, 100)일 때의 outcome을 예측하라는 뜻입니다.\n\nmdrug &lt;- lm(out~cbdrug+sex, drug)\npdrug &lt;- crosspred(cbdrug, mdrug, at=0:20*5)\n\ncrosspred 객체인 pdrug에 저장된 effect summaries는 다음과 같이 추출될 수 있습니다.\n\nwith(pdrug,cbind(allfit,alllow,allhigh)[\"50\",])\n\n  allfit   alllow  allhigh \n30.29584 20.12871 40.46298 \n\n\n위 코드는 약물 복용량이 50일때의 overall cumulative effects 추정치(allfit)와 95% 신뢰구간(alllow, allhigh) 추출한 것입니다. 이때, overall cumulative effects는 lag period인 28일동안 약물 복용량이 50으로 유지되었을 때 outcome의 전체적인 증가량 또는 약물 복용량 50이 28일 뒤 미치는 총 영향으로 해석될 수 있습니다.\n위에서처럼 all-로 시작하는 객체들을 통해서는 전반적인 추정값을 확인할 수 있고, mat-으로 시작하는 객체들을 통해서는 특정 약물 복용량과 시차의 조합에 따른 추정 결과를 확인할 수 있습니다. 다음 코드는 lag3에서의 약물 복용량이 20일 때, outcome의 증가량을 추출한 것입니다. ?crosspred를 통해 crosspred()의 더 많은 기능을 확인할 수 있습니다.\n\npdrug$matfit[\"20\",\"lag3\"]\n\n[1] 1.118139\n\n\nplot() 함수를 이용해 추정 결과를 시각화할 수 있습니다.\n\npar(mfrow=c(1,3))\nplot(pdrug, zlab=\"Effect\", xlab=\"Dose\", ylab=\"Lag (days)\")\nplot(pdrug, var=60, ylab=\"Effect at dose 60\", xlab=\"Lag (days)\", ylim=c(-1,5))\nplot(pdrug, lag=10, ylab=\"Effect at lag 10\", xlab=\"Dose\", ylim=c(-1,5))\n\n\n\n\n\n\n\n첫 번째 그래프는 회귀 모델을 통해 추정한 exposure-lag-response 관계를 3차원 공간에 그려놓은 것입니다. 약물 복용량과 시차의 변화에 따라 effect가 어떻게 달라지는지 확인할 수 있습니다. 그래프에 따르면 약물 복용량의 효과는 복용 후 첫 번째 날에 나타나고 15-20일 후에 사라지는 경향이 있음을 알 수 있습니다.\n두 번째와 세 번째 그래프는 약물 복용량이 60일 때의 lag-response curve와 lag가 10일 때 exposure-response curve를 그린 것입니다. 각각 var=60, lag=10을 지정하여 3차원상의 첫 번째 그래프의 단면을 자른 것과 같습니다. cross-basis matrix를 만들 때 basis function을 지정했던대로, lag-response 차원에서는 natural cubic spline, exposure-response 차원에서는 simple linear function 형태로 나타나는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#a-more-complex-dlnm",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#a-more-complex-dlnm",
    "title": "the Extended DLNM 소개",
    "section": "3.3 A more complex DLNM",
    "text": "3.3 A more complex DLNM\nneseted 데이터와 dlnm 패키지의 함수를 이용하여 노출 요인(exposure)에 대한 장기간의 노출이 암에 어떤 영향을 미치는지 분석하고자 합니다.\n최근 3년(lag0-2)간의 노출은 암에 영향을 주지 않는다고 가정해봅시다. 따라서 matrix of exposure histories에는 lag3부터의 데이터만 있으면 됩니다.\n먼저, cross-basis matrix를 만듭니다. crossbasis() 함수를 이용합니다. lag3부터 lag40까지의 데이터만 이용할 것이므로, lag에 c(3,40)을 넣어줍니다. exposure-response 차원에서는 basis 함수로 quadratic spline을 사용하고, exposure-lag 차원에서는 natural cubic spline을 사용합니다. quadratic spline의 자유도(df)와 차수(degree)를 입력해야하며, single knot는 별도로 지정하지 않으면 중앙값으로 지정됩니다. natural cubic spline은 intercept=F를 입력하여 intercept를 제외합니다. 위에서 lag0-2는 고려하지 않는다고 하였으므로, intercept를 제외함으로써 위 가정과 일치하게 시차 차원에서의 null effect를 예측할 수 있습니다.\n\ncbnest &lt;- crossbasis(Qnest, lag=c(3,40), argvar=list(\"bs\",degree=2,df=3),\n                     arglag=list(fun=\"ns\",knots=c(10,30),intercept=F))\nsummary(cbnest)\n\nCROSSBASIS FUNCTIONS\nobservations: 600 \nrange: 0 to 1064 \nlag period: 3 40 \ntotal df:  9 \n\nBASIS FOR VAR:\nfun: bs \nknots: 15 \ndegree: 2 \nintercept: FALSE \nBoundary.knots: 0 1064 \n\nBASIS FOR LAG:\nfun: ns \nknots: 10 30 \nintercept: FALSE \nBoundary.knots: 3 40 \n\n\n앞에서와 마찬가지로, cross-basis 객체를 회귀 모형에 포함시킵니다. nested 데이터는 nested case-control study 데이터이기 때문에, conditional logistic regression을 이용합니다. survival 패키지의 clogit() 함수를 이용합니다.\ncrosspred 객체를 만들 때, cen=0을 입력하여 reference value를 0으로 지정합니다. 즉, 노출 요인에 따른 암의 OR은 exposure=0일 때를 기준으로 계산됩니다.\n\nlibrary(survival)\nmnest &lt;- clogit(case~cbnest+strata(riskset), nested)\npnest &lt;- crosspred(cbnest, mnest, cen=0, at=0:20*5)\n\neffect summaries는 pnest를 통해 확인할 수 있습니다. 이번에는 allRR-이나 matRR-로 시작하는 객체를 추출하여 OR 추정값을 확인할 수 있습니다.\n\nwith(pnest,cbind(allRRfit,allRRlow,allRRhigh)[\"50\",])\n\n  allRRfit   allRRlow  allRRhigh \n 32.061676   2.561702 401.276652 \n\n\n\npnest$matRRfit[\"50\",\"lag5\"]\n\n[1] 1.058661\n\n\n앞에서와 마찬가지로, plot() 함수를 통해 추정 결과를 시각화할 수 있습니다.\n\npar(mfrow=c(1,3))\nplot(pnest, zlab=\"OR\", xlab=\"Exposure\", ylab=\"Lag (years)\")\nplot(pnest, var=50, ylab=\"OR for exposure 50\", xlab=\"Lag (years)\", xlim=c(0,40))\nplot(pnest, lag=5, ylab=\"OR at lag 5\", xlab=\"Exposure\", ylim=c(0.95,1.15))\n\n\n\n\n\n\n\n첫 번째 그래프는 노출 요인과 암 사이의 exposure-lag-response 관계를 3차원 상에서 시각화한 것입니다. 그래프에 따르면 노출 초기엔 암의 위험성을 증가시키다가 점차 감소한다는 사실을 확인할 수 있습니다.\n두 번째와 세 번째 그래프는 exposure이 50일 때의 lag-response curve와 lag가 5일 때 exposure-response curve를 그린 것입니다. 두 번째 그래프를 통해 노출 후 10-15년이 지났을 때 암의 위험성이 가장 크게 증가하며, 30년이 지났을 때쯤부턴 거의 원 상태로 되돌아온다는 사실을 확인할 수 있습니다. 세 번째 그림을 통해 노출 요인의 양이 20으로 증가할 때까지는 위험성이 크게 증가하지만 그 이후로는 위험성이 천천히 증가함을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#extended-prediction-summaries",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#extended-prediction-summaries",
    "title": "the Extended DLNM 소개",
    "section": "3.4 Extended prediction summaries",
    "text": "3.4 Extended prediction summaries\ncrosspred()의 at과 lag 인자를 통해 특정 exposure 값 또는 lag에서의 effect summaries를 얻을 수 있습니다. 이때, nested 데이터의 matrix of exposure histories를 만들 때 사용한 함수인 exphist()를 사용한다면, 특정 exposure history에서의 effect summaries를 얻을 수 있습니다.\n예를 들어, ’3.3 A more complex DLNM’에서 이용한 nested 데이터를 다시 분석해봅시다. 5년 동안의 exposure 값이 10이고, 그 이후 5년 간은 0, 그 이후 10년간은 13일 때의 effect summary를 구하고자 할 때, 아래와 같이 exposure history를 만들 수 있습니다.\n\nexpnested &lt;- rep(c(10,0,13), c(5,5,10))\nhist &lt;- exphist(expnested, time=length(expnested), lag=c(3,40))\nhist\n\n   lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13 lag14 lag15 lag16\n20   13   13   13   13   13   13   13     0     0     0     0     0    10    10\n   lag17 lag18 lag19 lag20 lag21 lag22 lag23 lag24 lag25 lag26 lag27 lag28\n20    10    10    10     0     0     0     0     0     0     0     0     0\n   lag29 lag30 lag31 lag32 lag33 lag34 lag35 lag36 lag37 lag38 lag39 lag40\n20     0     0     0     0     0     0     0     0     0     0     0     0\n\n\ntime 인자에는 expnested의 길이를 입력하고, lag는 lag3부터 40까지만 고려한다고 입력합니다. 이때, expnested의 길이는 20이기 때문에 lag20-40의 exposure 값은 0으로 입력됩니다.\n이렇게 만들어진 exposure history를 crosspred()의 at 인자에 넣어줍니다.\n\npnesthist &lt;- crosspred(cbnest, mnest, cen=0, at=hist)\nwith(pnesthist, c(allRRfit,allRRlow,allRRhigh))\n\n      20       20       20 \n3.503928 1.240109 9.900351 \n\n\n위의 코드를 통해 해당 exposure history에서의 OR 추정치는 3.5임을 알 수 있습니다.\n이 방법을 통해 여러 개의 time-varying exposure histories를 만들어 effect summaries를 구할 수 있습니다. ’3.2 A simple DLM’에서 이용한 drug 데이터를 다시 분석해봅시다. 환자의 2주 동안의 약물 복용량이 10이고, 그 후 1주간은 50이고, 그 후 1주간은 복용하지 않았을 때, 각 시점에서의 effect summary를 구해봅시다. 아래와 같이 각 시점에서의 exposure history을 만들어줍니다.\n\nexpdrug &lt;- rep(c(10,50,0,20),c(2,1,1,2)*7)\ndynhist &lt;- exphist(expdrug, lag=27)\ndynhist[1:10,]\n\n   lag0 lag1 lag2 lag3 lag4 lag5 lag6 lag7 lag8 lag9 lag10 lag11 lag12 lag13\n1    10    0    0    0    0    0    0    0    0    0     0     0     0     0\n2    10   10    0    0    0    0    0    0    0    0     0     0     0     0\n3    10   10   10    0    0    0    0    0    0    0     0     0     0     0\n4    10   10   10   10    0    0    0    0    0    0     0     0     0     0\n5    10   10   10   10   10    0    0    0    0    0     0     0     0     0\n6    10   10   10   10   10   10    0    0    0    0     0     0     0     0\n7    10   10   10   10   10   10   10    0    0    0     0     0     0     0\n8    10   10   10   10   10   10   10   10    0    0     0     0     0     0\n9    10   10   10   10   10   10   10   10   10    0     0     0     0     0\n10   10   10   10   10   10   10   10   10   10   10     0     0     0     0\n   lag14 lag15 lag16 lag17 lag18 lag19 lag20 lag21 lag22 lag23 lag24 lag25\n1      0     0     0     0     0     0     0     0     0     0     0     0\n2      0     0     0     0     0     0     0     0     0     0     0     0\n3      0     0     0     0     0     0     0     0     0     0     0     0\n4      0     0     0     0     0     0     0     0     0     0     0     0\n5      0     0     0     0     0     0     0     0     0     0     0     0\n6      0     0     0     0     0     0     0     0     0     0     0     0\n7      0     0     0     0     0     0     0     0     0     0     0     0\n8      0     0     0     0     0     0     0     0     0     0     0     0\n9      0     0     0     0     0     0     0     0     0     0     0     0\n10     0     0     0     0     0     0     0     0     0     0     0     0\n   lag26 lag27\n1      0     0\n2      0     0\n3      0     0\n4      0     0\n5      0     0\n6      0     0\n7      0     0\n8      0     0\n9      0     0\n10     0     0\n\n\nexphist() 함수의 time 인자가 지정되지 않았을 경우에는 모든 time point에 대하여 exposure history가 만들어집니다. 이렇게 만들어진 exposure histories를 crosspred()의 at 인자에 넣어줍니다.\n\npdyndrug &lt;- crosspred(cbdrug, mdrug, at=dynhist)\n\n아래 코드로 그린 plot을 통해 각 시점에서의 exposure history의 effect summary를 확인할 수 있습니다.\n\nplot(pdyndrug,\"overall\", ylab=\"Effect\", xlab=\"Time (days)\", ylim=c(-10,27),\n     xlim=c(1,50), yaxt=\"n\")\naxis(2, at=-1:5*5)\npar(new=TRUE)\nplot(expdrug, type=\"h\", xlim=c(1,50), ylim=c(0,300), axes=F, ann=F)\naxis(4, at=0:6*10, cex.axis=0.8)\nmtext(\"Dose\", 4, line=-1.5, at=30, cex=0.8)"
  },
  {
    "objectID": "posts/2024-06-05-theExtendedDLNM/index.html#applying-user-defined-functions",
    "href": "posts/2024-06-05-theExtendedDLNM/index.html#applying-user-defined-functions",
    "title": "the Extended DLNM 소개",
    "section": "3.5 Applying user-defined functions",
    "text": "3.5 Applying user-defined functions\ncross-basis 객체를 생성할 때, exposure와 lag 차원에서 사용할 basis function을 사용자 정의 함수로 지정할 수 있습니다. 이때, 사용자 정의 함수의 첫 번째 인자는 반드시 \\(X\\)값이어야하고, return 값은 변환된 벡터 혹은 행렬이어야 합니다.\n’3.3 A more complex DLNM’의 세 번째 plot을 보면, exposure-response 차원에서의 quadratic spline은 log 함수와 비슷한 형태임을 알 수 있습니다. cross-basis 객체를 생성할 때, exposure-response 차원에서의 basis function을 quadratic spline이 아닌 log 함수를 이용해봅시다.\n다음과 같이 log 함수를 정의합니다.\n\nmylog &lt;- function(x) log(x+1)\n\n위에서 정의한 mylog 함수를 crossbasis() 함수에 넣어줍니다.\n\ncbnest2 &lt;- crossbasis(Qnest, lag=c(3,40), argvar=list(\"mylog\"),\n                      arglag=list(fun=\"ns\",knots=c(10,30),intercept=F))\nsummary(cbnest2)\n\nCROSSBASIS FUNCTIONS\nobservations: 600 \nrange: 0 to 1064 \nlag period: 3 40 \ntotal df:  3 \n\nBASIS FOR VAR:\nfun: mylog \n\nBASIS FOR LAG:\nfun: ns \nknots: 10 30 \nintercept: FALSE \nBoundary.knots: 3 40 \n\n\n3.3에서 정의한 cross-basis cbnest와 비교하여 자유도가 9에서 3으로 감소한 것을 확인할 수 있습니다.\n아래 plot을 통해 추정 결과를 비교해봅시다.\n\nmnest2 &lt;- clogit(case~cbnest2+strata(riskset), nested)\npnest2 &lt;- crosspred(cbnest2, mnest2, cen=0, at=0:20*5)\n\npar(mfrow=c(1,3))\nplot(pnest2, zlab=\"OR\", xlab=\"Exposure\", ylab=\"Lag (years)\")\nplot(pnest2, var=50, ylab=\"OR for exposure 50\", xlab=\"Lag (years)\", xlim=c(0,40))\nlines(pnest, var=50, lty=2)\nplot(pnest2, lag=5, ylab=\"OR at lag 5\", xlab=\"Exposure\", ylim=c(0.95,1.15))\nlines(pnest, lag=5, lty=2)\n\n\n\n\n\n\n\n점선은 3.3에서 추정한 결과를 나타낸 것입니다. exposure-response 차원에서 basis function으로 log 함수를 사용했을 때와 quadratic spline을 사용했을 때의 결과가 유사함을 확인할 수 있습니다.\n’3.2 A simple DLM’의 두 번째 plot을 보면, lag-response 차원에서의 natural cubic spline은 지수적으로 감소하는 형태를 보입니다. 다음과 같이 exponential decay 함수를 정의합니다.\n\nfdecay &lt;- function(x,scale=5) {\n  basis &lt;- exp(-x/scale)\n  attributes(basis)$scale &lt;- scale\n  return(basis)\n}\n\n다음과 같이 crossbasis() 함수에 인자로 넣어주고, plot을 그려 결과를 확인합니다.\n\ncbdrug2 &lt;- crossbasis(Qdrug, lag=27, argvar=list(\"lin\"),\n                      arglag=list(fun=\"fdecay\",scale=6))\nsummary(cbdrug2)\n\nCROSSBASIS FUNCTIONS\nobservations: 200 \nrange: 0 to 100 \nlag period: 0 27 \ntotal df:  1 \n\nBASIS FOR VAR:\nfun: lin \nintercept: FALSE \n\nBASIS FOR LAG:\nfun: fdecay \nscale: 6 \n\nmdrug2 &lt;- lm(out~cbdrug2+sex, drug)\npdrug2 &lt;- crosspred(cbdrug2, mdrug2, at=0:20*5)\n\npar(mfrow=c(1,3))\nplot(pdrug2, zlab=\"Effect\", xlab=\"Dose\", ylab=\"Lag (days)\")\nplot(pdrug2, var=60, ylab=\"Effect at dose 60\", xlab=\"Lag (days)\", ylim=c(-1,5))\nlines(pdrug, var=60, lty=2)\nplot(pdrug2, lag=10, ylab=\"Effect at lag 10\", xlab=\"Dose\", ylim=c(-1,5))\nlines(pdrug, lag=10, lty=2)\n\n\n\n\n\n\n\n점선은 3.2에서 추정한 결과를 나타낸 것입니다. lag-response 차원에서 basis function으로 exponential decay 함수를 사용했을 때와 natural cubic spline을 사용했을 때의 결과가 유사함을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html",
    "href": "posts/2024-05-17-patchwork/index.html",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "",
    "text": "데이터 시각화는 데이터 분석에서 중요한 역할을 한다. 다행히 R은 이 방면에서는 ggplot2를 필두로 다른 프로그래밍 언어들 이상의 뛰어난 여러 기능들을 사용할 수 있다는 장점이 있다.\n한편 데이터 시각화는 제작 이후 색상이나 레이아웃 등의 추가적인 커스텀 수정을 필요로 하기도 한다. 이를 위해 R 내에서 할 수 있다면 더할 나위 없이 좋지만 떄로는 단순한 작업을 위해 여러줄 코드를 사용하는 것보다 간단히 ppt 같은 외부 프로그램을 활용하는 것이 더 간편한 경우도 많다.\n이전의 다른 아티클에서 officer 패키지를 활용해 MS powerpoint로 벡터 이미지를 만들고 편집하는 방법을 소개하였는데, 이번 글에서는 여러 장의 이미지를 대상으로 R에서 할 수 있는 고급 방법들과 이에 쓰이는 R 패키지를 소개한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#data-visualization",
    "href": "posts/2024-05-17-patchwork/index.html#data-visualization",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "",
    "text": "데이터 시각화는 데이터 분석에서 중요한 역할을 한다. 다행히 R은 이 방면에서는 ggplot2를 필두로 다른 프로그래밍 언어들 이상의 뛰어난 여러 기능들을 사용할 수 있다는 장점이 있다.\n한편 데이터 시각화는 제작 이후 색상이나 레이아웃 등의 추가적인 커스텀 수정을 필요로 하기도 한다. 이를 위해 R 내에서 할 수 있다면 더할 나위 없이 좋지만 떄로는 단순한 작업을 위해 여러줄 코드를 사용하는 것보다 간단히 ppt 같은 외부 프로그램을 활용하는 것이 더 간편한 경우도 많다.\n이전의 다른 아티클에서 officer 패키지를 활용해 MS powerpoint로 벡터 이미지를 만들고 편집하는 방법을 소개하였는데, 이번 글에서는 여러 장의 이미지를 대상으로 R에서 할 수 있는 고급 방법들과 이에 쓰이는 R 패키지를 소개한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#result",
    "href": "posts/2024-05-17-patchwork/index.html#result",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "result",
    "text": "result\n이번 글에서 소개하는 방법들을 적용한 ppt 결과물을 먼저 소개한다.\n\n우선 위 이미지는\n\nR에서 ggplot2를 사용하여 만든 시각화를\n\ncowplot을 이용하여 박스로 감싸고\n\npatchwork와 를 사용해 시각화와 설명을 위한 텍스트를 레이아웃에 따라 배치한 뒤\n\nofficer를 활용하여 와이드스크린(혹은 16:9) 해상도 크기의 MS powerpoint로 만들어 낸 결과물이다.\n\n이 결과물들은 벡터 그래픽스를 활용한 만큼, 다음 이미지처럼 ppt에서 편리하게 커스텀 수정이 가능하다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#patchwork",
    "href": "posts/2024-05-17-patchwork/index.html#patchwork",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "patchwork",
    "text": "patchwork\n예시에서 사용할 이미지는 ggplot2의 mtcars 데이터셋을 사용하는 patchwork의 예시 코드를 사용한다. ggplot2과 각 차트에 대해서는 별도로 설명하지 않는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\nMerc 280C\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\nMerc 450SE\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\nMerc 450SL\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n\n\nMerc 450SLC\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\nCadillac Fleetwood\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\nLincoln Continental\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\nChrysler Imperial\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\nFiat 128\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\nHonda Civic\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\nToyota Corona\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\nDodge Challenger\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\nAMC Javelin\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\nPontiac Firebird\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\nFord Pantera L\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\nmtcars\n\n\n\nlibrary(ggplot2)\n\np1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp))\np2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))\np3 &lt;- ggplot(mtcars) + geom_bar(aes(gear)) + facet_wrap(~cyl)\np4 &lt;- ggplot(mtcars) + geom_bar(aes(carb))\np5 &lt;- ggplot(mtcars) + geom_violin(aes(cyl, mpg, group = cyl))\np6 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + facet_wrap( ~ cyl)\n\npatchwork는 여러 개의 ggplot 결과물들을 하나(한장)의 그래픽에 간단하게 배치할 수 있게 하는 R 패키지이다. 유사한 목적으로 patchwork외에 gridExtra나 cowplot등의 다른 패키지도 사용할 수 있다.\npatchwork의 사용법은 크게 +, |, ( ), /로 구성된다.\n| (vertical bar)\n먼저 | 는 여러 이미지를 하나의 행에 배치하는 역할을 한다.\n\np1 | p2 | p3 | p4\n\n\npatchwork - vertical bar\n\n\n\n\n+\n두번째로 +는 여러 이미지를 배치하는데 이때 행과 열은 grid 형태로, 행 순서로 채우는 방식을 사용한다.\n\np1 + p2 + p3 + p4\n\n\npatchwork - plus\n\n\n\n\n이때 이미지 배치를 특별히 지정을 하기 위해서는 plot_layout이라는 함수를 사용한다.\n\np1 + p2 + p3 + p4 + p5 +\n  plot_layout(ncol = 3, byrow = FALSE)\n\n\npatchwork - plot_layout\n\n\n\n\n/\n이어서 /를 사용하면 이미지를 열로 이어서 배치할 수 있다.\n\np1 / p2 \n\n\npatchwork - slash\n\n\n\n\n( )\n마지막으로 ( )를 사용하면 이미지를 하나의 그룹으로 묶어서 배치할 수 있다.\n\np1 | (p2 / p3)\n\n\npatchwork - parenthesis\n\n\n\n\n물론 이 외에도 patchwork는 다양한 기능을 제공하는데, 자세한 내용은 공식 문서를 참고하자.\n이를 활용해서 이제 앞에서 만들었던 예시 이미지 6개를 한장의 ppt에 배치해보자.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\n\npatchwork - combined"
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#cowplot",
    "href": "posts/2024-05-17-patchwork/index.html#cowplot",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "cowplot",
    "text": "cowplot\n이어서 cowplot을 사용해 이미지 사이에 캡션과 박스를 추가하는 방법을 다루겠다.\n우선 첫 이미지 3장을 표현하는 가상의 캡션을 list 형태로 생성한다. 참고로 &lt;br&gt;은 줄넘김을 의미한다.\n내용은 lorem ipsum을 활용했다.\n\ntext &lt;- list(\n  p1 = \"Lorem ipsum dolor sit amet &lt;br&gt; consectetur adipiscing elit.\",\n  p2 = \"Integer lectus risus, &lt;br&gt; tincidunt eget felis non.\",\n  p3 = \"Cras varius sapien et est consectetur porttitor.\"\n)\n\n이를 이전 combined_plot에 추가한다.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  ( text$p1 | text$p2 | text$p3 ) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\n# combined_plot \n# ERROR !!\n\n그러나 이 상태로는 text 오브젝트가 ggplot 결과가 아닌 단순 텍스트만을 포함하기 때문에 에러가 발생한다. 이를 해결하기 위해 cowplot의 ggdraw 함수를 사용한다.\nggdraw\n우선 cowplot은 ggplot2의 결과물에 annotation, theme를 추가하는 기능등을 제공하는 R 패키지로, ggdraw는 ggplot2의 결과물에 추가적인 그래픽을 그릴 수 있게 제일 상위 레벨에 레이어를 추가한다고 생각하면 편하다.\n\nscatter &lt;- ggplot(mpg, aes(displ, cty)) +\n  geom_point() +\n  theme_minimal_grid()\n\ndraft &lt;- ggdraw(scatter) + \n  draw_label(\"Draft\", colour = \"#80404080\", size = 120, angle = 45)\n\nscatter | draft\n\n\ncowplot - ggdraw\n\n\n\n\n이 ggdraw를 사용해 이전의 text 내용 중 첫번째 라벨(p1)을 label로 갖는 ggplot 오브젝트를 생성하고 이를 combined_plot에 추가한다.\n\ncombined_plot &lt;- (p1 | p2 | p3) /\n  ( \n    ggdraw() + \n      labs(subtitle = text$p1) + \n      theme_void() +\n      theme(\n        text = element_text(size = 8),\n        plot.subtitle = ggtext::element_textbox_simple(\n          hjust = 0,\n          halign = 0,\n          margin = margin(3, 0, 0, 0)\n        ),\n        plot.margin = margin(0, 0, 0, 0)\n      ) \n  ) /\n  (p4 | p5 | p6) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\n\ncowplot - combined with caption\n\n\n\n\n이어서 남은 라벨을 추가하기 전, 라벨 커스텀에 반복적으로 쓰이는 기능들을 별도의 함수로 만들어 사용하자. 추가로 caption과 그래프를 동일한 1:1:1의 높이로 할당하지 않고 caption 부분을 줄이기 위해 plot_layout의 height로 높이를 조절한다.\n\n\n\ncowplot - caption function\n\n\n\n\n다음은 각 시각화를 박스(테두리)로 감싸는 방법을 다룬다. 이를 위해 각 시각화에 ggdraw를 사용하여 레이어를 만들고, 그 레이어에 draw_line 함수를 사용해 (0,0) 부터 (1,1)을 지나는 직선을 추가하는 방법을 사용한다.\n추가로 각 시각화에 text 속성을 조절하기 위해 theme 함수를 사용한다.\n\ntext_theme &lt;- theme(\n  text = element_text(size = 6), \n  axis.text = element_text(size = 6), \n  axis.title = element_text(size = 6),\n  axis.title.x = element_text(size = 6), \n  axis.title.y = element_text(size = 6), \n  plot.title = element_text(size = 6),\n  legend.text = element_text(size = 6),\n  legend.title = element_text(size = 6)\n)\n\n(\n  p1 | \n  ggdraw(p1 + text_theme) +\n    draw_line(\n      x = c(0, 1, 1, 0, 0), \n        y = c(0, 0, 1, 1, 0), \n        color = \"black\", \n        size = 0.5\n    )\n)\n\n\ncowplot - box\n\n\n\n\n이전과 마찬가지로 (반복되는) 박스를 만드는 기능들을 함수로 만들어 사용하자.\n\n\n\ncowplot - box function"
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#officer",
    "href": "posts/2024-05-17-patchwork/index.html#officer",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "officer",
    "text": "officer\n이제 officer 패키지를 사용해 위에서 만든 그래프를 ppt에 추가해보자.\n기본적인 officer에 대한 소개는 이전 아티클을 참고하면 좋다.\nofficer에서는 read_pptx 함수로 ppt 오브젝트를 생성하는데 이때 읽을 파일을 입력하지 않으면 너비와 높이가 4:3 비율인 새로운 오브젝트를 생성하여 사용한다.\n만약 이를 그대로 사용한다면 다음 그림과 같이 애써만든 레이아웃이 깨지는 상황이 발생할 수 있기 때문에, ppt에서 임의의 사이즈를 갖는 템플릿을 만들고 이를 파일로 읽어 사용한다.\n\n\n\n\n\n\n\nppt를 생성한 다음, 페이지 설정에서 16:9 혹은 와이드 스크린으로 변경하는 방법도 있지만, 이 방법 또한 마찬가지로 그래프 요소들을 다시 배치해야 한다는 점은 동일하다.\n\n\n\n\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  remove_slide(1) |&gt;\n  add_slide() |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")\n\n위 코드에서 2번째 줄 remove_slide 함수를 사용하지 않으면, 기존 템플릿의 슬라이드 이후 에 ggplot 결과를 담는 슬라이드를 만들기 때문에 아래와 같이 불필요한 첫페이지를 가지고 시작하게 된다.\n\n한편 remove_slide와 add_slide를 둘 다 제거하고 ph_with으로 이미지만 더하게 되면 아래와 같이 템플릿의 제목과 새로 추가한 제목이 겹쳐서 보여지게 된다.\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")\n\n\n그러므로 템플릿을 사용하는 경우에는 remove_slide와 add_slide를 활용하는 것을 권장한다."
  },
  {
    "objectID": "posts/2024-05-17-patchwork/index.html#summary",
    "href": "posts/2024-05-17-patchwork/index.html#summary",
    "title": "patchwork를 활용한 고급 시각화",
    "section": "summary",
    "text": "summary\n이번 아티클에서는 patchwork와 cowplot을 사용해 여러 그래프를 하나로 합치고 약간의 커스텀을 거쳐, officer를 사용해 ppt에 추가하는 방법을 알아보았다. 이처럼 R의 기능과 ppt를 연결하는 방법은 다양하게 활용할 수 있으며, 이를 통해 보다 효율적인 작업을 할 수 있을 것이다.\n\n최종 코드는 다음과 같다.\n\nCodelibrary(ggplot2)\nlibrary(patchwork)\nlibrary(cowplot)\nlibrary(officer)\n\np1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp))\np2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear))\np3 &lt;- ggplot(mtcars) + geom_bar(aes(gear)) + facet_wrap(~cyl)\np4 &lt;- ggplot(mtcars) + geom_bar(aes(carb))\np5 &lt;- ggplot(mtcars) + geom_violin(aes(cyl, mpg, group = cyl))\np6 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + facet_wrap( ~ cyl)\n\ntext &lt;- list(\n  p1 = \"Lorem ipsum dolor sit amet &lt;br&gt; consectetur adipiscing elit.\",\n  p2 = \"Integer lectus risus, &lt;br&gt; tincidunt eget felis non.\",\n  p3 = \"Cras varius sapien et est consectetur porttitor.\"\n)\n\ncap &lt;- function(text){\n  ggdraw() + \n    labs(subtitle = text) +\n    theme_void() +\n    theme(\n      text = element_text(size = 8),\n      plot.margin = margin(0, 0, 0, 0)\n    )\n}\n\ntext_theme &lt;- theme(\n  text = element_text(size = 6), \n  axis.text = element_text(size = 6), \n  axis.title = element_text(size = 6),\n  axis.title.x = element_text(size = 6), \n  axis.title.y = element_text(size = 6), \n  plot.title = element_text(size = 6),\n  legend.text = element_text(size = 6),\n  legend.title = element_text(size = 6)\n)\n\nwith.box &lt;- function(p){\n  ggdraw(p + text_theme) +\n    cowplot::draw_line(\n      x = c(0, 1, 1, 0, 0), \n      y = c(0, 0, 1, 1, 0), \n      color = \"black\", \n      size = 0.5\n    ) \n}\n\ncombined_plot &lt;- (with.box(p1) | with.box(p2) | with.box(p3)) /\n  ( cap(text$p1 + text_theme) | cap(text$p2 + text_theme) | cap(text$p3 + text_theme) ) /\n  (with.box(p4) | with.box(p5) | with.box(p6)) +\n  plot_layout(heights = c(5, 0.1, 5)) +\n  theme(plot.margin = margin(1, 10, 1, 10)) \n\ncombined_plot\n\nread_pptx(\"~/Documents/template.pptx\") |&gt;\n  ph_with(\n    value = \"Example Title (baseline ~ X)\", \n    location = ph_location_type(type = \"title\")\n  ) |&gt; \n  ph_with(\n    rvg::dml(ggobj = combined_plot), \n    location = ph_location(left = 0, top = 1.5, height = 6, width = 13.333)\n  ) |&gt;\n  print(target = \"output2.pptx\")"
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html",
    "href": "posts/2024-03-14-process-macro/index.html",
    "title": "Process macro 소개",
    "section": "",
    "text": "Process macro에 대해 알아보고 R에서 사용가능한 패키지를 소개합니다."
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#매개효과",
    "href": "posts/2024-03-14-process-macro/index.html#매개효과",
    "title": "Process macro 소개",
    "section": "매개효과",
    "text": "매개효과\n매개효과 분석은 설명변수가 반응변수에 영향을 미치는 경로, 매커니즘을 확인하기 위한 분석방법입니다. 단순매개모형(4번 모델)은 다음과 같은 다이아그램으로 표현할 수 있습니다.\n\n\n\n\nFigure.1\n\n\n\n먼저 R에서 process macro를 사용하려면 패키지를 설치하거나 파일을 다운받아야 합니다. 패키지로는 가톨릭대학교 문건웅 교수님이 만든 processR이라는 패키지를 다음과 같이 다운로드하고 불러올 수 있습니다.\n\ndevtools::install_github(\"cardiomoon/processR\")\nlibrary(processR)\n\nprocessR 패키지를 사용하려면 lavaan 패키지가 필요합니다. 아래 코드로 다운로드하고 불러올 수 있습니다.\n\ninstall.packages(\"lavaan\")\nlibrary(lavaan)\n\n아래 코드로 processR 패키지에서 지원하는 모델의 번호를 확인할 수 있습니다.\n\npmacro$no\n\n [1]  0.0  1.0  2.0  3.0  4.0  4.2  5.0  6.0  6.3  6.4  7.0  8.0  9.0 10.0 11.0\n[16] 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 28.0 29.0\n[31] 30.0 31.0 35.0 36.0 40.0 41.0 45.0 49.0 50.0 58.0 59.0 60.0 61.0 62.0 63.0\n[46] 64.0 65.0 66.0 67.0 74.0 75.0 76.0 25.0 26.0 27.0 58.2  4.3\n\n\n직접 다운받아 사용하시려면 process macro를 개발한 Andrew F. Hayes가 제공하는 파일을 여기서 내려받을 수 있습니다. process.R파일을 실행시키거나 분석을 진행할 R파일 상단에 source(\"process.R\")코드를 실행하면 함수를 사용할 수 있습니다. processR패키지와 process.R파일은 서로 다른 도구이니 혼동하지 않도록 주의해야 합니다. 이 포스트에서 processR 패키지에서 제공하는 함수는 코드 상단에 # processR로, process.R에서 제공하는 함수는 # process.R로 주석을 달아놓겠습니다.\n예시 데이터로 단순매개효과를 설명해보겠습니다.\n미국의 1,338명의 의료비용에 대한 데이터입니다.\n\ncost &lt;- read.csv(\"Medical_Cost.csv\")\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.924\n\n\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.552\n\n\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.462\n\n\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.471\n\n\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.855\n\n\n\n\n\nprocess.R의 process()함수를 실행해보겠습니다. 인자는 다음과 같습니다.\n\ndata = 데이터셋\nx = 설명변수\ny = 반응변수\nm = 매개변수\nmodel = 모델번호\nboot = 부트스트래핑 횟수\ntotal = 총효과 출력(0이면 출력하지 않음)\n\n\ncost$sex &lt;- ifelse(cost$sex == \"male\", 1, 0)\ncost$smoker &lt;- ifelse(cost$smoker == \"yes\", 1, 0)\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", m = \"bmi\", model = 4, boot = 0, total = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 4      \n    Y : charges\n    X : smoker \n    M : bmi    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: bmi\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.0038    0.0000   37.2152    0.0188    1.0000 1336.0000    0.8910\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant   30.6518    0.1870  163.8953    0.0000   30.2849   31.0187\nsmoker      0.0567    0.4133    0.1371    0.8910   -0.7541    0.8674\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8111    0.6579 50238769.3992 1283.9234    2.0000 1335.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -3459.0955  998.2795   -3.4651    0.0005 -5417.4628 -1500.7282\nsmoker   23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\nbmi        388.0152   31.7875   12.2065    0.0000   325.6564   450.3741\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.7873    0.6198 55804130.1996 2177.6149    1.0000 1336.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant  8434.2683  229.0142   36.8286    0.0000  7985.0017  8883.5348\nsmoker   23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\nDirect effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\n\nIndirect effect(s) of X on Y:\n       Effect\nbmi   21.9825\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n우선 Figure.1에서 보았던 경로의 이름을 정하겠습니다.\\(X → M : a\\)\\(M → Y : b\\)\\(X → Y : c'\\) 단순매개모형에서 설명변수가 0, 1로 이루어진 변수일때, \\(a = [\\bar{M}|(X = 1)] - [\\bar{M}|(X = 0)] = 0.0567\\)이며 X가 1일때 M의 평균과 X가 0일때 M의 평균의 차이와 같고 lm(bmi ~ smoker, data = cost)의 기울기와 같습니다.\\(b = [\\hat{Y}|(M = m, X = x)] - [\\hat{Y}|(M = m - 1, X = x)] = 388.0152\\)이며 이는 lm(charges ~ smoker + bmi, data = cost)에서 bmi의 기울기와 같고 아래처럼 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\ncost.1 &lt;- cost\ncost.1$bmi &lt;- cost.1$bmi - 1\n\npredict(model, cost)[1] - predict(model, cost.1)[1]\n\n       1 \n388.0152 \n\n\n\\(c' = [\\hat{Y}|(X = x, M = m)] - [\\hat{Y}|(X = x - 1, M = m)] = 23593.9810\\)이며 lm(charges ~ smoker + bmi, data = cost)의 smoker의 기울기와 같고 다음과 같이 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\nsmoke1 &lt;- cost\nsmoke1$smoker &lt;- 1\n\nsmoke0 &lt;- cost\nsmoke0$smoker &lt;- 0\n\npredict(model, smoke1)[1] - predict(model, smoke0)[1]\n\n       1 \n23593.98 \n\n\n단순매개모형에서 \\(ab\\)를 간접효과, \\(c'\\)을 직접효과, 이 둘을 더한 값을 \\(c\\)(총효과)라고 하며 총효과는 lm(charges ~ smoker, data = cost)의 기울기와 같습니다. 간접효과는 매개변수를 통했을 때 흡연자는 비흡연자보다 의료비용이 21.9825만큼 높다는 것을 의미하며, 직접효과는 매개변수가 고정되어있을 때 흡연자는 의료비용이 23593.981만큼 더 높다는 것을 의미합니다. 이제 processR 패키지를 실행해보겠습니다.\n\n# processR\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", M = \"bmi\")\nmeanSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\n \nY\nM\nY\n\n\n\n\ncharges\nbmi\nadjusted\n\n\n\n\nsmoker(X) = 0\nMean\n8434.268\n30.652\n8438.77\n\n\n\nSD\n5993.782\n6.043\n\n\n\nsmoker(X) = 1\nMean\n32050.232\n30.708\n32032.751\n\n\n\nSD\n11541.547\n6.319\n\n\n\n\nMean\n13270.422\n30.663\n\n\n\n\nSD\n12110.011\n6.098\n\n\n\n\n\n\n\nAdjusted mean은 \\(adjusted\\;mean(\\bar{Y}^*) = i_{Y} + b\\bar{M} + c'X\\)로 계산할 수 있습니다. 설명변수가 0일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 0\\)이고 설명변수가 1일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 1\\)로 계산할 수 있습니다. 보정평균은 \\(X\\)일때 평균적인 \\(M\\)의 값을 가지는 사람은 보정평균만큼의 \\(Y\\)를 갖는다는 것을 의미합니다.\n아래처럼 각 계수를 깔끔하게 출력하는 함수도 존재합니다.\n\n# processR\nmodelsSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\nbmi(M)\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\na\n0.057\n0.413\n0.137\n.891\n\nc'\n23593.981\n480.180\n49.136\n&lt;.001\n\n\nbmi(M)\n\n\n\n\n\n\nb\n388.015\n31.787\n12.207\n&lt;.001\n\n\nConstant\niM\n30.652\n0.187\n163.895\n&lt;.001\n\niY\n-3459.096\n998.279\n-3.465\n.001\n\n\nObservations\n\n1338\n\n\n1338\n\n\nR2\n\n0.000\n\n\n0.658\n\n\nAdjusted R2\n\n-0.001\n\n\n0.657\n\n\nResidual SE\n\n6.100 ( df = 1336)\n\n\n7087.931 ( df = 1335)\n\n\nF statistic\n\nF(1,1336) = 0.019, p = .891\n\n\nF(2,1335) = 1283.923, p &lt; .001\n\n\n\n\n\n\n간접효과, 직접효과, 총효과를 다음 함수로 출력할 수 있습니다.\n\n# processR\nmodel &lt;- tripleEquation(labels = labels)\nsemfit &lt;- sem(model = model, data = cost)\n\nmedSummaryTable(semfit)\n\n\n\n\n\nEffect\nEquation\nestimate\n95% CI\n\n\n\nindirect\n(a)*(b)\n21.983\n(-292.098 to 336.063)\n\n\ndirect\nc\n23593.981\n(22653.900 to 24534.062)\n\n\ntotal\ndirect+indirect\n23615.964\n(22624.816 to 24607.111)\n\n\nprop.mediated\nindirect/total\n0.001\n(-0.012 to 0.014)"
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#조절효과",
    "href": "posts/2024-03-14-process-macro/index.html#조절효과",
    "title": "Process macro 소개",
    "section": "조절효과",
    "text": "조절효과\n조절효과는 설명변수가 반응변수에 미치는 영향이 다른 변수에 의해 변화될 때, 이 변화를 조절효과라고 하며, 이러한 영향을 주는 변수를 조절변수라고 합니다.\n단순조절효과(1번모델)는 다음의 다이아그램으로 나타낼 수 있습니다.\n\n\n\n\nFigure.2\n\n\n\nprocess()함수로 단순조절효과를 알아보겠습니다. plot 인자는 출력결과 하단에 테이블을 만드어줍니다.\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", w = \"age\", model = 1, plot = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : charges\n    X : smoker \n    W : age    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8495    0.7217 40903347.7298 1153.1995    3.0000 1334.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -2091.4206  582.5654   -3.5900    0.0003 -3234.2647  -948.5764\nsmoker   22385.5487 1278.7311   17.5061    0.0000 19877.0057 24894.0917\nage        267.2489   13.9285   19.1872    0.0000   239.9247   294.5731\nInt_1       37.9887   31.0950    1.2217    0.2220   -23.0116    98.9890\n\nProduct terms key:\nInt_1  :  smoker  x  age      \n\nTest(s) of highest order unconditional interaction(s):\n      R2-chng         F       df1       df2         p\nX*W    0.0003    1.4925    1.0000 1334.0000    0.2220\n----------\nFocal predictor: smoker (X)\n      Moderator: age (W)\n\nData for visualizing the conditional effect of the focal predictor:\n     smoker       age    charges\n     0.0000   22.0000  3788.0555\n     1.0000   22.0000 27009.3554\n     0.0000   39.0000  8331.2870\n     1.0000   39.0000 32198.3946\n     0.0000   56.0000 12874.5186\n     1.0000   56.0000 37387.4338\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n단순조절효과의 계수는 lm(charges ~ smoker + age + smoker * charges, data = cost)의 계수와 동일합니다.\n\\(\\hat{Y} = i_{Y} + b_{1}X + b_{2}W + b_{3}XW\\)일때, \\(b_{1} = 23385.55\\), \\(b_{2} = 267.25\\), \\(b_{3} = 37.99\\)이며 각 계수를 다음과 같은 의미를 가지고 있습니다.\n\n\\(b_{1} = W\\)가 \\(0\\)일때 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{X→Y} = b_{1} + b_{3}W\\)로 계산합니다.\n\\(b_{2} = X\\)가 \\(0\\)일때 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{W→Y} = b_{2} + b_{3}X\\)로 계산합니다.\n\\(b_{3} = W\\)가 한 단위 바뀔 때, \\(X\\)의 한 단위 변화가 \\(Y\\)에 영향을 미치는 정도의 차이입니다.\n\nproceeR 패키지로 확인해보겠습니다.\n\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", W = \"age\")\nmodel &lt;- lm(charges ~ smoker + age + smoker * age, data = cost)\n\n# processR\nm.summary &lt;- modelsSummary(list(model), labels = labels)\nmodelsSummaryTable(m.summary)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\nc1\n22385.549\n1278.731\n17.506\n&lt;.001\n\n\nage(W)\nc2\n267.249\n13.929\n19.187\n&lt;.001\n\n\nsmoker:age(X:W)\nc3\n37.989\n31.095\n1.222\n.222\n\n\nConstant\niY\n-2091.421\n582.565\n-3.590\n&lt;.001\n\n\nObservations\n\n1338\n\n\nR2\n\n0.722\n\n\nAdjusted R2\n\n0.721\n\n\nResidual SE\n\n6395.573 ( df = 1334)\n\n\nF statistic\n\nF(3,1334) = 1153.199, p &lt; .001"
  },
  {
    "objectID": "posts/2024-01-23-RND-review/index.html",
    "href": "posts/2024-01-23-RND-review/index.html",
    "title": "R&D 시험인증 후기 및 개발, 행정 관련 느낀 점",
    "section": "",
    "text": "공개SW 기반의 클라우드 통계 패키지SW 과제를 수행하면서, 외부 공인 기관에 시험인증을 의뢰해야 했습니다. 이와 관련된 후기, 또 전반적인 과제 관련 정보를 남깁니다. 당사로서는 처음 수행하는 R&D였습니다. 저희 팀원들과 공동연구기관에서 다 같이 열심히 참여하였기에 잘 완수할 수 있었습니다. 그러나 그 과정에서 어쩔 수 없이 겪은 몇가지 어려움이 있었기에 이를 공유합니다.\n\n개발\n최초에 R&D 연구개발계획서를 작성하면서, 계획서 표지(요약 페이지)와 하단에, 높은 TRL단계를 달성하겠다고 기재하였습니다. 또, 하단에는 외부 기관으로부터 인증을 받겠다고도 기재하였습니다.\n이를 인지하지 못하고 있다, 2차년도 10/26에 최초로 위와 같이 기재한 것을 발견하였습니다. 관련하여 주관 기관에, 수행하지 않아도 될지에 대해 문의하였으나 외부 시험이 필요할 것이라는 회신을 받았습니다(사유: 최초 연구개발계획서에 기재되어 있고, TRL이 높은 단계인 점 등 고려)\n그 후, 12/18에 현장 테스트가 완료되었으며, 12/27경 최종 보고서 초안를 받았습니다. 시험 준비부터 완료까지 거의 2달이 소요됩니다. 만약 이러한 준비가 되어 있지 않은 경우 마감이 아주 촉박해 제때 마무리 하지 못할 우려가 있습니다. 우리만 준비되면 되는 것이 아니고, 시험인증 업체측 일정도 고려해야 합니다. SW인증업체가 전국에 10개소 정도밖에 되지 않는 것으로 알고 있습니다. SW R&D는 최소 100건은 될 것입니다. 모두가 시험인증을 거치는 것은 아니지만, 미리 준비해야 합니다. 제가 10월 말경 문의하였을때 이미 12월 말까지 일정이 있어 의뢰받지 못한다고 회신한 업체도 2~3개정도 있었습니다.\n시험인증은 상당히 시간이 많이 소요되는 작업입니다. 가능하다면 피하는 것이 좋으나, 현재 우리 회사의 기술 수준을 볼 때 대부분 상용화/성숙 단계 R&D일 것이므로 피하기 어려울 것입니다. 미리 준비해야 합니다.\n\n연구개발계획서 내용을 R&D 참가 인원과 잘 공유하고, 우리가 해야 할 것을 정리\n처음 연구개발계획서를 가장 잘 쓰는 것이 중요하고, 그 R&D 전 기간에 거쳐 참가할 사람과 공유하여 향후 어떠한 검증을 외부 업체에서 받아야 하는지를 사전에 알고 있는 것이 중요합니다. 그렇게 해야만 해당 부분의 유지보수를 조금 더 잘 할 수 있으리라 생각합니다.\n현재는 연구개발계획서를 다 같이 보는 것이 아니라, 대표님이 작성하시고 그것을 개개인이 읽어 보는 형태로 진행되고 있습니다. 하지만 앞으로는 한 자리에 모여 다같이 연구개발계획서를 검토하고, 비현실적인 부분이 있는지, 또는 미리 준비해야 하는 부분이 있는지를 검토하고 가능하면 쉽게 수행할 수 있도록 향후 우려되는 요소를 미리 제거한 연구개발계획서를 작성하는 것이 중요하겠습니다.\n\n\nRFP에 기반한 목표치를 알맞게 설정\nRFP의 ‘국내 기업 최고치’, ‘해외 기업 최고치’를 준용하지 않는 경우(즉, 해당 값을 변경하거나 새로운 기준을 만드는 경우)에는 ’국내 기업 최고치’, ’해외 기업 최고치’가 해당 값인 것도 우리가 입증해야 함에 주의하여야 합니다(간사 문의사항). 다행히도, 이번 과제의 경우 RFP상에서 국내/해외 기업 최고치를 제시하였기에 우리가 입증 할 필요가 없었습니다. 시험인증이 불가하여 외부에 위탁하지는 않았지만, 예를 들어 ’분석 속도’에 정확한 기준을 적지 않은 채, 해외 기업을 100%라고 한 경우 매우 곤란한 경우가 생길 수도 있으리라 생각됩니다. 연구개발계획서를 작성 할 때에는 상대적인 수치가 아닌, 절대치로 기재하는것이 좋겠으며 달성이 어렵더라도, 입증 자체는 쉽게 할 수 있는 항목을 기재하는 것이 최종 결과보고서 작성에 유리합니다.\n\n\n다른 업체와의 업무 분담 명확화\n이번 시험인증에서는 다른 공동연구개발기관와 함께 시험인증을 진행했습니다. 그 공동연구개발기관 역시 시험인증에 대한 준비가 되지 않은 상황이었으며, 상대측 실무자 또한 경험이 많이 없어 미숙한 면이 있었다고 보입니다. 마찬가지로 연구개발계획서를 작성 할 때에 언제쯤, 어떻게 시험을 진행할 지, 각자 서류작업(페이퍼워크)의 분담은 어떻게 할 지 등을 미리 검토 할 필요가 있습니다. 그렇지 않으면 서로 소통이 되지 않아 일을 이중으로 하게 됩니다.\n이번 시험인증의 경우 다른 공동연구개발기관에 요청한 서류 대부분을 사용하지 못하고 우리 회사측에서 새로 작성하였습니다. 주관기관으로 참여할 경우 대부분의 페이퍼워크를 우리가 부담해야 하니 미리 알고 있을 필요가 있습니다.\n\n\n사전에 검증 가능한 항목인지, 공인인증 업체에 미리 확인\n통계분석 시간 단축률 항목의 경우, 다른 상용 프로그램과의 비교가 불가능하다는 시험인증 업체의 회신을 받았습니다, 이는 최종 평가에서 분명히 마이너스 요소가 될 수 있는 사안입니다. 다음 연구개발계획서 작성 시에는, 평가 항목을 사전에 업체와 조율하여 평가가 가능한지를 파악한 후 이를 기재하는것이 바람직하리라 생각됩니다.\n\n\n개발 시 시험항목과 테스트 항목에 맞추어 개발\n기능을 개발 할 때에, 시험항목에 포함되는 개발 항목과, 시험 항목에 포함되지는 않지만 연구개발계획서에는 존재하는 항목이 있습니다. 시험인증을 하지 않아도 되는 항목의 경우 일단 구현만 하면 연구개발계획서를 어기지는 않은 반면, 시험인증을 해야 하는 항목의 경우 여러 번/여러 플랫폼에서 테스트를 할 필요가 있고, 기능 명세도 작성해야 합니다. 또 이러한 항목의 경우 코드에 주석 작업을 철저히 하여 추후 오류가 발생했을 때 쉽게 수정할 수 있어야 할 것입니다. 미리미리 해 두지 않으면 추후 유지보수가 굉장히 어려워, 시간이 2~3배 더 소요될 수 있습니다.\n\n\n양식 작성 시 스크린샷은 최대한 자세하게\n이번 시험인증을 진행한 업체를 포함해, 약 4개정도의 업체에서 견적을 받거나, 최소한 문의하여 양식을 작성했습니다. 공통적인(비슷한) 양식이 있는 것을 보니 정형화된 서류가 존재하는 듯 합니다.\n이번에 수행한 업체의 경우, 해당 서류를 작성해서 제출했음에도 불구하고, 그 이후에 해당 서류의 내용을 바탕으로 전체적인 테스트를 수행한 후 보내달라고 하였습니다. 최초에 테스트 할 항목을 작성할 때, 미리 테스트를 겸하여 어떻게 작동되는지를 스크린샷으로 촬영 해 두면 간편하게 시험인증을 마무리 할 수 있습니다.\n\n\n테스트 도구의 도입\n아직 한번도 이런 도구를 사용해보진 못했지만, 이러한 도구의 필요성을 절실히 느꼈습니다. 예를 들어, statgarten의 ML 중 lightGBM 패키지가 업데이트 되어, 특정 버전을 지정하여 설치하도록 한 경우에는 다른 ML에 영향이 없는지 확인이 필요합니다. 어떤 dependency가 업데이트 되었을 때, 테스트 도구가 없다면 매번 모든 항목을 수동으로 테스트 해야 합니다. 이런 번거로움을 막고자 테스트 도구 도입을 검토 할 필요가 있습니다.\n\n\n\n행정\n\n연구비는 인건비와 연구활동비 위주로 편성\n연구비로 회사에 필요한 비품을 구매하거나, 용역을 주는 것은 쉽지 않습니다, 이번 IITP에서 회계를 제가 담당할 때는 대부분 인건비와 식비였기에 예산 집행에 큰 어려움은 없었습니다. 하지만 그 후에, NIPA나 창업도약패키지 연구비 집행을 옆에서 지켜 본 결과, 사실상 인건비와 식사 이외에는 사용하지 말라는 의도라고 생각하게 되었습니다. 실물 재료가 필요한 다른 연구와는 달리, 우리는 SW개발업이기에 인건비가 가장 많은 비중을 차지하고, 최대한 인건비를 연구비로 계상하는 것이 중요할 것 같습니다. 만약 다른 항목(예: 장비 구매 등)이 꼭 필요하다면, “협약 초기”에 끝내는 것이 좋겠습니다.\n2024년 1월 추가: 2023년 12월 28일부터 회의비 사용이 어려워졌습니다. 대부분 인건비로 처리가 필요합니다.\n\n\n간사와의 연락은 메일로\n출장 등으로 전화가 잘 되지 않을 뿐더러, 상대가 전화를 선호하지 않습니다. 한 명의 간사가 10개 이상의 사업을 관리하는 경우도 있기에 메일이 좋습니다. 또 추후 문제가 생겼을 때 우리가 대응하기 굉장히 껄끄러운 상대방입니다. 따라서 가능한 한 기록이 남는 메일로 연락을 주고받는것이 좋겠습니다.\n\n\n협약 변경은 한번에\n비교적 자유로웠던 IITP과제와는 달리, 현재 수행중인 타 과제는 협약 변경이 매우 번거롭습니다. 그 기관의 특성으로 보이나, 연구 분야를 보았을 때 우리가 추가로 R&D를 수행한다면 그 기관 과제일 확률이 매우 높습니다. 앞에 언급 한 것처럼 최대한 최초 연구개발계획서에 구체적으로, 우리가 유리한 방향으로 작성하는것이 좋습니다.\n또 위 기관의 경우 한번 협약 변경에 오랜 시일이 걸립니다. 지난 10월경 협약변경한 건은 약 1개월 걸렸으며, 그 과정에서 반려도 최소 5회 이상 당했습니다. 또, 과제 종료 시기(11월 이후?)가 되면, 원래는 협약변경 가능한 시기임에도 불구하고 협약변경을 잘 해주지 않으려 합니다. 이 점 감안하셔서, 하반기 진입 시 과제 상황을 한 번 체크하시는 것이 좋습니다.\n\n\n전문연구사업자\n전문연구사업자 자격이 있으므로 기존인력 인건비 계상이 가능합니다. 현재 수행하고 있는 R&D의 주관기관은 거부하였는데 해당 법령 소관기관인 과기부 문의 결과 부처 상관없이 기존인력 인건비 계상이 가능하여야 합니다. 현재는 기존인력인건비 계상이 꼭 필요하지는 않았지만, 앞으로 필요시 과기부 문의 결과를 바탕으로 요청해보아야 할 것 같습니다.\n\n\nSlack을 통한 계획서 공유는 지양\nSlack으로 계획서를 공유하면 간편하지만, 버전 관리 문제가 발생합니다. 같은 파일에 각기 다른 내용이 추가되고, 이들을 합칠 수 없는 문제가 생깁니다. 그리 길지 않은 문서라면 대조하며 확인이 가능하지만, 백 페이지가 넘는 문서들은 그렇게 대조하는 것이 매우 고통스러운 작업일 것입니다. 가급적이면 버전 관리가 가능한 회사 드라이브를 통해 하는 것이 좋습니다.\n\n\n주관기관이 대부분의 일을 합니다\n만약 우리가 주관기관으로 참여하게 되면 대부분의 서류 작업을 우리가 해야 합니다. 각 기관별 예산/성과등은 각 기관에게 요청할 수 있지만, 대부분의 페이퍼워크를 우리가 해야 하므로 미리 준비해야 합니다.\n\n\n\n결론 요약\n\n과제 지원/시작 단계에서 다같이 모여 연구개발계획서와 (있는 경우) 과업지시서를 읽고 논의하자\n개발 단계부터 체계를 갖추고, 테스트를 잘 수행하자\n협약 관련 사항은 여름까지는 완료하고, 인건비 이외 지출항목은 가능하면 연초에 소진하자\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{lim2024,\n  author = {Lim, Changwoo},\n  title = {R\\&D {시험인증} {후기} {및} {개발,} {행정} {관련} {느낀}\n    {점}},\n  date = {2024-01-23},\n  url = {https://blog.zarathu.com/posts/2024-01-23-RND-review/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLim, Changwoo. 2024. “R&D 시험인증 후기 및 개발, 행정 관련\n느낀 점.” January 23, 2024. https://blog.zarathu.com/posts/2024-01-23-RND-review/."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html",
    "href": "posts/2023-12-11-quarto-dashboard/index.html",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "",
    "text": "Quarto는 R Markdown을 기반으로 하는 문서 작성 도구입니다.\nR Markdown은 RStudio에서 제공하는 문서 작성 도구로, R 코드와 문서를 한 번에 작성할 수 있습니다.\nQuarto는 R Markdown의 flexdashboard의 역할을 이어가는 기능으로써, R Markdown의 장점을 그대로 가져오면서, R 뿐만 아니라 Python, Julia 등 다양한 언어를 지원합니다.\n본 게시글은 Quarto 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#개요",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#개요",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "",
    "text": "Quarto는 R Markdown을 기반으로 하는 문서 작성 도구입니다.\nR Markdown은 RStudio에서 제공하는 문서 작성 도구로, R 코드와 문서를 한 번에 작성할 수 있습니다.\nQuarto는 R Markdown의 flexdashboard의 역할을 이어가는 기능으로써, R Markdown의 장점을 그대로 가져오면서, R 뿐만 아니라 Python, Julia 등 다양한 언어를 지원합니다.\n본 게시글은 Quarto 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#quarto-dashboard-소개",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#quarto-dashboard-소개",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "Quarto Dashboard 소개",
    "text": "Quarto Dashboard 소개\n\n\n\n\n\n\nQuarto Versione\n\n\n\nQuarto Dashboard는 현재 개발 중인 기능으로, 1.4 버전 이상의 Quarto를 사용해야 합니다.\n\n\n이번 글에서는 Quarto를 사용하여 Markdown과 R, Python, Julia등을 활용해 아래 이미지 같은 인터랙티브한 대시보드를 만드는 방법을 소개합니다.\n\n\n더 많은 예시는 링크에서 확인 할 수 있습니다.\nQuarto dashboard는 다양한 언어를 활용할 수 있기 때문에 이들로 부터 파생되는 Plotly, Leaflet, Jupyter Widgets, Htmlwidgets를 포함한 다양한 커스텀 위젯을 사용할 수 있습니다.\n\n\n\n\n\n\nPrerequisite\n\n\n\nQuarto와 Rmarkdown에 대한 설명은 이전의 글과 자료를 참고하시길 바랍니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드의-구성",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드의-구성",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드의 구성",
    "text": "대시보드의 구성\n보통 대시보드의 구성은 아래 그림과 같이 5개의 영역으로 구분합니다.\n\n\n\nMain: 대시보드에서 주요 지표를 포함한 내용을 표현하는 공간입니다.\nHeader / Footer : 대시보드에 대한 일반적인 메타 정보를 소개합니다.\nNavigator: 대시보드가 여러 개의 내용을 담고 있어 main을 구분해야하는 경우 각 페이지를 구분짓는 역할을 합니다.\nSide: 대시보드의 내용을 조절하는 역할을 합니다.\n\n즉, 위의 예시에서 보여진 Labor and Delivery Dashboard는 아래와 같이 영역을 구분해볼 수 있습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-만들기",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-만들기",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드 만들기",
    "text": "대시보드 만들기\nqmd 파일에서 (이후 배포를 고려하면 파일명은 index.qmd를 권장합니다.)\n---\nformat: dashboard\n---\n를 추가하여 quarto dashboard를 만들 수 있습니다.\nQuarto dashboard에서 반드시 알아야 하는 컨셉은 3가지이며 하나씩 소개하면더 대시보드를 만들어가겠습니다.\n\n카드\n대시보드 내부 요소 배치 (레이아웃)\n대시보드 구성 (페이지)\n\n카드\n카드는 대시보드의 Main을 구성하는 그래프나 테이블, 값등을 포함하는 하나의 단위입니다.\nquarto에서는 다음과 같이 사용합니다.\n::: {.card}\nThis text will be displayed within a card\n:::\n추가로\n```{r}\n...\n```\n를 사용하는 기본 코드 블록은 Dashboard format에서 자동으로 카드로 변경됩니다.\n이때 카드에 사용할 수 있는 주요 옵션은 아래와 같습니다.\n\n\ntitle: 카드의 제목\n\nexpandable: 카드를 접을 수 있을지의 여부\n\noutput: 결과를 출력할지 여부\n\nlayout-ncol: 카드 내용을 구분할 column의 개수 (layout-nrow)\n\n이 외에 기본 코드블록을 사용하기 때문에 코드 블록의 옵션들을 사용 가능합니다.\n이제 카드를 qmd에 2개 추가해보겠습니다.\n```{r}\n#| echo: false\nlibrary(ggplot2)\n```\n\n```{r}\n#| title: \"Card 1\"\n#| layout-ncol: 2\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = wt)) + \n  geom_point()\n\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = qsec)) + \n  geom_point()\n```\n\n```{r}\n#| title: \"Card 2\"\n#| output: false\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = vs)) + \n  geom_point()\n```\n\n```{r}\n#| title: \"Card 3\"\n#| expandable: false\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = vs)) + \n  geom_point()\n```\n위 코드의 실행결과는 아래 그림처럼 2개의 카드를 만들어냅니다. (2번째는 output:false)\n\n\n레이아웃\n대시보드의 레이아웃은 특별한 설정을 하지 않으면 (위 예시처럼) 1개의 카드가 1개의 행으로 배치됩니다.\n그런데 ## Row 태그를 사용하면 ## Row 태그 아래에 있는 카드들을 1개의 행에 배치할 수 있습니다.\n(마찬가지로 별도의 설정을 하지 않으면 Column에 같은 크기로 배치됩니다)\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n:::{.card}\nCard 2-1\n:::\n\n:::{.card}\nCard 2-2\n:::\n\n\n추가로, Row로 먼저 행을 구분 한뒤, Column을 사용해 디테일한 배치도 가능합니다.\n이때 Column은 ### Column으로 ## Row안에만 사용할 수 있습니다.\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n### Column {width=40%}\n:::{.card}\nCard 2-1\n:::\n\n### Column {width=60%}\n:::{.card}\nCard 2-2\n:::\n\n\n\n\n\n\n\n\nScroll\n\n\n\n별 다른 설정을 하지 않으면 각 Row에 배치된 요소의 높이들의 합이 100%에 맞추어 크기가 일괄적으로 조절되지만, Scrolling 옵션을 사용하여 요소의 원래 크기를 유지하며 화면이 스크롤 되게 변경할 수 있습니다.\nformat: \n  dashboard:\n    scrolling: true \n\n\nTabset\n카드를 행과 열로 배치하는 것 외에도, Tabset을 사용하여 카드를 탭으로 구분할 수 있습니다.\n## Row {.tabset}\n\n\n페이지\n대시보드에 여러 요소들을 담아야 한다면 별도의 페이지로 구분하여 만들 수 있습니다.\n이때 페이지는 # 태그를 사용하고, yaml에 “title”을 설정해야합니다.\n---\ntitle: \"dashboard\"\nformat: dashboard\n---\n# Page A\n\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n### Column {width=40%}\n:::{.card}\nCard 2-1\n:::\n\n### Column {width=60%}\n:::{.card}\nCard 2-2\n:::\n\n# Page B\n\n:::{.card}\nCard 3\n:::\n\n\n이처럼 title을 설정하면 Header가 생성되며 Page 가 존재한다면 Navigation 역할도 같이 수행합니다.\nHeader\nHeader에서는 title 외에 author를 사용해 일종의 “subtitle” 역할을 할 수 있습니다. 추가로 logo를 사용하여 대시보드의 로고를 설정하거나, 외부 링크나 자료로 연동할 수 있는 nav-buttons도 사용가능합니다.\n네비게이션 버튼을 커스텀으로 제작하는 것에 대해서는 설명하지 않고 공식문서의 링크로 대체합니다.\n\n\n\n\n\n\nQuarto Versione\n\n\n\nQuarto 1.4.455 버전을 기준으로, 네비게이션 버튼은 버그로 아직 작동하지 않습니다.\n\n\nSidebar\nSidebar는 sidebar 태그를 사용하여 만들 수 있으며 특정 페이지에 종속되지 않습니다.\n\n# {.sidebar}\n\nSidebar content \n\n로고와 sidebar를 설정하고 난 대시보드의 결과는 다음과 같습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-채우기",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-채우기",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드 채우기",
    "text": "대시보드 채우기\n대시보드에 들어갈 수 있는 내용은 주로 r로 만든 결과물이지만, 다른 형태의 요소들도 활용할 수 있습니다.\n테이블\n여러 옵션이 있으며, kable, DT, reactable 정도가 사용됩니다.\n\nknitr::kable(mtcars) # kable\n\nDT::datatable(mtcars) # DT\n\nreactable::reactable(mtcars) # reactable\n\n\n\nValue Box\nQuarto 대시보드에서만 사용할 수 있는 “지표를 표현하기 위한” 특별한 방법입니다.\n아래 예시처럼 {.valuebox} 코드로 사용할 수 있습니다.\n옵션으로 색상은 primary, secondary, success, danger, warning, info, light, dark를 사용할 수 있으며, 사용 가능한 (bootstrap) 아이콘의 종류는 링크에서 확인 가능합니다.\n\n::: {.valuebox icon=\"pencil\" color=primary}\nArticles per day\n\n`r articles`\n:::\n\n\n\nTheme\n다른 Quarto 기능들과 마찬가지로 Quarto Dashboard도 yaml에서 사용되어 색상과 스타일을 꾸밀 수 있는 Theme 기능을 제공합니다.\n기본 값은 cosmo이고, 가능한 값의 목록은 링크에서 확인 가능합니다.\n---\nformat: \n  dashboard:\n    logo: Zarathu.png\n    theme: sandstone\n---\n\n\n\n\n\n\n\n\n대시보드 게시하기\n\n\n\nQuarto로 만든 Dashboard는 Quarto pub, Github page, Posit connect, Netlify 등의 서비스를 활용하여 공유할 수 있습니다.\n이 글에서는 다루지 않으며, 차라투의 Quarto 교육자료 17페이지를 참고하시길 바랍니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#다른-대시보드-툴과-quarto-dashboard의-차이",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#다른-대시보드-툴과-quarto-dashboard의-차이",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "다른 대시보드 툴과 Quarto Dashboard의 차이",
    "text": "다른 대시보드 툴과 Quarto Dashboard의 차이\n\n\nQuarto Dashboard는 Tableau, Power BI, Shiny나 Streamlit과 같은 대시보드를 만들 수 있는 도구와 비교될 수 있습니다.\n제일 먼저 대시보드 사용에 필요한 비용입니다. 즉 Tableau, Power BI등 상용 서비스는 클릭으로 쉽게 만들 수 있지만 사용하기 위해 비용이 발생합니다.\n한편 Quarto Dashboard는 별도의 비용은 없지만, 개발을 통해 대시보드를 만들어야 합니다.\n두번째로는 대시보드의 내용이 변하는 가의 여부입니다.\n즉 사용자의 선택 값에 따라 값이 동적으로 바뀌어야 한다면 Shiny나 Streamlit을 사용하는 것이 좋습니다.\n반면 사용자의 선택은 없고 값이 정적으로 고정되어 있다면 Quarto Dashboard를 사용하는 것이 좋습니다.\n이 둘의 차이는 동적 대시보드는 사용자의 입력을 처리하고 결과를 호스팅할 서버가 필요하고 이를 한 비용이 필요합니다. (정적은 비용이 들지 않습니다)\n\n\n\n\n\n\n정적/동적\n\n\n\n예를 들어, 어제의 매출 데이터를 계산하여 보여주는 대시보드는 날마다 값이 변하긴 하지만, 사용자의 입력이 없기 때문에 정적으로 이루어져 Quarto Dashboard로도 충분합니다.\n그런데 동일한 매출 데이터이지만, 사용자로부터 날짜를 입력 받고, 그에 따라 매출 데이터를 계산하여 보여주는 대시보드는 데이터가 변해야하기 때문에 Shiny나 Streamlit 과 같은 동적 대시보드가 필요합니다.\n\n\n(Quarto Dashboard에서의 페이지나 탭셋은 모든 결과를 만들어두고 필요에 따라 보여주는 내용을 다르게 한다의 관점으로 데이터가 변하지 않는 정적 기능입니다.)"
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#마치며",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#마치며",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "마치며",
    "text": "마치며\n이번 아티클에서는 새롭게 공개된 Quarto Dashboard를 통해 대시보드를 구성하는 방법에 대해 알아보았습니다.\n꼭 헬스케어, 메디컬 IT 업계에서뿐 아니라 다양한 산업과 역할에서 데이터를 활용하기 위해 대시보드를 활용하고 있습니다.\nQuarto와 사용 가능한 R이나 Python을 활용할 수 있다면 Quarto Dashboard를 사용해 정적 대시보드 구성과 공유가 더욱 편리하게 이루어지기를 기대하며 글을 마칩니다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#개요",
    "href": "posts/2023-11-10-discourse1/index.html#개요",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "개요",
    "text": "개요\n이번 글에서는 차라투에서 사용하는 커뮤니티 플랫폼인 Discourse를 사용해서 커뮤니티를 구축하는 과정에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#discourse",
    "href": "posts/2023-11-10-discourse1/index.html#discourse",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "Discourse",
    "text": "Discourse\nDiscourse는 쉽게 모던한 커뮤니티를 만들 수 있는 오픈소스 커뮤니티 플랫폼이며 사이트 개발과 배포를 완벽히 조절할 수 있도록 다양한 커스텀 옵션과 설치 방식을 제공합니다. 또한 10년이 넘게 수많은 테스트를 거쳐왔으며 카카오, Zoom, Jetbrain 등의 다양한 회사에서 사용 되고 있고 지속적으로 업데이트 되고 있습니다.\n또한 적절히 분리된 게시판에 Markdown 게시글을 작성하는 기능부터 채팅 기능까지 사용할 수 있고, 다양한 테마를 다운받아 사용하거나 직접 코드를 수정해서 커뮤니티를 꾸밀 수 있습니다. 마지막으로 AI를 통한 챗봇이나 Data Explorer를 사용한 SQL 분석 기능 등 다양한 범위의 플러그인을 제공하며 이를 직접 개발할 수도 있습니다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#discourse-커뮤니티-만들기",
    "href": "posts/2023-11-10-discourse1/index.html#discourse-커뮤니티-만들기",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "Discourse 커뮤니티 만들기",
    "text": "Discourse 커뮤니티 만들기\nDiscourse는 다양한 설치 방식을 지원하지만 공식적으로 지원되는 유일한 방법은 Docker 기반입니다.\nDocker를 사용하면 빌드에 시간이 걸리는 단점이 있지만 Discourse를 제작하는데 사용된 Rails 웹 애플리케이션 프레임워크의 복잡한 설정을 하지 않아도 되며, 쉽게 배포하고 업데이트 할 수 있습니다.\n반면에 macOS / Ubuntu / Windows 환경에 직접 설치하는 방법도 공식 GitHub에 자세히 작성되어 있습니다.\n\n1. 메일서버 설정\n메일 서비스는 계정 관리, 알림에 쓰이며 필수적으로 구성해야 합니다. 일반적인 Gmail과 같은 서비스가 아닌 transactional 이메일 서버를 사용해야 합니다.\n저는 지원되는 이메일 서비스 중 Mailjet을 사용했습니다. Mailjet은 무료 요금제를 사용했을 때 일일 200건/월 6000건의 이메일을 보낼 수 있기에 적합하다고 생각했습니다.\n\n도메인 인증받기\nMailjet 서비스에 회원 가입을 했다면 설정 페이지에 들어가서 도메인을 등록해줍니다.\n\nPending 우측 톱니바퀴 버튼 → Validate → TXT DNS 레코드 등록을 통해서 도메인을 인증해줍니다.\n\n도메인 인증이 되었다면 Authenticate this domain 버튼을 클릭해서 SPF 및 DKIM 설정을 진행합니다.\n\nTXT DNS 레코드를 추가해서 SPF 및 DomainKeys를 설정해서 mailjet 서비스가 도메인을 사용할 수 있도록 합니다.(반영까지는 시간이 걸릴 수 있으니 새로고침 해주시면 됩니다)\n\n설정이 모두 되었다면 위 사진처럼 Active 라고 나옵니다.\n\n\n발신 이메일 정하기\n도메인 인증 바로 밑에 발송 주소를 정할 수 있는 부분이 있으며 메일의 발송자를 지정할 수 있는 부분입니다.\nDiscourse는 알림 용도로 메일 서비스를 사용하기 때문에 noreply를 붙여서 설정했습니다.\n\n\n\nAPI Key 발급받기\n계정관리 페이지에서 Main account의 API KEY를 확인하고 SECRET KEY를 받을 수 있습니다. 추후에 다시 다운받으려면 재발급이 필요하기에 꼭 어딘가에 저장해둬야 합니다.\n\n\n\n\n2. Discourse 설치\nDiscourse를 도커에 설치하는 방법은 공식 가이드를 참조했습니다.\n먼저 명령어를 실행해서 도커 이미지를 다운로드 받습니다. root 권한은 빌드 등에 필요합니다.\n$ sudo -s # root 권한 얻기\n$ git clone https://github.com/discourse/discourse_docker.git /var/discourse\n$ cd /var/discourse\n$ chmod 700 containers # 권한설정\n$ vim containers/discourse.yml # 설정파일 생성(이름을 다르게 만들어서 구분이 용이하게 하려고 합니다.)\n공식 가이드 에서는 discourse-setup 라는 자동 설정 툴을 사용해서 질문에 답변을 하는 형태로 직관적인 설정을 지원하나, 디테일한 설정을 위해 설정 파일을 직접 만들어 주었습니다.\n## discourse.yml\n## 밑에 expose에서 443을 제외했기 때문에 ssl template를 주석처리 했습니다.\ntemplates:\n  - \"templates/postgres.template.yml\"\n  - \"templates/redis.template.yml\"\n  - \"templates/web.template.yml\"\n  ## Uncomment the next line to enable the IPv6 listener\n  #- \"templates/web.ipv6.template.yml\"\n  - \"templates/web.ratelimited.template.yml\"\n  ## Uncomment these two lines if you wish to add Lets Encrypt (https)\n  #- \"templates/web.ssl.template.yml\"\n  #- \"templates/web.letsencrypt.ssl.template.yml\"\n\n## which TCP/IP ports should this container expose?\n## If you want Discourse to share a port with another webserver like Apache or nginx,\n## see https://meta.discourse.org/t/17247 for details\n## Nginx에서 직접 SSL 설정을 하려고 하므로 443을 제외했습니다.\nexpose:\n  - \"80:80\"   # http\n    #  - \"443:443\" # https\n\nparams:\n  db_default_text_search_config: \"pg_catalog.english\"\n\n  ## Set db_shared_buffers to a max of 25% of the total memory.\n  ## will be set automatically by bootstrap based on detected RAM, or you can override\n  #db_shared_buffers: \"256MB\"\n\n  ## can improve sorting performance, but adds memory usage per-connection\n  #db_work_mem: \"40MB\"\n\n  ## Which Git revision should this container use? (default: tests-passed)\n  ## 기본 설정은 beta 버전으로 설치됩니다. 안정성을 위해 stable 버전으로 꼭 지정해주어야 합니다.\n  version: v3.1.2\n\n## 다중 언어를 지원하므로 꼭 ko로 하지 않아도 됩니다.\nenv:\n  LC_ALL: en_US.UTF-8\n  LANG: en_US.UTF-8\n  LANGUAGE: en_US.UTF-8\n  # DISCOURSE_DEFAULT_LOCALE: en\n\n  ## How many concurrent web requests are supported? Depends on memory and CPU cores.\n  ## will be set automatically by bootstrap based on detected CPUs, or you can override\n  #UNICORN_WORKERS: 3\n\n  ## Discourse를 배포할 도메인\n  ## TODO: The domain name this Discourse instance will respond to\n  ## Required. Discourse will not work with a bare IP number.\n  DISCOURSE_HOSTNAME: 'community.zarathu.com'\n\n  ## Uncomment if you want the container to be started with the same\n  ## hostname (-h option) as specified above (default \"$hostname-$config\")\n  #DOCKER_USE_HOSTNAME: true\n\n  ## 관리자 이메일\n  ## TODO: List of comma delimited emails that will be made admin and developer\n  ## on initial signup example 'user1@example.com,user2@example.com'\n  DISCOURSE_DEVELOPER_EMAILS: 'office@zarathu.com'\n\n  ## 메일서버 설정값\n  ## TODO: The SMTP mail server used to validate new accounts and send notifications\n  # SMTP ADDRESS, username, and password are required\n  # WARNING the char '#' in SMTP password can cause problems!\n  DISCOURSE_SMTP_ADDRESS: in-v3.mailjet.com\n  DISCOURSE_SMTP_PORT: 587\n  DISCOURSE_SMTP_USER_NAME: API KEY\n  DISCOURSE_SMTP_PASSWORD: \"SECRET KEY\"\n  #DISCOURSE_SMTP_ENABLE_START_TLS: true           # (optional, default true)\n  DISCOURSE_SMTP_DOMAIN: community.zarathu.com # mailjet에 등록한 도메인\n  DISCOURSE_NOTIFICATION_EMAIL: noreply@community.zarathu.com # 등록해둔 발신자 주소\n\n  ## If you added the Lets Encrypt template, uncomment below to get a free SSL certificate\n  #LETSENCRYPT_ACCOUNT_EMAIL: me@example.com\n\n  ## The http or https CDN address for this Discourse instance (configured to pull)\n  ## see https://meta.discourse.org/t/14857 for details\n  #DISCOURSE_CDN_URL: https://discourse-cdn.example.com\n  \n  ## The maxmind geolocation IP address key for IP address lookup\n  ## see https://meta.discourse.org/t/-/137387/23 for details\n  #DISCOURSE_MAXMIND_LICENSE_KEY: 1234567890123456\n\n## 도커 컨테이너와 연결될 호스트 볼륨 경로 / 로그파일 경로\n## The Docker container is stateless; all data is stored in /shared\nvolumes:\n  - volume:\n      host: /var/discourse/shared/standalone\n      guest: /shared\n  - volume:\n      host: /var/discourse/shared/standalone/log/var-log\n      guest: /var/log\n\n## 플러그인 설정\n## 빌드 전/후 실행될 명령어를 작성할 수 있음\n## Plugins go here\n## see https://meta.discourse.org/t/19157 for details\nhooks:\n  after_code:\n    - exec:\n        cd: $home/plugins\n        cmd:\n          - git clone https://github.com/discourse/docker_manager.git\n\n## Any custom commands to run after building\nrun:\n  - exec: echo \"Beginning of custom commands\"\n  ## If you want to set the 'From' email address for your first registration, uncomment and change:\n  ## After getting the first signup email, re-comment the line. It only needs to run once.\n  #- exec: rails r \"SiteSetting.notification_email='info@unconfigured.discourse.org'\"\n  - exec: echo \"End of custom commands\"\n위 설정 파일은 /var/discourse/sample/standalone.yml 을 베이스로 만들어진 예시입니다.\nDiscourse는 SSL을 위한 Let’s encrypt 인증서 관리를 자동으로 해주지만 추후에 와일드카드 인증서를 사용하기 위해 비활성화 시켰습니다. 기본 SSL을 활성화 시키려면 설정 파일에서 아래 부분을 수정해주시면 됩니다.\n## discourse.yml\ntemplates:\n    ...\n  - \"templates/web.ssl.template.yml\"\n  - \"templates/web.letsencrypt.ssl.template.yml\"\n...\nexpose:\n  - \"80:80\"   # http\n  - \"443:443\" # https\n...\nenv:\n    ...\n    LETSENCRYPT_ACCOUNT_EMAIL: me@example.com\nDiscourse 버전은 공식 GitHub Tags 에서 버전명을 확인한 뒤에 안정적인 최신 버전으로 수정할 수 있습니다.\n\n\n3. Discourse 시작\n설정 파일을 저장하고 Discourse를 실행하는 단계입니다. Discours는 bootstrap(빌드)을 하는데 약 2-8분이 소요되므로 설정을 수정하면 꽤 오랜 시간을 기다려야 합니다.\n설정 파일의 이름이 discourse.yml 이므로 파일명을 꼭 명시해줘야 합니다. 만약 파일명을 잘못 입력한다면 충돌이 일어나서 DB가 초기화될 수 있습니다.(경험담)\n## /var/discourse/launcher 명령어 설정파일명\n$ /var/discourse/launcher rebuild discourse\nrebuild 명령어는 아래 3개의 명령어를 실행하는 것과 동일하게 작동합니다. 만약 설정파일을 수정했다면, 간단하게 위 명령어를 사용해서 다시 배포하면 새로운 수정 사항이 반영됩니다.\n## equivalent with launcher rebuild\n$ /var/discourse/launcher stop discourse\n$ /var/discourse/launcher bootstrap discourse\n$ /var/discourse/launcher start discourse\n작업이 완료됐다면, 설정 파일에 명시한 HOST_NAME인 http://community.zarathu.com 으로 접속할 수 있으며 관리자 계정 생성을 하고 가이드를 따라서 초기 설정을 해주시면 됩니다. (SSL이 비활성화된 상태이므로 http로 접속가능)\n\n\n4. 외부 Nginx를 이용한 배포\n서비스 관리의 단순화, 서버 리소스 효율성, 와일드카드 인증서를 사용하기 위해 단일 Nginx 도커 인스턴스를 사용해서 Discourse를 운영하기로 결정했습니다.\n이를 위해 설정 파일을 수정해서 외부 포트 노출을 제거합니다.\n## discourse.yml\n...\n\n## \"80:80\" -&gt; \"80\"\nexpose:\n  - \"80\"   # http\n    #  - \"443:443\" # https\nDocker Network의 Bridge Network를 통해 Nginx와 Discourse 컨테이너를 연결해줍니다. Nginx 컨테이너는 실행되고 있다고 가정합니다.\n## mybridge라는 이름의 bridge 네트워크 생성\n$ docker network create --driver bridge mybridge\n\n## nginx, discourse 컨테이너를 mybridge 네트워크에 연결\n$ docker network connect mybridge nginx\n$ docker network connect mybridge discourse\nNginx sites-available 설정 파일을 생성합니다.\n## community.conf\n\n## 80포트(http)\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name community.zarathu.com;\n\n    add_header Content-Security-Policy upgrade-insecure-requests;\n        \n        ## https로 upgrade\n    location / {\n        return 301 https://$server_name$request_uri;\n    }\n}\n\n## 443 포트(https)\nserver {\n    listen 443 ssl http2;\n    server_name community.zarathu.com;\n    underscores_in_headers on;\n\n        ## 따로 만들어둔 와일드카드 인증서 경로\n    ssl_certificate /etc/letsencrypt/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/privkey.pem;\n    ssl_protocols        SSLv3 TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers          HIGH:!aNULL:!MD5;\n\n        ## 보안을 위한 헤더\n    add_header Strict-Transport-Security max-age=31536000;\n    add_header X-Content-Type-Options \"nosniff\" always;\n    add_header X-Frame-Options \"SAMEORIGIN\" always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Content-Security-Policy upgrade-insecure-requests;\n\n    ssl_stapling on;\n    ssl_stapling_verify on;\n    client_max_body_size 4G; # 사이트 업로드 크기 제한\n\n        ## Docker network를 이용한 프록시\n        ## discourse 컨테이너 내부 80포트로 연결됩니다.\n    location / {\n        proxy_pass http://discourse;\n\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header X-Forwarded-Proto https;\n                proxy_set_header X-Forwarded-Scheme https;\n\n        proxy_buffer_size          128k;\n        proxy_buffers              4 256k;\n        proxy_busy_buffers_size    256k;\n    }\n    location = /robots.txt {\n        return 200 \"User-agent: *\\nDisallow: /\";\n    }\n}\n설정 파일을 활성화 시키기 위해서 sites-enabled에 심볼릭 링크를 생성합니다.\n## nginx 설정 파일 위치는 /etc/nginx 로 가정\n$ ln -s /etc/nginx/sites-available/community.conf /etc/nginx/sites-enabled/community.conf\n변경된 설정을 반영하기 위해 Nginx를 다시 로드해줍니다. 기존에 배포된 서비스의 중단을 최소화 하기 위해 reload를 사용했습니다.\n## nginx 컨테이너의 nginx reload 명령 실행\n## docker exec -it 컨테이너명 nginx -s reload\n$ docker exec -it nginx nginx -s reload\n\n\n5. 개발 서버를 위한 다중 사이트 구성\nDiscourse는 설정 변경 후 bootstrap(빌드)을 하는데 매우 많은 시간이 소요되며 그 동안 기존의 서비스의 운영이 중단됩니다. 또한 여러 설정을 수정하거나 플러그인을 추가했을 때 오류가 발생할 수 있으므로 매우 위험합니다. 따라서 테스트를 할 수 있도록 개발용 Discourse 커뮤니티를 배포하게 되었습니다.\n메인 커뮤니티와 동일한 파일 경로를 공유하고 같은 DB를 사용하게 할 수도 있지만 사이트의 안정성을 위해 완전한 분리를 하려고 하므로 새로운 discourse를 설치했습니다.\n$ git clone https://github.com/discourse/discourse_docker.git /var/discourse-dev\n$ cd /var/discourse-dev\n$ chmod 700 containers # 권한설정\n$ vim containers/discourse-dev.yml # 설정파일 생성(이름을 다르게 만들어서 구분이 용이하게 하려고 합니다.)\n새로운 Discourse 설정 파일을 생성합니다. 대부분의 내용은 메인 커뮤니티와 동일합니다.\n## discourse-dev.yml\n...\nDISCOURSE_HOSTNAME: 'community.dev.zarathu.com'\n...\nvolumes:\n  - volume:\n      host: /var/discourse-dev/shared/standalone\n      guest: /shared\n  - volume:\n      host: /var/discourse-dev/shared/standalone/log/var-log\n      guest: /var/log\n...\n개발용 Discourse 커뮤니티를 실행합니다.\n$ /var/discourse-dev/launcher rebuild discourse-dev\n이후에 외부 Nginx를 이용한 배포 설정도 메인 커뮤니티 설정과 동일하나 dev 커뮤니티 도메인과 컨테이너 이름 등을 수정해서 배포해 주시면 됩니다.\n배포가 완료됐다면 메인 커뮤니티와 개발용 커뮤니티의 데이터를 동기화하기 위해 메인 커뮤니티를 백업합니다.\n\n백업이 완료됐다면 목록에 표시되며 다운로드 버튼을 클릭해서 파일로 저장합니다.\n\n저장한 파일을 개발 커뮤니티에 업로드한 뒤 복구를 진행하면 메인 커뮤니티의 사용자, 글, 설정 등이 개발용 서버에도 동일하게 적용됩니다.\n\n디스코스를 설치 한 다음, 몇가지 커스텀을 거쳐 실제로 사용하는 커뮤니티의 이미지는 다음과 같습니다.\n\n이어지는 글에서 커스텀 과정을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html",
    "href": "posts/2023-09-27-high-dpi-slide/index.html",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "",
    "text": "지난 게시글에서 officer 패키지를 활용해 R으로 만든 그림을을 벡터 이미지로 저장하는 방법을 다루었습니다. 이렇게 저장한 벡터 이미지는 확대를 해도 깨지지 않고 파워포인트에서 편집이 가능하다는 장점이 있습니다. 하지만 파워포인트 슬라이드를 그림으로 내보내기하면 저해상도의 이미지로 저장된다는 문제가 있습니다. 따라서 이번 글에서는 파워포인트로 저장한 벡터 이미지를 300DPI의 고해상도로 내보내는 방법을 알아보고자 합니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#개요",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#개요",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "",
    "text": "지난 게시글에서 officer 패키지를 활용해 R으로 만든 그림을을 벡터 이미지로 저장하는 방법을 다루었습니다. 이렇게 저장한 벡터 이미지는 확대를 해도 깨지지 않고 파워포인트에서 편집이 가능하다는 장점이 있습니다. 하지만 파워포인트 슬라이드를 그림으로 내보내기하면 저해상도의 이미지로 저장된다는 문제가 있습니다. 따라서 이번 글에서는 파워포인트로 저장한 벡터 이미지를 300DPI의 고해상도로 내보내는 방법을 알아보고자 합니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#dpi란",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#dpi란",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "DPI란?",
    "text": "DPI란?\n\n\n이미지 출처: https://itwiki.kr/w/DPI\n\n\nDPI란 Dot Per Inch의 약자로, 인쇄물에서 1인치(= 2.54cm)에 몇 개의 점이 찍히는지를 나타내는 단위입니다. DPI 값이 높을 수록 고해상도의 결과물을 얻을 수 있으며, 깨끗한 이미지를 얻기 위해서는 300DPI 이상이 권장됩니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#고해상도로-슬라이드-내보내기",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#고해상도로-슬라이드-내보내기",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "고해상도로 슬라이드 내보내기",
    "text": "고해상도로 슬라이드 내보내기\n1. officer 패키지를 사용해 파워포인트로 이미지 저장하기\n우선 지난 게시글에서 다루었던 officer과 rvg 패키지를 활용해 벡터 이미지를 파워포인트로 저장하겠습니다.\n\nlibrary(officer)\nlibrary(rvg)\nlibrary(ggplot2)\n\n# 이미지 생성\nplotObj &lt;- iris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n# ppt\nread_pptx() |&gt; # ppt 생성, 별도의 오브젝트로 저장하지 않아도 됨.\nadd_slide() |&gt; # 슬라이드 추가\n  ph_with( # 이미지 추가\n    dml(ggobj = plotObj), \n    location = ph_location_fullsize() \n  ) |&gt;\nprint('image.pptx') # ppt 저장 \n\n파워포인트의 파일 탭에서 다른 이름으로 저장/이미지로 저장을 선택하면 슬라이드가 각각 JPEG 파일로 저장됩니다. 저장된 이미지의 속성을 살펴보면 파워포인트 이미지 내보내기의 디폴드 해상도인 96DPI로 저장된 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\n2. 내보내기 해상도 설정 변경하기\n슬라이드를 고해상도 이미지로 저장하려면, 파워포인트의 내보내기 해상도 설정을 변경해야 합니다. 설정을 변경하기에 앞서, 모든 Windows 기반 프로그램을 종료하시길 바랍니다. 실행 중인 프로그램은 Ctrl + Shift + ESC 단축키를 통해 확인할 수 있습니다.\n\n시작 단추를 우클릭한 뒤, 실행을 선택합니다.\n열기 상자에 regedit을 입력한 다음 확인을 선택합니다.\n\n사용 중인 파워포인트 버전에 따라 아래 레지스트리 하위 키를 찾습니다.\n파워포인트 버전별 레지스트리 하위 키는 다음과 같습니다.\n\n\n\nOption 하위 키를 선택하고, 편집 탭의 새로 만들기/DWORD(32비트) 값을 선택합니다\nExportBitmapResolution을 입력한 다음 엔터키를 누릅니다.\nExportBitmapResolution이 선택되어 있는지 확인한 다음 편집 탭의 수정을 선택합니다.\nDWORD 값 편집 대화 상자에서 10진수를 선택한 뒤, 값 데이터에 300을 입력하고 확인을 선택합니다.\n\n\n\n파일 메뉴에서 끝내기를 선택해 레지스트리 편집기를 종료합니다.\n3. 슬라이드를 고해상도 그림으로 내보내기\n앞서 저장했던 파워포인트 파일을 다시 열어 파일 탭의 다른 이미지로 저장/이미지로 저장을 선택해 다시 슬라이드를 JPEG 파일로 저장합니다. 이번에는 300DPI의 고해상도 이미지로 잘 저장된 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#정리",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#정리",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 레지스트리 편집기에서 해상도 설정을 변경하여 파워포인트 슬라이드를 고해상도 이미지로 내보내는 방법에 대해 알아보았습니다. officer 패키지를 다룬 지난 글과 함께 R을 통해 이미지를 자유자재로 다루는 데 도움이 되기를 기대합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html",
    "href": "posts/2023-09-10-wasm2/index.html",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "",
    "text": "이번 글은 이전 글 에이어 정적 페이지에서 shiny application을 webR로 제공하는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#개요",
    "href": "posts/2023-09-10-wasm2/index.html#개요",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "",
    "text": "이번 글은 이전 글 에이어 정적 페이지에서 shiny application을 webR로 제공하는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#webr-r과-shiny의-차이",
    "href": "posts/2023-09-10-wasm2/index.html#webr-r과-shiny의-차이",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "webR (R)과 shiny의 차이",
    "text": "webR (R)과 shiny의 차이\nwebR을 사용하기 위해 R과 Shiny의 차이점중 한가지를 짚고 넘어가는 것이 좋습니다.\nR은 한번에 하나의 코드만 실행 가능합니다. 즉, 새로운 코드를 실행하기 위해서는 이전의 연산이 종료된 상태여야 합니다.\n그런데 Shiny는 (app.R이라는) 코드를 실행한 상태에서 사용자로부터 입력을 받고, 이를 통해 새로운 계산을 진행합니다.\n\n\n\n이는 webR과 유사하게 shiny에서 ( 로컬에 ) 별도의 server를 만들고, 브라우저의 입력값과 계산된 출력 값을 서버와 주고 받으면서 ui로 보내주는 과정을 거칩니다.\n그래서 webR과 Shiny를 이어주려면 아래 그림처럼 또 하나의 브라우저 (service worker)를 만든다고 이해하는 것이 편합니다.\n\n\n\nShiny를 사용자의 브라우저에서 실행시키면 얻을 수 있는 장점과 단점은 webR의 장단점과 대체로 유사합니다.\n그러나 service worker라는 추가 개념이 등장했고, 이로 인한 Cross-Origin Isolation이라는 새로운 문제점이 발생합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#cross-origin-isolation",
    "href": "posts/2023-09-10-wasm2/index.html#cross-origin-isolation",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "Cross-Origin Isolation",
    "text": "Cross-Origin Isolation\nShiny를 정적 페이지에서 제공하기 위해 Cross-Origin Isolation의 정확한 개념을 이해할 필요는 없지만, 설명을 하면 다음과 같습니다.\nservice worker는 별도의 프로세스를 위해 외부에서 (자체 실행을 위한) 파일을 다운로드 받고 이를 사용합니다.\n그런데 최근의 웹페이지에서는 보안 이슈로 인해 (검증되지 않은) 외부의 파일을 실행하는 것을 정책적으로 막고 있습니다.\n이를 위해 정적 페이지를 제공하는 서비스 (github, netlify, wordpress 등)에서 외부 파일의 실행을 허용하는 설정을 해야합니다.\n차라투 블로그가 사용하는 github page를 위해서는  enable-thread.js 라는 스크립트를 실행해야합니다.\n마찬가지로 Quarto를 활용해 웹페이지를 만들고, Shiny에서 기본으로 제공하는 01_hello 심도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#quarto-페이지의-구성",
    "href": "posts/2023-09-10-wasm2/index.html#quarto-페이지의-구성",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "Quarto 페이지의 구성",
    "text": "Quarto 페이지의 구성\nShiny를 정적페이지로 제공하는 quarto page에는 3가지가 필요합니다.\n\nJavascript 파일:\n\n\nservice worker를 만들기 위한 httpuv-serviceworker.js (from Inspired from Here)\n(github blog 기준) Cross origin isolation을 풀기 위한 enable-threads.js\n\n\nButton & iframe: shiny는 webR에 비해 조금 더 로딩이 필요하기 때문에 진행 상태를 보여줄 버튼과, shiny app을 심을 iframe이 필요합니다.\nJavascript 파일: webR을 준비하고, iframe에 Shiny를 심는 loadshiny.js\n\n&lt;!-- 1. scripts --&gt;\n&lt;script src='httpuv-serviceworker.js'&gt;&lt;/script&gt;\n&lt;script src='enable-threads.js'&gt;&lt;/script&gt;\n\n&lt;!-- 2. button & iframe --&gt;\n&lt;button class=\"btn btn-success btn-sm\" type=\"button\" style=\"background-color: dodgerblue\" id=\"statusButton\"&gt;\n  &lt;i class=\"fas fa-spinner fa-spin\"&gt;&lt;/i&gt;Loading webR...\n&lt;/button&gt;\n&lt;div id=\"iframeContainer\"&gt;&lt;/div&gt;\n\n&lt;!-- 3. webR & shiny--&gt;\n&lt;script type=\"module\" src='loadshiny.js'&gt;&lt;/script&gt;\n추가적으로 loadshiny.js의 두 부분을 커스텀 해야합니다. (TODO로 표기)\n\n임베드할 shiny app을 담고 있는 url 코드 (app.R): 이 글의 경우 차라투 블로그의 github에 (공개 된) 있는 코드를 사용합니다.\nservice worker를 적용할 범위: httpuv-serviceworker.js를 등록하고, index.html을 담고 있는 디렉토리를 scope에 저장합니다.\n\n마지막으로 디렉토리 구조는 다음과 같습니다.\n\n2023-09-10-wasm2\n\nenable-threads.js\nindex.qmd\nindex.html\nloadshiny.js\nhttpuv-serviceworker.js"
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#실제-webr-shiny-결과",
    "href": "posts/2023-09-10-wasm2/index.html#실제-webr-shiny-결과",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "실제 webR + shiny 결과",
    "text": "실제 webR + shiny 결과\n\n\n\n\n\n  Loading webR..."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#정리",
    "href": "posts/2023-09-10-wasm2/index.html#정리",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "정리",
    "text": "정리\n단순히 webR을 심는 것보다 shiny를 심는 것은 3가지 이유에서 조금 더 복잡합니다.\n\nservice worker 의 구성 및 연결\nshiny app의 실행을 위한 shiny 패키지를 webR에 설치\n실제 정적 페이지에 배포하기 전까지 로컬 PC 에서는 shiny 결과를 확인할 수 없음\n\n또한 이전의 webR과 마찬가지로 단순한 shiny app만 webR로 구현이 가능합니다.\n가벼운 (특히 shiny 교육) 용도로는 shiny app을 별도의 서버를 구성하지 않고도 정적 페이지로 사용자에게 제공할 수 있다는 점은 꽤 흥미롭고, (수초의 시간이 더 필요하긴 하지만) 여전히 개선할 수 있는 부분이 존재합니다.\n다만 R과 quarto를 넘어서 javascript를 포함한 웹 개발 지식과 service worker 구성을 위한 네트워크와 인프라 구성에 대한 경험이 없다면 많은 시행착오가 필요하기도 합니다.\n이 webR에 대해 appsilon은 초창기 기술인 만큼 아쉬운 점도 있지만 동적 페이지에서 제공되는 shiny app을 대체할 기술이라기보단, 상호간에 보조할 수 있는 방법이라고 평가했습니다.\n\n이전 글과 이번 글을 통해서 webR과 shiny를 정적페이지로 사용자에게 제공하는 방법을 알아보았습니다.\n배경 지식이 없다면 한번에 이해하긴 어렵고, 이렇게 사용할 수 도 있다는 것만 기억하셔도 충분합니다.\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html",
    "href": "posts/2023-07-01-officer/index.html",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "",
    "text": "이번 글에서는 R 패키지 officer를 사용하여 PPT 프레젠테이션에 벡터 그래픽을 만드는 과정에 대해 소개합니다. officer를 포함하여 몇몇 패키지들이 officeverse라고 불리는 생태계를 구성하고 있으며 PPT외에도 엑셀이나 워드로 R의 결과를 만들어 낼 수 있습니다만, 이번 글에서는 벡터 그래픽을 저장하는 목적으로의 officer에 한정합니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#오브젝트-만들기",
    "href": "posts/2023-07-01-officer/index.html#오브젝트-만들기",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "1. 오브젝트 만들기",
    "text": "1. 오브젝트 만들기\n\nppt &lt;- read_pptx()\n\nprint(ppt)\n\npptx document with 0 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\n\nread_pptx는 원래 ppt 파일을 R 오브젝트 형태로 읽기 위한 함수이지만, 만약 함수에 파일을 입력하지 않으면 새로운 ppt 오브젝트를 생성합니다.\n\nppt &lt;- read_pptx(\"mypptx.pptx\") # 기존 ppt 읽기 \nppt &lt;- read_pptx() # 새로운 ppt 생성\n\n한편 officer에는 read_pptx 외에도 read_docx(워드), read_xlsx(엑셀)도 존재합니다.\nppt 오브젝트를 콘솔에 입력하면, 몇개의 슬라이드로 구성되어있는지 확인할 수 있습니다. (layout과 master는 신경쓰지 않으셔도 좋습니다.)"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#슬라이드-만들기",
    "href": "posts/2023-07-01-officer/index.html#슬라이드-만들기",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "2. 슬라이드 만들기",
    "text": "2. 슬라이드 만들기\n처음 만든 ppt 오브젝트에는 pptx document with 0 slide(s), 즉 슬라이드가 없습니다.\n이 오브젝트에 슬라이드를 추가하는 것은 add_slide()로 할 수 있습니다.\n\nppt |&gt;\n  add_slide() # ppt &lt;- ppt |&gt; add_slide() 로 안해도 됨\n\npptx document with 1 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\nppt # pptx document with 1 slide(s)\n\npptx document with 1 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\n\nadd_slide()에는 layout과 master라는 옵션을 지정할 수 있고 가능한 값은 다음과 같습니다.\n\nTitle Slide\nTitle and Content (기본값)\nSection Header\nTwo Content\nComparison\nTitle Only\nBlank\n\n아마 눈치 채셨을 수도 있겠지만 레이아웃은 콘솔에서 ppt 오브젝트를 확인할 때 나오는 것들이며,\n우리의 목적은 슬라이드 구성이 아닌 이미지 저장이기 때문에 어떤 값을 선택해도 동일한 결과를 얻을 수 있습니다.\n아래의 이미지는 각 옵션들을 적용하여 만든 슬라이드의 결과물로 모두 동일한 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#벡터-그래픽스-슬라이드에-추가",
    "href": "posts/2023-07-01-officer/index.html#벡터-그래픽스-슬라이드에-추가",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "3. 벡터 그래픽스 슬라이드에 추가",
    "text": "3. 벡터 그래픽스 슬라이드에 추가\n앞서 만든 ggplot의 결과를 ph_with이라는 함수로 슬라이드에 추가할 수 있습니다.\n\nppt |&gt; ph_with( # paragraph의 ph가 아닐까 생각\n  dml(ggobj = plotObj), # 앞에서 만들었던 ggplot 이미지 오브젝트\n  location = ph_location_fullsize() # 쉬운 편집을 위해 이미지의 크기를 슬라이드에 가득 채움\n)\n\n여기서 dml은 DrawingML이라는 오피스 프로덕트(pptx)에 XML로 이미지를 만들기 위한 내용입니다. location에는 다른 옵션도 있지만 ph_location_fullsize를 권장합니다\n만약 여러개의 이미지를 여러장의 슬라이드로 집어넣어 만들고 싶다면 다음처럼 ph_with를 pipe (|&gt;)로 이어서 사용 할 수 있습니다.\n\nppt |&gt; \n  add_slide() |&gt; # 1번째 슬라이드 \n  ph_with(\n    dml(ggobj = plotObj),\n    location = ph_location_fullsize()\n  ) |&gt;\n  add_slide() |&gt; # 2번째 슬라이드\n  ph_with( \n    dml(ggobj = plotObj2),\n    location = ph_location_fullsize()\n  ) |&gt;\n  add_slide() |&gt; # 3번째 슬라이드 \n  ph_with( \n    dml(ggobj = plotObj3),\n    location = ph_location_fullsize()\n  )"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#ppt-저장",
    "href": "posts/2023-07-01-officer/index.html#ppt-저장",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "4. ppt 저장",
    "text": "4. ppt 저장\n마지막으로 add_slide와 ph_with를 통해 만든 슬라이드는 print로 현재 작업중인 디렉토리에 (getwd()로 확인) 저장할 수 있습니다.\n\ngetwd() # ppt가 저장되는 위치\nppt |&gt; \n  print(target = \"myPrint.pptx\")\n\n이렇게 만들어진 pptx는 파워포인트와 키노트, 그리고 구글 슬라이드에서 작업할 수 있습니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#정리",
    "href": "posts/2023-07-01-officer/index.html#정리",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "정리",
    "text": "정리\n위 4개의 단계를 1개의 코드로 연결하면 다음과 같습니다.\n\nlibrary(officer)\nlibrary(rvg)\nlibrary(ggplot2)\n\n# 이미지 생성\nplotObj &lt;- iris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n# ppt\n\nread_pptx() |&gt; # ppt 생성, 별도의 오브젝트로 저장하지 않아도 됨.\nadd_slide() |&gt; # 슬라이드 추가\n  ph_with( # 이미지 추가\n    dml(ggobj = plotObj), \n    location = ph_location_fullsize() \n  ) |&gt;\nprint('image.pptx') # ppt 저장 \n\n한편, officer를 활용하여 더 자세한 ppt 생성과 편집도 가능하지만, 이 글에서는 다루지 않으며 quarto를 활용한 revealjs 슬라이드 생성하는 방법을 링크로 대신 첨부해드립니다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html",
    "href": "posts/2023-04-28-tidycdisc/index.html",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "",
    "text": "Clinical trials 데이터가 CDISC 형태로 통일되어 가며, CDISC에 대한 관심도가 많아지고 있다. 오늘 CDISC와 대략적인 data format에 대해 배워보고, CDISC ADaM 데이터를 시각화 해주는 Shiny App 오픈소스 패키지인 tidyCDISC에 대해서 소개하려고 한다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#cdisc란-왜-중요할까",
    "href": "posts/2023-04-28-tidycdisc/index.html#cdisc란-왜-중요할까",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "1. CDISC란? 왜 중요할까?",
    "text": "1. CDISC란? 왜 중요할까?\n임상시험에서 수많은 종류의 데이터들이 수집된다. 임상시험 데이터는 의약품의 안전성과 유효성을 증명하며, 임상효과를 확인하고 이상반응을 조사하는 데 사용될 수 있어 매우 중요하다.\n그러나 문제는 각 기관마다 통일된 규정이 없어서 데이터들의 변수 이름도 제각각이고, 데이터의 구조와 정의가 다 다르다.\n▶️ 데이터를 정리하고 분석하는지에 대한 국제적인 표준이 없다면, 데이터를 해석하고 설명하는 데만 상당한 시간과 노력이 필요하여서 비효율적이다.\n이런 비효율성을 없애고자 CDISC라는 비영리 단체가 1997년에 설립되어 규제기관, 제약회사, 임상연구 조직 등을 규합해서 임상시험 데이터의 표준을 정의하였다.\n이제 미국 FDA나 일본의 PMDA와 같은 규제기관들도 신약개발 및 임상시험에서 데이터를 CDISC 표준으로 제출하기를 요구하게 되는 만큼 CDISC 표준은 널리 사용되고 있기에, 이에 대해서 잘 알고 있는 것이 중요하다.\n  image from Kor J Clin Pharmacol Ther \nCDISC에는 데이터와 관련된 SDTM (Study Data Tabulation Model) 과 ADaM (Analysis Data Model)이 있다.\n\nSDTM은 임상시험의 데이터를 제출하기 위해 정의한 표준으로, Raw Data를 정해진 형식으로 정리/정의한다.\nSDTM을 이용하여 데이터분석이 가능하도록 변환한 형태로 변환하는게 ADaM이다.  How we build ADaM from SDTM\n\n 즉, ADaM은 데이터의 도출과 분석, SDTM은 Raw Data를 형식에 맞게 잘 정리하여 테이블에 정렬하는 것!\n이 글에서는 ADaM에 대해서 알아본 후, tidyCDISC에 대해서 소개하고자 한다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#adam이란",
    "href": "posts/2023-04-28-tidycdisc/index.html#adam이란",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "2. ADaM이란?",
    "text": "2. ADaM이란?\n우선 ADaM의 데이터 구조에 대해서 배우기 전에, CDISC에서 정의한 ADaM의 Fundamental Principles 를 확인해 보자.\n  image from PharmaSUG \nADaM의 Fundamental Principles 는 데이터셋 구조가 아닌 분석 요구와 이해에 중점을 둔다. ADaM은 분석 가능한 데이터셋을 만드는 역할을 한다.\n ADaM에서 정의된 Data Structure는 4개가 있다:\n\nADSL (subject-level analysis dataset)\nBDS (basic data structure)\nOCCDS (occurence data structure)\nOTHER\n\n  copy of “ADaMIG v1.1 Figure 1.6.1 Categories of Analysis Datasets”\n\n이런 Data Structure가 왜 필요할까?\n\n분석을 위한 structure가 필요하기 때문이다!\n\n분석 목적에 따라 맞는 특정한 data structure를 사용한다.\n\n\n\n2.1. ADSL (Analysis Data Subject Level)\nADaM의 첫 번째 데이터 구조는 ADSL(Analysis Data Subject Level)이다.\nADSL은:\n\n개별 연구대상자를 가진다, contains one record per subject\n한 행에 대상자의 정보, 임상시험의 정보 등 SDTM의 한 대상자의 모든 자료가 들어가고, 다른 ADaM datasets의 분석에 필요한 data가 있다.\n주요 변수는 ID (USUBJID), 약물그룹(TRT01P), 시작일(RFSTDTC), 종료일 (RFENDTC) 등이 있다.\n\nADSL example in tidyCDISC\n 다른 변수들의 의미는 여기를 확인할 수 있다.\n\n\n2.2. BDS (Basic Data Structure)\nADaM의 두 번째 데이터 구조는 BDS(Basic Data Structure)이다.\n\n한 대상자에 대한 반복적인 혹은 여러 번의 결과가 나타나 있는 데이터이다.\n대상자, 분석 변수, 분석 시점별로 하나 이상의 데이터가 존재한다.\nBDS 에서는 분석하려는 매개변수(예: PARAM 및 관련 변수들)을 설명하고 분석할 값 (예: AVAL 및 AVALC 등 관련 변수들)을 포함하는 중앙 변수 집합이 포함된다.\n\nPARAM: 분석하고자 하는 값에 대한 설명\nAVAL/AVALC: 분석하고자 하는 값\n\nADSL 및 OCCDS와 같은 다른 데이터 구조의 기초 또는 시작점이기 때문에 “Basic”라고 한다.\n기본 데이터 집합(예: 치료, 인구학 및 안전성 데이터)을 처리하는 데 사용되는 데이터 구조이다.\n반복측정이 계획되어 있거나, 이미 반복적으로 측정한 값이다.\nBDS는 부작용이나 기타 발생 데이터(other occurrence data)의 발생률 분석은 지원하지 않는다.\n모든 ADSL 변수가 BDS dataset에 있을 필요가 없다.\n\n\n2.2.1. ADLB (Laboratory Data Analysis Dataset):\n\n검사 데이터, laboratory test results data.\n주요 변수는 ID (USUBJID), 검사항목 (LBTESTCD, LBTEST), 결과값 (LBORRES)이 있다.\n\n CDISC. (2011). Analysis Data Model Implementation Guide: ADaM Version 1.1.\n\n\n2.2.2. ADEFF (Analysis Dataset Definition):\n\n메타데이터 (다른 데이터를 설명해 주는 데이터) 테이블이다.\n분석하려는 data set의 내용 및 구조를 설명한다.\nADaM dataset를 만드는데 ADEFF table이 사용된다.\n\n The ADEFF table should be completed before creating the analysis datasets to ensure consistency in variable definitions and to allow traceability of the analysis datasets back to their source data.\n table from CDISC.org\n\n\n2.2.3. ADTTE (Analysis Data Time-to-Event):\n\n임상 시험에서 기록하고자하는 사건(event)의 발생 시간\nmore detailed information about ADTTE could be found here \n\n [PARAMCD:Parameter Code, STARTDT: Time to event origin date for subject, ADT: Analysis date, SRCDOM: Source Data] \n\n\n\n2.3. OCCDS (Occurence Data Structure)\nADaM Dataset structure 중 세 번째 OCCDS는 한 대상자에 대한 반복적인 결과가 나타난다는 점에서 BDS와 비슷하다.\n 하지만, OCCDS는 BDS처럼 반복적으로 측정되지 않으며, 한 대상자에 대한 결과가 한 건도 발생하지 않거나, 이와 반대로 무수히 많이 발생할 수도 있다.\n OCCDS 는 부작용과 같은 discrete event를 분석하는 데 사용된다.\nCDISC ADaM structure for OCCDS v1.0\n OCCDS의 종류를 알아보자!\n\n2.3.1. ADAE (Adverse Events Analysis Datset):\n\n부작용 정보에 대한 dataset\nOne record per subject per adverse event\n주요 변수는 임상시험 과정: ID (USUBJID), 부작용종류 (AETERM), 발생일 (AESTDTC), 해결일 (AEENDTC), 중증도(AESER)\n\nADAE example in tidyCDISC\n\n\n2.3.2. ADCM (Concomitant Medications Analysis Datase)\n\n복용 약물정보\n주요 변수는 ID (USUBJID), 약물명 (CMTRT), 시작일 (CMSTDTC), 종료일 (CMENDTC), 용량 (CMDOSFRQ)\nOne record or multiple records per subject per recorded medication occurrence or constantdosing interval\n\nexample ADCM dataset: \n\n\n\n2.4. OTHER\nOTHER의 dataset은:\n\nADaM의 Fudamental Principles 및 기타 ADaM 규칙(naming convention 등등)을 따르지만, ADaM의 정의된 3개의 데이터 구조(ADSL, BDS, OCCDS)를 따르지 않는다.\n\n details on other"
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#r-shiny-app-tidycdisc",
    "href": "posts/2023-04-28-tidycdisc/index.html#r-shiny-app-tidycdisc",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "3. R Shiny App tidyCDISC",
    "text": "3. R Shiny App tidyCDISC\ntidyCDISC는 오픈소스 프로그램이며, ADaM-ish 데이터로 인터랙티브한 표, 그래프, 그리고 환자들의 프로필 생성을 할 수 있는 shiny app이다.\n\ntidyCDISC의 데모를 같이 실행해보자 ▶️ https://rinpharma.shinyapps.io/tidyCDISC/\ntidyCDISC의 세 가지 주요 기능:\n\nDrag-and-Drop Table Generator\nPopulation Explorer (Graph Generator)\nIndividual Explorer/ Patient Profile Viewer\n\n 각 기능에 대해 세 개의 R 패키지를 사용한다:\n\nTable Generator ▶️ GT 패키지\nPopulation Explorer ▶️ plotly 패키지지\nPatient Profile Viewer ▶️ timevis 패키지지\n\n 차근차근 각 패널이 어떤 기능이 있는지 확인해 보자!\n\n3.1. Data Upload 패널\n맨 처음 tidyCDISC 데모에 들어가면 아래와 같이 Data Upload 패널이 랜딩 페이지로 보일 것이다. 데모에는 CDISC Pilot Data 예시 데이터가 사용되고 있다.\n\ntidyCDISC 앱은 ADaM(-ish) 데이터가 없으면 사용할 수가 없다. 앱을 실행하려면 최소한 ADSL sas7bdat 파일이 필요하며, 더 많은 데이터가 있을수록 더 많은 기능과 인사이트를 탐색할 수 있다.\n\n\n\n3.2. Table Generator 패널\n다음 Table Generator 탭에 들어가면 인터랙티브한 테이블을 만들 수 있다.\nTable Generator 탭은 두 개로 구분된다. 왼쪽 영역은 테이블을 만드는 데 사용되는 드래그 앤 드롭 인터페이스이고 오른쪽 영역은 실시간 테이블 출력한다.\n우선 테이블을 생성하려면 왼쪽의 변수 블록을 “Variable” 드롭 영역으로 끌어다 놓고, ANOVA, CHG, MEAN, FREQ 등등을 “Stats” 드롭 영역에 끌어다 놓으면 된다.\n\n\n변수들 위에 Standard Analysis Tables 드롭 다운에는 규제 당국에 제출할 문서에 공통으로 포함되는 테이블 list가 나온다.\n\n테이블 중 하나를 선택하면, 해당 테이블을 생성할 때 필요한 변수와 Stats가 올바른 순서로 선택되어 원하는 테이블을 생성합니다.\n\nGroup Data By 드롭다운을 사용하여 범주형 변수들의 통계량을 계산할 수도 있다.\n\n\n\n밑에 Table Title로 테이블의 이름을 설정하고 파일로(RTF, CSV, 그리고 HTML) 저장이 가능하다.\n\n\n\n\n\n3.3. Population Explorer 패널\n다음으로 Population Explorer 탭에서 여러 차트로 데이터 시각화를 할 수 있다.\n\n\n\n\n\n\n\n\n\n\nType of Chart:\n\n원하는 차트의 종류를 고를 수 있다.\n\nGeneral plot controls:\n\n차트 유형에 따라 바뀐다.\n일반적으로 BDS 데이터 소스의 변수 또는 매개 변수 등을 사용하여 축을 설정하도록 된다.\n\n\n\n\n\n위 설정에 따라 메인 패널에는 아래와 같은 인터랙티브 그래프가 표시될 것이다.\n\n\n\n\n3.4. Individual Explorer 패널\n마지막 탭은 Individual Explorer이다. 이 탭에서는 특정 환자 데이터를 탐색하기 위해 사용됩니다. 처음 들어가시면 환자의 USUBJID로 환자를 선택할 수 있는 기능이 있습니다. 특정 그룹에 포함된 환자 데이터를 탐색해야 할 경우 (예를 들어, 나이가 10세 이하), Advanced Pre-Filtering을 이용하면 된다.\n\n Details on filtering. \n 특정 환자를 선택한 후, 밑에 Events에서는 환자 타임라인과 events에 대한 데이터 테이블을 확인할 수 있다.\n\nEvents 탭 바로 옆에 Visits탭에서는 BDS data sets에서의 PARAMS과 Study Visit의 plot과 데이터 테이블을 보여준다.\n\n환자의 특정 변수별 plot을 담은 파일을 png 혹은 html 파일로 다운 가능하다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#마치며",
    "href": "posts/2023-04-28-tidycdisc/index.html#마치며",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "마치며",
    "text": "마치며\n지금까지 CDISC의 ADaM에 대해서 4개의 data structure를 살표보며 알아갔고, ADaM 데이터를 활용한 오픈소스 프로그램인 tidyCDISC를 사용하는 방법에 대해서 배웠다.\n여러 해외 규제기관에서 임상이나 비임상시험 데이터 제출시 CDISC 적용을 의무하고 있는 만큼, CDISC 표준을 잘 이해하고 자료 관리에 있어 적용이 필요해 보인다.\nCDISC의 전문가가 되지 않는한 어렵고 복잡한 것을 알 필요는 없지만, 임상시험 결과가 ADaM을 이용해서 만들어야 하기 때문에 적어도 CDISC data format과 임상시험의 전체 흐름에 대해서 잘 파악하고 있는 것이 중요할 것 같다.\n\n\nReference\n\n\nJeong, Sunok, et al. “International Standard in Electronic Clinical Trial.” Journal of Korean Society for Clinical Pharmacology and Therapeutics, vol. 15, no. 1, Korean Society for Clinical Pharmacology and Therapeutics, 2007, p. 20. Crossref, https://doi.org/10.12793/jkscpt.2007.15.1.20.\n“Get Started With {tidyCDISC}.” Get Started With {tidyCDISC}, cran.r-project.org/web/packages/tidyCDISC/vignettes/getting_started.html.\n“SDTMIG v3.3.” SDTMIG v3.3 | CDISC, www.cdisc.org/standards/foundational/sdtmig/sdtmig-v3-3/html#Representing+Relationships+and+Data.\n“TidyCDISC an Open Source Application to Interactively Create Tables, Figures, and Patient Profiles.” YouTube, 17 Aug. 2020, www.youtube.com/watch?v=EFGkHrV0WbY.\nLi, Chengxin. “The Dataset Generation for Survival Analysis With the ADaM Basic Data Structure for Time-to-Event Analyses (ADTTE) Standard.” Pharmaceutical Programming, vol. 5, no. 1–2, Informa UK Limited, Dec. 2012, pp. 1–4. Crossref, https://doi.org/10.1179/1757092112z.0000000001."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html",
    "href": "posts/2023-03-17-pkgdown/index.html",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "",
    "text": "R 패키지를 개발할 때, 개발 자체도 힘들지만, 패키지를 잘 설명하는 문서화 역시 매우 중요합니다.\n아무리 좋은 기능을 개발해두었어도, 어떤 기능이 있는지, 어떤 방식으로 사용할 수 있는지… 등을 작성해 두지 않으면 (코드를 열어보기 전까진 모르기 때문에) 패키지를 사용하려는 사람들로부터 사랑받기 어렵습니다.\n특히나, 패키지를 설치해서 ? 를 통해 확인할 수 있는 것과, 설치 하기 전에 깃헙의 패키지 리포지토리에서 확인할 수 있는 것은 꽤 차이가 큽니다.\n하지만 문서화와 이 결과물을 웹페이지로 만드는 것은 많은 시간과 노력이 필요한 작업입니다.\n이런 문제를 해결하기 위해 pkgdown이라는 R 패키지가 등장했습니다. 이번 글에서는 pkgdown을 사용하여 R 패키지를 문서화하는 페이지를 만드는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#pkgdown-설치-및-환경-설정",
    "href": "posts/2023-03-17-pkgdown/index.html#pkgdown-설치-및-환경-설정",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "pkgdown 설치 및 환경 설정",
    "text": "pkgdown 설치 및 환경 설정\n위에서 서술한 것처럼 pkgdown 또한 하나의 R package이기 때문에 설치를 해야합니다.\n\n# install.packages('pkgdown') CRAN version\nremotes::install_github('r-lib/pkgdown') # Github version\n\nlibrary(pkgdown)\nlibrary(usethis) \n\npkgdown은 패키지의 웹페이지를 만드는 역할을 하기 때문에 pkgdown의 “대상이 되는” R 패키지로 pkgdown.tutorial이라는 간단한 패키지를 먼저 만들었습니다. (위 링크 참조)\npkgdown.tutorial\n\n💡 ttest와 ttest2는 동일한 내용의 함수이며, roxygen2의 효과를 보기 위해 비교용도로 사용합니다.\n\n\n# ttest.R (= ttest2.R)\nttest &lt;- function(x, y = NULL, alternative = 'two.sided', \n                  mu = 0, paired = FALSE, var.equal = FALSE, \n                  conf.level = 0.95, ...){\n  t.test(x, y, alternative, mu, paired, var.equal, conf.level, ...)\n}\n\n패키지에 ttest와 ttest2라는 함수를 만들고 패키지 빌드 직후의 구성 상태는 아래와 같습니다.\n/pkgdown.tutorial\n  - .gitignore\n  - .Rbuildignore\n  - DESCRIPTION\n  - NAMESPACE\n  - pkgdown.tutorial.Rproj\n  - /R\n    - ttest.R\n    - ttest2.R\n이후 pkgdown.tutorial의 작업 디렉토리에서 (.Rproj를 열어) usethis::use_pkgdown()을 실행합니다.\n그 결과 아래 이미지처럼 _pkgdown.yml이라는 파일이 생기는 것을 확인 할 수 있습니다.\n\n\n💡 .gitignore에서 docs를 삭제해주세요.\n\nbuild_site\npkgdown의 핵심 코드를 하나만 고르라면 pkgdown::build_site()입니다.\n이는 현재 작업된 내용들을 기반으로 웹사이트를 만드는 역할을 하는 함수입니다.\n바로 실행해보면 아래와 같은 결과를 확인할 수 있습니다.\n\n💡 library(pkgdown)을 실행했다면 앞의 pkgdown::은 붙이지 않아도 좋습니다"
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#pkgdown의-구성-요소",
    "href": "posts/2023-03-17-pkgdown/index.html#pkgdown의-구성-요소",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "pkgdown의 구성 요소",
    "text": "pkgdown의 구성 요소\npkgdown에서 웹페이지 제작을 위해 제공하는 주요 요소들을 소개하겠습니다.\n_yml\n_pkgdown.yml은 보여지는 웹사이트를 구성하는 파일입니다.\n\n💡 yml은 들여쓰기 (indent)를 깐깐하게 사용하기 때문에 에러가 난다면 이를 확인해보는 것이 좋습니다.\n들여쓰기를 하나도 하지 않은 (처음 url과 같은 위치) 경우를 lv0이라 표현합니다.\n\n1. template (lv0)\ntemplate: \n  bootstrap: 5\n  bootswatch: flatly \n처럼 변경하여 웹사이트의 테마를 바꿀 수 있습니다. 아래의 예시에서는 flatly를 사용했습니다.\n\ntheme에서 사용할 수 있는 옵션은 bootswatch의 theme를 소문자로 입력한 값이며, 필요한 경우 bslib 옵션을 활용하여 더 자세한 커스터마이즈를 할 수 있습니다.\ntemplate:\n  bootstrap: 5\n  bslib:\n    bg: \"#202123\"\n    fg: \"#B8BCC2\"\n    primary: \"#306cc9\"\n2. navbar (lv0)\nnavbar는 웹페이지 윗부분의 네비게이션 바 구성을 설정할 수 있는 옵션입니다.\n만약 yml의 내용을\nnavbar:\n  structure:\n    left:  [intro, reference, articles, tutorials, news]\n    right: [search, github]\n처럼 작성한다면\n\n\nnavbar의 왼쪽정렬로 intro, reference., articles, tutorials, news\n\n\n오른쪽 정렬로 search, github를 배치할 수 있습니다.\n\n위의 예시에서 표기된 기본 제공되는 구성요소의 설명은 이러합니다.\n\n\nintro: Get Started 페이지\n\nreference: R패키지의 함수(예시의 ttest) 기능 설명\n\narticles: 추가로 만든 rmd 아티클 파일\n\ntutorials: 튜토리얼 (개인적으로는 헷갈리니 articles로의 사용을 권장합니다)\n\nnews: NEWS.md 설명\n\nsearch: 웹페이지의 검색창\n\ngithub: 패키지를 담고 있는 깃헙 리포지토리 링크 (패키지 DESCRIPTION 에서 설정)\n2-1. intro\nGet Started 페이지는 패키지와 동일한 이름을 갖는 rmd(예시는 pkgdown.tutorial.rmd)로 아티클을 추가해야만 합니다.\n아티클을 추가하는 것에 대해서는 아래에서 자세하게 다루겠습니다. (지금은 아래 코드를 실행만 하면 됩니다.)\n\n💡 usethis::use_article(“pkgdown.tutorial”, “intro”)\n\n2-2. reference\nttest.R에서 roxygen2를 활용하여 함수 description을 만들고 나면 그 결과가 reference에 나타납니다. (/man 디렉토리에 .rd 파일을 생성합니다.)\nroxygen2에서 사용 가능한 태그의 종류는 다양하며, 보통은 @import, @export, @title, @description, @details, @param, @returns, @examples 정도가 권장됩니다.\n앞서 만들었던 ttest.R에 아래 내용을 코드의 맨 위에 추가한 다음, CTRL/CMD + SHIFT + D를 통해 일부 내용만 reference를 만들어 보겠습니다. (ttest2.R은 비교용)\n#' @title ttest\n#' @description run t test\n#' @details\n#' alternative = \"greater\" is the alternative that x has a larger mean than y. For the one-sample case: that the mean is positive.\n#' If paired is TRUE then both x and y must be specified and they must be the same length. \n#' Missing values are silently removed (in pairs if paired is TRUE). \n#' If var.equal is TRUE then the pooled estimate of the variance is used. \n#' By default, if var.equal is FALSE then the variance is estimated separately \n#' for both groups and the Welch modification to the degrees of freedom is used.\n#' If the input data are effectively constant (compared to the larger of the two means) \n#' an error is generated.\n#' @param x a (non-empty) numeric vector of data values.\n#' @param y an optional (non-empty) numeric vector of data values.\n#' @returns A list with class \"htest\" containing the following components:\n#' @examples t.test(1:10, y = c(7:20))      # P = .00001855\n#' @export\n이후 pkgdown::build_site()를 실행하면 아래 이미지처럼 reference 페이지가 navbar에 생성 되는 것을 확인할 수 있습니다.\n\n2-3. articles\nusethis::use_article(&lt;ARTICLENAME&gt;, &lt;PAGETITLE&gt;)의 형태로 사용 할 수 있습니다.\n\n💡 여기서 ARTICLENAME에는 숫자, 문자 그리고 -와 _ 만 활용할 수 있습니다. (소문자를 권장합니다)\n\nusethis::use_article(\"using-ttest\", \"perform t-test\") 코드를 실행하면 using-ttest.Rmd라는 파일이 생성되며 build_site()를 통해 그 결과를 반영 할 수 있습니다.\n\n3. components (navbar &gt; lv1)\nnavbar에서 소개 되지 않은, 기본 제공 되지 않는 구성요소는 아래처럼 작성하여 사용할 수 있습니다.\nnavbar:\n components:\n   articles: \n    text: Articles\n    menu:\n    - text: Category A\n    - text: Title A1\n      href: articles/a1.html\n    - text: Title A2\n      href: articles/a2.html\n    - text: -------\n    - text: \"Category B\"\n    - text: Article B1\n      href: articles/b1.html\n이는 이렇게 해석할 수 있습니다.\nArticles라는 (text:) 메뉴의 하위 구성요소로\n\nCategory A (그룹)\nTitle A1 (a1.rmd에서 생성)\nTitle A2 (a2.rmd에서 생성)\n구분선 (——)\nCategory B (그룹)\nArticle B1 (b1.rmd에서 생성)\n\n\n💡 usethis::use_article(“a1”,“A1 article”)…로 a1,a2,b1 아티클을 추가하세요\n\n4. footer (lv0)\n크게 중요한 것은 아니지만, 모든 페이지에 공통으로 나타날 수 있게 하는 역할을 합니다.\nfooter:\n  structure: \n    left: developed_by\n    right: built_with\n\n5. DESCRIPTION\n자세한 설명은 링크를 참조하세요.\n원래는 패키지 개발을 하면서 채워졌어야 하지만, pkgdown.tutorial에서는 미처 채워지지 못한 부분들로 아래와 같이 채우겠습니다.\n\n💡 먼저 usethis::use_mit_license()등을 통해 라이센스를 설정하고, 그 다음 DESCRIPTION을 채우는 것을 권장합니다.\n\nPackage: pkgdown.tutorial\nTitle: tutorial pkgdown\nVersion: 0.0.1\nAuthors@R: \n    person(\"Jinhwan\", \"Kim\", , \"jinhwan@zarathu.com\", role = c(\"aut\", \"cre\"))\nDescription: contains base ttest function \nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nSuggests: \n    rmarkdown\nURL: https://github.com/jhk0530/pkgdown.tutorial\nURL을 추가 한 것에 유의하세요 (navbar의 github 버튼에 사용됩니다)\n\n\n오른쪽의 Links, License, Developers 등이 채워졌음을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#메인-페이지-추가",
    "href": "posts/2023-03-17-pkgdown/index.html#메인-페이지-추가",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "메인 페이지 추가",
    "text": "메인 페이지 추가\n여기까지 잘 따라왔다면, 패키지의 구조는 아래 이미지와 같습니다.\n\n이제 usethis::use_readme_md()를 사용하여 README.MD를 추가하여 메인 페이지를 만들어줍니다.\ngithub repository를 만들면서, add readme를 통해 만들었어도 상관 없지만, 위 함수를 사용하면 최소 템플릿을 만들어 주기 때문에 조금 더 편리할 수 있습니다.\n\n💡 rmd를 선호한다면 usethis::use_readme_rmd()를 사용해도 좋습니다.\n\n최종 결과는 아래 이미지와 같습니다.\n\n단, 지금은 주소창이 https://로 시작하지 않는, 작업자의 pc에서만 확인 할 수 있는 형태라는 것을 확인해야합니다."
  },
  {
    "objectID": "posts/2023-02-14-shiny.likert/index.html",
    "href": "posts/2023-02-14-shiny.likert/index.html",
    "title": "shiny.likert 패키지 소개",
    "section": "",
    "text": "개요\n\n이전 글을 보고 오면 이해에 조금 더 도움이 됩니다*\n\n순서를 가진 범주형 데이터는 A~ E, 좋음 ~ 안좋음. 상위 10% ~ 하위 10%, NPS 1-10과 같은 예시들로 꼭 의료 도메인이 아니더라도 다양한 분야에서 활용되고 있습니다.\n이러한 데이터를 수집하는 방법은 보통, 설문조사를 위해 쓰이는 Google Forms나 Typeform과 같은 온라인 설문조사 도구를 활용할 수 있습니다.\n이제 이 결과를 활용하기 위한 방법은 정말 많지만, 설문 데이터를 시각화를 하는 방법은 대부분 의 경우 pie chart 혹은 barchart 정도만 활용하게 됩니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nR이나 Python 과 같은 프로그래밍 경험이 없거나 혹은 google studio, tableau public과 같이 “상용 시각화 툴”에서 제공하지 않는 차트라면 likert chart와 같은 방법을 활용하는 것은 꽤나 골치 아픈 일입니다.\n다행히 R에서는 Shiny라는 R의 기능을 웹으로 보내주는 라이브러리가 있고, 이를 활용하면 누구나 web application의 주소만 알고 있으면 해당 페이지에 접속하여 R의 기능을 사용하여 likert chart를 만들어 낼 수 있습니다.\n이 글에서는 likert 패키지를 웹에서 사용할 수 있게 하는 shiny app을 만드는 과정에 대하여 가볍게 다뤄보겠습니다.\nshiny design\n\nUI 디자인에 대한 내용은 언급하지 않습니다.\n\nshiny application에 제일 먼저 필요한 기능은 사용자의 설문조사 결과 데이터 (csv)를 업로드 하는 기능입니다.\n처음에는 googlesheets4라이브러리를 사용하여 많이 쓰이는 google sheets를 url만 복사하여 사용하게 만드는 것도 고려했으나, 이렇게 하기 위해서는 해당 시트를 외부에 노출 하거나, 구글 권한 문제를 shiny에서 같이 해결해야하기 때문에 너무 복잡하여 고려하지 않고, 대신 다운로드 받은 csv만 작업할 수 있게 합니다.\n\n이후 사용자의 혹은 예시로 github에 올려둔 데이터를 사용자가 업로드 하면 여러 column 중에 어떤 부분을 차트로 그릴지 선택해야 합니다.\n다행히 likert 패키지에서는 항목만 같다면 여러개의 차트도 동시에 그릴수 있기 때문에 multiple select의 형태로 만들어줍니다.\n\ncolumn을 선택하고 나면, column의 내용과 차트 옵션이 나타납니다.\n\n단, column의 내용. 데이터의 경우 좋음 ~ 안좋음. 동의함 ~ 동의하지 않음 등과 같이 사용자가 설계한 내용에 따라 다른 항목을 가질 수 있고, 이러한 항목의 순서가 likert에서는 중요하기 때문에 입력한 column의 순서를 사용자가 다시 바꿀 수 있게 구현합니다.\n\n이후 옵션을 조절한 후 draw button 을 클릭하여 차트를 만들어낼 수 있습니다.\n그러나 사용자는 처음에 어떤 옵션이 어떤 역할을 하는지 알 수 없기 때문에 차트가 만들어지고 난 이후에 옵션을 변경해도 차트에 반영되도록 구현합니다.\n\n차트는 ggplot + plotly를 같이 활용하여 interactive하게 만들어 지기 때문에 사용자가 png로 다운로드하여 활용할 수 있게 구현합니다.\n\n이 상태에서의 shiny application은 말 그대로 “works-on-my-machine ¯_(ツ)_/¯” 이기 때문에 이를 다른 사람도 웹에서 사용할 수 있게 배포를 해야합니다.\ndeploy shinyapps\nshiny application을 배포하는 방법은 shinyapps.io, shiny server, Rstudio connect 3가지로 볼 수 있는데 각각의 특징은 이러합니다.\n\nshinyapps.io:\n\n\nposit의 클라우드 인프라를 이용한 배포 방식. Rstudio와 바로 연동하여 편리하게 올릴 수 있다는 장점이 있다.\n다양한 요금제를 제공하며 무료 요금제의 경우 사이즈가 작은 shiny application을 5개까지 운영할 수 있다.\n\n\nshiny server:\n\n\n자체 서버/인프라를 가지고 있는 경우(온프레미스) 이를 설치하여 배포하는 방법\n비용이 들지 않음.\n\n\nRstudio connect:\n\n\n이전의 shiny server pro에 몇 기능을 더 추가한 옵션.\nRstudio와 바로 연동하여 편리하게 배포 할 수 있다.\ncommercial product이기 때문에 팀 단위로 비용이 든다.\n\n다행히 shiny.likert는 복잡한 기능, 많은 패키지 등을 사용하지 않는 “가벼운” Application이기 때문에, 그냥 제 개인 shinyapps.io 계정에 배포를 해도 문제가 없습니다.\n이 app을 배포하는 방법은 간단합니다. Rstudio에서 편집한, 잘 돌아가는 app.R로 이동하여 오른쪽 위의 connect 버튼을 누르고 이후의 몇번 더 클릭만 하면 됩니다. 물론 shinyapps.io 계정은 미리 만들어두어야 합니다.\n\n몇분 정도 지난 후 배포를 마치고 나면 https://jhkim.shinyapps.io/shiny-likert/ 와 같은 주소를 통해 누구나 shiny.likert 패키지를 활용하여 likert chart를 만들 수 있습니다.\n아래의 이미지는 shiny.likert를 이용하여 만든 이미지 입니다.\n\nshiny app to R package\nshinyapps를 통해 배포하는 방법은 편리하지만, 3가지 문제점을 가지고 있습니다.\n\n사용량이 많은 경우 요금제가 막힘\n클라우드 무료 요금제의 인스턴스는 로컬에 비해 성능이 많이 모자람 (물론 제약이 생길 정도까진 아닙니다)\nR을 사용할 수 있는 사람도 shiny.likert를 사용하기 위해 shinyapps만 사용해야함\n\n물론 이 외에도 나중에 유지보수를 위해 로컬에서 작업해야하는 경우를 위해 package의 형태로도 개발하겠습니다.\n이전에 만든 shiny package 중 하나는 끌어다 사용하는 라이브러리의 api 변화로 로컬에서 작동하게 하려면 최근 버전에 맞추어 수정을 해야합니다.\n이 방법은 dean attali님의 아티클을 참조하였습니다.\n\n현재 작업중인 디렉토리에 R Package를 생성\n\n\ndevtools::create_package(getwd())\n\n\nR package의 Description을 변경.\n\n\n\nImports를 추가합니다.\n\n\n\ninst 디렉토리를 만들고, shinyApp이라는 디렉토리를 만들어 작업했던 app.R이나 www/styles.css를 이동합니다.\n\n\n\ninst는 고정이고, shinyApp은 이름을 바꾸어도 상관없습니다.\n\n\n\nR 디렉토리에 shiny.likert라는 함수를 추가합니다. 내용은 아래와 같습니다.\n\n\n\nshinyApp과 package 이름 shiny.likert에 주의합니다.\n\n\n#' @importFrom shiny runApp\n#' @export\nshiny.likert &lt;- function(){  \n    appDir &lt;- system.file(\"shinyApp\", package = 'shiny.likert' )  \n    shiny::runApp(appDir)\n}\n\n\n이후 roxygen2를 활용하여 export등의 documentation을 하고, package를 build합니다.\n\n그 결과, 아래의 코드를 통해 누구나 로컬에서도 shiny.likert를 활용할 수 있습니다.\n\nremotes::install_github('zarathucorp/shiny.likert')\nlibrary(shiny.likert)\nshiny.likert()\n\ngithub package\nshiny.likert는 github에 올려진 R package이기 때문에 shinyreadme와 polaroid를 사용하여 readme.md를 수정하고, pkgdown을 활용해 사이트를 제작합니다.\npkgdown에서의 Get started는\n\npkgdown::build_article(\"shiny.likert\", \"shiny.likert\")\n\n를 통해 만들 수 있습니다. 이외의 과정들은 별도로 설명하지 않으며, 결과는 아래와 같습니다.\n\n\n\n\nReuseCC BY-NC 4.0CitationBibTeX citation:@online{kim2023,\n  author = {Kim, Jinhwan},\n  title = {Shiny.likert {패키지} {소개}},\n  date = {2023-02-15},\n  url = {https://blog.zarathu.com/posts/2023-02-14-shiny.likert/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKim, Jinhwan. 2023. “Shiny.likert 패키지 소개.” February\n15, 2023. https://blog.zarathu.com/posts/2023-02-14-shiny.likert/."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html",
    "href": "posts/2023-02-05-likert/index.html",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "사용자로 부터 수집되는 데이터중 많은 부분은 설문조사를 통해 얻을 수 있으며, 제시된 문장에 얼마나 동의하는지의 단계를 나타내는 표현하는 방법으로 리커트 척도 라는 방법이 쓰이기도 합니다.\n하나의 예시를 들면 다음과 같습니다.\n\n\n\n\n  \n  매우 그렇다\n  \n\n  \n  그런 편이다\n  \n  \n  \n  보통이다\n  \n  \n  \n  그렇지 않다\n  \n  \n  \n  전혀 그렇지 않다\n  \n\n\n이 설문을 10명에게 진행했고, 그 결과가 다음 테이블과 같다고 가정해보겠습니다.\n\n\nLoading required package: xtable\n\n\n\n\n\n읽기쉬움\n\n\n\nid1\n그런 편이다\n\n\nid2\n보통이다\n\n\nid3\n매우 그렇다\n\n\nid4\n보통이다\n\n\nid5\n전혀 그렇지 않다\n\n\nid6\n그렇지 않다\n\n\nid7\n그런 편이다\n\n\nid8\n그런 편이다\n\n\nid9\n그렇지 않다\n\n\nid10\n전혀 그렇지 않다\n\n\n\n\n\n이렇게 순서가 있는, 여러 카테고리의 데이터를 어떻게 시각화 할 수 있을까요? (Likert 외에도 NPS도 해당합니다.)\n\n제일 먼저 해볼 수 있는 것은, 문항별로 개수를 보여주는 것입니다.\n\n\n\n\n\n\n\n\n이 방법 자체는 나쁘지는 않지만, 1개가 아닌 여러개 문항의 결과를 보여줘야 한다면 아쉬운 문제점이 생깁니다.\n\n다음과 같이 데이터를 세줄 더 추가해보겠습니다.\n\n\n\n\n\n읽기쉬움\n도움이됨\n따라하기쉬움\n\n\n\nid1\n그런 편이다\n전혀 그렇지 않다\n그런 편이다\n\n\nid2\n보통이다\n전혀 그렇지 않다\n그렇지 않다\n\n\nid3\n매우 그렇다\n그런 편이다\n그렇지 않다\n\n\nid4\n보통이다\n보통이다\n그런 편이다\n\n\nid5\n전혀 그렇지 않다\n매우 그렇다\n그렇지 않다\n\n\nid6\n그렇지 않다\n전혀 그렇지 않다\n그런 편이다\n\n\nid7\n그런 편이다\n그렇지 않다\n보통이다\n\n\nid8\n그런 편이다\n그렇지 않다\n그렇지 않다\n\n\nid9\n그렇지 않다\n그렇지 않다\n그런 편이다\n\n\nid10\n전혀 그렇지 않다\n전혀 그렇지 않다\n전혀 그렇지 않다\n\n\n\n\n\n이 여러개의 데이터를 표기 하는 방법은 문항을 하나의 축 (x)에, 그리고 문항의 결과들을 나머지 축(y)을 활용하여 그려야 하기 때문에 “stacked bar”를 활용하는 방법이 있습니다.\n\nImage from The Data Visualisation Catalogue\n\n\n\n\n\n\n\n\n문항별로 응답자 수는 같기 때문에 (NA는 고려하지 않습니다) 전부 높이가 동일한 결과가 나오게 됩니다.\n그런데 주의해야할 점으로는, 당연하게도 문항에 따라 분포가 다를 수 있습니다. “한쪽으로 몰리는” 답이 나오는 경우가 있을 수 있다는 의미입니다.\n위의 그림에서 보통이다는 동일하게 각각 1개씩 답변이 있음에도 불구하고 도움이됨의 위치는 높게 되어있어 시각적 비교에 혼선을 줄 수 있습니다.\n이를 위해서 아래 이미지처럼 Neutral(회색)의 위치를 고정시키고, 이를 기준으로 위 아래로 Positive / Negative를 붙여 표현하는 방법을 고려할 수 있습니다.\n\nImage from daydreamingnumbers\n이러한 차트의 정확한 이름은 모르겠지만, 이 글에서는 편의상 Likert chart라 표현하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#barplot",
    "href": "posts/2023-02-05-likert/index.html#barplot",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "제일 먼저 해볼 수 있는 것은, 문항별로 개수를 보여주는 것입니다.\n\n\n\n\n\n\n\n\n이 방법 자체는 나쁘지는 않지만, 1개가 아닌 여러개 문항의 결과를 보여줘야 한다면 아쉬운 문제점이 생깁니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#stacked-bar-plot",
    "href": "posts/2023-02-05-likert/index.html#stacked-bar-plot",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "다음과 같이 데이터를 세줄 더 추가해보겠습니다.\n\n\n\n\n\n읽기쉬움\n도움이됨\n따라하기쉬움\n\n\n\nid1\n그런 편이다\n전혀 그렇지 않다\n그런 편이다\n\n\nid2\n보통이다\n전혀 그렇지 않다\n그렇지 않다\n\n\nid3\n매우 그렇다\n그런 편이다\n그렇지 않다\n\n\nid4\n보통이다\n보통이다\n그런 편이다\n\n\nid5\n전혀 그렇지 않다\n매우 그렇다\n그렇지 않다\n\n\nid6\n그렇지 않다\n전혀 그렇지 않다\n그런 편이다\n\n\nid7\n그런 편이다\n그렇지 않다\n보통이다\n\n\nid8\n그런 편이다\n그렇지 않다\n그렇지 않다\n\n\nid9\n그렇지 않다\n그렇지 않다\n그런 편이다\n\n\nid10\n전혀 그렇지 않다\n전혀 그렇지 않다\n전혀 그렇지 않다\n\n\n\n\n\n이 여러개의 데이터를 표기 하는 방법은 문항을 하나의 축 (x)에, 그리고 문항의 결과들을 나머지 축(y)을 활용하여 그려야 하기 때문에 “stacked bar”를 활용하는 방법이 있습니다.\n\nImage from The Data Visualisation Catalogue\n\n\n\n\n\n\n\n\n문항별로 응답자 수는 같기 때문에 (NA는 고려하지 않습니다) 전부 높이가 동일한 결과가 나오게 됩니다.\n그런데 주의해야할 점으로는, 당연하게도 문항에 따라 분포가 다를 수 있습니다. “한쪽으로 몰리는” 답이 나오는 경우가 있을 수 있다는 의미입니다.\n위의 그림에서 보통이다는 동일하게 각각 1개씩 답변이 있음에도 불구하고 도움이됨의 위치는 높게 되어있어 시각적 비교에 혼선을 줄 수 있습니다.\n이를 위해서 아래 이미지처럼 Neutral(회색)의 위치를 고정시키고, 이를 기준으로 위 아래로 Positive / Negative를 붙여 표현하는 방법을 고려할 수 있습니다.\n\nImage from daydreamingnumbers\n이러한 차트의 정확한 이름은 모르겠지만, 이 글에서는 편의상 Likert chart라 표현하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#예시-데이터",
    "href": "posts/2023-02-05-likert/index.html#예시-데이터",
    "title": "likert 패키지 소개",
    "section": "예시 데이터",
    "text": "예시 데이터\n시각화 패키지의 특성상 사용자의 데이터의 형태와 패키지가 요구하는 형태가 다를 수 있어, likert 패키지에서는 활용할 수 있는 예시데이터를 제공합니다.\n\n# install.package('likert')\nlibrary(likert)\ndata(pisaitems)\nhead(pisaitems[,1:3])\n\n         CNT           ST24Q01           ST24Q02\n68038 Canada          Disagree    Strongly agree\n68039 Canada             Agree Strongly disagree\n68040 Canada    Strongly agree Strongly disagree\n68041 Canada          Disagree          Disagree\n68042 Canada Strongly disagree          Disagree\n68043 Canada             Agree Strongly disagree\n\n# str(pisaitems)\n# View(pisaitems)\n\npisaitems 데이터의 경우, - rownames를 가지고 있으며 - 첫 CNT 이후로는 전부 scale을 갖는 factor의 형태로 이루어져 있습니다. - 개수는 각 column 마다 다르며, NA를 포함하고 있기도 합니다. (ex: ST36Q01)\n이후 이 데이터를 likert 오브젝트로 변환해야 합니다. 다만 모든 데이터를 다 사용하진 않고, CNT 이후의 처음 3개만 사용하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#likert-print",
    "href": "posts/2023-02-05-likert/index.html#likert-print",
    "title": "likert 패키지 소개",
    "section": "likert: print",
    "text": "likert: print\n\nsummary(pisaitems[, 2:4])\n\n              ST24Q01                   ST24Q02                   ST24Q03     \n Strongly disagree:14947   Strongly disagree:13323   Strongly disagree:13900  \n Disagree         :23515   Disagree         :23811   Disagree         :22072  \n Agree            :20000   Agree            :20935   Agree            :23525  \n Strongly agree   : 7029   Strongly agree   : 7487   Strongly agree   : 5917  \n NA's             : 1199   NA's             : 1134   NA's             : 1276  \n\n100 * 14947 / (14947 + 23515 + 20000 + 7029) # ST24Q01: Strongly disagree \n\n[1] 22.82298\n\nll &lt;- likert(pisaitems[, 2:4])\nprint(ll)\n\n     Item Strongly disagree Disagree    Agree Strongly agree\n1 ST24Q01          22.82298 35.90570 30.53855      10.732772\n2 ST24Q02          20.32308 36.32162 31.93453      11.420770\n3 ST24Q03          21.24927 33.74201 35.96325       9.045464\n\n\n위의 summary와 바로 비교해보면 알 수 있듯, likert 를 출력했을때는 NA를 제외한 각 문항별 factor의 백분율을 출력합니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#likert-summary",
    "href": "posts/2023-02-05-likert/index.html#likert-summary",
    "title": "likert 패키지 소개",
    "section": "likert: summary",
    "text": "likert: summary\n한편 summary 함수를 사용하면 low와 high를 표기해주는데 이는 각각 Neutral보다 낮은 / 높은 값의 백분율 합을 표기합니다. (예시의 경우 Strongly disagree + Disagree, Strongly agree + agree)\n\nsummary(ll)\n\n     Item      low neutral     high     mean        sd\n3 ST24Q03 54.99129       0 45.00871 2.328049 0.9090326\n2 ST24Q02 56.64470       0 43.35530 2.344530 0.9277495\n1 ST24Q01 58.72868       0 41.27132 2.291811 0.9369023\n\n22.82298 + 35.90570 # ST24Q01's Low\n\n[1] 58.72868\n\n\n\n물론 문항이 4-5개가 아닌, 여러개도 가능하기 때문에 Neutral은 가운데를 기준으로 설정하지만, 사용자가 center를 통해 지정할 수도 있습니다.\n\nmean과 sd는 크게 신경쓰지 않아도 괜찮습니다.\n\n\nsummary(ll, center = 1.5)\n\n     Item      low neutral     high     mean        sd\n2 ST24Q02 20.32308       0 79.67692 2.344530 0.9277495\n3 ST24Q03 21.24927       0 78.75073 2.328049 0.9090326\n1 ST24Q01 22.82298       0 77.17702 2.291811 0.9369023"
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#liert-plot",
    "href": "posts/2023-02-05-likert/index.html#liert-plot",
    "title": "likert 패키지 소개",
    "section": "liert: Plot",
    "text": "liert: Plot\nlikert object는 그냥 plot에 넣는 것으로도 결과를 바로 만들어 낼 수 있습니다.\n\nplot(ll)\n\n\n\n\n\n\n\n기본 center는 가운데, 이 경우 2와 3사이인 2.5를 기준으로 그려지고, 문항별 High, Low를 차트 양옆에 추가로 표기하게 됩니다.\ninformation\n차트에서의 factor 별 %, high, low 는 plot.percents, plot.percent.low, plot.percent.high를 통해 설정할 수 있습니다.\n\nplot(ll, plot.percents = TRUE, plot.percent.low = FALSE, plot.percent.high = FALSE)\n\n\n\n\n\n\n\ncolors\n기본 색상외에 colors를 사용하여 색상을 커스텀 할 수 있습니다. 단, 개수는 factor의 수와 동일해야합니다.\n\nplot(ll, colors=c('orange','darkorange','darkblue','blue'))\n\n\n\n\n\n\n\ncenter\n만약 위의 summary 처럼 Neutral을 바꾸고 싶다면, center를 통해 값을 지정하여 그릴 수 있습니다.\n여기서 center가 정수라면 해당하는 factor의 색상이 자동으로 회색으로 바뀌는 것을 확인 할 수 있습니다.\n\nplot(ll, center = 2)\n\n\n\n\n\n\n\ninclude.center\n차트에서 Neutral을 제외하고 그리고 싶다면 include.center = FALSE를 사용하여 제거할 수 있습니다.\n\nplot(ll, center = 2, include.center = FALSE)\n\n\n\n\n\n\n\ncentered\n단순히 stacked bar chart를 그리고 싶다면 centered = FALSE를 사용하면 됩니다.\n\nplot(ll, centered = FALSE)\n\n\n\n\n\n\n\nNA info (histogram)\n문항에서 결측치 (NA) 정보를 같이 표현하고 싶은 경우, include.histogram = TRUE를 설정하여 그릴 수 있습니다.\n\nsummary(pisaitems[,2])\n\nStrongly disagree          Disagree             Agree    Strongly agree \n            14947             23515             20000              7029 \n             NA's \n             1199 \n\n100 * 1199 / 66690 # 1.79% NA\n\n[1] 1.797871\n\nplot(ll, include.histogram = TRUE)\n\n\n\n\n\n\n\ndensity plot\nlikert 오브젝트는 type = 'density'를 사용하여 bar chart의 형태가 아닌 density plot으로도 표현할 수 있습니다.\n\nplot(ll, type = 'density')\n\n\n\n\n\n\n\n여러개의 density plot을 facet을 사용하여 하나로 겹치게 보여줄 수 있으며, 추가로 legend를 지정하는 예시입니다. (구분은 잘 안가지만, fill의 색상은 미세하게 다르게 표현되어 있습니다)\n\nplot(ll, type='density', facet=FALSE) + \n  guides(\n    color = guide_legend(title=\"Legend with Color\"),\n    fill = guide_legend(title=\"Legend with Fill\")\n  )\n\n\n\n\n\n\n\nHeat map\nlikert 오브젝트는 type = 'heat'를 사용해 heatmap으로 도 표현할 수 있습니다.\n\nplot(ll, type='heat')"
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#group-likert",
    "href": "posts/2023-02-05-likert/index.html#group-likert",
    "title": "likert 패키지 소개",
    "section": "Group Likert",
    "text": "Group Likert\n한편 likert 오브젝트를 만들때, 사용자의 그룹을 설정해준다면 (예시데이터의 CNT와 같이) dplyr의 group by를 사용한 것과 유사한 결과를 낼 수 있습니다.\n사용 가능한 parameter는 위와 동일합니다.\n\nllg &lt;- likert(pisaitems[,2:4], grouping=pisaitems$CNT)\nprint(llg) \n\n          Group    Item Strongly disagree Disagree    Agree Strongly agree\n1        Canada ST24Q01          25.69810 35.12856 24.88383      14.289507\n2        Canada ST24Q02          26.77758 35.18871 24.63608      13.397637\n3        Canada ST24Q03          25.22917 31.68150 33.47062       9.618706\n4        Mexico ST24Q01          21.87500 36.76845 33.45526       7.901293\n5        Mexico ST24Q02          15.26451 36.42523 37.79077      10.519491\n6        Mexico ST24Q03          18.44410 34.78607 37.89150       8.878331\n7 United States ST24Q01          17.16996 33.00426 33.97213      15.853659\n8 United States ST24Q02          29.08282 40.51858 21.03328       9.365325\n9 United States ST24Q03          24.31646 35.13671 32.79038       7.756448\n\n\n\nplot(llg, group.order=c('Mexico', 'Canada', 'United States'))\n\n\n\n\n\n\n\nGroup horizontal Plot\n그룹을 사용하면 차트를 여러개의 그룹에 따라 구분지어 그려야 하는 만큼 ggplot의 facet처럼 차트를 나누어야 하는데, 이 과정에서 위아래가 아닌, 좌우로도 설정 할 수 있습니다.\n\nplot(llg, panel.arrange='h', wrap=20)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html",
    "href": "posts/2023-02-01-streamlit/index.html",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "",
    "text": "https://docs.streamlit.io/library/api-reference 를 바탕으로 정리한 글입니다.\n2023년 2월 기준) streamlit version 1.17.0 을 기준으로 작성하였습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "1.1 streamlit과 shiny 예제 비교",
    "text": "1.1 streamlit과 shiny 예제 비교\n\nstreamlit 활용 사례\n\nhttps://streamlit.io/gallery\n\nshiny 활용 사례\n\nhttps://shiny.rstudio.com/gallery/"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.1 스트림릿 설치",
    "text": "2.1 스트림릿 설치\n파이썬, 가상환경 설치와 관한 내용은 다른 게시물을 참고하시기 바랍니다.\npip install streamlit \n파이썬 가상환경에 streamlit패키지를 설치합니다\napp.py를 생성한 후 다음 과 같이 수정하여 저장합니다.\n\nimport streamlit as st\n\nst.title('Hello Streamlit')\n\n이후 터미널에서\nstreamlit run app.py \nstreamlit run app.py 명렁어를 실행하면, 로컬서버로 페이지가 만들어지게 됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.2 스트림릿 기능 소개",
    "text": "2.2 스트림릿 기능 소개\n더 자세한 내용은 https://docs.streamlit.io/library/api-reference에서 확인하실 수 있습니다.\n\n2.2.1 강조 문구\n\ntitle\nheader\nsubheader\n\n\nimport streamlit as st\n\nst.title('this is title')\nst.header('this is header')\nst.subheader('this is subheader')\n\n\n제목과 헤더,서브헤더를 구현할 수 있다.\n\netc\n\n추가적으로 Markdown 문법을 st.markdown으로, caption, Latex Code block을 활용가능합니다\n\n\n\n2.2.2 layout 짜기\n페이지의 공간을 레이아웃을 통해 웹페이지를 분할할 수 있다.\n\ncolumn\n\n\nimport streamlit as st\n\ncol1,col2 = st.columns([2,3])\n# 공간을 2:3 으로 분할하여 col1과 col2라는 이름을 가진 컬럼을 생성합니다.  \n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# with 구문 말고 다르게 사용 가능 \ncol1.subheader(' i am column1  subheader !! ')\ncol2.checkbox('this is checkbox2 in col2 ') \n#=&gt;위에 with col2: 안의 내용과 같은 기능을합니다\n\n\n결과물\n\n\ntab\n\n\nimport streamlit as st\n\n# 탭 생성 : 첫번째 탭의 이름은 Tab A 로, Tab B로 표시합니다. \ntab1, tab2= st.tabs(['Tab A' , 'Tab B'])\n\nwith tab1:\n  #tab A 를 누르면 표시될 내용\n  st.write('hello')\n    \nwith tab2:\n  #tab B를 누르면 표시될 내용 \n  st.write('hi')\n\n 다음을 실행하면 tab A 를 눌렀을 경우 hello, tab B를 눌렀을 경우 hi가 나오게 됩니다.\n탭의 특징으로는, 탭을 클릭과 동시에 데이터가 만들어지는 것이 아니라,탭에 표시될 데이터가 이미 만들어져 았는 것이 특징입니다. 장점이 될 수도 있고, 단점이 될 수도 있습니다.\n \n\nsidebar\n\n\nimport streamlit as st\n\n#st.sidebar는 \n\nst.sidebar.title('this is sidebar')\nst.sidebar.checkbox('체크박스에 표시될 문구')\n# 사이드바에 체크박스, 버튼등 추가할 수 있습니다! \n\n \n\netc\n\n추가적으로 Expander, Container ,Empty가 있습니다\n\n\n2.2.3 이미지 불러오기\n\nimport streamlit as st\nfrom PIL import Image\n\n#PIL 패키지에 이미지 모듈을 통해 이미지 열기 \n# Image.open('이미지 경로')\nzarathu_img = Image.open('zarathu.png')\n\ncol1,col2 = st.columns([2,3])\n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# 컬럼2에 불러온 사진 표시하기\ncol2.image(zarathu_img)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "href": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.3 웹사용자로부터 input 받기",
    "text": "2.3 웹사용자로부터 input 받기\n이제 UI적인 부분을 전반적으로 살펴봤기 때문에, 사용자로 하여금 input을 받아 interactive하게 데이터를 보여주는 페이지를 만들어보겠습니다\n데이터는 사이킷런의 아이리스 데이터를 가져와 사용하도록 하겠습니다. \n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.datasets import load_iris \nimport matplotlib.pyplot as plt\nimport streamlit as st\n\niris_dataset = load_iris()\n\ndf= pd.DataFrame(data=iris_dataset.data,columns= iris_dataset.feature_names)\ndf.columns= [ col_name.split(' (cm)')[0] for col_name in df.columns] # 컬럼명을 뒤에 cm 제거하였습니다\ndf['species']= iris_dataset.target \n\n\nspecies_dict = {0 :'setosa', 1 :'versicolor', 2 :'virginica'} \n\ndef mapp_species(x):\n  return species_dict[x]\n\ndf['species'] = df['species'].apply(mapp_species)\nprint(df)\n\n\nstreamlit 에서 데이터 프레임을 보여주는 방식은 table과 dataframe 두가지를 사용할 수 있습니다.\ndataframe의 head함수를 이용하여 첫 5행의 데이터에 대해 table과 dataframe으로 출력한 경우입니다.\n\nst.subheader('this is table')\nst.table(df.head())\n\nst.subheader('this is data frame')\nst.dataframe(df.head())\n\n\n이제 버튼을 동작시키는 방법을 배워보도록 하겠습니다.\n 1. Select Box\n\n# 사이드바에 select box를 활용하여 종을 선택한 다음 그에 해당하는 행만 추출하여 데이터프레임을 만들고자합니다.\nst.sidebar.title('Iris Species🌸')\n\n# select_species 변수에 사용자가 선택한 값이 지정됩니다\nselect_species = st.sidebar.selectbox(\n    '확인하고 싶은 종을 선택하세요',\n    ['setosa','versicolor','virginica']\n)\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species']== select_species]\n# 선택한 종의 맨 처음 5행을 보여줍니다 \nst.table(tmp_df.head())\n\n\n사용자가 sidebar에서 종을 바꿀 때마다 자동으로 해당하는 종의 테이블 정보가 불러와지게 됩니다.\n\n\n\nmulti select\n\n\n# 여러개 선택할 수 있을 때는 multiselect를 이용하실 수 있습니다 \n# return : list\nselect_multi_species = st.sidebar.multiselect(\n    '확인하고자 하는 종을 선택해 주세요. 복수선택가능',\n    ['setosa','versicolor','virginica']\n\n)\n\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species'].isin(select_multi_species)]\n# 선택한 종들의 결과표를 나타냅니다.  \nst.table(tmp_df)\n\n\n\n\nRadio / Slider\n\n\n# 라디오에 선택한 내용을 radio select변수에 담습니다\nradio_select =st.sidebar.radio(\n    \"what is key column?\",\n    ['sepal length', 'sepal width', 'petal length','petal width'],\n    horizontal=True\n    )\n# 선택한 컬럼의 값의 범위를 지정할 수 있는 slider를 만듭니다. \nslider_range = st.sidebar.slider(\n    \"choose range of key column\",\n     0.0, #시작 값 \n     10.0, #끝 값  \n    (2.5, 7.5) # 기본값, 앞 뒤로 2개 설정 /  하나만 하는 경우 value=2.5 이런 식으로 설정가능\n)\n\n# 필터 적용버튼 생성 \nstart_button = st.sidebar.button(\n    \"filter apply 📊 \"#\"버튼에 표시될 내용\"\n)\n\n# button이 눌리는 경우 start_button의 값이 true로 바뀌게 된다.\n# 이를 이용해서 if문으로 버튼이 눌렸을 때를 구현 \nif start_button:\n    tmp_df = df[df['species'].isin(select_multi_species)]\n    #slider input으로 받은 값에 해당하는 값을 기준으로 데이터를 필터링합니다.\n    tmp_df= tmp_df[ (tmp_df[radio_select] &gt;= slider_range[0]) & (tmp_df[radio_select] &lt;= slider_range[1])]\n    st.table(tmp_df)\n    # 성공문구 + 풍선이 날리는 특수효과 \n    st.sidebar.success(\"Filter Applied!\")\n    st.balloons()\n\nslider_range : list 형식으로 2개의 값이 저장됩니다. 양쪽 앞뒤로 두개의 값을 저장합니다.\nslider_range[0] : 최솟값 \nslider_range[1] : 최댓값  \n\npetal_width 컬럼값이 0에서 1.38사이인 값들로 정보를 filtering한 결과물이 표시됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#plotly",
    "href": "posts/2023-02-01-streamlit/index.html#plotly",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.1 plotly",
    "text": "3.1 plotly\npython의 시각화로 자주 이용되는 패키지는 matplotlib,seaborn,bokeh,plotly 등이 있습니다.\n스트림릿에서는 bokeh, plotly, matplotlib 등의 패키지를 통해 생성한 그림(figure)를 streamlit을 통해 웹에서 표시하는 기능을 제공합니다. \n\n3.1.1 st.plotly_chart\n\n#st.plotly_chart(figure_or_data, use_container_width=False, theme=\"streamlit\", **kwargs)\n\n 인자로 줄 수 있는 옵션들에 대해서 하나씩 설명해드리도록 하겠습니다.\n\nfigure_or_data : 첫번째 인자로 plotly로 생성한 그림의 이름이 들어가는 위치입니다.\nuse_container_width : 레이아웃으로 지정한 사이즈에 그림이 해상도를 조절해서 들어갈 것인지 (-&gt;‘True’)  아니면 원래그림 크기대로 표시될 것인지 (-&gt; ‘False’) 선택하는 옵션입니다.\ntheme : 스트림릿 웹에 어떻게 표시될지 테마를 설정합니다. 인자로는 “streamlit” 과 None(입력하지 않음)을 선택할 수 있습니다.\n\n\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\n\nfig = px.scatter(\n    df.query(\"year==2007\"),\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=60,\n)\nfig.show()\n\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\nwith tab1:\n    # Use the Streamlit theme.\n    # This is the default. So you can also omit the theme argument.\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\nwith tab2:\n    # Use the native Plotly theme.\n    st.plotly_chart(fig, theme=None, use_container_width=True)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#지도표시",
    "href": "posts/2023-02-01-streamlit/index.html#지도표시",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.2 지도표시",
    "text": "3.2 지도표시\n\nimport numpy as np\nimport pandas as pd \n\n#지도 위에 표시될 점 좌표 값을 위도경도에 담습니다 .\nbase_position =  [37.5073423, 127.0572734]\n#중심점의 위도, 경도 좌표를 리스트에 담습니다.\n\n# base_position에, 랜덤으로 생성한 값을 더하여 5개의 좌표를 데이터 프레임으로 생성하였고,\n# 컬럼명은 위도 :lat  경도 lon으로 지정하였습니다. \n\n\nmap_data = pd.DataFrame(\n    np.random.randn(5, 1) / [20, 20] + base_position , \n    columns=['lat', 'lon'])\n# map data 생성 : 위치와 경도 \n\nprint(map_data)\n\n이어서 이 위도 경도 데이터를 스트림릿 웹 페이지에 지도로 나타내는 과정은 다음과 같습니다. \n\nst.code('st.map(map_data)')\n# 웹사이트에 어떤 코드인지 표시해주기 \nst.subheader('Map of Data ')\n# 제목 생성 \nst.map(map_data)\n# 지도 생성 \n\n결과물"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.1 st.session_state 세션 스테이트란?",
    "text": "4.1 st.session_state 세션 스테이트란?\n상태가 자꾸 변하는 것들을 세션스테이트에 관리해두면 바뀌는 값에 따라 내용이 바뀌는 것들을 기록할 수 있다.\n** 아주 중요한 역할을 하는 기능입니다. **"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.2 st.session_state 활용",
    "text": "4.2 st.session_state 활용\n\n\n\nwhy we use sessionstate\n\n\n세션스테이트에 key값의 초기값이 없으면, 초기값을 생성하여 놓는 작업을 if문을 통해 진행합니다.\n세션스테이트에 사용자가 입력한 인풋에 따라서 dataframe이 재가공 되는데 이 값이 interactive하게 지정되게 하기 위해 st.session_state값으로 사용합니다.\n\n# 예시코드 \n# import streamlit as st\n\n\n\n# if 'final_dataframe' not in st.session_state:\n#   # session state 에 final 이라는 값이 없으면, \n#   st.session_state['final_dataframe']= df\n#   # 초기 값 설정 : session_state에 final_dataframe키 값에 초기값 데이터를 집어넣습니다 .\n\n# #아래 코드는 df의 테이블 값이 바뀌더라도 interactive하게 연동되서 바뀌지 않습니다\n# st.table(df)\n\n# #  아래 코드는  이제 dataframe가 조작될 때 마다 session_state객체 안에 final_dataframe값을 변경하면, \n# #  수정 될 때  계속 바뀌어서 보여줍니다. \n# st.table(st.session_state.final_dataframe)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "href": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.3 cache 란 무엇인가?",
    "text": "4.3 cache 란 무엇인가?\n\n\n\n캐싱기능의 필요성\n\n\n캐싱에 관한 간단한 개념은 주문이 들어왔을 때 우리가 만들기 시작하면 코드가 결과물을 만들어내는데 시간이 오래 걸리는 경우 유저가 결과물을 오랜시간 기달려야하는 경우가 발생합니다.\n따라서 만들어내는데 오래걸리는 결과물을 미리 만들어두고, 보이지 않는 곳에 캐싱하여 필요할때 찾아 꺼내는 것을 cache기능이라고 간단히 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.4 streamlit 에서 cache 기능 사용하기",
    "text": "4.4 streamlit 에서 cache 기능 사용하기\nstreamlit에서는 시간이 오래걸리는 작업: 데이터 로드 등을 할때 위에 @st.cache를 추가하여 캐싱할 수 있게 할 수 있다.\n\nimport streamlit as st\nimport pandas as pd \n\nfile_path = '~~~filepath'\n@st.cache\ndef load_data():\n  data = pd.read_csv(file_path)\n  return data\n\n큰 데이터를 로드하거나, 실행이 오래걸리는 복잡한 연산을 해야할 때 cache기능을 이용하면 용이하다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "href": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.5 로딩상태 구현",
    "text": "4.5 로딩상태 구현\n\n4.5.1 st.progress\n\nimport time \n\n# 방법 1 progress bar \nlatest_iteration = st.empty()\nbar = st.progress(0)\n\nfor i in range(100):\n  # Update the progress bar with each iteration.\n  latest_iteration.text(f'Iteration {i+1}')\n  bar.progress(i + 1)\n  time.sleep(0.05)\n  # 0.05 초 마다 1씩증가\n\nst.balloons()\n# 시간 다 되면 풍선 이펙트 보여주기 \n\n코드 수행 소요시간이 긴 경우, progress bar와 streamlit의 spinner를 통해서 로딩페이지를 만들 수 있다!\n \n\n\n4.5.2 st. spinner 사용\n\n#방법2  st.spinner 사용 \nimport time \n\nwith st.spinner('Wait for it...'):\n  time.sleep(5)\n  st.success('Done!')"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "href": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.1 근본적 한계",
    "text": "5.1 근본적 한계\nstreamlit 에서 제공하는 UI들이 대부분 예쁘지만, 디테일적으로 맘에 들지 않을 수가 있다.\n\n예를 들어 4.87로 표시된 부분의 폰트가 맘에들지 않는다고 하자. 그렇다면 streamlit를 markdown을 통해서 style을 직접 수정해주어야한다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "href": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.2 극복방안 : CSS hack",
    "text": "5.2 극복방안 : CSS hack\n이 작업을 streamlit CSS hack이라고 한다\n\n브라우저에 검사기능을 이용하여 css해당하는 부분을 마크다운으로 직접 style tag를 넣어서 수정하면 된다.\n아래 유튜브를 통해서 더욱 자세한 내용을 배울 수 있습니다.\nhttps://www.youtube.com/watch?v=gr_KyGfO_eU"
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html",
    "href": "posts/2022-09-30-GAM/index.html",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "",
    "text": "김진섭 대표는 성균관대학교 바이오헬스 규제과학과 강의에서 비선형모델인 GAM(Generalized Additive Model) 을 소개할 예정입니다. 전체 강의자료는 https://github.com/jinseob2kim/R-skku-biohrs 에 있습니다."
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#요약",
    "href": "posts/2022-09-30-GAM/index.html#요약",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "요약",
    "text": "요약\nGAM 은 비선형관계를 다루는 통계방법이다\n\nLOWESS: 구간 촘촘하게 나눈 후 평균값\nCubic spline(cs): 구간 몇개로 나눈 후 각각 3차함수 fitting\nNatural cubic spline(ns): cs 맨 처음과 끝구간만 선형 fitting\nSmoothing spline(GAM default): 최적화때 smoothing penalty(λ) 부여\n\n종속변수 형태따라 여러종류\n\nContinuous: normal\nBinary: logistic\nCount: poisson, quasipoisson(평균 ≠ 분산 일 때)\nSurvival: coxph"
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#slide",
    "href": "posts/2022-09-30-GAM/index.html#slide",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/gam 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html",
    "href": "posts/2022-07-25-collapse/index.html",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "R의 고급 데이터 변환 및 통계 컴퓨팅을 위한 C/C++ 기반 패키지입니다.\n\n유연하고 간결한 구문을 통해 매우 빠르고 클래스에 구애받지 않습니다,\n기본 R, ‘dplyr’, ‘tibble’, ‘data.table’, ‘sf’, ‘plm’ 과 잘 통합됩니다.\n\n\n\n##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr)\nlibrary(data.table) \nlibrary(dplyr)\nlibrary(collapse)\nlibrary(microbenchmark)\n\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하겠습니다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#setup",
    "href": "posts/2022-07-25-collapse/index.html#setup",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr)\nlibrary(data.table) \nlibrary(dplyr)\nlibrary(collapse)\nlibrary(microbenchmark)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#load-file",
    "href": "posts/2022-07-25-collapse/index.html#load-file",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하겠습니다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#load",
    "href": "posts/2022-07-25-collapse/index.html#load",
    "title": "collapse 패키지 소개",
    "section": "load",
    "text": "load\n\ncollapse는 데이터를 불러오는 함수가 존재하지 않기 때문에, data.table를 이용하여 읽습니다.\nfselect()는 컬럼명을 명시하거나 인덱스를 전달하면 원하는 컬럼을 불러올 수 있습니다.\n\n\n## data.table(Only specific column)\ndt1 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\",select = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"))\ndt2 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", select = 1:5)\ndt3 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", drop = 6:10)\n\n## collapse(Only specific column)\ndt4 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM)\ndt5 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),1:5)\ndt6 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),-(6:10)) \n\n\n예시로 dt6를 확인해보면, dt3와 마찬가지로 인덱스 6부터 10까지 제외되서 출력된 것을 볼 수 있습니다.\n\n\ndt3\n\n\n\n\n  \n\n\n\n\ndt6"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#row",
    "href": "posts/2022-07-25-collapse/index.html#row",
    "title": "collapse 패키지 소개",
    "section": "row",
    "text": "row\n\ncollapse::fselect와 dplyr::select는 유사하지만 fselect가 x100배 정도 빠릅니다.\nss()함수는 컬럼명이 아닌 인덱스로 행,열을 출력을 할 때 사용합니다. fsubset() 보다 빠르지만 기능이 제한적이기 떄문에 간단한 행,열 출력할 때 사용가능합니다.\n\n\n## data.table(row)\ndt[1:10]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)]\ndt[order(HME_YYYYMM)]\ndt[order(HME_YYYYMM, -HGHT)]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)]        \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)] %&gt;% .[order(HGHT)]  #same\n\n## collapse(row)                                       \nfsubset(dt, 1:10)                                                   #ss(dt,1:10)\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 )               \nroworder(dt, HME_YYYYMM)                                            \nroworder(dt, HME_YYYYMM, -HGHT)\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25)\n\n\n예시로 두번째 코드와 다섯번째 코드를 확인해보겠습니다.\n\n두번째\n\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 ) \n\n\n\n\n  \n\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)]\n\n\n\n\n  \n\n\n\n다섯번째\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)] \n\n\n\n\n  \n\n\n\n\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column",
    "href": "posts/2022-07-25-collapse/index.html#column",
    "title": "collapse 패키지 소개",
    "section": "column",
    "text": "column\n\n열 이름을 정규식으로 선택하기 위해서는 fselect()가 아니라 get_vars() 함수를 이용해야합니다. regex = TRUE 정규식을 사용하겠다는 의미이고 return = “names” 컬럼명을 출력하겠다는 의미입니다. get_var()는 fselect()함수와 유사하지만, 수행속도가 좀 더 빠르며 벡터, 정수 형태로 값을 전달합니다.\n유의사항은 컬럼명을 리스트로 전달하면 ERROR 발생합니다.\nfsubset는 빠르고 부분적인 작업을 위해 base::subset 패키지 의 C 함수를 사용하는 향상된 버전입니다.\n\n\n## data.table(column)\ndt[, 1:10]\ndt[, c(\"HGHT\", \"WGHT\")]\ndt[, .(HGHT, WGHT)]\ndt[, .(Height = HGHT, Weight = WGHT)]   \ndt[, .(HGHT)] \ndt[, \"HGHT\"]\ncolvars1 &lt;- grep(\"Q_\", names(dt), value = T)\ndt[, ..colvars1]\ndt[, colvars1, with = FALSE]      \ndt[, .SD, .SDcols = colvars1]     \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars]\ndt[, !..colvars1]\ndt[, -..colvars1]\ndt[, .SD, .SDcols = -colvars1]\n\n## collapse(column)\nfselect(dt, 1:10)                 \nfselect(dt, c(\"HGHT\", \"WGHT\"))    #get_vars(dt, 1:10)\nfselect(dt, HGHT, WGHT)           #get_vars(dt, c(\"HGHT\", \"WGHT\"))\nfselect(dt, Height = HGHT, Weight = WGHT)\nfselect(dt, .(HGHT))              #ERROR\nfselect(dt, \"HGHT\")\ncolvars2 &lt;-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")    #regex = TRUE 정규식 사용/ return = \"names\" 컬럼명 출력\nfselect(dt, colvars2)                                            #fselect(dt, c(colvars))\nget_vars(dt, colvars2)                                           #get_var(dt, c(colvars))\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars2)\nfselect(dt, -(4:12))\nfselect(dt, -(Q_PHX_DX_STK:Q_DRK_FRQ_V09N))\n\n\n예시로 같은 값을 출력하는지 확인해 보겠습니다.\n\n\ncolvars1 &lt;- grep(\"Q_\", names(dt), value = T)\n\nqDT(colvars1)\n\n         colvars1\n           &lt;char&gt;\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ncolvars2 &lt;-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")\n\nqDT(colvars2)\n\n         colvars2\n           &lt;char&gt;\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars1]\n\n\n\n\n  \n\n\n\n\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars2)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column-summary",
    "href": "posts/2022-07-25-collapse/index.html#column-summary",
    "title": "collapse 패키지 소개",
    "section": "Column summary",
    "text": "Column summary\n\nfmean는 (열별) 평균을 계산하고, (선택적으로) 그룹화 및 가중치를 계산하는 일반적인 함수입니다.\ndapply는 데이터에 대한 정보(속성)를 잃거나 데이터의 클래스 또는 형식을 변경하지 않고 데이터의 행이나 열에 함수를 적용하는 효율적인 함수입니다.\n\n\n## data.tanble(Column summary)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))] \n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n#collapse(Column summary)\nfselect(dt,HGHT,WGHT,BMI) %&gt;% fmean()     #fmean(fselect(dt,HGHT,WGHT,BMI))\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n두번째 코드를 비교해보겠습니다.\n\n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n\n\n\n\nHGHT\nWGHT\nBMI\n\n\n164.5487\n65.09672\n23.92257\n\n\n\n\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n\n\n\n\nfmean\n\n\n\nHGHT\n164.54866\n\n\nWGHT\n65.09672\n\n\nBMI\n23.92257"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#by",
    "href": "posts/2022-07-25-collapse/index.html#by",
    "title": "collapse 패키지 소개",
    "section": "By",
    "text": "By\n\ncollap()는 ‘Fast Statistical Functions’ 사용하여 각 컬럼에 여러 함수를 적용할 수 있습니다.(#Fast Statistical Functions: fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct)\nadd_stub() 연산을 통해 열을 추가할 수 있는 명령입니다. ““를 통해 열 이름을 설정할 수 있습니다.\n\n\n##data.table(By)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY]\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = \"EXMD_BZ_YYYY\"]  #same\ndt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY]       #same\ndt[HGHT &gt;= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)]        \ndt[HGHT &gt;= 175, .N, by= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                               #same\ndt[HGHT &gt;= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                            #same(정렬)\n\n#collapse(By)                                                  \ncollap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16))                                # ~ ≒ by\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY)                       #same\nadd_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")  #dplyr::count()\n\n\n마지막 코드를 비교해보겠습니다. 결측치 값의 정렬 순서의 차이가 있지만, 같은 기능을 수행할 수 있습니다.\n\n\ndt[HGHT &gt;= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]\n\n\n\n\n  \n\n\n\n\nadd_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#new-variable",
    "href": "posts/2022-07-25-collapse/index.html#new-variable",
    "title": "collapse 패키지 소개",
    "section": "New variable",
    "text": "New variable\n\nftransform 새 열을 계산하거나 기존 열을 수정 및 삭제하는 데 사용할 수 있으며 항상 전체 데이터 프레임을 반환합니다.\nftransform은 base::transform 데이터 프레임 및 목록 의 향상된 버전입니다.\n\n\n## data.table(New variable)\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\ndt[, `:=`(BP_SYS140 = factor(as.integer(BP_SYS &gt;= 140)), BMI25 = factor(as.integer(BMI &gt;= 25)))]\n\ndt[, BMI2 := NULL]\n\n## collapse(New variable)\nftransform(dt, BMI2 = round(WGHT/(HGHT/100)^2, 1))\n\nftransform(dt,BP_SYS140 = factor(as.integer(BP_SYS &gt;= 140)),BMI25 = factor(as.integer(BMI &gt;= 25)))\n\nftransform(dt, BMI2 = NULL)\n\n\n첫번째와 두번째 코드를 확인해보겠습니다.\ndata.table\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\ncollapse"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "href": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "title": "collapse 패키지 소개",
    "section": "Specific symbol .N, .SD, .SDcols",
    "text": "Specific symbol .N, .SD, .SDcols\n\n## data.table(Specific symbol .N, .SD, .SDcols)\ndt[, .SD]\n\ndt[, lapply(.SD, class)]\n\ndt[, .N, keyby = \"RN_INDI\"]\n\n## collapse\nfselect(dt,1:32)\n\ndapply(dt,class)\n\nadd_stub(count(dt,RN_INDI),\"N\")\n\n\n두번째 코드를 비교해보겠습니다. 비슷하지만 출력하는 형태가 다릅니다. class()를 사용하여 확인해보면 형태가 다른 것을 알 수 있습니다.\n\n\ndt[, lapply(.SD, class)]\n\n\n\n\n  \n\n\n\n\ndapply(dt,class)\n\n\n\n\n  \n\n\n\n\nclass(dt[, lapply(.SD, class)])\n\n[1] \"data.table\" \"data.frame\"\n\nclass(dapply(dt,class))\n\n[1] \"character\""
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#order",
    "href": "posts/2022-07-25-collapse/index.html#order",
    "title": "collapse 패키지 소개",
    "section": "order",
    "text": "order\n\n#data.table(order)\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n#collapse(order)\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI)\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(-BMI)\n\n\n첫번째 코드를 통해 data.table과 collapse 출력 결과를 확인해보겠습니다.\n\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n  \n\n\n\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 행 연산",
    "text": "기본 행 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[1:10] ,\n               collapse = fsubset(dt, 1:10))\n\nWarning in microbenchmark(data.table = dt[1:10], collapse = fsubset(dt, : less\naccurate nanosecond times to avoid potential integer overflows\n\n\nUnit: microseconds\n       expr    min     lq     mean median     uq     max neval cld\n data.table 38.335 40.426 45.42554 41.656 43.788 151.003   100  a \n   collapse  5.945  6.601  7.41772  7.052  7.585  18.614   100   b\n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)],\n               collapse = fsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 ))\n\nUnit: microseconds\n       expr    min      lq     mean median      uq     max neval cld\n data.table 58.794 61.9715 66.39786 64.001 66.8300 163.303   100  a \n   collapse 25.338 27.9620 30.63520 29.889 30.9755  71.545   100   b\n\nmicrobenchmark(data.table = dt[order(HME_YYYYMM, -HGHT)],\n               collapse = roworder(dt, HME_YYYYMM, -HGHT))\n\nUnit: microseconds\n       expr     min       lq     mean   median      uq      max neval cld\n data.table 135.177 140.9375 198.2108 146.2265 157.399 4024.929   100   a\n   collapse  53.997  62.9350 103.2974  65.7845  71.504 3298.860   100   a\n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)],\n               collapse = roworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25))\n\nUnit: microseconds\n       expr     min      lq     mean   median      uq      max neval cld\n data.table 119.064 125.419 186.6652 130.4005 139.236 4590.770   100   a\n   collapse  65.477  78.556 102.0277  83.5580  91.471 1484.241   100   a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 열 연산",
    "text": "기본 열 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, 1:10],\n               collapse = fselect(dt, 1:10))\n\nUnit: microseconds\n       expr    min      lq     mean median      uq     max neval cld\n data.table 35.547 36.9820 40.57483 37.843 39.2165 129.437   100  a \n   collapse  3.936  4.3665  5.30868  4.633  5.0840  41.369   100   b\n\nmicrobenchmark(data.table = dt[, .(Height = HGHT, Weight = WGHT)],\n               collapse = fselect(dt, Height = HGHT, Weight = WGHT))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq     max neval cld\n data.table 148.666 152.4380 168.39028 156.1075 168.3665 409.590   100  a \n   collapse   5.084   5.6785   7.38574   6.6010   7.1955  51.947   100   b\n\n#base::grep() more faster \nmicrobenchmark(data.table = colvars &lt;- grep(\"Q_\", names(dt), value = T),\n               collapse = colvars &lt;-get_vars(dt, \"Q_\",regex = TRUE, return = \"names\"))\n\nUnit: microseconds\n       expr   min    lq    mean median    uq    max neval cld\n data.table 5.002 5.125 5.74615  5.207 5.412 51.168   100  a \n   collapse 6.191 6.355 6.78632  6.519 6.724 25.215   100   b\n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = get_vars(dt, colvars))\n\nUnit: microseconds\n       expr    min      lq     mean median      uq     max neval cld\n data.table 33.210 36.8180 41.42394 37.761 40.7745 125.870   100  a \n   collapse  2.829  3.2595  4.56494  3.649  4.0590  78.351   100   b\n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = fselect(dt, colvars) )\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval cld\n data.table 33.497 35.8955 39.87373 37.3100 40.0365 126.608   100  a \n   collapse  4.715  5.2890  6.26521  5.7605  6.1910  48.626   100   b\n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars],\n               collapse = fsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars))\n\nUnit: microseconds\n       expr    min      lq     mean median      uq     max neval cld\n data.table 66.092 67.4655 71.12967 68.716 70.7865 184.131   100  a \n   collapse 23.821 24.7230 26.42696 25.338 26.0350  99.302   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#평균-연산",
    "href": "posts/2022-07-25-collapse/index.html#평균-연산",
    "title": "collapse 패키지 소개",
    "section": "평균 연산",
    "text": "평균 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(mean(HGHT), mean(WGHT), mean(BMI))],\n               collapse = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq     mean   median       uq      max neval cld\n data.table 178.145 182.9010 227.1097 185.5045 194.1965 3139.698   100  a \n   collapse   9.881  10.6395  12.5009  11.4185  12.3205   36.285   100   b\n\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))],\n               collaspe = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq      mean  median       uq      max neval cld\n data.table 175.480 179.7850 214.44148 183.885 193.6430 1898.546   100  a \n   collaspe   9.799  10.5165  12.43899  11.439  12.3205   42.763   100   b\n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)],\n               collapse = dapply(fselect(dt,HGHT,WGHT,BMI),fmean))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq      max neval cld\n data.table 184.705 189.6045 218.93303 193.7045 201.3305 1603.100   100  a \n   collapse  16.605  17.6505  20.41226  18.8395  21.0125   62.033   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "href": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "title": "collapse 패키지 소개",
    "section": "그룹별 통계 연산",
    "text": "그룹별 통계 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY],\n               collapse.collap = collap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16)),\n               collapse.fmean = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n            expr     min       lq      mean  median       uq      max neval cld\n      data.table 243.745 258.7920 283.10254 267.115 277.9595  550.179   100 a  \n collapse.collap  62.484  70.9505  90.54891  74.497  81.6925  938.244   100  b \n  collapse.fmean  36.080  38.7450  54.37092  40.303  42.0660 1197.528   100   c\n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq      max neval cld\n data.table 246.123 252.7445 295.35006 259.2635 266.9715 2695.586   100  a \n   collapse  35.752  37.6380  42.12914  38.9295  40.6310   97.744   100   b\n\n#data.table more faster\nmicrobenchmark(data.table = dt[HGHT &gt;= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)],\n               collapse = add_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\") )\n\nUnit: microseconds\n       expr      min       lq     mean    median        uq      max neval cld\n data.table  238.497  260.022  358.199  305.4295  377.3845 2476.646   100  a \n   collapse 1634.137 1770.421 2089.203 1925.2780 2118.9825 7643.589   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "href": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "title": "collapse 패키지 소개",
    "section": "특수 심볼",
    "text": "특수 심볼\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .SD],\n               collapse = fselect(dt,1:32))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 215.537 228.6775 253.58541 234.0485 243.909 762.108   100  a \n   collapse   4.182   4.8995  13.80265   5.5145   6.355 769.406   100   b\n\nmicrobenchmark(data.table = dt[, lapply(.SD, class)],\n               collapse = dapply(dt,class))\n\nUnit: microseconds\n       expr     min      lq      mean  median       uq      max neval cld\n data.table 473.058 505.489 631.67511 530.048 590.7485 7756.503   100  a \n   collapse   7.954   8.856  12.01915  10.086  12.3615   51.988   100   b\n\n#data.table more faster\nmicrobenchmark(data.table = dt[, .N, keyby = \"RN_INDI\"],\n               collapse = add_stub(count(dt,RN_INDI),\"N\"))\n\nUnit: microseconds\n       expr      min        lq      mean   median       uq      max neval cld\n data.table  193.069  224.1675  285.7011  259.489  341.284  555.714   100  a \n   collapse 2592.430 2787.4670 3227.2088 3122.027 3329.856 6645.690   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "href": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "title": "collapse 패키지 소개",
    "section": "정렬 연산",
    "text": "정렬 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI))\n\nUnit: microseconds\n       expr     min       lq      mean  median      uq      max neval cld\n data.table 313.732 336.9585 427.11053 353.379 406.269 2012.485   100  a \n   collapse  45.141  49.4665  64.50489  53.710  64.124  251.781   100   b\n\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(-BMI))\n\nUnit: microseconds\n       expr     min      lq      mean   median       uq     max neval cld\n data.table 304.097 311.764 326.76016 316.8275 324.3305 576.993   100  a \n   collapse  46.289  49.159  54.02201  51.7830  54.1405 155.267   100   b"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "",
    "text": "김진섭 대표는 7월 15일(금) 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 “데이터과학자가 갖춰야할 기술” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "요약",
    "text": "요약\n분석: 의학 + 연구는 R 추천\n\n데이터 전처리는 data.table 과 %&gt;%.\nR로 ppt/xlsx 만들기(분석결과 반출)\n일반, 반복측정, 표본추출, Propensity, meta, 시계열, ML\n코드관리는 Github\n\n서비스: 리포트(Rmarkdown), 웹 애플리케이션(Shiny)\n서버: 분석환경 or 웹 서비스\n\n리눅스, 클라우드, Docker"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/short-2022summer 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html",
    "href": "posts/2022-03-25-graph/index.html",
    "title": "R로 논문용 그래프 그리기",
    "section": "",
    "text": "이번 시간에는 R을 이용해서 데이터와 통계 분석 결과를 한 눈에 전달할 수 있는 그래프를 만들 것이다. 예제 데이터로 지난 시간에 사용한 건강검진 데이터를 이용한다.\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/main/data/example_g1e.csv\")\nrmarkdown::paged_table(head(data))"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram",
    "href": "posts/2022-03-25-graph/index.html#histogram",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\n연속형 데이터를 히스토그램으로 나타내보자.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\")\n\n\n\n\n\n\n\nbreaks=n 옵션을 이용해서 계급구간의 수를 설정하고, freq=F 옵션을 이용하면 y축을 빈도수가 아닌 확률밀도로 표시할 수 있다. 그래프의 색도 간단하게 설정할 수 있다.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\",\n     breaks = 30, freq=F, col=\"grey\", border=\"white\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot",
    "href": "posts/2022-03-25-graph/index.html#barplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n히스토그램과 유사하지만, X축에 표현하고자 하는 변수가 이산형 변수일 때는 빈도수를 바 그래프로 나타낼 수 있다. table() 함수를 이용해 빈도표를 만들고, 바 그래프로 나타낸다.\n\ntable &lt;- table(data$Q_SMK_YN)\nprint(table)\n\n\n  1   2   3 \n995 256 391 \n\nbarplot(table, main=\"Distribution of smoking\", names.arg=c(\"Never\", \"Ex-smoker\", \"Current\"), ylab=\"frequency\")\n\n\n\n\n\n\n\n연도에 따른 흡연 여부의 분포를 하나의 그래프로 나타낼 수 있다. beside=T 옵션을 사용하면 적층형 그래프가 그룹형 그래프로 바뀐다.\n\ntable &lt;- table(data$Q_SMK_YN, data$EXMD_BZ_YYYY)\nprint(table)\n\n   \n    2009 2010 2011 2012 2013 2014 2015\n  1  125  132  140  146  141  157  154\n  2   34   42   35   36   35   38   36\n  3   53   62   48   52   67   59   50\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"), beside=T)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot",
    "href": "posts/2022-03-25-graph/index.html#boxplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n범주형 변수(흡연 여부, X축)에 따른 연속형 변수(수축기 혈압, Y축)의 분포를 나타내는 데는 박스 그래프를 이용할 수 있다.\n\nboxplot(BP_SYS ~ Q_SMK_YN, data = data, names=c(\"Never\", \"Ex-smoker\", \"Current\"), \n        main=\"SBP average by smoking\", ylab=\"SBP(mmHg)\", xlab=\"Smoking\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n두 연속형 변수 간의 관계는 산점도로 한 눈에 보여줄 수 있다. pch=n 옵션은 점의 모양, cex=n 옵션은 점의 크기를 지정한다.\n\nplot(HGHT ~ WGHT, data=data,\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\n\n\n\n\n\n\n\n범주형 변수에 따른 점의 분포를 표현하고자 할 때는 점의 색깔(col= 옵션)로 구분해서 표현할 수 있다. 2009년과 2015년에 실시한 검사에서 수검자의 신장, 체중 분포에 차이가 있는지 확인해보자.\n또, legend() 함수를 이용하면 범례에 사용될 옵션을 따로 설정할 수 있다.\n\ndata2 &lt;- data %&gt;% filter(EXMD_BZ_YYYY %in% c(2009, 2015))\nplot(HGHT ~ WGHT, data=data2, col=factor(EXMD_BZ_YYYY),\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\nlegend(x=\"bottomright\", legend=c(\"2009\", \"2015\"), col=1:2, pch = 19)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#line-plot",
    "href": "posts/2022-03-25-graph/index.html#line-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Line plot",
    "text": "Line plot\nplot() 함수에 type=“l” 옵션을 사용하면 선 그래프를 그릴 수 있다.summarize 함수를 이용해 연도에 따른 흡연자 비율(Q_SMK_YN=3)을 계산한 뒤, 선 그래프로 표현해보자.\n\ntable &lt;- data %&gt;% group_by(EXMD_BZ_YYYY) %&gt;% \n  summarize(smoker= mean(Q_SMK_YN==3, na.rm=T))\nprint(table)\n\n# A tibble: 7 × 2\n  EXMD_BZ_YYYY smoker\n         &lt;int&gt;  &lt;dbl&gt;\n1         2009  0.25 \n2         2010  0.263\n3         2011  0.215\n4         2012  0.222\n5         2013  0.276\n6         2014  0.232\n7         2015  0.208\n\nplot(table$EXMD_BZ_YYYY, table$smoker, type=\"l\",\n     xlab=\"Year\", ylab=\"prop of current smoker\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n앞에서 만들었던 산점도를 ggplot2를 이용해 다시 만들어보며 ggplot의 기본 문법을 이해해 보자.\nggplot 문법의 첫번째 요소는 시각화할 데이터, x축과 y축 변수, 기하학적 object의 모양, 색, 크기를 지정하고 변수의 스케일을 결정하는 aesthetic mapping이다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY)))\n\n\n\n\n\n\n\n위 코드를 통해 기본적인 그래프의 배경이 그려진다. 여기에 + 연산자를 이용해 기하학적 object를 추가한다. + 연산자는 magrittr에서의 %&gt;%와 같이 ggplot2 함수들을 연결해주는 역할을 한다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point()\n\n\n\n\n\n\n\nggtitle(), xlab(), ylab() 함수를 이용해 각각 그래프 제목, X축 라벨과 Y축 라벨을 추가할 수 있다. scale_color_manual() 함수를 이용하면 범례에 사용될 옵션들을 지정할 수 있다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point() +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\")\n\n\n\n\n\n\n\nggplot 문법의 장점은 + 연산자를 이용해서 기존 그래프 위에 새로운 요소를 추가해서 덧씌우는 것이 용이하다는 점이다.\n위의 산점도에 geom_smooth() 함수를 추가하면 그래프 위에 추세선을 덧씌울 수 있다. 이때, aes(col=) 옵션을 ggplot() 함수에서 제외하고 geom_point() 함수 내로 이동시키면, aes(col=) 옵션은 geom_point object에만 영향을 미치고 geom_smooth object는 영향을 받지 않게 된다. 마찬가지로 각 object 함수 내에 지정된 alpha(투명도), size, color 옵션은 해당 object에만 영향을 미친다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT)) +\n  geom_point(aes(col=factor(EXMD_BZ_YYYY)), alpha=0.5) +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\") +\n  geom_smooth(color=\"brown\", size=0.8)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-1",
    "href": "posts/2022-03-25-graph/index.html#boxplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\nggplot의 문법을 이해하고 나면 이를 응용해서 다양한 그래프를 그릴 수 있다.\n앞서 만들었던 흡연 여부에 따른 수축기 혈압의 분포 그래프를 다시 만들어 보자.\n\ndata2 &lt;- data %&gt;% filter(!is.na(Q_SMK_YN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n여기에 하나의 변수를 더 추가해서, facet으로 구분된 그래프를 만들 수 있다.\n\ndata2 &lt;- data2 %&gt;% filter(!is.na(Q_PHX_DX_HTN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_wrap(~Q_PHX_DX_HTN, labeller=label_both)\n\n\n\n\n\n\n\nfacet_grid를 이용해 2X2 grid 형태로 그래프를 그릴 수 있다. 고혈압 과거력 변수에 더해, 당뇨 과거력 변수에 따라서도 구분해서 그래프가 나타난다. labeller 함수를 사용해 facet의 label도 원하는 대로 설정할 수 있다.\n\ndata2 &lt;- data2 %&gt;% filter(!is.na(Q_PHX_DX_DM))\n\nHTN.labs &lt;- c(\"No HTN\", \"HTN\")\nnames(HTN.labs) &lt;- c(\"0\", \"1\")\nDM.labs &lt;- c(\"No DM\", \"DM\")\nnames(DM.labs) &lt;- c(\"0\", \"1\")\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_grid(Q_PHX_DX_DM~Q_PHX_DX_HTN,\n             labeller = labeller(Q_PHX_DX_HTN = HTN.labs, Q_PHX_DX_DM = DM.labs))"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot-1",
    "href": "posts/2022-03-25-graph/index.html#barplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n앞서 만들었던 바 그래프도 ggplot 패키지를 이용하면 table 변환 과정 없이 바로 그릴 수 있다.\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN))) +\n  geom_bar(fill=\"grey\", color=\"black\") +\n  ggtitle(\"Distribution of smoking\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n연도에 따른 흡연 여부의 분포도 table 변환 과정 없이 그릴 수 있다. geom_bar()의 position=‘fill’ 옵션을 설정하면 100% 누적 비율 바 그래프가 그려진다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"fill\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"proportion\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015)\n\n\n\n\n\n\n\n누적 비율이 아닌 count를 나타내고 싶다면 geom_bar()의 옵션을 position=‘stack’으로 변경한다. 적층형 그래프가 아닌 그룹형 그래프로 나타내고 싶다면 position=‘dodge’로 변경한다.\n그래프의 X축과 Y축 위치를 뒤집고 싶을 때는 coord_flip() 함수를 이용한다. X축과 Y축의 위치가 서로 바뀌는데, 축의 scale과 label을 다시 설정하지 않아도 되기 때문에 편리하다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"dodge\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"count\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015) +\n  coord_flip()"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram-1",
    "href": "posts/2022-03-25-graph/index.html#histogram-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\nggpubr 패키지의 기본 문법을 활용해서 히스토그램을 그려보자. 고혈압 병력이 있는 군과 없는 군 간에 체중 분포에 차이가 있을지 확인해본다.\n\ndata3 &lt;- data2 %&gt;% mutate(HTN = as.factor(ifelse(Q_PHX_DX_HTN==1, \"Yes\", \"No\")))\np &lt;- gghistogram(data=data3, x=\"WGHT\",\n                     color=\"HTN\", fill = \"HTN\", add=\"mean\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\nplot1 &lt;- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               xlab=\"Weight(kg)\",\n               legend.title=\"HTN Dx history\")\nprint(plot1)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-2",
    "href": "posts/2022-03-25-graph/index.html#boxplot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n같은 분포를 박스 그래프로도 나타낼 수 있다. 여기에 stat_compare_means() 함수를 활용하면, 고혈압 병력군 간 체중 평균에 통계적으로 유의한 차이가 있는지 확인할 수 있다.\n\np &lt;- ggboxplot(data=data3, x=\"HTN\", y=\"WGHT\", color=\"HTN\") +\n        stat_compare_means(method = \"t.test\", label.x.npc = \"middle\")\n\nplot2 &lt;- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               ylab=\"Weight(kg)\",\n               xlab=\"HTN Dx history\",\n               legend=\"none\")\n\nprint(plot2)\n\n\n\n\n\n\n\n세 개 이상의 범주로 나누어졌을 때도 마찬가지로 통계적 유의성을 검정할 수 있다. scale_x_discrete() 함수의 사용에서 확인할 수 있듯이, ggpubr 패키지는 ggplot의 문법을 기반으로 하고 있다.\n\nmy_comparisons &lt;- list(c(\"1\", \"2\"), c(\"2\", \"3\"), c(\"1\", \"3\"))\np &lt;- ggboxplot(data=data3, x=\"Q_SMK_YN\", y=\"WGHT\", color=\"Q_SMK_YN\") +\n        stat_compare_means(comparisons = my_comparisons) +\n        stat_compare_means(label.y = 150) +\n        scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\nplot3 &lt;- ggpar(p,\n               main=\"Weight distrubution by smoking\",\n               ylab=\"Weight(kg)\",\n               xlab=\"Smoking\",\n               legend=\"none\")\n\nprint(plot3)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n위에서 여러번 만들었던 신장과 체중의 산점도를 다시 그려보자. add = “reg.line” 옵션을 이용해 그래프 위에 추세선을 그린 뒤, stat_cor() 함수로 두 변수 간의 상관계수와 p value를 구할 수 있다.\n\np &lt;- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", \n               add = \"reg.line\", conf.int = TRUE,\n               add.params = list(color = \"navy\", fill = \"lightgray\")) +\n        stat_cor(method = \"pearson\")\n\nplot4 &lt;- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot4)\n\n\n\n\n\n\n\n같은 산점도를 고혈압 병력에 따른 두 그룹으로 나누어서 다른 색으로 표현하고, 상관계수와 p-value를 따로 계산할 수도 있다.\n\np &lt;- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", color=\"HTN\", alpha=0.5,\n               add = \"reg.line\", conf.int = TRUE) +\n        stat_cor(aes(color = HTN))\n\nplot5 &lt;- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot5)\n\n\n\n\n\n\n\nggarange 함수를 사용하면 여러 개의 그래프를 한 페이지에 배열해서 보여줄 수 있다.\n\nggarrange(plot2, plot3,\n          labels = c(\"A\", \"B\"),\n          ncol = 2, nrow = 1)"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html",
    "href": "posts/2022-02-11-datatable/index.html",
    "title": "data.table 패키지 소개",
    "section": "",
    "text": "본 자료는 빠른 속도와 메모리 효율성에 강점이 있는 data.table 패키지에 관해 기본 개념과 문법, 함수들을 예제를 통해 다루어 볼 것이다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "href": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "title": "data.table 패키지 소개",
    "section": "Load & save data: fread & fwrite\n",
    "text": "Load & save data: fread & fwrite\n\nfread 함수로 빠르게 csv 파일을 읽어와서 data.table 자료로 만들 수 있고, fwrite 함수로 csv 파일을 쓸 수 있다. fread와 fwrite 는 이름답게 매우 빠르며 Base R의 함수보다 40배 더 빠른 속도를 자랑한다. 파일을 읽어와서 data.table 자료로 만들 때, 로컬 file path를 입력하거나 http:// 로 시작하는 url을 입력하는 방법을 사용한다. fread로 파일을 읽으면 그 class는 data.frame에 data.table이 추가되며 문법이 원래의 data.frame과 달라지는 점을 유의하자.\nfread를 통해 데이터를 불러와 data.table 형태로 만들어보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\nSetup\n\n## Setup\n\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 8.6.0 with LibreSSL/3.3.6\n\n\nLoad file\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndf &lt;- read.table(url,header=T)\ndt &lt;- fread(url,header=T)\n# Class\nprint(class(df))\n\n[1] \"data.frame\"\n\nprint(class(dt))\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 볼 수 있다.\n\n\n# dt = data.table(df)\n# df = data.frame(dt)\n## See \ndt\n\n\n\n\n  \n\n\n\nSave file\n\n# Save file\nwrite.csv(dt, \"aa.csv\",row.names=F)\nfwrite(dt, \"aa.csv\")"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#row-operation",
    "href": "posts/2022-02-11-datatable/index.html#row-operation",
    "title": "data.table 패키지 소개",
    "section": "row operation",
    "text": "row operation\n행을 선택하는 slice는 data.table에서 DT[c(row1, row2, …),] 또는 DT[c(row1, row2, …)]의 형식으로 data.frame과 동일하게 쓰인다.\n\ndt[c(3,5)]\n\n\n\n\n  \n\n\n\n\n특정한 조건을 만족하는 행을 선택하는 filter는 data.table에서 DT[cond]의 형식으로 쓰인다.\n(이때, cond는 논리형 벡터이다.)\n\ndt[BMI&gt;=30 & HGHT&lt;150]\n\n\n\n\n  \n\n\n\n미리 key들을 지정하면 더 빠르게 검색할 수 있다. 자세한 내용은 뒤에서 다루도록 하겠다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#column-operation",
    "href": "posts/2022-02-11-datatable/index.html#column-operation",
    "title": "data.table 패키지 소개",
    "section": "column operation",
    "text": "column operation\n열을 이름 또는 순번으로 선택하는 select는 DT[, .(cols)] 또는 DT[, list(cols)]의 형식으로 data.frame과 비슷하나 몇 가지 차이점이 있다. 변수 이름으로 선택하는 경우 앞에 .()이나 list를 붙이지 않으면 결과로 벡터가 반환된다.\n\n순번으로 열 선택\n\n\ndt[, c(13, 14)]\n\n\n\n\n  \n\n\n\n\n\n이름으로 열 선택\n\n\n## same\n# dt[, list(HGHT, WGHT)]\ndt[, .(HGHT, WGHT)]\n\n\n\n\n  \n\n\n\n\n열을 선택할 때 DT[, .(new_col = col)] 형식을 사용하여 열 이름을 지정해서 출력할 수 있다.\n\n# rename\ndt[, .(Height = HGHT, Weight = WGHT)]\n\n\n\n\n  \n\n\n\n\n\n변수로 열 선택\n\n\n## same\n# vars &lt;- c(13,14) \nvars &lt;- c(\"HGHT\", \"WGHT\")\n\ndt[, ..vars]\n\n\n\n\n  \n\n\n\n변수로 열을 선택하는 경우, 변수 앞에 ..을 붙이지 않으면 오류가 발생하므로 주의하도록 한다. .. 대신 with = F 를 뒤에 붙이거나 .SD, .SDcols 옵션을 사용하기도 한다. .SD는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, vars, with = F]\n\n\n\n\n  \n\n\n\n\n\n열 제외\n\n필요없는 열을 제외할 때는 - 또는 ! 를 붙인다.\n\nicols = c(1:12)\n\n## same\n# dt[, -..icols]\ndt[, !..icols]\n\n\n\n\n  \n\n\n\ndata.table의 column operation에서는 열을 선택할뿐만 아니라 연산하는 식을 처리할 수 있다. 앞에서 배운 내용을 통해 특정 조건을 만족하는 행을 대상으로 mean 연산을 수행하여 보자.\n\ndt[HGHT &gt;= 180 & WGHT &lt;= 80, .(m_chol = mean(TOT_CHOL), m_TG = mean(TG))]\n\n     m_chol     m_TG\n      &lt;num&gt;    &lt;num&gt;\n1: 181.1143 144.8857"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#by-operation",
    "href": "posts/2022-02-11-datatable/index.html#by-operation",
    "title": "data.table 패키지 소개",
    "section": "by operation",
    "text": "by operation\nby 옵션을 이용하여 그룹별로 함수를 적용할 수 있다. by=.(그룹1, 그룹2, …)을 사용해 두 개 이상의 그룹별로 함수를 적용할 수 있다. 이때 괄호 앞에 있는 점(‘.’)은 list()를 의미하므로 꼭 포함시키도록 한다. by 대신 keyby를 사용할 경우, 그룹별 집계 결과가 정렬되어 나타난다.\n연도 변수인 EXMD_BZ_YYYY을 기준으로 집단을 분리한 후 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같다.\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n\n기준으로 사용되지 않은 모든 열에 대해 평균을 구할때는 .SD를 사용한다. 이는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n\n이번에는 두 개의 그룹 변수를 지정해 행의 개수를 출력해보자.\n\ndt[HGHT &gt;= 175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\n정렬\n그룹별로 함수를 적용한 결과를 정렬하고자 할 때는 keyby를 사용하거나 마지막에 [order()]를 붙인다.\n\nkeyby\n\n\ndt[HGHT &gt;= 175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\nby를 사용한 예제와는 달리 Q_SMK_YN에 대해서 정렬된 것을 볼 수 있다.\n\n\n[order()]\n\n\n# BMI에 대해 내림차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n  \n\n\n\n\n# BMI에 대해 오름차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n\n\n\n  \n\n\n\nExpressions in by\n\nby 옵션에는 변수뿐만 아니라 식을 지정할 수도 있다.\n약물 치료 여부에 따른 환자수를 확인하려는 경우, 다음과 같이 식을 지정한다.\n\ndt[, .N, by=.(Q_PHX_DX_STK &gt; 0, Q_PHX_DX_HTDZ &gt; 0)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "href": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "title": "data.table 패키지 소개",
    "section": "key를 이용한 탐색 setkey()",
    "text": "key를 이용한 탐색 setkey()\nkey를 사용하면 데이터의 탐색 및 처리 속도가 매우 향상된다. setkey(DT, col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용한다. 설정되어 있는 키를 제거할 때는 setkey(DT, NULL)를 설정한다.\n\n키 설정\n\nsetkey를 활용해 데이터 테이블에 키를 설정하고, key 함수로 설정된 키를 확인할 수 있다.\n\n# 1 key\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\n\n# 2 keys\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\n\n키를 활용한 행 선택\n\ndt[.(a)], dt[J(a)], dt[list(a)], dt[col == a] 중에서 아무거나 사용하여 행을 선택할 수 있다.\n\n## same\n# dt[.(2011)]\n# dt[list(2011)]\n# dt[EXMD_BZ_YYYY==2011]\ndt[J(2011)]\n\n\n\n\n  \n\n\n\n\n## same\n# dt[.(2011, 2)]\n# dt[list(2011, 2)]\n# dt[EXMD_BZ_YYYY==2011 & Q_HBV_AG==2]\ndt[J(2011, 2)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 병합 merge()",
    "text": "데이터 테이블 병합 merge()\ndata.table은 key를 사용하거나, on=을 활용하여 두 데이터 데이블을 병합할 수 있다.\n\n\n\n\nMerge in data.table\n\n\n\n기존 데이터를 가공하여 새로운 data.table인 dt1, dt2에 저장하고 연도에 따라 merge해 보자.\n\n# dt1\n(dt1 &lt;- dt[c(1, 300, 500, 700, 1000)])\nsetkey(dt1, EXMD_BZ_YYYY)\n\n\n\n\n  \n\n\n\n\n# dt2\n(dt2 &lt;- dt[c(400, 600, 800 ,1200, 1500)])\nsetkey(dt2, EXMD_BZ_YYYY)\n\n\n\n\n  \n\n\n\n1. inner join\n두 데이터에 모두 존재하는 경우 dt1[dt2, on=‘key’, nomatch=0] 또는 merge(dt1, dt2, by=‘key’, all=F) 형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY', nomatch=0]\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all = F)\n\n\n\n\n  \n\n\n\n2. left_outer_join\n첫 번째 데이터에 존재하는 경우 dt2[dt1, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.x=T) 형식 사용\n\n\n\n\n\n\n\n\n\ndt2[dt1, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.x=T)\n\n\n\n\n  \n\n\n\n3. right_outer_join\n두 번째 데이터에 존재하는 경우 dt1[dt2, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.y=T)형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.y=T)\n\n\n\n\n  \n\n\n\n4. full_outer_join\n어느 한 쪽에 존재하는 경우 merge(dt1, dt2, by=‘key’, all=T) 형식 사용\n\n\n\n\n\n\n\n\n\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all=TRUE)"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 수정 연산자 :=",
    "text": "데이터 테이블 수정 연산자 :=\n데이터 테이블에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용한다. 수정 또는 생성할 열이 하나인 경우, dt[ , newcol1 := ] 형식을 쓰며 열이 두 개 이상인 경우 dt[, ‘:=’ (col1=, col2=)]을 사용한다.\n다음의 예시를 통해서 자세히 알아보자.\n기존의 데이터 테이블에서 HDL - LDL을 구해서 diff라는 이름의 열을 새로 생성한다. 그리고, HGHT와 WGHT는 HGHT*0.9, WGHT+5로 수정한다.\n열 생성/수정\n위 코드를 실행시키면 갱신이 눈에 보이지 않는 상태로 실행되며, 만약 갱신 결과를 눈에 보이도록 출력하려면 제일 뒤에 [] 를 붙여주어야 한다.\n\n# 열 생성\ndt[, diff := HDL-LDL][]\n\n\n\n\n  \n\n\n\n\n# 열 수정\ndt[, ':=' (HGHT = HGHT*0.9, WGHT = WGHT+5)][]\n\n\n\n\n  \n\n\n\n열 삭제\n데이터 테이블에서 열을 삭제하려면 col := NULL 형식을 사용한다.\n\n# BMI 삭제\ndt[, BMI := NULL]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#특수-기호",
    "href": "posts/2022-02-11-datatable/index.html#특수-기호",
    "title": "data.table 패키지 소개",
    "section": "특수 기호",
    "text": "특수 기호\n.SD\n.SD 는 ’Subset of Data’의 약자로, by로 지정한 그룹 칼럼을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용한다.\n\n# 모든 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n 또한 .SD 기호를 사용하여 연도별로 처음 두 개의 행을 추출할 수 있다.\n\ndt[, head(.SD, 2), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n.SDcols\n.SDcols는 연산 대상이 되는 특정 칼럼을 지정하는 특수 기호이다. 특정 열을 대상으로 연산을 할 때 by 다음에 .SDcols = c(“col1”, “col2”, …)로 연산 대상을 지정한다.\n\n# HGHT, WGHT 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]\n\n\n\n\n  \n\n\n\n.N\n.N는 부분 데이터의 행의 수를 나타내며, 요약 통계치를 구할 때 대상 데이터의 수를 간편하게 구할 수 있다.\n\n# 특정 조건을 만족하는 행의 수\ndt[LDL &gt;= 150, .N]\n\n[1] 228\n\n\n\n# HGHT, WGHT 칼럼의 연도별 평균값과 행의 수\ndt[, c(.N, lapply(.SD, mean)), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#melt",
    "href": "posts/2022-02-11-datatable/index.html#melt",
    "title": "data.table 패키지 소개",
    "section": "melt",
    "text": "melt\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있다. melt(data, id.vars, measure.vars, variable.name, value.name) 형식으로 쓰이며, id.vars에는 고정 칼럼을 measure.vars는 stack 처리할 칼럼을 넣는다. melt함수를 써서 길게 재구조화한 후의 “variable”, “value” 변수 이름을 바꾸고 싶다면 variable.name=“new_var_name”, value.name=“new_val_name” 처럼 새로운 칼럼 이름을 부여하여 지정할 수 있다.\n\n# wide to long\ndt.long1 &lt;- melt(dt,\n                id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HGHT\", \"WGHT\"),\n                measure.vars = c(\"TOT_CHOL\", \"HDL\", \"LDL\"),\n                variable.name = \"measure\",\n                value.name = \"val\")\ndt.long1\n\n\n\n\n  \n\n\n\nEnhanced melt\nmelt 함수는 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있다.\n\n\nlist에 복수의 칼럼 이름을 입력하는 방법\n\nmelt 함수에 measure=list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣는다. 이때 공통의 value.name을 지정할 수 있다.\n\ncol1 &lt;- c(\"BP_SYS\", \"BP_DIA\")\ncol2 &lt;- c(\"HDL\", \"LDL\")\ndt.long2 &lt;- melt(dt, \n                 measure = list(col1, col2),\n                 value.name = c(\"BP\", \"Chol\"))\ndt.long2\n\n\n\n\n  \n\n\n\n\n\n특정 패턴을 정규 표현식으로 매칭하는 방법\n\nmelt 함수에 measure=patterns() 형식으로 특정 패턴을 따르는 복수의 칼럼을 정규 표현식을 통해 설정한다.\n\ndt.long3 &lt;- melt(dt, \n            measure=patterns(\"^Q_PHX_DX\", \"^BP\"), \n            value.name=c(\"Q_PHX_DX\", \"BP\"))\ndt.long3"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#dcast",
    "href": "posts/2022-02-11-datatable/index.html#dcast",
    "title": "data.table 패키지 소개",
    "section": "dcast",
    "text": "dcast\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용한다. dcast(data, formula, value.var, fun.aggregate) 형식으로 쓰이며, formula의 LHS에는 id.vars, RHS에는 variable을 입력하고 value.name에는 “value”를 입력한다. 이때 “variable”과 “value” 변수 이름을 바꿨다면, 새로 지정한 칼럼명을 넣는다.\n\n# long to wide\ndt.wide1 &lt;- dcast(dt.long1, EXMD_BZ_YYYY + RN_INDI + HGHT + WGHT ~ measure, value.var = \"val\")\ndt.wide1\n\n\n\n\n  \n\n\n\n\ndcast 함수에 집계 함수(fun.aggregate)를 사용하여 그룹별 요약 통계량을 계산한 결과를 재구조화하여 반환할 수 있다.\n\n# 연도별 TOT_CHOL, HDL, LDL의 평균값\ndt.wide2 &lt;- dcast(dt.long1, EXMD_BZ_YYYY ~ measure, value.var = \"val\", fun.aggregate = mean, na.rm =T)\ndt.wide2\n\n\n\n\n  \n\n\n\nEnhanced dcast\ndcast 함수의 value.vars에 복수의 칼럼을 넣어 여러 개의 칼럼을 동시에 재구조화할 수 있다.\n\ndt.wide3 &lt;- dcast(dt.long2,\n                  ... ~ variable,\n                  value.var = c(\"BP\", \"Chol\"))\ndt.wide3"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html",
    "href": "posts/2022-02-07-tableone/index.html",
    "title": "tableone 패키지 소개",
    "section": "",
    "text": "본 자료는 데이터셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. tableone 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "href": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "title": "tableone 패키지 소개",
    "section": "Categorical variable conversion",
    "text": "Categorical variable conversion\nfactorVars 인자를 사용하여 범주형 변수를 지정할 수 있다. 이때 vars 인자를 통해 전체 데이터 셋 중 테이블에 들어갈 변수를 설정할 수 있고, 지정하지 않을 시 데이터 셋의 모든 변수가 포함된다.\n\n# Variables\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n범주형 변수로 설정한 컬럼의 요약값이 mean(sd)에서 n(percentage)로 바뀐 것을 볼 수 있다.\n두 개의 범주가 있는 범주형 변수의 경우, 두 번째 범주의 요약값만 출력된다. 예를 들어 0과 1의 범주가 있을 때, 범주1의 개수와 백분율이 출력된다. 이는 옵션 설정을 통해 전체 범주의 요약값을 출력하도록 변경할 수 있다.\n3개 이상의 범주가 있을 때에는 모든 범주의 값이 요약되며, 백분율은 누락된 값을 제외한 후 계산된다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "href": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "title": "tableone 패키지 소개",
    "section": "Multiple group summary",
    "text": "Multiple group summary\nstrata 인자를 설정하여 그룹별 연산을 할 수 있다. strata는 dplyr 패키지의 group_by() 함수와 유사하며, 그룹 연산을 할 변수를 지정하여 사용할 수 있다.\n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nt2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)\n\n\n\n\n\n\n\n\n연속형 변수의 경우, 기본적으로 one-way ANOVA test가 적용되며 nonnormal일 경우 옵션 설정을 통해 Kruskal–Wallis one-way ANOVA test를 적용할 수 있다.\n범주형 변수의 경우, 기본적으로 chisq-test가 적용되며 print 함수의 exact 옵션 설정을 통해 fisher-test를 적용할 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "href": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "title": "tableone 패키지 소개",
    "section": "Showing all levels",
    "text": "Showing all levels\n범주형 변수에서 모든 범주의 요약값을 확인하려면 ShowAllLevels 또는 cramVars 옵션을 사용한다. ShowAllLevels = T 를 설정하거나 cramVars 옵션에 원하는 변수명을 지정하여 사용할 수 있다.\n\n1.use showAllLevels\n\n\n\nprint(t1, showAllLevels = T)\n\n\n\n\n\n\nlevel\nOverall\n\n\n\nn\n\n1644\n\n\nHGHT (mean (SD))\n\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n\n65.10 (12.53)\n\n\nBMI (mean (SD))\n\n23.92 (3.38)\n\n\nHDL (mean (SD))\n\n55.90 (19.47)\n\n\nLDL (mean (SD))\n\n118.69 (201.99)\n\n\nTG (mean (SD))\n\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n\n25.98 (27.18)\n\n\nQ_PHX_DX_STK (%)\n0\n1059 (98.9)\n\n\n\n1\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ (%)\n0\n1052 (97.6)\n\n\n\n1\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n1\n77 ( 4.7)\n\n\n\n2\n1102 (67.1)\n\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n1\n995 (60.6)\n\n\n\n2\n256 (15.6)\n\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n2.use cramVars\n\n\n\nprint(t1, cramVars=\"Q_PHX_DX_STK\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 0/1 (%)\n1059/12 (98.9/1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "href": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "title": "tableone 패키지 소개",
    "section": "nonnormal variables",
    "text": "nonnormal variables\n비모수통계를 사용하는 연속형 변수에는 nonnormal 옵션을 설정한다. nonnormal 설정 시 mean(sd)에서 median(IQR)로 요약값이 변경된다.\n\nprint(t1, nonnormal=\"LDL\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (median [IQR])\n112.00 [90.00, 134.00]\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#exact",
    "href": "posts/2022-02-07-tableone/index.html#exact",
    "title": "tableone 패키지 소개",
    "section": "exact",
    "text": "exact\nexact 옵션을 통해 fisher-test를 진행할 범주형 변수를 설정할 수 있다. 범주형 변수는 기본적으로 chisq-test가 적용되며, exact 옵션에 fisher-test를 적용할 변수를 지정하여 사용할 수 있다.\n\nprint(t2, exact=c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.045\nexact\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.281\nexact\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#smd",
    "href": "posts/2022-02-07-tableone/index.html#smd",
    "title": "tableone 패키지 소개",
    "section": "smd",
    "text": "smd\nsmd 옵션을 통해 smd(standardized mean difference)를 table1에 포함할 수 있다. default는 FALSE이고, smd=TRUE 설정 시 각 변수의 smd 값이 출력된다.\n\nprint(t2, smd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\nSMD\n\n\n\nn\n995\n256\n391\n\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n0.972\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n0.611\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n0.181\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n0.197\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n0.091\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n0.341\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n0.196\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n0.137\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n0.084\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n0.109\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\nNaN\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html",
    "href": "posts/2022-01-25-doctorskku2022/index.html",
    "title": "창업 경험 공유",
    "section": "",
    "text": "김진섭 대표는 1월 28일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정으로, 발표 슬라이드를 미리 공유합니다. 자세한 내용은 메디게이트뉴스 http://medigatenews.com/news/3852895813 참고 부탁드립니다."
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "href": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "title": "창업 경험 공유",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n데이터 입력/관리/분석 통합서비스\n공동연구에 코인 인센티브 = 공동연구플랫폼\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n연구지원전문가 새로운 직업 창출"
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "href": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "title": "창업 경험 공유",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/doctorskku2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html",
    "href": "posts/2022-01-06-internship-django-1/index.html",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 1주차 동안 학습한 내용에 대해 공유합니다.\n차라투에서는 사용자에게 여러가지 통계 웹앱을 제공하는 Openstat 서비스를 준비중입니다. 서비스를 이용하는 사용자 본인이 불필요한 기능을 제거하고, 본인에게 필요한 기능만을 제공받을 수 있는 기능 도입 준비 중입니다. Django를 사용하여 사용자에게 보여질 기능을 DB로 유지하며 해당 DB기반으로 사용자에게 서비스를 제공해야합니다. 이에 사용자가 직접 DB의 필드값을 수정하며 서비스를 제공받는 기능을 구현했습니다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html#목차",
    "href": "posts/2022-01-06-internship-django-1/index.html#목차",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "목차",
    "text": "목차\n\nModel.Boolean 값으로 화면 출력 통제\n\nCheckBox를 활용하여 Model.BooleanField 핸들링\n\n\n\nModel.Boolean 값으로 화면 출력 통제\n\n\n[models.py]\nClass Post(models.Model):\n  user_id = models.CharField(max_length = 50, default=\"\")\n  postname = models.CharField(max_length =50)\n  content = models.TextField()\n  important = models.BooleanField(default = True)  \n\n Model 필드에 BooleanField 형식의 필드를 추가합니다.\n이 예제에서는 important라는 필드명을 사용했지만, 필드명은 다른 이름으로 사용해도 괜찮습니다.\n(기존 Model에 새로운 필드를 추가하는 경우, 기존 Model 속 데이터들의 필드들도 일괄적으로 변경됩니다. 새로운 필드에 값을 넣지 않을 경우, 에러가 발생하므로 Model에 새로운 필드를 추가할때는 default 값을 넣어주는것이 좋습니다.)\n\n\n\n[views.py]\nfrom .models import Post\n\ndef show_important_post(request):\n    postlist = Post.objects.all()\n    return render(request,'blog/ImportantPost_Posting.html',{'postlist':postlist})\n\n\npostlist = Post.objects.all()\n\n현재 생성된 Models의 Post를 import하고 현재 모델 Post 속에 담겨있는 모든 objects들을 postlist에 담습니다.  \n\nretrun render(request,‘blog/ImportantPost_Posting.html’,{‘postlist’:postlist})\n\n{‘postlist’:postlist}로 {key,value} 값을 맞춰서 blog/ImportantPost_Posting.html에 넘깁니다.\n(예시에서는 blog/ImportantPost_Posting.html에 넘겼지만 본인의 상황에 적합한 templates에 넘기면 됩니다.)   \n\n\n[templates-(ImportantPost_Posting.html)]\n{% block contents %}\n&lt;h1&gt;중요한 게시판 &lt;/h1&gt;\n    {% for list in postlist %}\n        {% if list.important is True %}\n        &lt;ul&gt;\n            &lt;li&gt;\n                작성자: {{list.user_id}} \n                &lt;a href=\"/blog/showImportant/{{list.pk}}/\"&gt;{{list.postname}}&lt;/a&gt;\n            &lt;/li&gt;\n        &lt;/ul&gt;\n        {% endif %}\n    {%endfor%}\n    &lt;button&gt;&lt;a href=\"/blog/showImportant/new_post/\"&gt;글쓰기&lt;/a&gt;&lt;/button&gt;\n    &lt;input type=\"button\" value = \"돌아가기\" onclick = \"back()\"&gt;\n{% endblock %}\n\n\n{{%for list in postlist%}}\n　　　{%if list.important is True%}\n{% endfor %}\n\n앞서 views에서 넘겨 받은 postlist를 순회합니다. 만약 postlist에 해당하는 부분을 전부 출력하면 현재 가지고 있는 POST DB에 존재하는 모든 내용을 출력하게 됩니다. 하지만 저희는 if 문을 활용하여 DB 속 모든 객체들 중 important 필드의 값이 True 인 경우만 &lt;li&gt; 태그에 넣어 화면에 출력하도록 합니다. \n\n\n{{list.postname}}\n\n위 예제에서는 list의 postname 필드를 출력하였지만, list의 다른 필드를 출력해도 상관없습니다.\n\n\n\n\n\nCheckBox활용 Model.BooleanField 핸들링\n\n\n[Views.py]\ndef edit_post(request,pk):\n    post = Post.objects.get(pk=pk)\nif request.method == 'POST':\n        if len(request.POST.getlist('important')) == 0:\n            important = False\n        else: important = True\n        post.postname = request.POST['postname']\n        post.content = request.POST['contents']\n        post.important = important\n        post.save()\n\n\npost = Post.objects.get(pk=pk)\n\nedit_post함수를 통해 request 메시지와 (pk:primary key)를 받으면 Model class인 Post에서 pk가 동일한 객체를 찾아 post에 넘겨줍니다.  \n\nif len(request.POST.getlist(‘important’))==0\n　　important = False\nelse: important = True\n\n’important’값은 체크박스를 통하여 값을 넘깁니다. 만약 위 예제 코드 속 다른 코드에서 쓰이는 것처럼 post.important = request.POST[‘important’] 형식을 사용하여 important 값을 넣으려고 시도한다면 문제가 생깁니다. 체크 박스를 체크하여 request 요청했다면 문제가 되지 않지만, 체크 박스를 체크하지 않고 값을 넘기면 MultiValueDictKeyError 문제가 발생하기에 코드를 이와 같이 수정해야 합니다.\nMultiValueDictKeyError를 피하기 위하여 우리는 important의 값을 list 형태로 가지고 옵니다. 만약 체크 박스를 체크했다면 list에 값이 들어있을 것이고 체크박스를 체크하지 않았다면 list에 값이 없을 것입니다. 그렇게 list의 길이를 len을 통해 측정하여 길이가 0이면 False를, 길이가 0이 아니면 True를 important 값에 넣어 post 필드 값을 수정하여 새롭게 save() 해줍니다.\n \n\n\n[templates-(editPost.html)]\n{% if Post.important is True %}\n  &lt;input type = \"checkbox\" name= \"important\" value= \"True\" checked&gt; important &lt;br&gt;\n{% else %}\n  &lt;input type = \"checkbox\" name = \"important\" value = \"False\"&gt; important &lt;br&gt;\n{% endif %}\n Post의 필드 중 important 부분을 체크박스를 통하여 수정하는 templates의 일부를 가지고 왔습니다. 현재 Post의 important 부분이 True 이면 체크된 상태로 화면에 출력되도록 하고 False이면 체크되지 않은 상태로 화면에 출력되도록 했습니다. \n \n\n\n\n출력\n\n\n\n\n\nimportant = True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportant = False\n\n\n\n\n\n\n\n\n\n\n\n상기 이미지에 보이는 것처럼 체크박스의 체크 유무가 필드값인 important로 대입되어 important의 값에 따라 True 인 경우에는 화면에 출력되고 False인 경우에는 출력되지 않는 모습을 확인할 수 있습니다.\n\n\n\n결론\nModel의 booleanField 값을 CheckBox를 통해 제어하며 화면에 출력을 통제하는 방법에 대해 알아보았습니다.\n해당 방법을 활용하여 User 맞춤 서비스 제공 등 다양한 방식으로 DB의 내용을 기준하여 서비스 제공에 기여할 것으로 기대됩니다."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html",
    "href": "posts/2021-09-28-shinyecrf2/index.html",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이티 입력웹을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "href": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "요약",
    "text": "요약\n4월 발표 개발완료 후 서비스 중\n\n수백개 변수 추가, 입력화면 디자인 개선(Thanks to 김진환)\nshinydashboard 적용, 환자수 대시보드 추가(Thanks to 고현준)\nshinymanager 로 로그인 모듈: 특정 ID만 삭제권한.\n\n타 연구 빠른적용 위해 모듈화 필요, 설문지 Builder 가능할까?"
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "href": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea2 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html",
    "href": "posts/2021-08-19-pubicdatawithr/index.html",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "",
    "text": "김진섭 대표는 9월 12일 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에 참석, R 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "요약",
    "text": "요약\nR 로 보험공단/심평원 빅데이터 분석가능.\n\n공단표본코호트 V1 은 내부 분석환경 구축\n공단표본코호트 V2, 심평원 데이터는 원격 분석환경 이용\n대용량데이터 위한 R 패키지: data.table, fst, parallel\n\n자체개발 R 패키지 CRAN 배포, 원격분석환경에서도 이용가능.\n\nKaplan-meier 그림: jskm\n논문용 테이블: jstable\nGUI 분석: jsmodule\n\nShiny 로 웹기반 실시간 분석서비스.\n\n내부 분석환경: 웹에서 실시간 분석수행\n원격 분석환경: 모든 분석결과 반출 후 웹기반 시각화\nExcel/PPT 다운로드\n\nCDM 다기관 메타분석 서비스\n\nTable 1 합치기, Forest/Funnel plot 등"
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/publicdata_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html",
    "href": "posts/2021-07-25-googleauth/index.html",
    "title": "googleAuth",
    "section": "",
    "text": "7월 17일 Virtual Box를 이용해 ShinyProxy 환경을 구축하는 강의가 있었습니다. 강의 시간에 직접 해보지 못한 Social Login 중 Google Authentication 방법에 대해 알아보겠습니다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#요약",
    "href": "posts/2021-07-25-googleauth/index.html#요약",
    "title": "googleAuth",
    "section": "요약",
    "text": "요약\n\n지난 Lecture에서 구축한 환경설정 간단하게 리뷰\nApplication.yml을 project folder에 생성하여 Social login을 사용하도록 설정\nGoogle Developer Console에서 OAuth 설정을 통해 Client Id, Key 발급과 Redirection URL 설정"
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#contents",
    "href": "posts/2021-07-25-googleauth/index.html#contents",
    "title": "googleAuth",
    "section": "Contents",
    "text": "Contents\nReview of previous lecture\n\n\nShinyManager (https://datastorm-open.github.io/shinymanager/)\n\nShiny App의 Authentication service 제공\nSigle thread 기반으로 구성되기 때문에 동시에 여러 유저가 로그인 불가\n\n\n\nShinyProxy (https://www.shinyproxy.io/)\n\nShinyManager를 사용할 때 동시에 하나의 유저만 로그인할 수 있는 한계를 극복하기 위해 도입\n여러 사용자의 동시 접근을 가능한 Shiny App deploy\n\n\n\nGoogle OAuth 설정으로 넘어가기 전에…\n\n강의시간에 만든 Virtual box의 가상이미지 실행 후 shinyproxy docker 이미지가 실행되고 있는지 확인\n\n\nsudo docker images | grep shinyproxy\n\n\n\nRstudio server 로그인 후 Terminal에서 shinyproxy 실행\n\n브라우저에서 127.0.0.1:8787로 접속 후 Virtual Box 이미지 생성 시 입력했던 아이디와 패스워드로 로그인\nRstudio Terminal에서 shinyproxy 실행\n\n\njava -jar shinyproxy-2.5.0.jar\n\n\n\n127.0.0.1:8080으로 접속하여 admin 계정을 통해 login 기능 작동 확인\n\n\n\n\n\n\n\n\n\nGoogle OAuth\n\nFacebook, Github 등 다양한 소셜 로그인 중 하나\nGoogle Developer Console https://console.cloud.google.com/apis/에서 프로젝트 생성 후 Client Id와 Client Key 발급\n발급받은 Id, Key를 application.yml 파일에 입력\nGoogle Devloper Console에서 로그인 후 redirection할 URL을 입력하면 끝!\n구체적인 과정은 네이버 블로그를 따라하세요\nShinyProxy Setting\n\napplication.yml : Authentication 방법을 정의하는 파일로 simple, LDAP, openid, social 등의 방법 선택 가능 (authentication: {원하는 로그인 방식})\nRStudio server에서 홈 디렉토리에 application.yml 파일 생성(아래 코드를 복사하세요)\n\n\nproxy:\n    title: Shiny App\n    logo-url: https://www.openanalytics.eu/shinyproxy/logo.png\n    landing-page: /\n    heartbeat-rate: 10000\n    heartbeat-timeout: 60000\n    port: 8080\n    authentication: social\n    admin-groups: scientists\n    openid:\n      auth-url: https://accounts.google.com/o/oauth2/v2/auth\n      token-url: https://www.googleapis.com/oauth2/v4/token\n      jwks-url: https://www.googleapis.com/oauth2/v3/certs\n      client-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n      client-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n      # Docker configuration\n    social:\n      google:\n        app-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n        app-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n    docker:\n      url: http://localhost:2375\n      port-range-start: 20000\n    specs:\n    - id: 01_hello\n      display-name: Hello Application\n      description: Application which demonstrates the basics of a Shiny app\n      container-cmd: [\"R\", \"-e\", \"shinyproxy::run_01_hello()\"]\n      container-image: openanalytics/shinyproxy-demo\n      access-groups: [scientists, mathematicians]\n    - id: 06_tabsets\n      container-cmd: [\"R\", \"-e\", \"shinyproxy::run_06_tabsets()\"]\n      container-image: openanalytics/shinyproxy-demo\n      access-groups: scientists\n\n  logging:\n    file:\n      shinyproxy.log\n\n\nauthenticaiton을 openid로 설정할 경우\n\n\n위 코드의 openid 부분과 동일하게 auth, token, jws url 작성\nclient-id와 client-secret을 Google Developer Console에서 확인 후 입력\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/login/oauth2/code/shinyproxy\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행 (localhost:8080로 접근하기 위해서는 redirect url에 http://localhost:8080/login/oauth2/code/shinyproxy 등록)\n\n\nauthentication을 social로 설정할 경우\n\n\n위 코드의 social 부분과 동일하게 작성\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/signin/google\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행\n마무리\n\nShinyProxy는 Spring framework 기반으로 작성되어 application.yml 수정을 통해 여러 인증방법을 적용할 수 있습니다.\nFacebook, Github 등 여러 API provider들이 구글과 비슷한 인증 서비스를 제공하고 있어 손쉽게 여러 social 인증을 추가할 수 있습니다.\n발급받은 client id와 secret은 외부로 노출되어서는 안됩니다(Github Repo에 commit 금지!)\nsocial과 openid 모두 적용할 수 있으나 redirect url에 차이가 있음에 주의 (정확한 차이는 연구 필요)"
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html",
    "href": "posts/2021-07-05-channel.io-install-review/index.html",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "",
    "text": "얼마 전 회사 웹사이트에 채널톡 서비스를 추가했습니다. 우리 회사는 Hugo를 이용해 홈페이지를 운영합니다. Hugo는 Jekyll과 비슷한 서비스로 Static Site Generator의 한 종류입니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#서론",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#서론",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "",
    "text": "얼마 전 회사 웹사이트에 채널톡 서비스를 추가했습니다. 우리 회사는 Hugo를 이용해 홈페이지를 운영합니다. Hugo는 Jekyll과 비슷한 서비스로 Static Site Generator의 한 종류입니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설치",
    "text": "설치\n우선 채널톡 서비스는 Javascript 코드를 추가하는 것만으로도 쉽게 설치할 수 있습니다. 회원가입을 하고 나면 페이지에 추가해야 할 Javascript 코드가 제공됩니다. 설정을 위해, 제공되는 Javascript 코드를 파일화 하여 홈페이지 프로젝트 폴더의 /public/js에 넣고 config.toml파일에 해당 Javascript 파일을 사용하겠다고 설정해주는 것으로 간단히 설치가 끝납니다.\n\n\n\nchannel.js는 제가 임의로 지정한 이름입니다. 원하시는 이름으로 바꾸셔도 무방합니다.\n\n\n설치 직후에는 두 번째 행이\n\ncustom_js = []\n\n와 같은 상태입니다. 원하는 Javascript 파일의 경로와 이름을 큰따옴표로 묶어 대괄호 속에 넣어주면 사이트에 해당 파일이 적용됩니다. CSS또한 동일한 방법으로 custom_css에 원하는 파일의 경로와 이름을 입력한 후 선택적으로 적용할 수 있습니다. 이후 Netlify를 통해 사이트를 빌드 후 Publish 하였습니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설정",
    "text": "설정\n \n처음 실행 후 먼저 회사의 로고와 설명을 추가하고, 채널톡 주소도 설정하였습니다.\nZarathu는 Medical Research Supporters입니다. 위와 같이 설정하고 나면\n\n위와 같이 고객님들께서 상담 버튼을 누르자마자 나오는 메시지들과 이미지가 변경됩니다.\n고객님들께서 상담을 시작하시면\n\n위에서 설정한 것과 같은 메시지가 표시됩니다. 이는 회사의 응대를 기다리는 동안 정보를 입력하게 하여 상담을 원활히 진행하게 하는 것에 큰 도움이 됩니다.\n템플릿 설정도 있어, 매크로를 통해 빠르게 응대할 수도 있었습니다. \n\n마지막으로 운영 시간을 설정하였습니다. 출근, 퇴근 시 따로 설정할 필요 없이 자동으로 온/오프라인 설정을 할 수도 있었고 수동으로 설정할 수도 있었습니다. 저희는 회사 특성상 수동으로 설정하였습니다.\n\n종합적인 테스트를 해 보았습니다. 웹으로 들어가 테스트라고 메시지를 남기니\n\n와 같은 정보를 물을 수 있습니다. 이후 담당자를 배정한 후 테스트 계정과 소통할 수 있었습니다."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 2월 Shinykorea 밋업에 참석, 서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 공유할 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "요약",
    "text": "요약\n\n대표적인 감염병 모형은 SIR/SEIR 이고, 초기값과 parameter(예: 감염률)가 주어진 미분방정식으로 표현.\n확진자수 등 실제 데이터를 활용, parameter 들을 추정.\nSEIR 에 서울시 확진자수를 적용, 시간에 따라 변화하는 감염률을 계산함.\nParameter와 그 파생지표의 범위를 제한하고 신뢰구간을 계산하기 위해, 베이지안통계 이용 예정."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/covidmodel-seoul/modelling/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html",
    "href": "posts/2020-10-05-yu-seminar/index.html",
    "title": "의학통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 영남대학교 “의사과학자 역량 배가 프로젝트” 에 참석, 그동안 해온 지원업무에 대해 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#요약",
    "href": "posts/2020-10-05-yu-seminar/index.html#요약",
    "title": "의학통계지원 소개",
    "section": "요약",
    "text": "요약\n다양한 의료데이터를 다뤄본 경험으로, 의학연구 활성화에 기여하겠습니다.\n\n의과대학, 유전체연구 박사과정, 삼성전자 무선사업부를 거치며 임상, 유전체, 모바일헬스 등 다양한 데이터를 다루었습니다.\n현재 연구지원법인 차라투 를 운영 중입니다.\nShiny 밋업 의 진행과 후원을 맡아, 의료/축산/게임/반도체/신용평가/IPTV 등 다양한 분야의 사람들과 함께 Shiny 를 알아가는 중입니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#slide",
    "href": "posts/2020-10-05-yu-seminar/index.html#slide",
    "title": "의학통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/yu/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "title": "메타분석 웹 개발 후기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 8월 Shinykorea 밋업에 참석, 메타분석 ShinyApps 만든 후기를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "title": "메타분석 웹 개발 후기",
    "section": "요약",
    "text": "요약\nhttp://app.zarathu.com/meta-analysis 에 메타분석 ShinyApps 를 공개하였다.\n\n메타분석은 여러 연구결과를 합쳐서 보여주는 분석, meta 패키지를 ShinyApps 로 구현하였다.\nrhandsontable 로 연구결과를 직접 입력한다.\n서버에 한글폰트 설치 후 showtext 로 불러와 한글깨짐을 해결한다.\ncolourpicker 로 글자색 조절한다.\nEMF 포맷 다운로드를 지원, PPT에서 직접 그림수정할 수 있다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "title": "메타분석 웹 개발 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/meta-analysis 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html",
    "href": "posts/2020-07-08-table1inmed/index.html",
    "title": "의학 연구에서의 기술통계",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 1주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#요약",
    "href": "posts/2020-07-08-table1inmed/index.html#요약",
    "title": "의학 연구에서의 기술통계",
    "section": "요약",
    "text": "요약\n\n연속변수의 2그룹 비교: 정규분포 인정하면 t-test, 아니면 Wilcox-test\n연속변수의 3그룹 이상 비교: 정규분포 인정하면 one-way ANOVA, 아니면 Kruskal–Wallis one-way ANOVA\n범주형 변수의 그룹 비교: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n본사가 개발한 웹 과 R 패키지 에서 바로 Table 1 을 얻을 수 있다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#slide",
    "href": "posts/2020-07-08-table1inmed/index.html#slide",
    "title": "의학 연구에서의 기술통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/table1 를 클릭하면 볼 수 있다. 강의록은 과거 내용인 https://blog.zarathu.com/posts/2018-11-24-basic-biostatistics 를 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 2주차에는 %&gt;% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nProject 생성\n\n\n\n강의록은 과거 글 https://blog.zarathu.com/posts/2019-01-03-rdatamanagement/ 을 참고하자."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "요약",
    "text": "요약\ntidyverse는 직관적인 코드를 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\n\nmagrittr 패키지의 %&gt;% 연산자로 의식의 흐름대로 코딩한다.\ndplyr 패키지의 select, mutate, filter, group_by, summarize 함수는 %&gt;% 와 찰떡궁합이다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-snuhlab/tidyverse 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html",
    "href": "posts/2020-02-11-tokyor81/index.html",
    "title": "TokyoR 81회 리뷰",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 3월 Shinykorea 밋업에 참석, 일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#요약",
    "href": "posts/2020-02-11-tokyor81/index.html#요약",
    "title": "TokyoR 81회 리뷰",
    "section": "요약",
    "text": "요약\n일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰하였다.\n\n초심자세션: R과 shiny 기초를 다루었다.\n응용세션1: 비동기 프로그래밍, 30분만에 적당히 사용할 shiny 앱 만들기 를 다루었다.\n응용세션2: leaflet 이용 지도앱 만들기, shiny앱을 지탱하는 엔지니어링 패키지 를 다루었다.\nLT: Shiny로 만드는 사진편집앱, Shiny로 사내용 범용 통계 앱 만들기, 오픈소스 약물동태 시뮬레이션, shiymeta로 재현가능한 R code 생성하기 등을 다루었다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#slide",
    "href": "posts/2020-02-11-tokyor81/index.html#slide",
    "title": "TokyoR 81회 리뷰",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/TokyoR81 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html",
    "href": "posts/2019-11-30-rseleniumtip/index.html",
    "title": "RSelenium 이용 팁",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 1월 Shinykorea 밋업에 참석, RSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "href": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "title": "RSelenium 이용 팁",
    "section": "요약",
    "text": "요약\n웹에 로그인 후 클릭기반 데이터 다운받는 과정을 RSelenium 으로 자동화 하였다.\n\nSelenium docker image 를 이용, 복잡한 설치과정 없이 Selenium 을 실행하고 다운로드 경로를 설정하였다.\nfindElement 와 sendKeysToElement, clickElement 를 이용, 아이디와 비번을 입력하고 로그인버튼을 클릭하였다.\nclickElement 이 안될 때 mouseMoveToLocation 과 click 을 이용, 마우스로 클릭하였다.\n작업 팝업창을 바꾸는 switchToWindow 가 안될 때, queryRD 로 자체 함수를 만들어 작업하였다.\n50개 일별 데이터 다운로드에 성공하였다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "href": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "title": "RSelenium 이용 팁",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RSelenium 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "",
    "text": "김진섭 대표는 11월 14일 서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정 에 강사로 초청, Shiny 기초학습을 위한 워크샵을 진행할 계획입니다. 강의 슬라이드를 포함한 실습 자료를 미리 공유합니다. daattalli 의 NBA 2018/2019 선수 스탯 데이터를 이용한 워크샵 을 그대로 활용했습니다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습 목표",
    "text": "실습 목표\n\nRStudio cloud 를 이용, 클라우드 환경에서 R을 쓸 수 있다.\napp.R 파일에 Shiny의 ui와 server 코드를 입력할 수 있다.\nfluidPage의 sidebarLayout 레이아웃을 이용, 왼쪽에는 UI 옵션, 오른쪽에는 해당되는 결과를 보여줄 수 있다.\nDT 패키지로 데이터를, ggplot2 로 히스토그램을 보여줄 수 있다.\nReactivity 를 이해한다.\nshinyapps.io 에 app 을 배포할 수 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: RStudio cloud\n",
    "text": "실습환경 만들기: RStudio cloud\n\nStep 1: https://rstudio.cloud 회원 가입\nStep 2: https://rstudio.cloud/spaces/30306/join?access_code=s4hEiPXQF%2BjosPclQEzgTtR0mPWDuh7Dhr2O7wAg 들어가서 “Join Space” 클릭\nStep 3: 위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 선택. Repo 주소는 https://github.com/jinseob2kim/shiny-workshop-odsc2019\n\n\nStep 3\n\n모든 강의자료는 RStudio cloud 안에 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: 개인 PC",
    "text": "실습환경 만들기: 개인 PC\nStep 1: 패키지 설치\n\ninstall.packages(c(\"shiny\", \"ggplot2\", \"dplyr\", \"DT\", \"colourpicker\", \"readr\")) \n\nStep 2: https://github.com/jinseob2kim/shiny-workshop-odsc2019 들어간 후\nStep 3: 녹색 “Clone or download” 클릭 후 “Download ZIP” 을 눌러 자료 다운."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/shiny-workshop-odsc2019 를 클릭하면 볼 수 있고, 전체 워크숍 자료는 https://github.com/jinseob2kim/shiny-workshop-odsc2019에서 다운받을 수 있다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html",
    "href": "posts/2019-09-19-nhiswithr/index.html",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "",
    "text": "김진섭 대표는 9월 25일 을지의대 학술원 특강에 참석, R 을 이용하여 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#요약",
    "href": "posts/2019-09-19-nhiswithr/index.html#요약",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "요약",
    "text": "요약\nR 로 심평원, 공단 빅데이터의 데이터 정리와 통계분석을 수행하였다.\n\nhaven 패키지로 SAS 파일을 직접 읽을 수 있다.\ndplyr, data.table 을 활용, 기존 R 문법보다 빠르게 데이터 를 정리할 수 있다.\ndbplyr 를 활용, R 코드를 PROC SQL 문으로 바꿔 복잡한 SAS 작업을 수행할 수 있다.\n자체 개발한 jsmodule 패키지를 이용, GUI 환경에서 기술통계와 회귀/생존분석을 수행하고 테이블과 그림을 바로 다운받는다.\n\njsmodule 의 표본조사 데이터 분석 기능을 활용, GUI 환경에서 국건영 데이터 분석을 수행하였다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#slide",
    "href": "posts/2019-09-19-nhiswithr/index.html#slide",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/NHIS_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "",
    "text": "본 내용은 필자가 준비하던 박사논문 선형모형의 다차원공간으로의 확장 의 후속으로 계획했던 내용으로, 선형모형 벡터공간에 허수축(Imaginary Axis) 을 도입, Inverted U-shape 을 선형관계로 재해석하였습니다. 아인슈타인 일반상대성이론의 4차원 시공간 중, 시간을 허수축으로 해석하는 것에서 아이디어를 얻었습니다. 이전 내용 의 요약은 아래 슬라이드를 참고해 주세요."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#시작하면서",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#시작하면서",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "",
    "text": "본 내용은 필자가 준비하던 박사논문 선형모형의 다차원공간으로의 확장 의 후속으로 계획했던 내용으로, 선형모형 벡터공간에 허수축(Imaginary Axis) 을 도입, Inverted U-shape 을 선형관계로 재해석하였습니다. 아인슈타인 일반상대성이론의 4차원 시공간 중, 시간을 허수축으로 해석하는 것에서 아이디어를 얻었습니다. 이전 내용 의 요약은 아래 슬라이드를 참고해 주세요."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Abstract",
    "text": "Abstract\n이전 연구 에서 저자는 선형모형을 휘어진 다차원공간으로 확장하여 \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제시하였는데, Inverted \\(U\\)-shape은 이 방법으로 표현할 수 없었다. 이에 본 연구에서는 선형모형의 무대를 Imaginary Axis를 포함한 다차원공간까지 확장하여 Inverted \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제안한다. 본 연구를 활용하여 Health science 연구자들이 새로운 관점에서 연구데이터를 해석할 수 있을 것이다.\nkeywords : Multidimension, Linear Model, Vector Space, Metric Tensor"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Introduction",
    "text": "Introduction\n이전 연구에서 선형모형을 다차원 벡터공간으로 확장한 Multidimensional Vectorized Linear Model(MDVLM)을 제안하였는데, 변수들간의 dependency를 적절하게 조절하여 기존 선형모형에서 정확히 표현하기 어려웠던 관계를 표현할 수 있었다(Jinseob Kim 2017). 특히 MDVLM을 통해 점점 느리게 감소하는 혹은 점점 빠르게 증가하는 모양을 표현할 수 있는데, 이는 MDVLM의 표현식 \\(Y^2= \\beta_1^2X_1^2 + 2r\\beta_1\\beta_2X_1X_2+ \\beta_2^2X_2^2 = (\\beta_1X_1+r\\beta_2X_2)^2 + (1-r^2)\\beta_2^2X_2^2\\)에서 \\(X_1\\)과 \\(Y\\)의 관계가 쌍곡선 모양을 보이기 때문이다(Figure 1).\n\n\n\n\nExpressable relationship in conventional linear model and MDVLM\n\n\n\n이제 자연스러운 질문이 생긴다. “MDVLM은 점점 빠르게 감소하는 혹은 느리게 증가하는 관계를 표현할 수 있을까?” \\(Y^2=X_1^2 - X_2^2\\)의 간단한 예를 통해 이를 살펴보도록 하자.\nLinear relationship can’t explained via MDVLM\n\\(Y^2 = X_1^2 - X_2^2\\)의 그래프(\\(X, Y &gt; 0\\))를 살펴보면 \\(Y\\)와 \\(X_2\\)는 타원모양(이 예시에서는 원)으로 감소하는 관계를 보이고 MDVLM으로 잘 표현할 수 없다(Figure 2(a)). MDVLM은 직선 혹은 쌍곡선 모양만 표현할 수 있기 때문이다.\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n즉, \\(Y^2 = X_1^2 - X_1^2\\) 간단한 표현임에도 불구하고, 선형모형을 벡터공간으로 일반화한 MDVLM으로도 잘 표현될 수 없다. 그렇다면 이것은 선형모형이 아닌 것일까? 분명히 점점 빠르게 감소하는 타원 모양은 선형관계가 아니다. 그러나 \\(X_2^2\\)이 1 증가할 때 마다 \\(Y^2\\)가 정확히 1이 감소하는 관계가 있는 것도 사실이다(Figure 2(b)). 이 또한 선형관계라고 할 수 있는 것 아닐까?\n이를 확인해 보기 위해 MDVLM에서와 마찬가지로 다차원 벡터공간에서 \\(Y^2 = X_1^2 - X_2^2\\)을 표현해보자.\n\\[\\vec{Y} = \\vec{X_1} + \\vec{X_2}\\]\n\\(\\vec{X_1}\\)와 \\(\\vec{X_2}\\)가 독립된 axis를 갖고 있다면 \\(Y^2 = \\vec{Y}\\cdot\\vec{Y} = X_1^2 + X_2^2\\)을 얻는다. 한편 \\(\\vec{X_2}\\)가 허수축(imaginar axis)를 갖고 있다고 생각하면 아래와 같이 \\(Y^2 = X_1^2 - X_2^2\\)을 얻는다.\n\\[Y^2 = \\vec{Y}\\cdot\\vec{Y} =  X_1^2 + (iX_2)^2 = X_1^2 - X_2^2\\]\n이 경우에도 \\(X_1\\)이 고정된 상태에서는 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}= d\\vec{X_2}\\)가 성립하며, 방향과 허수축을 고려했을 때 \\(dY\\)와 \\(dX_2\\)은 선형관계가 있다고 할 수 있다.\n허수 \\(i\\)는 실제로 존재하는 것이 아니지만 \\(i^2 = -1\\)임을 이용해서 내적이 음수인 허수축을 상상할 수 있고, 선형관계를 허수축을 포함한 벡터공간으로 확장할 수 있다. 허수축의 활용은 물리학에서 많이 볼 수 있는데, 대표적으로 아인슈타인의 특수상대성이론에서는 time coordinate를 허수축으로 두고 4차원 시공간(Minkowski space)에서의 거리 \\(ds\\)를 아래와 같이 정의한다(Corry 1997).\n\\[(ds)^2 = (dx)^2 + (dy)^2 + (dz)^2 + (idt)^2 = (dx)^2 + (dy)^2 + (dz)^2 - (dt)^2\\]\n일반적으로 \\(p\\)개의 실수축과 \\(q\\)개의 허수축으로 구성된 manifold를 pseudo-Riemannian manifolds라 하고 거리 \\(g\\)는 아래와 같이 정의한다(Kulkarni 1981).\n\\[g = dx_1^2 + \\cdots + dx_p^2 - dx_{p+1}^2 - \\cdots - dx_{p+q}^2\\]\n점점 느리게 증가하는 타원모양도 마찬가지로 MDVLM으로는 표현하기 어려우며 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 원을 예로 들 수 있다(Figure 3(a), 3(b)).\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n제안: MDVLM with Imaginary Axis\n지금까지의 고찰을 토대로 본 연구에서는 허수축을 포함한 다차원 벡터공간에서 선형관계를 표현하는 MDVLM with Imaginary Axis(MDVLM-IA)를 제안한다. 이것은 기존의 MDVLM에 Imaginary Axis의 개념을 추가하여 일반화한 것인데, MDVLM의 개념을 간단하게 리뷰한 후 여기에 허수축을 추가하여 본 연구의 모형과 추정방법을 설명하겠다. 그 후 몇 가지 example을 통해 이것이 어떻게 활용되는지 살펴볼 것이다."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Formula",
    "text": "Formula\n이전에 연구했던 MDVLM에 대해 간단히 리뷰를 한 후, 이것을 확장하여 본 연구의 모형과 추정을 설명하도록 하겠다.\nBrief Review of MDVLM\n음이 아닌 실수 \\(Y\\)와 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\) 들의 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ (\\beta_2X_2+\\beta_{02})\\vec{e_2} + \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p}\\]\n이 때, \\(\\vec{e_i}\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며, \\(X_i\\)와 \\(X_j\\)가 완전히 독립적인 정보라면 \\(\\vec{e_i}\\cdot\\vec{e_j} = 0\\)이고 일반적으로 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)의 값은 0에서 1까지의 값을 갖는다.\n한편 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\beta_2dX_2\\vec{e_2} + \\cdots + \\beta_pdX_p\\vec{e_p}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고, 기존의 선형모형의 해석에 vector의 개념만 추가된 것이다.\n\\(\\beta\\)값들의 추정은 다음과 같은 스칼라식을 토대로 이루어진다.\n\\[\n\\begin{aligned}\nY^2 = (\\vec{Y})\\cdot(\\vec{Y}) &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i}) \\\\\n&=\\sum_{i=1}^p(\\beta_iX_i+\\beta_{i0})^2 + 2\\sum_{i&lt;j}r_{ij}(\\beta_iX_i+\\beta_{i0})(\\beta_jX_j+\\beta_{j0})\\\\\n\\end{aligned}\n\\]\n이제 최소제곱추정을 위한 오차제곱의 합(Sum of Squared Error: SSE)을 다음과 같이 정의하면, 기존 선형모형의 최소제곱추정을 자연스럽게 확장한 것이 된다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i&lt;j}r_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n예를 들어 \\(r_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 optimization technique를 이용하며, Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다Nelder and Mead (1965).\nMDVLM with Imaginary Axis\nMDVLM에서 허수축을 갖는 \\(X_{p+1}\\),\\(\\cdots\\),\\(X_{p+q}\\)를 추가하여 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p} + (\\beta_{p+1}X_{p+1}+\\beta_{0(p+1)})\\vec{e_{p+1}} + \\cdots + (\\beta_{p+q}X_{p+q}+\\beta_{0(p+q)})\\vec{e_{p+q}}\\]\n\\(\\vec{e_1},\\cdots,\\vec{e_p}\\)들은 실수축을 가진 단위벡터로서 자기자신과의 내적값인 \\(\\vec{e_i}\\cdot\\vec{e_i}\\)의 값이 1 이다. 반면 \\(\\vec{e_{p+1}},\\cdots,\\vec{e_{p+q}}\\)는 허수축을 가진 단위벡터로서 자기자신과의 내적값은 -1이다. \\(1\\le i, j \\le p\\)일 때는 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)가 0에서 1까지의 값을 갖으며, \\(p+1\\le i, j \\le p+q\\)라면 -1에서 0까지의 값을 갖고, 그 외에는 \\(r_{ij}=0\\)이다. 즉 Axis들의 dependency는 실수축끼리, 혹은 허수축끼리만 정의한다.\n\\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\cdots + \\beta_pdX_p\\vec{e_p} + \\beta_{p+1}dX_{p+1}\\vec{e_{p+1}} + \\cdots + \\beta_{p+q}dX_{p+q}\\vec{e_{p+q}}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고 이는 MDVLM에서의 해석과 동일하다.\n추정을 위한 스칼라 관계식은 MDVLM때와 비슷하게 아래와 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\nY^2 &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{1\\le i&lt;j \\le p}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{p&lt; i&lt;j\\le p+q}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j}))\\\\\n\\end{aligned}\n\\] \\(Y\\)의 값의 변화량 \\(dY\\)는 \\[\n\\begin{aligned}\n(dY)^2 &= (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i})\\cdot (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 + \\sum_{1 \\le i&lt;j \\le p}2r_{ij}\\beta_i\\beta_jdX_idX_j) - (\\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2 + \\sum_{p &lt; i&lt;j \\le p+q}2r_{ij}\\beta_i\\beta_jdX_idX_j)\\\\\n\\end{aligned}\n\\]\n이고, 모든 \\(r_{ij}\\)들이 0이라면 \\(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 - \\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2\\)로 간단히 표현할 수 있다.\n최소제곱 추정을 위한 \\(SSE(\\beta)\\)값도 비슷하게 정의할 수 있으며 추정방법은 MDVLM의 경우와 같이 optimization technique를 이용한다.\\[\n\\begin{aligned}\nf(\\beta,X_k) &= (\\sum_{i=1}^p(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{1\\le i&lt;j \\le p}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{p&lt; i&lt;j\\le p+q}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) \\\\\nSSE(\\beta) &= \\sum_{k=1}^N (Y_k - \\sqrt{f(\\beta,X_k)})^2\n\\end{aligned}\n\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value, \\(X_k= (X_{k1},\\cdots,X_{k(p+q)})\\))"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Apply to Data",
    "text": "Apply to Data\n앞서 언급했던 \\(Y^2 = X_1^2 - X_2^2\\)과 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 경우의 데이터에 적용해보도록 하겠다. 모든 계산은 R 3.3.3의 optim 함수를 이용하였다.\nExample 1: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\\(Y^2 = X_1^2 - X_2^2\\)인 양수 \\(Y, X_1, X_2\\)의 쌍을 100번 random sampling 해서 MDVLM과 본 연구의 모형을 비교해 보았다.\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - X_2^2\\): Dependency and MSE\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-0.224 + 1.163X_1  -0.634X_2)^2\\)\n\\(Y^2 = X_1^2 -X_2^2\\)\n\n\nMSE\n0.176\n0\n\n\n\n\n\n실제로 \\(Y^2\\)과 \\(X_1^2, X_2^2\\)의 값을 이용해서 선형모형으로 추정하면 정확한 추정 결과를 얻는다. 그러나 제곱한 값이 아닌 원래값을 이용해서 선형모형에 적용하면 \\(Y =\\) \\(-0.224\\) \\(+\\) \\(1.163\\) \\(X_1 +\\) \\(-0.634\\) \\(X_2\\)이 되어 정확한 추정을 얻지 못하고, MDVLM으로 확장해도 이보다 정확한 추정은 얻을 수 없다.\nExample 2: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - (5-X_2)^2\\): Dependency and MSE\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-2.78 + 1.079X_1 + 0.567X_2)^2\\)\n\\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\nMSE\n0.047\n0"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Discussion",
    "text": "Discussion\n예상대로 \\(Y\\)가 빨리 감소하거나 천천히 증가하는 타원모양의 관계는 MDVLM으로 잘 표현할 수 없었으며, 허수축을 활용했을 때 정확히 표현할 수 있었다.\n본 연구가 Health Status를 설명하기 위해 처음으로 허수 \\(i\\)의 개념을 이용하였다는 의의가 있다. 본 연구에서는 \\(i^2 = -1\\)이라는 특징을 이용해서 타원모양을 허수축에서의 선형관계로 재해석하여 MDVLM보다 더 확장된 선형모형을 제시하였는데, 이를 통해 연구자들이 선형관계라는 직관적 해석을 잃지 않으면서 지금보다 훨씬 다양하게 건강현상을 설명할 수 있을 것이라 확신한다.\n허수 \\(i\\)의 또하나의 큰 특징은 복소수 표현을 통해 실수체계를 확장할 수 있다는 것인데, 이것을 적극적으로 활용한 분야 중 하나가 양자역학이다. 양자역학의 대표적인 방정식인 슈뢰딩거 방정식(Schrödinger equation)은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용인데 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특징이며, 복소수가 포함된 파동함수 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현할 수 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 건강상태를 설명하는 새로운 방법이 될 수 있을 것이라 예상한다.\n본 연구를 시작으로 향후 Health science에서 복소수를 활용한 모형이 활발히 제안되길 기대한다."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html",
    "href": "posts/2019-05-13-rmedicalresearch/index.html",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에 참석, 의학연구를 지원하면서 다양하게 R을 활용했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공하는 것을 업으로 삼고 있다.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포한다.\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 통계지원을 맡고 있다.\n심평원/보험공단 빅데이터 연구도 개별적으로 지원중."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/CRScube 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html",
    "href": "posts/2019-04-03-reviewmoneypin/index.html",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "",
    "text": "법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "법인 설립 후 세무/회계 고민",
    "text": "법인 설립 후 세무/회계 고민\n법인을 설립하고 느낀 가장 큰 문제가 세무와 회계였습니다. 세무사에게 맡기자니 매출도 없는데 비용이 부담되었고, 평소 회계에 관심이 많아 이 기회에 회계를 배우고 싶었습니다. 그러던 중 인터넷 검색을 통해 머니핀과 자비스를 알게되어 둘다 이용하기 시작했습니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n머니핀으로 결정한 계기",
    "text": "머니핀으로 결정한 계기\n두 서비스 모두 국세청, 법인카드, 기업통장을 연계하여 자동으로 수입, 지출을 기록할 수 있습니다. 계정과목을 지정하면 바로 재무제표에 반영되어 확인할 수 있고, 이것을 부가세 등 세금을 낼 때 이용할 수 있습니다. 작년 말부터는 머니핀만 사용하기 시작했는데 이유는 다음과 같습니다.\n\n\n앱 하나로 다 됩니다. 회계관리, 부가세/법인세 납부까지 앱에서 해결할 수 있습니다(Figure @ref(fig:screen)).\n\n\n자비스는 영수증 등록만 앱으로 등록하고 나머지 업무들은 전부 웹에서 해야 합니다.\n\n\n\n\n\n\n\n앱 화면(출처: 머니핀 홈페이지)\n\n\n\n\n\n운영진 피드백이 매우 좋습니다. 회계를 처음 겪는 입장에서 큰 도움이 되었습니다(Figure @ref(fig:talk)).\n\n앱 내 메신저를 통해 운영진에게 질문을 자주 하는데, 항상 빠르고 친절하게 설명해 주십니다. 질문을 할 때마다 미안한 마음이 들 정도입니다.\n\n\n\n\n\n\n\n운영진 피드백\n\n\n\n\n\n쌉니다. 작년에는 무료였고 지금은 기본요금제가 월 9,900원입니다(Figure @ref(fig:bill)).\n\n아직 거래가 별로 없는 1인기업이 세무대행을 이용하는 것이 부담이었습니다.\n\n\n\n\n\n\n\n요금제"
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n차라투의 머니핀 이용법",
    "text": "차라투의 머니핀 이용법\n가장 많이 쓰는 기능은 국세청 세금계산서, 법인카드, 통장내역 자동 반영입니다. 거래가 일어난 후 앱 새로고침을 누르면 바로 거래내역이 반영되고, 어떤 회계항목에 해당되는지 메뉴에서 선택할 수 있습니다(Figure @ref(fig:content)). 어느 계정에 넣을지 헷갈릴때는 운영진께 질문하면 친절하게 답변해 주십니다.\n\n\n\n\n거래내역 고르기(출저: 머니핀)\n\n\n\n부득이하게 현금이나 개인카드를 이용한 경우 영수증을 카메라로 찍어 업로드하면 해당 내역이 앱에 반영됩니다. 저의 경우는 법인 설립 전 지출했거나 깜빡 잊고 개인카드를 이용한 내역을 반영하는 데 이용했습니다(Figure @ref(fig:bill2)).\n\n\n\n\n영수증 업로드\n\n\n\n부가세를 낼 때는 그동안의 내역을 바탕으로 바로 신고서와 전자파일을 만든 후, 국세청에 업로드하면 됩니다(유료, Figure @ref(fig:tax)). 지난 3월에는 법인세를 납부했는데 이 또한 머니핀을 통해 쉽게 마무리하였습니다(유료).\n\n\n\n\n18년 부가세 신고\n\n\n\n차라투는 아직 1인기업이라 직원급여와 관련된 기능은 이용하지 못했습니다. 올해 말에는 꼭 같이할 팀원이 생겨서 이 기능도 이용해보고 싶네요."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "아쉬운 점",
    "text": "아쉬운 점\n차라투의 회계는 100% 머니핀에 의존하다보니, 앱에서 작은 문제만 생겨도 큰 어려움을 겪습니다. 다음은 앱을 이용하면서 아쉬웠거나 개선이 필요한 내용입니다.\n\n\n앱 안정성 문제\n\n앱이 갑자기 멈추거나, 홈택스/법인카드 거래내역 반영이 안될 때가 있습니다. 운영진께 문의하면 바로 처리해 주십니다.\n\n\n\n버그, 오류 문제.\n\n앱 안정성보다 이것이 더 문제인데요, 같은 내역이 2번 반영되는 등 기본 숫자가 잘못되는 경우가 있습니다. 이번에 법인세를 내면서 잘못된 숫자들이 있다는 것을 알게 되었고, 운영진의 도움으로 무사히 수정하였습니다.\n\n\n\n회계 항목 추가\n\n일반적인 항목들은 다 있어 별 문제점은 없습니다만, 거래처 경조사같은 몇 가지 세부 항목이 추가되었으면 좋겠습니다. 현재 운영진께 건의드린 내용입니다.\n\n\n\n머니핀도 창업한지 얼마 안된 스타트업인 만큼, 서비스 운영을 하며 여러 시행착오를 겪는 것으로 느껴집니다. 저희가 겪는 시행착오랑 비슷한 경우도 많아 감정이입이 됩니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "마치며",
    "text": "마치며\n창업 후 7개월간 머니핀을 사용한 느낌을 적어 보았습니다. 간단히 3줄 요약하자면 다음과 같습니다.\n\n앱 하나로 모든 세무, 회계를 처리할 수 있다.\n싸다.\n앱이 아직 불안정하다. 그러나 운영진의 피드백이 매우 빠르다.\n\n이제 막 사업을 시작하셨거나, 1인 법인 등 작은 규모의 업체를 운영하는 분께 적극 추천합니다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html",
    "href": "posts/2019-02-06-sccs/index.html",
    "title": "Self-controlled case series",
    "section": "",
    "text": "김진섭 대표는 2월 18일(월) 성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#요약",
    "href": "posts/2019-02-06-sccs/index.html#요약",
    "title": "Self-controlled case series",
    "section": "요약",
    "text": "요약\n\nSelf-controlled methods는 자기 자신을 대조군으로 비교, time-invariant confounders의 영향을 최소화할 수 있다.\nSelf-controlled case series (SCCS), case-crossover (CCO) design, sequence symmetry analysis (SSA)가 대표적이다.\nSCCS는 위험에 노출된 기간과 그렇지 않은 기간의 상대위험도 (RR) 를 구한다.\n한 사람에게 일어나는 각 사건(ex: 노출, 발생, 나이)이 변화할 때 마다 데이터를 만든다 (Long format data).\nMatched case-control study와 유사, Conditional logistic regression으로도 분석할 수 있다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#slide",
    "href": "posts/2019-02-06-sccs/index.html#slide",
    "title": "Self-controlled case series",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureSCCS/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html",
    "href": "posts/2019-01-03-rmarkdown/index.html",
    "title": "R Markdown 기초",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, R Markdown으로 R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 강의할 예정입니다. 강의 내용을 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "title": "R Markdown 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR Markdown은 R 코드와 분석을 포함한 컨텐츠를 만드는 툴이며 크게 3가지 활용법이 있다.\n\n문서(pdf, html, docx): 글쓰기, 분석 결과, 참고문헌 등 논문의 모든 작업을 R Markdown으로 수행한다.\n프리젠테이션(pdf, html, pptx): R 코드나 분석결과가 포함된 프리젠테이션을 만든다. 기본 템플릿1 외에 xaringan2 패키지가 최근 인기를 끌고 있다.\n웹(html): 웹사이트나 블로그를 만든다. blogdown3 이나 distill4 패키지가 대표적이다. 이 글의 블로그도 distill로 만들었으며, 과거 차라투 홈페이지는 blogdown을 이용하였다.\n\n본 강의는 1의 가장 기초에 해당하는 강의로 간단한 문서를 작성하는 것을 목표로 한다. pdf 문서를 만들기 위해서는 추가로 LaTeX 문서작성 프로그램인 Tex Live를 설치해야 하며 본 강의에서는 생략한다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "href": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "title": "R Markdown 기초",
    "section": "Rmd 문서 시작하기",
    "text": "Rmd 문서 시작하기\nR Markdown은 Rmd 파일로 작성되며 rmarkdown5 패키지를 설치한 후, Rstudio에서 File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R markdown… 의 순서로 클릭하여 시작할 수 있다(Figure @ref(fig:rmdfilemenu), @ref(fig:rmdstart)).\n\n\n\n\nRstudio File 메뉴6\n\n\n\n\n\n\n\nR markdown 시작 메뉴7\n\n\n\n문서의 제목과 저자 이름을 적은 후 파일 형태를 아무거나 고르면(나중에도 쉽게 수정 가능) Figure @ref(fig:rmdfile)처럼 확장자가 Rmd인 문서가 생성될 것이다.\n\n\n\n\nR markdown 기본 문서8\n\n\n\n파일 내용을 보면 맨 먼저 제목을 쓰는 부분이 있고 글과 코드를 작성하는 부분도 있다. 일단 이 파일을 문서로 만들어보자. 문서 이름이 있는 바로 아래의 knit 탭을 누르거나, 그 옆의 아래방향 화살표를 누르고 원하는 파일 형태를 클릭하면 된다(Figure @ref(fig:knittab)). 처음에 언급했듯이 pdf는 Tex Live를 설치한 후 이용할 수 있다.\n\n\n\n\nknit 탭9\n\n\n\n다음은 각각 html, pdf, docx로 생성된 문서이다.\n\n\n\n\nhtml 문서10\n\n\n\n\n\n\n\npdf 문서11\n\n\n\n\n\n\n\nword 문서12\n\n\n\n생각보다 간단하지 않은가? 이제 본격적으로 Rmd 파일의 내용을 살펴보면서 어떻게 글과 R 코드를 작성하는지 알아보자. Rmd는 크게 제목을 적는 YAML Header, 글을 쓰는 Markdown Text와 코드를 적는 Code Chunk로 나눌 수 있다(Figure @ref(fig:rmdcontents)).\n\n\n\n\nRmd 파일 구성13"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "href": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "title": "R Markdown 기초",
    "section": "YAML Header",
    "text": "YAML Header\nYAML은 YAML Ain’t Markup Language의 재귀형식의 이름을 갖고 있는 언어로 가독성에 초점을 두고 개발되었다. R Markdown은 Rmd의 시작 부분에 문서 형식을 설정하는 용도로 이 포맷을 이용한다. 다음은 기초 정보만 포함된 YAML이다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2024-05-25\"\noutput: html_document\n---\nKnit 버튼 오른쪽의 설정() \\(\\rightarrow\\) Output Options…를 클릭하여 html, pdf, word 포맷 각각에 대한 기본 설정을 할 수 있다(Figure @ref(fig:outputoption), @ref(fig:outputhtml)).\n\n\n\n\nOutput Options14\n\n\n\n\n\n\n\nHTML Option15\n\n\n\n설정을 마치면 업데이트 된 YAML을 볼 수 있다. 모든 포맷 공통인 설정값은\n\ntoc(yes or no): 목차 포함 여부\n그림의 높이(fig_height) 와 넓이(fig_width): R 코드로 만든 그림에는 해당되지 않는다. Figures 에서 다시 설명하겠다.\n\n이며, 자동으로 현재 날짜를 입력하려면 아래와 같이 `r format(Sys.Date())`를 이용하면 된다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\" \n---\n아래는 필자가 Rmd 문서를 만들 때 흔히 쓰는 YAML 설정이다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\"\noutput:\n  html_document:\n    fig_height: 6\n    fig_width: 10\n    highlight: textmate\n    theme: cosmo\n    toc: yes\n    toc_depth: 3\n    toc_float: yes\n  pdf_document:\n    fig_height: 6\n    fig_width: 10\n    toc: no\n  word_document:\n    fig_height: 6\n    fig_width: 9\n    toc: no\n---\nhtml은 theme16에서 테마, highlight17에서 글씨 강조 스타일을 설정할 수 있으며, toc_float 옵션으로 움직이는 목차를 만들 수 있다(@ref(fig:tocfloat)).\n\n\n\n\ntoc_float- 움직이는 목차18\n\n\n\ndocx는 미리 설정을 마친 docx 문서를 아래와 같이 YAML에 추가하여 템플릿으로 이용할 수 있다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2024-05-25\"\noutput: \n    word_document:\n      reference_docx: mystyles.docx\n---\ndocx에 대한 자세한 내용은 Rstudio 블로그19를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "href": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "title": "R Markdown 기초",
    "section": "Markdown 글쓰기",
    "text": "Markdown 글쓰기\nR Markdown은 이름에서 알 수 있듯이 마크다운(Markdown) 을 기반으로 만들어졌다. 마크다운은 문법이 매우 간단한 것이 특징으로 깃허브의 README.md가 대표적인 마크다운 문서이다. 아래의 [R markdown reference]20에 흔히 쓰는 문법이 정리되어 있다.\n2 가지만 따로 살펴보겠다.\nInline R code\n문장 안에 분석 결과값을 적을 때, 분석이 바뀔 때마다 바뀐 숫자를 직접 수정해야 한다. 그러나 숫자 대신 `r &lt;코드&gt;` 꼴로 R 코드를 넣는다면 재분석시 그 숫자를 자동으로 업데이트 시킬 수 있다.\nThere were  `r nrow(cars)` cars studied\n\nThere were 50 cars studied\n\n수식\nLaTeX 문법을 사용하며 hwp 문서의 수식 편집과 비슷하다. inline 삽입은 $...$, 새로운 줄은 $$...$$ 안에 식을 적으면 된다.\nThis summation expression $\\sum_{i=1}^n X_i$ appears inline.\n\nThis summation expression \\(\\sum_{i=1}^n X_i\\) appears inline.\n\n$$\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n$$\n\\[\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\]\n수식 전반은 LaTeX math and equations21을 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "href": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "title": "R Markdown 기초",
    "section": "R chunk",
    "text": "R chunk\nRmd 문서에서 R 코드가 들어가는 방식은 4가지이다.\n\n몰래 실행. 코드와 결과는 다 숨긴다 - 최초 설정 때 쓰임.\n실행. 코드와 결과를 모두 보여준다.\n실행. 코드는 숨기고 결과만 보여준다.\n실행하지 않음. 코드 보여주기만 한다.\n\n하나씩 살펴보도록 하자.\n최초 설정\n문서를 처음 생성했을 때 최초로 보이는 R 코드는 다음과 같다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\ninclude=FALSE 옵션으로 문서에는 포함시키지 않고 몰래 실행할 수 있으며, 주로 최초 설정에 이용된다. setup은 이 코드에 해당하는 라벨로 생략 가능하다. knitr::opts_chunk$set 에서 디폴트 옵션을 설정할 수 있으며 echo = TRUE는 코드를 보여준다는 뜻이다. 흔히 쓰는 옵션들은 아래와 같다.\n\n\neval=F - 코드를 실행하지 않는다.\n\necho=F - 코드를 보여주지 않는다.\n\ninclude=F - 실행 결과를 보여주지 않는다.\n\nmessage=F - 실행 때 나오는 메세지를 보여주지 않는다.\n\nwarning=F - 실행 때 나오는 경고를 보여주지 않는다.\n\nerror=T - 에러가 있어도 실행하고 에러코드를 보여준다.\n\nfig.height = 7 - 그림 높이, R로 그린 그림에만 해당한다.\n\nfig.width = 7 - 그림 너비, R로 그린 그림에만 해당한다.\n\nfig.align = 'center' - 그림 위치, R로 그린 그림에만 해당한다.\n\n다음은 필자가 논문을 Rmd로 쓸 때 흔히 쓰는 디폴트 옵션이다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=F, fig.align = \"center\", message=F, warning=F, fig.height = 8, cache=T, dpi = 300, dev = \"jpg\")\n```\n전체 옵션은 knitr::opts_chunk$get 함수로 확인할 수 있다.\nknitr::opts_chunk$get()\n\n\n$eval\n[1] TRUE\n\n$echo\n[1] FALSE\n\n$results\n[1] \"markup\"\n\n$tidy\n[1] FALSE\n\n$tidy.opts\nNULL\n\n$collapse\n[1] FALSE\n\n$prompt\n[1] FALSE\n\n$comment\n[1] NA\n\n$highlight\n[1] TRUE\n\n$size\n[1] \"normalsize\"\n\n$background\n[1] \"#F7F7F7\"\n\n$strip.white\n[1] TRUE\n\n$cache\n[1] FALSE\n\n$cache.path\n[1] \"index_cache/html/\"\n\n$cache.vars\nNULL\n\n$cache.lazy\n[1] TRUE\n\n$dependson\nNULL\n\n$autodep\n[1] FALSE\n\n$cache.rebuild\n[1] FALSE\n\n$fig.keep\n[1] \"high\"\n\n$fig.show\n[1] \"asis\"\n\n$fig.align\n[1] \"center\"\n\n$fig.path\n[1] \"index_files/figure-html/\"\n\n$dev\n[1] \"png\"\n\n$dev.args\nNULL\n\n$dpi\n[1] 96\n\n$fig.ext\nNULL\n\n$fig.width\n[1] 7\n\n$fig.height\n[1] 5\n\n$fig.env\n[1] \"figure\"\n\n$fig.cap\nNULL\n\n$fig.scap\nNULL\n\n$fig.lp\n[1] \"fig:\"\n\n$fig.subcap\nNULL\n\n$fig.pos\n[1] \"\"\n\n$out.width\nNULL\n\n$out.height\nNULL\n\n$out.extra\nNULL\n\n$fig.retina\n[1] 2\n\n$external\n[1] TRUE\n\n$sanitize\n[1] FALSE\n\n$interval\n[1] 1\n\n$aniopts\n[1] \"controls,loop\"\n\n$warning\n[1] TRUE\n\n$error\n[1] FALSE\n\n$message\n[1] TRUE\n\n$render\nNULL\n\n$ref.label\nNULL\n\n$child\nNULL\n\n$engine\n[1] \"R\"\n\n$split\n[1] FALSE\n\n$include\n[1] TRUE\n\n$purl\n[1] TRUE\n\n$fig.asp\nNULL\n\n$fenced.echo\n[1] FALSE\n\n$ft.shadow\n[1] FALSE\n\n\nChunk 별 설정\n최초 설정 이후부터는 아래와 같이 간단하게 코드를 보여주거나 실행할 수 있다.\n```{r}\nhead(mtcars)\n```\nhead(mtcars)\n\n\n\n  \n\n\n\n기본 설정과 다른 옵션을 적용하려면 chunk에 옵션을 따로 적으면 된다. 예를 들어 코드는 숨기고 결과만 보여주려면 echo=F 를 추가하면 된다.\n```{r, echo=F}\nhead(mtcars)\n```\n\n\n\n  \n\n\n\n반대로 실행은 안하고 코드만 보여주려면 eval=F를 추가하면 된다.\n```{r, eval=F}\nhead(mtcars)\n```\nhead(mtcars)"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#figures",
    "href": "posts/2019-01-03-rmarkdown/index.html#figures",
    "title": "R Markdown 기초",
    "section": "Figures",
    "text": "Figures\nRmd 문서에 그림이 들어가는 방법은 2가지가 있다.\n\nR 코드로 생성 : plot 함수, ggplot2 패키지 등\n외부 그림 삽입\n\n앞서도 언급했듯이 주의할 점은 그림이 만들어지는 방법에 따라 서로 다른 옵션이 적용된다는 것이다. 일단 전자부터 살펴보자.\nFigures with R\n\nR 코드에서 자체적으로 만든 그림은 전부 chunk 옵션의 지배를 받아 간단하다.\n```{r, fig.cap = \"scatterplot: cars\", fig.width = 8, fig.height = 6}\nplot(cars, pch = 18)\n```\nplot(cars, pch = 18)\n\n\n\n\nscatterplot - cars\n\n\n\nExternal figures\n외부 그림은 R 코드로도 삽입할 수 있고 마크다운 문법을 쓸 수도 있는데, 어떤 방법을 쓰느냐에 따라 다른 옵션을 적용받는다는 것을 주의해야 한다. R에서는 knitr::include_graphics 함수를 이용하여 그림을 넣을 수 있고 이 때는 chunk 내부의 옵션이 적용된다.\n```{r, fig.cap = \"tidyverse logo\", fig.align = \"center\"}\nlibrary(knitr)\ninclude_graphics(\"https://www.tidyverse.org/images/tidyverse-default.png\")\n```\n\n\n\n\ntidyverse logo\n\n\n\n같은 그림을 chunk없이 바로 마크다운에서 삽입할 수도 있다. 이 때는 YAML의 옵션이 적용된다.\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n\n\ntidyverse logo\n\n{ width=50% } 는 그림의 크기를 조절하는 옵션이며 R chunk에서도 같은 옵션 out.width=\"50%\"이 있다. 위치를 가운데로 조절하려면 &lt;center&gt;...&lt;/center&gt; 를 포함시키자.\n&lt;center&gt;\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n&lt;/center&gt;\n\n\n\ntidyverse logo\n\n\n개인적으로는 외부 이미지도 chunk 내부에서 읽는 것을 추천한다. chunk 내부의 옵션들이 마크다운의 그것보다 훨씬 체계적이고 쉬운 느낌이다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#tables",
    "href": "posts/2019-01-03-rmarkdown/index.html#tables",
    "title": "R Markdown 기초",
    "section": "Tables",
    "text": "Tables\n논문을 쓸 때 가장 귀찮은 부분 중 하나가 분석 결과를 테이블로 만드는 것으로, knitr::kable() 함수를 쓰면 문서 형태에 상관없이 Rmd에서 바로 테이블을 만들 수 있다. 아래는 데이터를 살펴보는 가장 간단한 예시이다.\n```{r}\nkable(iris[1:5, ], caption = \"A caption\")\n```\n\n\n\nA caption\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\nepiDisplay 패키지의 regress.display, logistic.display 함수를 활용하면 회귀분석의 결과를 바로 테이블로 나타낼 수 있다(Table @ref(tab:regtable)).\n```{r}\nmtcars$vs &lt;- as.factor(mtcars$vs)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nmodel &lt;- glm(mpg ~ disp + vs + cyl, data = mtcars)\nmodel.display &lt;- epiDisplay::regress.display(model, crude = T, crude.p.value = T)\nmodel.table &lt;- model.display$table[rownames(model.display$table)!=\"\", ]\nkable(model.table, caption = model.display$first.line)\n```\n\n\n\nLinear regression predicting mpg\n\n\n\n\n\n\n\n\n\n\ncrude coeff.(95%CI)\ncrude P value\nadj. coeff.(95%CI)\nP(t-test)\nP(F-test)\n\n\n\ndisp (cont. var.)\n-0.04 (-0.05,-0.03)\n&lt; 0.001\n-0.03 (-0.05,0)\n0.019\n&lt; 0.001\n\n\nvs: 1 vs 0\n7.94 (4.6,11.28)\n&lt; 0.001\n0.04 (-3.81,3.89)\n0.984\n0.334\n\n\ncyl: ref.=4\n\n\n\n\n0.041\n\n\n6\n-6.92 (-10.11,-3.73)\n&lt; 0.001\n-4.77 (-8.56,-0.98)\n0.016\n\n\n\n8\n-11.56 (-14.22,-8.91)\n&lt; 0.001\n-4.75 (-12.19,2.7)\n0.202\n\n\n\n\n\n\n테이블을 좀 더 다듬으려면 kableExtra 패키지가 필요하며, 자세한 내용은 cran 설명서22를 참고하기 바란다. html 문서의 경우 kable()외에도 다양한 함수들을 이용할 수 있는데, 대표적인 것이 rmarkdown::paged_table() 함수와 DT 패키지이다. 전자는 아래와 같이 YAML에서 테이블 보기의 기본 옵션으로 설정할 수도 있다.\n---\ntitle: \"Motor Trend Car Road Tests\"\noutput:\n  html_document:\n    df_print: paged\n---\nDT 패키지에 대한 설명은 Rstudio DT 홈페이지23를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "href": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "title": "R Markdown 기초",
    "section": "마치며",
    "text": "마치며\n본 강의를 통해 R Markdown으로 기본적인 문서를 만드는 법을 알아보았다. 본 강의에서는 시간 관계상 참고문헌 다는 법을 언급하지 않았는데 궁금하다면 Bibliographies and Citations24을 참고하자. 이 내용까지 숙지한다면 R Markdown으로 논문을 쓸 준비가 된 것이다. R Markdown에 대한 전반적인 내용은 아래의 R Markdown Cheet Sheet25에 잘 요약되어 있으니 그때그떄 확인하면 좋다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#footnotes",
    "href": "posts/2019-01-03-rmarkdown/index.html#footnotes",
    "title": "R Markdown 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://rmarkdown.rstudio.com/lesson-11.html↩︎\nhttps://github.com/yihui/xaringan↩︎\nhttps://github.com/rstudio/blogdown↩︎\nhttps://rstudio.github.io/distill/↩︎\nhttps://github.com/rstudio/rmarkdown↩︎\nhttps://rachaellappan.github.io/rmarkdown/↩︎\nhttps://rachaellappan.github.io/rmarkdown/↩︎\nhttps://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T5-RMarkdown/Images/New_Markdown_v2.png↩︎\nhttps://rstudioblog.files.wordpress.com/2014/06/r-markdown-formats.png↩︎\nhttps://stackoverflow.com/questions/47317229/rmarkdown-knit-pdf-to-look-exactly-like-html↩︎\nhttps://stackoverflow.com/questions/47317229/rmarkdown-knit-pdf-to-look-exactly-like-html↩︎\nhttps://rmarkdown.rstudio.com/articles_docx.html↩︎\nhttps://rfriend.tistory.com/311↩︎\nhttps://richardlent.github.io/post/rstudio-as-a-research-and-writing-platform/↩︎\nhttps://stackoverflow.com/questions/24934781/rstudio-knitr-themes↩︎\nhttp://www.datadreaming.org/post/r-markdown-theme-gallery/↩︎\nhttps://eranraviv.com/syntax-highlighting-style-in-rmarkdown/↩︎\nhttps://rfriend.tistory.com/311↩︎\nhttps://rmarkdown.rstudio.com/articles_docx.html↩︎\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf↩︎\nhttps://www.latex-tutorial.com/tutorials/amsmath/↩︎\nhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html↩︎\nhttps://rstudio.github.io/DT/↩︎\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html↩︎\nhttps://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf↩︎"
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html",
    "href": "posts/2018-11-24-basic-biostatistics/index.html",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "",
    "text": "김진섭 대표는 11월 28일(수) 중앙보훈병원 정신건강의학과를 방문, 의학 연구에 필요한 기술 통계(descriptive statistics)에 대해 강의하고 자체 제작한 웹 애플리케이션과 Rstudio Addins를 이용하여 실습을 진행하였습니다. 강의 내용을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n의학연구에서 R 활용 능력을 대략 5단계로 구분할 수 있다.\n\n데이터 정리는 미리 excel로 완료, 통계분석만 R 이용.\nR로 데이터 정리와 통계분석을 모두 수행.\nR로 그림을 그린다.\n논문에 들어갈 Table과 figure를 모두 R로 만든다.\n글쓰기, 참고문헌 등 논문의 모든 작업을 R에서 직접 수행한다.\n\n본 강의는 1단계에 해당하는 연구자가 R을 쉽게 이용할 수 있도록 돕는 내용에 해당하며 R과 Rstudio의 설치과정은 생략한다. 혹시 설치를 못했다면 https://www.r-project.org 와 https://www.rstudio.com/products/rstudio/download 를 참조하여 설치하길 바란다. R의 전반적인 도움말은 help.start() 명령어를 활용하고 특정 함수를 보려면 help(which) 형태로 실행하면 된다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "의학 연구에서의 기술 통계",
    "text": "의학 연구에서의 기술 통계\n기술 통계는 원래 평균(mean), 중위수(median), 분산(variance), 빈도표(frequency table)등의 데이터를 설명하는 숫자들이나 히스토그램(histogram), 상자그림(box-plot)같은 그래프를 의미한다. 그러나 대부분의 의학 연구에서는 단순한 기술 통계가 아닌 그것들의 그룹 비교(ex: 성별, 질환 유무)가 Table 1에 기술 통계란 제목으로 제시된다.\n\n\n\n\nTable 1 example(Balaji 2011)\n\n\n\n보통 연구의 흐름은 기술 통계로 데이터를 보여주고 단변량(univariate) 분석을 통해 가설 검정을 수행한 후, 다변량(multivariate) 혹은 소그룹(subgroup) 분석 을 이용하여 다른 변수들의 효과를 보정한 결과를 보여주는 것으로 이루어진다. 그러나 단변량 분석에서 끝나는 간단한 연구도 많고 이것은 본질적으로 기술 통계의 그룹 비교와 같으므로, Table 1에 필요한 통계를 알고 쉽게 구현할 수 있다면 그것만으로 간단한 의학 연구를 수행할 수 있다.\n본 강의에서는 Table 1의 그룹 비교에 필요한 통계 방법들을 알아보고 R을 이용해서 실제 분석을 수행할 것이다. 통계 방법을 선택하는 기준은 크게 (1) 연속 변수(continuous variable) vs 범주형 변수(categorical variable), (2) 비교할 그룹 수, (3) 샘플 수 혹은 정규분포 여부 의 3가지가 있으며 추가로 짝지은 그룹인 경우를 살펴보겠다.\n마지막으로 자체 개발한 웹 애플리케이션과 Rstudio Addins을 사용하여 간단히 Table 1을 만들어 볼 것이다(Figure @ref(fig:appgif), @ref(fig:addingif))."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "연속 변수의 그룹 비교",
    "text": "연속 변수의 그룹 비교\n연속 변수의 그룹 비교는 2 그룹일 때는 t-test, 3 그룹 이상이면 ANOVA라고 생각하면 되며, 2 그룹일 때 ANOVA 결과는 t-test 결과와 거의(?) 같다. 따라서 연속 변수는 무조건 ANOVA라고 생각해도 대충 맞다.\nT-test\nT-test는 2 그룹의 평균값을 비교하는 통계 방법1으로 필요한 숫자는 각 그룹의 평균과 분산이다. 실제로 데이터 없이 두 그룹의 평균과 분산만 있어도 t-test를 수행할 수 있으며 https://www.evanmiller.org/ab-testing/t-test.html 를 통해 웹에서도 간단히 계산할 수 있다. 아래 남녀의 총 콜레스테롤 데이터를 이용해 t-test를 수행해 보자.\n\n\n\n\n\n\n이제 t.test 함수를 이용하여 남녀의 총 콜레스테롤 수치를 비교한다.\n\nnev.ttest &lt;- t.test(tChol ~ sex, data = data.t, var.equal = F)\nnev.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  tChol by sex\nt = -1.9211, df = 27.992, p-value = 0.06496\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -48.410207   1.553065\nsample estimates:\nmean in group Female   mean in group Male \n            134.5714             158.0000 \n\n\n여자의 평균 콜레스테롤 값은 134.6, 남자는 158 이고 \\(p\\)-value는 0.065임을 확인할 수 있다.\n위의 t.test 함수의 옵션 중 var.equal = F는 등분산 가정 없이 분석하겠다는 뜻으로 옵션을 적지 않아도 기본적으로 F가 적용된다. 등분산 가정이란 두 그룹의 분산이 같다고 가정하는 것인데, 계산이 좀 더 쉽다는 이점이 있으나 아무 근거 없이 분산이 같다고 가정하는 것은 위험한 가정이다. 위의 분석에 var.equal = T를 적용해보자.\n\nev.ttest &lt;- t.test(tChol ~ sex, data = data.t, var.equal = T)\nev.ttest\n\n\n    Two Sample t-test\n\ndata:  tChol by sex\nt = -1.9007, df = 28, p-value = 0.06767\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -48.677187   1.820044\nsample estimates:\nmean in group Female   mean in group Male \n            134.5714             158.0000 \n\n\n앞서는 Welch t-test였는데 이름이 바뀐 것을 확인할 수 있고 \\(p\\)-value도 0.068로 아까와 다르다. 논문 리뷰어가 요구하는 등의 특별한 경우가 아니고서야 위험한 등분산가정을 할 필요가 없고, 이제부터 var.equal 옵션은 신경쓰지 않아도 좋다.\n\nANOVA2\n\n3 그룹 이상일 때는 2그룹씩 짝을 지어서 t-test를 여러 번 수행할 수 있다. 그러나 Table 1에서는 대부분 하나의 \\(p\\)-value만 제시하고 이것은 전체적으로 튀는 것이 하나라도 있는가?를 테스트하는 ANOVA를 이용한다.3 ANOVA는 비교할 모든 그룹에서 분산이 같다는 등분산 가정 하에 분석을 수행하며, 실제로 2 그룹일 때 ANOVA를 수행하면 등분산 가정 하에 수행한 t-test와 동일한 결과를 얻는다. 위에도 언급했듯이 모든 그룹에서 분산이 같다는 것은 너무 위험한 가정이나, 3 그룹 이상인 경우 마땅한 대안이 없고 Table 1에서 엄밀한 통계를 요구하는 것도 아니기 때문에 그냥 ANOVA를 쓰는 것이 관행이다. 아래 세 그룹의 총 콜레스테롤 데이터를 활용해 분석을 수행해 보자.\n\n\n\n\n\n\n이제 aov함수를 이용하여 3 그룹의 평균 콜레스테롤 값을 한번에 비교한다.\n\nres.aov &lt;- aov(tChol ~ group, data = data.aov)\nsummary(res.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup        2   1546   773.0    0.78  0.468\nResiduals   27  26751   990.8               \n\n\n결과에서 나온 \\(p\\)-value인 0.468가 Table 1에 이용되며 의미는 “3 그룹에서 총콜레스테롤 값이 비슷하다(다른 것이 있다고 할 수 없다)” 이다.\n비모수 통계: 정규분포 아닐 때\nT-test나 ANOVA는 모두 변수가 정규분포를 이룬다고 가정하고 분석을 수행하는데, 언뜻 생각하기에 의학에서 정규분포가 아닌 변수는 없을 것 같지만4 일부 지표(ex: CRP, 자녀 수)들은 정규분포를 따르지 않는다고 알려져 있다. 이 때는 변수의 값 자체가 아닌 순위 정보만을 이용하는 비모수 검정을 이용한다. T-test에 대응되는 비모수 분석은 Wilcoxon rank-sum test 혹은 Mann–Whitney U test 로 불리며 앞서 이용한 남녀별 총 콜레스테롤 데이터로 분석을 수행하면 아래와 같다.\n\nres.wilcox &lt;- wilcox.test(tChol ~ sex, data = data.t)\nres.wilcox\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  tChol by sex\nW = 72.5, p-value = 0.1046\nalternative hypothesis: true location shift is not equal to 0\n\n\n위 결과에서 \\(p\\)-value는 0.105임을 확인할 수 있다. ANOVA에 대응되는 비모수 분석은 Kruskal–Wallis one-way ANOVA이며 역시 앞서 이용한 그룹별 총 콜레스테롤 데이터에 적용하면 아래와 같다.\n\nres.kruskal &lt;- kruskal.test(tChol ~ group, data = data.aov)\nres.kruskal\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tChol by group\nKruskal-Wallis chi-squared = 2.0192, df = 2, p-value = 0.3644\n\n\n마찬가지로 \\(p\\)-value는 0.364임을 확인할 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "범주형 변수의 그룹 비교",
    "text": "범주형 변수의 그룹 비교\n범주형 변수의 그룹 비교는 그룹 수나 정규분포를 고려할 필요가 없어 연속 변수일 때보다 훨씬 간단하며 딱 하나, 샘플 수가 충분한지만 확인하면 된다.\n샘플 수 충분: Chi-square test\nChi-square test는 두 범주형 변수가 관계가 있는지 없는지를 파악하는 테스트로5 아래의 혈압약과 당뇨약 복용 여부를 조사한 데이터로 분석을 수행해 보겠다.\n\n\n\n\n\n\n두 약물 복용 여부를 테이블로 나타내면\n\ntb.chi &lt;- table(data.chi)\ntb.chi\n\n        DM_medi\nHTN_medi  0  1\n       0 15 13\n       1 14  8\n\n\n이며 언뜻 봐서는 관계가 있는지 아닌지 잘 모르겠다. 이제 chisq.test함수를 이용해서 Chi-square test를 수행하자.\n\nres.chi &lt;- chisq.test(tb.chi)\nres.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.chi\nX-squared = 0.18246, df = 1, p-value = 0.6693\n\n\n\\(p\\)-value는 0.669가 나오고 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 말할 수 있다.\n샘플 수 부족: Fisher’s exact test\n이번엔 다른 사람들의 혈압, 당뇨약 복용 데이터로 chi-square test를 수행해 보겠다.\n\n\n\n\n\n\n아까와 마찬가지로 테이블로 두 약물 복용상태를 비교하면 아래와 같다.\n\ntb.fisher &lt;- table(data.fisher)\ntb.fisher\n\n        DM_medi\nHTN_medi  0  1\n       0 31  8\n       1  9  2\n\n\n혈압약과 당뇨약을 모두 복용한 사람이 2명으로 좀 작아보이지만 무시하고 chi-square test를 수행하면 결과는 나오나 Warning 메시지가 뜬다.\n\nchisq.test(tb.fisher)\n\nWarning in chisq.test(tb.fisher): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.fisher\nX-squared = 4.5971e-31, df = 1, p-value = 1\n\n\n이는 두 약을 모두 복용한 사람이 2명뿐이라서 일어나는 문제로, 일반적으로 분석할 테이블에서 샘플 수가 너무 작은 항이 있으면 chi-square test의 계산이 부정확해진다. 이 때는 fisher’s exact test를 수행하며 아래와 같이 fisher.test함수를 이용하면 된다.\n\nres.fisher &lt;- fisher.test(tb.fisher)\nres.fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb.fisher\np-value = 1\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07627205 5.55561549\nsample estimates:\nodds ratio \n 0.8636115 \n\n\n\\(p\\)-value는 1로 확인되고 마찬가지로 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 할 수 있다.\n여기서 의문점이 들 수 있다. 무조건 fisher’s test만 하면 간단한데 도대체 chi-square test는 왜 하는 것일까? 샘플 수가 작을 때는 fisher’s test만 하는 것이 실제로 더 간단하고 방법론적으로도 아무 문제가 없다. 그러나 샘플 수나 그룹 수가 늘어날수록 fisher’s test는 필요한 계산량이 급격하게 증가하는 문제가 있어 chi-square test를 먼저 수행하는 것을 권유한다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "추가: 짝지은 2 그룹",
    "text": "추가: 짝지은 2 그룹\n각 사람의 혈압을 한 번은 사람이 직접, 한 번은 자동혈압계로 측정했다고 하자. 이 때 직접 잰 혈압과 자동혈압계의 측정값을 비교한다면 t-test로 충분할까? t-test는 혈압 재는 방법마다 평균을 먼저 구한 후 그것이 같은지를 테스트하므로 짝지은 정보를 활용하지 못한다. 이 때는 각 사람마다 두 혈압값의 차이를 먼저 구한 후 평균이 0인지를 테스트하면, 짝지은 정보를 활용하면서 계산도 더 간단한 방법이 된다.\n연속변수: Paired t-test\n위에 언급한 대로 각 사람마다 차이값을 먼저 구한 후 그 평균이 0인지를 테스트하는 방법이 paired t-test이다. 아래의 수축기 혈압 데이터를 통해 t-test와의 차이점을 알아보자.\n\ndata.pt &lt;- data.frame(SBP_hand = round(rnorm(30, mean = 125, sd = 5)), SBP_machine = round(rnorm(30, mean = 125, sd = 5)))\nrownames(data.pt) &lt;- paste(\"person\", 1:30)\ndatatable(data.pt, rownames = T, caption = \"data.pt: systolic blood pressure measured by hand & machine\")\n\n\n\n\n\n위 데이터는 30명의 사람이 앞서 말한 두 가지 방법으로 수축기 혈압을 측정한 데이터이다. 먼저 t-test를 수행하자.\n\npt.ttest &lt;- t.test(data.pt$SBP_hand, data.pt$SBP_machine)\npt.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.45768, df = 57.863, p-value = 0.6489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.224307  2.024307\nsample estimates:\nmean of x mean of y \n    125.0     125.6 \n\n\n위 결과를 보면 각 방법의 평균을 먼저 구한 후 그것을 비교한 것을 확인할 수 있고 \\(p\\)-value는 0.649이다. 이제 paired t-test를 수행하자.\n\npt.ttest.pair &lt;- t.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\npt.ttest.pair\n\n\n    Paired t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.46171, df = 29, p-value = 0.6477\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.257804  2.057804\nsample estimates:\nmean difference \n           -0.6 \n\n\n이번에는 사람마다 차이값을 먼저 구한 후 그것이 0인지 테스트 한 것을 확인할 수 있고 \\(p\\)-value는 0.648이다.\nPaired t-test의 비모수버전은 wilcoxon-signed rank test 이며 아래와 같이 실행한다.\n\npt.wilcox.pair &lt;- wilcox.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with zeroes\n\npt.wilcox.pair\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nV = 214, p-value = 0.9482\nalternative hypothesis: true location shift is not equal to 0\n\n\n본 강의에서는 다루지 않겠지만 짝지은 3개 이상의 그룹은 repeated measure ANOVA6라는 방법을 이용한다.\n범주형 변수: Mcnemar test, Symmetry test for a paired contingency table\n이번에는 측정값이 0,1과 같은 범주형 변수인 경우를 살펴보자. 아래 데이터를 활용해 약 복용 전후로 복통증상 발생에 차이가 있는지 알아본다고 하자.\n\n\n\n\n\n\n이 데이터를 2 by 2 테이블로 정리하면 아래와 같다.\n\ntable.mc &lt;- table(data.mc)\ntable.mc\n\n           Pain_after\nPain_before 0 1\n          0 8 8\n          1 9 5\n\n\n먼저 앞서 배운 Chi-sqaure test 를 이용한 결과를 보자.\n\nmc.chi &lt;- chisq.test(table.mc)\nmc.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table.mc\nX-squared = 0.17514, df = 1, p-value = 0.6756\n\n\n이것은 약 복용 전 복통 증상과 복용 후의 복통 증상이 얼마나 관계가 있는지 알아보는 테스트로 짝지은 정보를 활용하지 않는다. 이번엔 짝지은 정보를 활용하는 mcnemar test를 수행하자.\n\nmc.mcnemar &lt;- mcnemar.test(table.mc)\nmc.mcnemar\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  table.mc\nMcNemar's chi-squared = 0, df = 1, p-value = 1\n\n\nMcnemar test는 약 복용후 증상발생이 달라진 사람 즉, discordant pair만 분석에 이용한다. 따라서 condordant pair 의 구성과 어떻더라도 통계결과는 동일하게 나온다.\n한편 측정값이 3개 이상일 때는 chi-square test는 그대로 이용할 수 있으나, mcnemar test는 그대로 쓰지 못하고 symmetry test for a paired contingency table7라는 일반화된 테스트를 사용한다. 본 강의에서는 간단한 실행법만 살펴볼 것이며 먼저 rcompanion R package를 설치하자.\n\n## For symmmetry test\n#install.packages(\"rcompanion\")\nlibrary(rcompanion)\n\n예제 테이블(3 \\(\\times\\) 3)을 아래와 같이 불러온 후\n\n## Example data\ndata(AndersonRainGarden) \nAndersonRainGarden       \n\n             Yes.after No.after Maybe.after\nYes.before           6        0           1\nNo.before            5        3           7\nMaybe.before        11        1          12\n\n\nnominalSymmetryTest 함수로 분석을 수행한다.\n\n## Symmetry test\nnominalSymmetryTest(AndersonRainGarden)\n\n$Global.test.for.symmetry\n  Dimensions  p.value\n1      3 x 3 0.000476\n\n$Pairwise.symmetry.tests\n                                       Comparison p.value p.adjust\n1       Yes.before/Yes.after : No.before/No.after  0.0736   0.0771\n2 Yes.before/Yes.after : Maybe.before/Maybe.after 0.00937   0.0281\n3   No.before/No.after : Maybe.before/Maybe.after  0.0771   0.0771\n\n$p.adjustment\n  Method\n1    fdr\n\n$statistical.method\n        Method\n1 McNemar test"
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "실습",
    "text": "실습\n웹 애플리케이션\nAnpanman 에서 만든 기초통계 앱을 소개한다(Figure @ref(fig:appgif)). 5메가 이하의 excel, csv 형태 혹은 sas, spss 프로그램으로 만든 데이터를 업로드하면 Table 1과 회귀분석, 로지스틱 회귀분석을 간단하게 수행하고 결과를 excel로 바로 다운받을 수 있다.\n\n\n\n\nApplication mady by Anpanman\n\n\n\nRstudio Addins\n5메가보다 큰 데이터는 R에서 데이터를 읽은 후, 자체적으로 만든 jsmodule R package를 설치하여 앱을 이용할 수 있다.\n\n## For private package install \ninstall.packages(\"devtools\")   \ndevtools::install_github(c(\"jinseob2kim/jstable\", \"jinseob2kim/jsmodule\")) \n\n패키지를 설치한 후 Rstudio 프로그램의 Addins 탭을 누르면 Basic statistics 항목이 보일 것이다. 데이터를 읽고 그것의 이름을 드래그 한 상태로 Basic statistics 를 누르면 된다(Figure @ref(fig:addingif)).\n\n\n\n\nRstudo Addins made by Anpanman8\n\n\n\n직접 R에서 코드를 실행하고 싶은 유저는 tableone R package를 참고하기 바란다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "마치며",
    "text": "마치며\n지금까지 의학 연구에서 쓰이는 그룹 비교 통계들을 알아보고 웹 앱과 Rstudio Addins 을 이용하여 직접 Table 1을 만들어보았다. 앞으로 연구자들은 어려운 통계 프로그램을 이용할 필요 없이 Anpanman의 서비스를 활용, 빠르게 통계 분석을 수행하고 테이블과 그림을 바로 다운받을 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#footnotes",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#footnotes",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n1 그룹의 평균값을 특정 숫자와 비교할 수도 있다.↩︎\n본 강의에서는 One-way ANOVA만 다룬다.↩︎\n사후(post-hoc) 분석을 이용, 어떤 것이 튀는지를 알아볼 수도 있다.↩︎\n정규분포에 대한 내용은 https://jinseob2kim.github.io/Normal_distribution.html 를 참고하기 바란다.↩︎\n세 범주형 변수일 때도 이용할 수 있으나 본 강의에서는 생략한다.↩︎\nhttps://statistics.laerd.com/statistical-guides/repeated-measures-anova-statistical-guide.php↩︎\nhttp://rcompanion.org/handbook/H_05.html↩︎\nhttps://github.com/jinseob2kim/jsmodule↩︎"
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "title": "Redefine Null Hypothesis",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "title": "Redefine Null Hypothesis",
    "section": "Abstract",
    "text": "Abstract\n통계적 가설검정에서 광범위하게 이용되는 \\(p\\)-value는 샘플 숫자만 늘리면, 아무리 작은 차이라도 유의미한 결과로 해석되는 문제가 있다. 이것은 대부분의 연구에서 차이 또는 효과가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 발생하는 문제이며, 실제 차이가 0.0000001이라도 정확히 0만 아니라면 샘플 수만 늘려도 결국 \\(p &lt;0.05\\)인 유의한 결과를 얻을 수 있다. 그러나 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며, 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못한다. 이에 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였고, 이 방법에 따른 새로운 \\(p\\)-value가 기존의 \\(p\\)-value와 effect size를 종합한 지표로서 지나치게 작은 차이를 유의한 결과로 해석하는 문제를 해결할 수 있음을 보였다. 한편 다중비교에서 검정할 가설의 갯수가 증가하는 것을 각 가설의 uncertainty가 증가하는 것으로 재해석할 수 있었고, 이 접근법이 기존의 family-wise error rate(FWER), false discovery rate(FDR) control이 지나치게 큰 sample size를 필요로 하는 단점을 보완할 수 있었다. 이 새로운 가설검정방법이 기존 가설검정의 단점을 극복한 새로운 표준이 될 것으로 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "title": "Redefine Null Hypothesis",
    "section": "Introduction",
    "text": "Introduction\n통계적 가설검정은 물리학, 생물학, 의학 등의 자연과학뿐만 아니라 경제학, 심리학 등의 사회과학에서도 어떤 주장을 하기위한 필수적인 도구로 이용되며 핵심 개념은 가설검정(hypothesis test)이다(Anderson, Burnham, and Thompson 2000). R.A Fisher에 의해 처음으로 사용된 귀무가설(null hypothesis)은 반박하려는 가설로 이용되며, 이 가설 하에서 일어날 확률이 낮은 사건을 제시함으로서 이것이 틀렸음을 설명하는데, 이는 수학의 증명법 중 하나인 귀류법과 비슷하다(Yates 1964). 여기서 일어날 확률이 낮은 사건을 정량적으로 표현하는 것이 \\(p\\)-value이며 흔히 기준이 되는 \\(p &lt;0.05\\)는 연구의 결과보다 더 극단적인 사건이 발생할 확률이 5%미만임을 의미한다(Wasserstein and Lazar 2016).\n\\(p &lt;0.05\\)는 지금까지 과학 연구에서 새로운 발견의 절대적인 기준으로 이용되었는데, 이에 대한 비판도 꾸준히 제시되어 왔으며 최근에는 유의성의 기준을 0.05에서 0.005로 바꿔야한다는 주장까지 나왔다(JA 2018; Anderson, Burnham, and Thompson 2000; Benjamin et al. 2018; Wasserstein and Lazar 2016). 그러나 \\(p\\)-value의 개념은 그대로 두고 유의수준만 낮추는 것은 지금부터 언급할 가설검정법의 근본적인 문제를 해결하지 못한 임시방편에 불과하다.\n본 연구자들은 비현실적인 귀무가설을 가설검정법의 근본적인 문제로 판단한다. 철학자 쇼펜하우어는 논쟁에서 이기는 38가지 비법이라는 책에서 상대방의 주장을 단순화하라고 주장했는데, 가설검정에서도 귀무가설은 명확하게 제시하는 것이 원칙이며(예: \\(\\mu =0\\)), 가설이 명확해야 그것을 반박하기도 쉽다(Goffey 2005). 그러나 명확한 귀무가설은 현실에서는 존재하지 않는다. 예를 들어 어떤 값이 정확히 0이라고 주장하는 사람이 있을까? 대부분은 어느 정도 uncertainty를 마음속에 갖고 있으며 실제 측정값이 0.0000001이기 때문에 값이 0이라는 가설은 틀렸다고 주장한다면 설득력을 얻기 어렵다. 그러나 현재의 가설검정방법은 uncertainty가 전혀 없는 명확한 귀무가설을 사용하기 때문에 실제 측정값과 가정의 차이가 아무리 작아도 샘플수만 커지면 결국은 \\(p &lt; 0.05\\)인 결과를 얻을 수 있고, 이는 유의수준을 아무리 올려도 마찬가지이다. 예를 들어 Two-sided \\(z\\)-test의 검정통계량은 \\(z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\) 꼴인데 \\(\\bar{X}-\\mu\\)가 0만 아니라면 \\(n\\)을 늘려서 \\(p\\)값을 무한히 0에 가깝게 만들 수 있다. 이에 \\(p\\)-value와 별개로 실제 차이값도 살펴봐야 한다는 주장이 제기되었는데, 이 차이를 표준화한 개념이 effect size로 원래도 required sample size를 계산할 때 이용하는 지표이며, 흔히 차이를 표준편차로 나눈 값을 사용한다(Sullivan and Feinn 2012). 이처럼 현실적인 귀무가설을 설정하는 것은 그 자체로 중요한 것 뿐만 아니라 effect size를 고려하는 것과도 연결되어 있지만 이것을 개념화한 이론이 없어 실제 연구설계에 반영되기 어렵다.\n이에 저자는 uncertainty를 포함한 귀무가설과 이것을 이용한 가설검정방법을 제안할 것이며 이를 통해 계산된 새로운 \\(p\\)-value가 기존 \\(p\\)-value와 effect size를 종합적으로 고려할 수 있는 지표임을 보일 것이다. 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 직접적으로 연결되어 있음을 보임으로서 가설검정 시 effect size를 고려해야 하는 이유에 대한 이론적 배경을 탄탄히 할 수 있으며, 새로운 \\(p\\)를 기준으로 유의수준을 설정하면 샘플수의 힘만으로 작은 차이를 유의한 결과로 해석하는 것을 방지할 수 있다. 간단한 Two-sided \\(z\\)-test를 예로 들어 개념을 정리 한 후 다른 test로 확장할 것이며, 마지막으로 이 개념을 다중비교에 활용하여 새로운 다중비교법을 제안하고 FWER, FDR control방법과 비교해 보겠다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "title": "Redefine Null Hypothesis",
    "section": "Basic Concept",
    "text": "Basic Concept\nBrief Review of Hypothesis Test\n평균은 모르고 분산이 \\(\\sigma^2\\)인 모집단에서 \\(n\\)개의 샘플 \\(X_1, X_2, \\cdots, X_n\\)을 뽑아 모평균이 \\(\\mu_0\\)인지 가설검정을 한다고 하자. 기존에는 귀무가설 \\(H_0\\)를 \\(\\mu = \\mu_0\\)로 설정한 후, 이 가정 하에서 \\(\\bar{X}\\sim N(\\mu_0, \\frac{\\sigma^2}{n})\\)임을 이용해서(\\(n\\)이 적당히 클 때 중심극한정리) \\((\\bar{X}-\\mu_0)\\sim N(0,\\frac{\\sigma^2}{n})\\)을 얻을 수 있으며 검정통계량은 아래와 같이 표현된다.\n\\[ z= \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\nTwo-sided test라면 \\(z\\)값이 1.96보다 크거나 -1.96보다 작을 때 \\(p &lt;0.05\\)로 유의한 결과로 판단한다. 한편, \\(\\bar{X}-\\mu_0\\)이 딱 0만 아니라면 \\(n\\)이 무한히 커질 때 \\(z\\)도 무한히 커져 어떠한 \\(p\\)-value 기준으로도 유의한 결과가 되는 문제가 있다.\nNull hypothesis with uncertainty\n위의 귀무가설에 uncertainty 개념을 추가해 보자. \\(H_0: \\mu \\sim N(\\mu_0,\\tau^2)\\)는 모평균이 \\(\\mu_0\\)이라고 생각하지만 \\(\\tau\\)정도의 uncertainty를 갖고 있다는 뜻으로 \\(\\tau=0\\)이면 기존의 귀무가설과 일치한다(Figure @ref(fig:fig1)).\n\n\n\n\nNull hypothesis with uncertainty: \\(\\mu_0=0\\)\n\n\n\n앞에서는 \\(\\bar{X}-\\mu_0\\)의 분산이 \\(\\frac{\\sigma^2}{n}\\)이었지만 이번에는 모평균 \\(\\mu_0\\)이 uncertainty를 갖고 있으므로 \\(\\bar{X}-\\mu_0\\)의 분산은 \\(\\frac{\\sigma^2}{n}\\)이 아니고 \\(\\frac{\\sigma^2}{n}+ \\tau^2\\)이 된다. 이를 토대로 검정통계량을 새로 표현하면\n\\[z_{\\tau}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + \\tau^2}} \\sim N(0,1)\\]\n이고 이 때의 two-sided \\(p\\)-value를 \\(p_{\\tau} = 2\\times P(Z\\ge |z_{\\tau}|)\\)로 정의한다. \\(p_0\\)은 기존의 \\(p\\)-value와 같으며 \\(\\tau\\)가 0이 아니라면 \\(n\\)이 아무리 커져도 \\(|z_{\\tau}| &lt; |\\frac{\\bar{X}-\\mu_0}{\\tau}|\\) 이 되어 \\(1.96\\times \\tau\\) 이하의 차이는 \\(p_{\\tau}&lt;0.05\\)를 얻을 수 없다.\nUncertainty interpretation\nUncertainty \\(\\tau\\)의 의미는 그것을 모표준편차 \\(\\sigma\\)와 비교했을 때 더 잘 드러난다. \\(\\tau = c\\sigma\\)로 치환한 후 이를 \\(z_{\\tau}\\)에 적용하면 아래와 같다.\n\\[z_{\\tau}=z_{c\\sigma}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + c^2\\sigma^2}} = \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{z}{\\sqrt{1+nc^2}}\\] 이제 통계적으로 유의한결과를 얻으려면 기존 가설검정 대비 \\(\\sqrt{1+nc^2}\\)배의 \\(z\\)값이 필요함을 알 수 있으며, 기존 \\(p\\)-value와 \\(z_{c\\sigma}\\)로 계산한 \\(p_{c\\sigma}\\)의 관계를 Figure @ref(fig:fig2)에 그래프로 나타내었다.\n\n\n\n\n\\(p\\) and \\(p_{c\\sigma}\\)-values according uncertainty and sample size\n\n\n\n한편 \\(|z_{c\\sigma}|\\)를 effect size \\(d=|\\frac{\\bar{X}-\\mu_0}{\\sigma}|\\)에 대해 표현하면\n\\[|z_{c\\sigma}|= |\\dfrac{\\bar{X}-\\mu_0}{\\sigma}| \\cdot \\dfrac{1}{\\sqrt{1/n + c^2}} = \\dfrac{d}{\\sqrt{1/n+c^2}}\\]\n의 관계를 얻는다(Figure @ref(fig:fig3)).\n\n\n\n\n\\(p_{c\\sigma}\\) according to effect size, sample size and uncertainty of null hypothesis\n\n\n\n만약 \\(d\\)가 \\(1.96\\times c\\) 이하라면\n\\[|z_{c\\sigma}| = \\dfrac{d}{\\sqrt{1/n+c^2}} &lt;  \\dfrac{d}{c} \\le \\dfrac{1.96\\times c}{c}  = 1.96\\] 이 되어 샘플수 \\(n\\)을 아무리 늘리더라도 \\(p_{c\\sigma} &lt;0.05\\)를 얻을 수는 없다. 연구자는 이 사실을 이용하여 역으로 \\(c\\)를 조절함으로서 연구자가 생각하기에 너무 작은 effect size가 유의한 결과가 되는 것을 막을 수 있는데, \\(c=0.01\\) 즉, 모집단 표준편차 대비 1%의 불확실성을 설정한다면 \\(d\\)가 0.0196 이하인 매우 작은 effect가 유의한 결과를 얻는 것을 막을 수 있다. \\(n\\)이 작을 때는 1%의 uncertainty가 기존의 \\(p\\)-value에 거의 영향을 끼치지 않지만, \\(n=10,000\\)일 때는 유의한 \\(p\\)-value 기준을 0.005로 강화한 것과 같은 효과를 갖는다(Figure 2).\nSample Size Calculation\nUncertainty의 개념이 추가되면 Required sample size도 바뀌게 되는데, 먼저 기존 two-sided \\(z\\)-test의 샘플수 구하는 공식을 standard error에 대해 정리하면 아래와 같다(\\(\\alpha\\): Type 1 error rate, \\(\\beta\\): Type 2 error rate)(Noordzij et al. 2011).\n\\[\n\\begin{aligned}\nn &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{(\\mu-\\mu_0)/\\sigma})^2 \\\\\n\\dfrac{n}{\\sigma^2} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{\\sigma^2\\cdot(1/n)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2\n\\end{aligned}\n\\]\n이제 uncertainty parameter \\(c\\)를 추가하려면 좌변의 분모에 \\(c^2\\sigma^2\\)을 더해주면 되고 새로운 샘플수 구하는 공식은 아래와 같이 바뀌게 된다.\n\\[\n\\begin{aligned}\n\\dfrac{1}{\\sigma^2\\cdot(1/n_{c\\sigma}+c^2)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{n_{c\\sigma}} &= (\\dfrac{(\\mu-\\mu_0)/\\sigma}{Z_{1-\\alpha/2} + Z_{1-\\beta}})^2 -c^2 \\\\\n               &= \\dfrac{1}{n}-c^2 \\\\\nn_{c\\sigma} &= \\dfrac{1}{1/n-c^2}\n\\end{aligned}\n\\]\n따라서 \\(\\frac{1}{n} -c^2 \\le 0\\) 즉, 기존 가설검정방법으로도 \\(1/c^2\\) 이상의 샘플수가 필요했었다면, uncertainty를 포함한 가설검정으로는 아무리 샘플수를 늘려도 Type 1,2 error를 컨트롤할 수 없다. Figure @ref(fig:fig4)에 \\(\\alpha = 0.05\\), \\(1-\\beta=0.8\\)인 경우의 effect size와 required sample size의 관계가 그래프로 표현되어 있으며, uncertainty가 존재한다면 아무리 샘플수를 늘려도 어떤 effect size이하는 검정할 수 없음을 확인할 수 있다.\n\n\n\n\nRequired sample size according effect size in some uncertainty scenarios: \\(\\alpha=0.05\\), \\(1-\\beta=0.8\\)"
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "title": "Redefine Null Hypothesis",
    "section": "Extend to Other Statistics",
    "text": "Extend to Other Statistics\n이번엔 \\(Z_{c\\sigma}\\)의 논리를 다른 통계량으로 확장해보자.\n\n\\(t\\)-test: One or paired 2 sample\nOne sample \\(t\\)-test의 귀무가설은 위의 \\(z\\)-test와 일치하며, 모분산 \\(\\sigma^2\\)을 표본분산인 \\(s^2\\)으로만 바꾸면 통계량은 자유도 \\(n-1\\)인 \\(t\\)-distribution을 따르는 것이 알려져 있다(Kim 2015). 이제 귀무가설에 \\(cs\\)만큼의 uncertainty를 추가하면 아래와 같다.\n\\[t_{cs} = \\dfrac{\\bar{X}-\\mu_0}{\\sqrt{s^2/n + c^2s^2}} = \\dfrac{\\bar{X}-\\mu_0}{s/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{t}{\\sqrt{1+nc^2}}\\]\nPaired \\(t\\)-test의 경우도 \\(\\bar{X}\\)를 두 그룹의 차이의 평균인 \\(\\bar{d}\\), \\(s\\)를 차이의 표준편차인인 \\(s_d\\)로 바꾸면 위와 동일하다.\nIndependent 2 sample \\(t\\)-test\n독립된 2집단(equal variance)의 평균이 같은지를 비교할 때 귀무가설은 \\(H_0: \\mu_1-\\mu_2=0\\)의 형태가 되고 통계량은 아래와 같다(Kim 2015).\n\\[t=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\] (\\(s_p\\): pooled variance, \\(n_1, n_2\\): sample numbers of group 1, 2 )\n이 \\(t\\)값이 자유도 \\(n_1+n_2-2\\)인 \\(t\\)-distritution을 따른다고 알려져 있으며, 귀무가설 \\(H_0: \\mu_1-\\mu_2=0\\) \\(s_p\\)의 \\(c\\)배만큼 uncertainty를 추가하면 통계량은\n\\[t_{cs_p}=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})+c^2s_p^2)}}= \\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\cdot \\dfrac{1}{\\sqrt{1+\\frac{1}{\\frac{1}{n_1}+\\frac{1}{n_2}}}c^2} = \\dfrac{t}{\\sqrt{1+\\frac{n_1n_2}{n_1+n_2}c^2}}\\] 이 된다.\nRegression coefficient\n종속변수 \\(Y\\)와 독립변수 \\(X_1\\)의 선형관계를 살펴보는 \\(Y=\\beta_0 + \\beta_1X_1\\)에서 \\(H_0: \\beta_1=0\\)을 검정하는 아래 통계량은 자유도 \\(n-2\\)인 \\(t\\)-distribution을 따른다는 것이 알려져 있다(@ Altman and Krzywinski 2015).\n\\[t= \\dfrac{\\hat{\\beta}_1}{se({\\hat{\\beta}_1})}=\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/ \\sqrt{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}} \\]\n(\\(\\hat\\sigma_{\\varepsilon}\\): Mean squared error(MSE), \\(s_{x_1}\\): standard deviance of \\(X_1\\))\n이제 \\(\\dfrac{\\hat\\sigma_{\\varepsilon}}{{s_{x_1}}}\\)의 \\(c\\)배 만큼 귀무가설에 uncertainty를 추가하면 통계량은 아래와 같다.\n\\[t_{c\\hat\\sigma_{\\varepsilon}/s_{x_1}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n+c^2}} = (\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}}) \\cdot \\dfrac{1}{\\sqrt{1+nc^2}}= \\dfrac{t}{\\sqrt{1+nc^2}}\\]\n일반적으로 \\(Y\\)와 \\(X_1, X_2, \\cdots, X_p\\)들의 선형모형은 행렬을 이용해서 \\(\\mathbf{Y}= \\mathbf{X\\beta}\\)로 표현할 수 있으며, \\(\\beta_i=0\\)을 검정하는 경우 앞서 \\(\\frac{1}{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}\\)대신 \\((\\mathbf{X'X})^{-1}\\)의 \\(i+1\\)번째 diagonal인 \\(g_{ii}\\)을 대입한 통계량을 사용하며 자유도는 \\(n-p-1\\)이다(Alexopoulos 2010). 이제 uncertainty를 추가한 통계량을 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)로 정의하면 simple linear model의 경우를 자연스럽게 확장한 셈이 된다. 선형모형을 \\(Y\\)가 정규분포가 아닐 때로 확장한 Generalized linear model(GLM)의 경우에는 \\(z=\\frac{\\hat{\\beta}_i}{se{(\\hat{\\beta}_i)}}\\)가 asymptotically normal distribution을 따르는 것을 이용하며, 앞서와 마찬가지로 uncertainty를 추가한 통계량을 \\(\\dfrac{z}{\\sqrt{1+nc^2}}\\)로 정의하면 일관성을 유지할 수 있다(Nelder and Wedderburn 1972; Wedderburn 1974). .\nCorrelation coefficients\n두 변수 \\(X\\), \\(Y\\)의 상관관계를 볼 때 흔히 사용되는 pearson correlation coefficient(\\(r\\))은 아래 통계량을 이용한다.\n\\[t= \\dfrac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\]\n이것이 자유도 \\(n-2\\)인 \\(t\\)-distribuition을 따른다는 것이 알려져 있으며 실제로 이 값을 계산해보면 앞서 simple linear model의 \\(\\dfrac{\\hat{\\beta}_1}{se(\\hat{\\beta}_1)}\\)과 정확히 일치한다. 따라서 uncertainty를 추가한 \\(t\\)를 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)으로 정의하면 일관성을 유지할 수 있으며, 이는 \\(H_0: r'=\\dfrac{r}{\\sqrt{1-r^2}}=0\\)을 검정할 때 \\(\\dfrac{n}{n-2}\\)의 \\(c\\)배 만큼 uncertainty를 준 것으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "title": "Redefine Null Hypothesis",
    "section": "New Approach for Multiple Comparison: Uncertainty control",
    "text": "New Approach for Multiple Comparison: Uncertainty control\n마지막으로 귀무가설의 uncertainty를 조절하여 multiple comparison 문제를 다루는 uncertainty control을 제안한다.\nDefinition of uncertainty control\n여러 개의 귀무가설을 동시에 검정할 때 원래의 유의수준을 그대로 적용하면 위양성이 늘어나는 문제가 있는데, 각 가설마다의 유의수준이 \\(\\alpha\\)라면 전체 \\(m\\)개의 가설에서 하나라도 위양성결과를 얻을 확률은 \\(1-(1-\\alpha)^m \\simeq m\\alpha\\) 이 되어 원래 위양성 확률의 \\(m\\)배가 된다. 이를 해결하기 위해 흔히 이용되는 방법은 family-wise error rate(FWER) control로 bonferroni correction이 대표적이다. Bonferroni correction에서는 가설이 \\(m\\)개라면 각 가설의 유의수준을 \\(\\alpha_m=\\frac{\\alpha}{m}\\)로 낮춰 family-wise error rate를 \\(\\alpha\\)이하로 control 할 수 있으며 식으로 표현하면 아래와 같다(Armstrong 2014).\n\\[-\\text{log}\\alpha_m = -\\text{log}\\alpha + \\text{log}m\\]\n한편, 여러 개의 귀무가설을 동시에 검정한다는 것을 각 가설의 uncertainty가 늘어나는 것으로 바라볼 수도 있다. Genome-Wide Association Study(GWAS)가 대표적인 예로 이것은 어떤 유전자가 질병과 관계있는지 아무런 사전정보도 없지만 일단 갖고있는 유전자들을 전부 각각 검정해보는 방법이며, 각 검정의 유의수준은 보통 100만개의 유전자를 동시검정할 때의 bonferroni correction에 해당하는 \\(5\\times 10^{-8}\\)를 사용한다(Jannot, Ehret, and Perneger 2015). 그런데 어떤 유전자가 질병과 관련있는지 사전정보가 없음에도 불구하고 각 효과가 정확히 0이라고 생각하는 사람이 과연 있을까? 검정하려는 귀무가설의 갯수가 증가할수록 가설의 uncertainty도 증가한다고 간주하는 것이 자연스러우며 가설이 \\(m\\)개일 때의 uncertainty \\(c_m\\)을 아래와 같이 정의하겠다.\n\\[\\text{log}c_m = \\text{log}c + b\\cdot\\text{log}m\\]\n이것은 아까의 bonferroni correction과 비슷한 형태로 원래의 \\(c\\)(log scale)에 \\(\\text{log}m\\)에 비례하는 penalty를 추가로 부여하며, \\(1.96\\times c_m\\)보다 작은 effect size는 \\(n\\)이 아무리 커도 통계적으로 유의한 결과를 얻을 수 없다. 연구자는 \\(c\\)를 조절하여 initial uncertainty를, 상수 \\(b\\)를 조정하여 uncertainty control의 정도를 조절할 수 있으며, \\(c=0.01\\), \\(b=0.069\\)으로 정하면 \\(n=10000\\)에서 bonferroni correction과 같은 정도의 control을 줄 수 있다 (Figure @ref(fig:fig5)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and required \\(p\\)-values(\\(p_{\\text{req}}\\)): bonferroni correction vs uncertainty control(\\(b=0.069\\), \\(c=0.01\\)\n\n\n\nFigure @ref(fig:fig5)를 보면 uncertainty control도 bonferroni correction과 마찬가지로 \\(m\\)이 증가함에 따라 \\(p\\)-value의 기준을 강화함을 알 수 있다. 차이점은 샘플 수에 따라 기준을 강화하는 정도가 다르다는 것인데, \\(m=10^6\\) 기준으로 \\(N=1500\\)일 때는 \\(5.46\\times 10^{-3}\\), \\(N=3000\\)일 때는 \\(6.61\\times 10^{-4}\\), \\(N=5000\\)일 때는 \\(4.22\\times 10^{-5}\\)의 \\(p\\)-value 기준을 갖는다. \\(N=10000\\)일 때는 아까 언급했듯이 \\(p\\)-value 기준 \\(5.06\\times 10^{-8}\\)로 bonferroni correction과 비슷한 정도의 control을 주게 된다.\nWe can look at the change in significance levels in terms of the effect size. Unlike the Bonferroni correction, the degree of significance correction can be changed by the effect sizes even if they have the same \\(p_{c_{m\\sigma}}\\)-values of the Uncertainty control (Figure @ref(fig:fig6)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and \\(p_{c_m}\\) by effect size(\\(d\\)): uncertainty control\n\n\n\nIn Figure @ref(fig:fig6), when applying \\(c=0.01\\) and \\(b=0.069\\), the result with an effect size of \\(0.5\\) and \\(p_{c_{m\\sigma}}=5\\times 10^{-5}\\) is hardly affected by the increase in the number of hypotheses; however, the result with an the effect size of \\(0.05\\) cannot gain significance when the number of hypotheses increases to \\(10^6\\) even if the initial \\(p_{c_{m\\sigma}}\\)-value is same as above.\nSome scenarios: compare with bonferroni correction, false discovery rate(FDR)\n몇 가지 시나리오를 설정하여 uncertainty correction 방법을 흔히 이용하는 multiple comparison 방법인 bonferroni correction, FDR control과 비교해보겠다(Benjamini and Hochberg 1995). \\(10^6\\)개의 귀무가설 중 1% 즉, 10000개가 실제로 참인 가설이고 그 effect size는 \\(d\\)라고 가정하자. 통계적 유의성은 \\(d\\)와 sample size \\(N\\), 그리고 multiple comparison 방법에 따라 달라지게 되며 이론상의 결과와 데이터를 생성해서 simulation한 결과를 Table 1,2에 정리하였다. FWER와 FDR control의 기준은 모두 0.05이며 simulation 때 FDR control은 Benjamini-Hochberg Procedure를 이용하였다(Benjamini and Hochberg 1995).\n\n\n\nThe number of false positive/negative results among \\(10^6\\) hypotheses(1% true) with some scenarios\n\n\n\n\n\n\n\n\n\n\n\nSample size\nd\nBonferroni: False (+)\nBonferroni: True (+)\nFDR: False (+)\nFDR: True (+)\nUncertainty: False (+)\nUncertainty: True (+)\n\n\n\n2000\n0.02\n0\n0\n0\n0\n2655\n176\n\n\n2000\n0.04\n0\n1\n0\n2\n2655\n1125\n\n\n2000\n0.06\n0\n28\n41\n788\n2655\n3750\n\n\n2000\n0.08\n0\n305\n245\n4652\n2655\n7176\n\n\n2000\n0.10\n0\n1637\n437\n8311\n2655\n9292\n\n\n3000\n0.02\n0\n0\n0\n0\n654\n104\n\n\n3000\n0.04\n0\n6\n4\n81\n654\n1123\n\n\n3000\n0.06\n0\n152\n167\n3172\n654\n4526\n\n\n3000\n0.08\n0\n1424\n424\n8050\n654\n8355\n\n\n3000\n0.10\n0\n5103\n515\n9776\n654\n9809\n\n\n4000\n0.02\n0\n0\n0\n0\n164\n62\n\n\n4000\n0.04\n0\n17\n24\n450\n164\n1082\n\n\n4000\n0.06\n0\n488\n301\n5727\n164\n5115\n\n\n4000\n0.08\n0\n3477\n496\n9429\n164\n9021\n\n\n4000\n0.10\n0\n8087\n525\n9979\n164\n9947\n\n\n5000\n0.02\n0\n0\n0\n0\n42\n37\n\n\n5000\n0.04\n0\n44\n64\n1217\n42\n1027\n\n\n5000\n0.06\n0\n1134\n400\n7596\n42\n5588\n\n\n5000\n0.08\n0\n5814\n519\n9857\n42\n9409\n\n\n5000\n0.10\n0\n9474\n526\n9998\n42\n9985\n\n\n10000\n0.02\n0\n3\n1\n20\n0\n3\n\n\n10000\n0.04\n0\n733\n350\n6656\n0\n736\n\n\n10000\n0.06\n0\n7084\n523\n9944\n0\n7091\n\n\n10000\n0.08\n0\n9946\n526\n10000\n0\n9946\n\n\n10000\n0.10\n0\n10000\n526\n10000\n0\n10000\n\n\n\n\n\nTable 1을 보면 uncertainty control는 나머지 둘과는 달리 sample size가 작을 때 유의한 결과를 많이 얻을 수 있음을 알 수 있다. 구체적으로 \\(d\\)가 0.02일 때 bonferroni correction, FDR control로는 \\(n\\)이 5000이하일 때 유의한 결과를 하나도 얻을 수 없지만 uncertainty control로는 \\(n=1500,3000,5000\\)일 때 각각 5175, 646, 62개의 유의한 결과를 얻을 수 있다. \\(d=0.05\\)인 경우 \\(n=1500, 3000\\)이면 uncertainty control &gt; FDR control &gt; bonferroni correction, \\(n=5000\\)이면 FDR control &gt; uncertainty control &gt; bonferroni correction 순으로 유의한 결과를 많이 얻을 수 있다. 한편 \\(n=10000\\)인 모든 시나리오에서 uncertainty control은 bonferroni correction보다도 강한 기준으로 나타나고 sample size의 힘만으로 작은 effect size를 유의한 결과로 판단하는 것에 penalty를 주게 된다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "title": "Redefine Null Hypothesis",
    "section": "Discussion",
    "text": "Discussion\n본 연구에서는 uncertainty의 개념을 포함한 현실적인 귀무가설을 만들고 그것을 검정할 수 있는 가설검정법을 제안하였으며, 이를 통해 계산된 \\(p_{c\\sigma}\\)-value가 기존의 \\(p\\)-value와 effect size를 종합적으로 고려한 지표로 작은 effect size가 sample size의 힘만으로 유의한 결과를 얻는 것에 penalty를 줄 수 있음을 보였다. 이는 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 어떻게 연결되는지를 개념화했다는 의의가 있으며, 이제 연구자들은 차이가 정확히 0이라는 비현실적인 귀무가설에서 벗어나 현실적인 가설검정을 수행하면서 effect size를 가설검정의 틀 안에서 고려할 수 있을 것이다.\n얼마만큼의 uncertainty를 고려해야 하는지는 검증가능한 최소 effect size가 어느정도인지에 달려 있는데, 이는 연구분야와 주제에 따라서 달라질 수 있기 때문에 각 분야의 연구자들의 공감대가 형성되어야 할 것으로 생각된다. 앞서 언급한 \\(c=0.01\\)이 기존의 \\(p\\)-value에 큰 영향을 끼치지 않으면서 10000이상의 매우 큰 sample size에 0.005정도의 유의수준을 부여할 수 있는 적절한 기준이라 판단한다. 한편 effect size로 표준화하지 않더라도 차이값 자체를 기준으로 uncertainty를 고려할 수도 있는데, 사실 어떤 지표든지 그것을 측정하는 순간 측정의 최소단위로 인한 uncertainty가 존재할 수 밖에 없다. 예를 들어 수축기 혈압을 측정해서 120이 나왔을 때 그것의 의미는 실제값이 정확히 120이라는 것이 아니라 실제값이 119.5~120.5 사이에 존재한다는 것이다. 따라서 차이가 1이하인 것을 가설검정하는 것은 그 자체로 비현실적이라 할 수 있으며 \\(\\tau= \\frac{1}{1.96}\\)로 두면 \\(|z_{\\tau}| &lt; |\\frac{\\bar{X}-\\mu_0}{\\tau}| \\le \\frac{1}{\\tau} = 1.96\\)이 되어 1이하의 차이를 유의한 결과로 판단하는 것을 방지할 수 있다. 꼭 측정의 최소단위가 아니더라도 연구자가 생각하는 차이의 최소 단위를 \\(\\tau\\)에 반영할 수도 있다.\n베이지안 통계에서도 prior를 통해 사전지식의 uncertainty를 다룰 수 있는데, prior의 정보가 데이터의 정보가 합쳐져서 posterior가 된다는 것에서 본 연구와는 차이가 있다. 예를 들어 분산이 \\(\\sigma\\)인 정규분포 에서 \\(n\\)개의 sample을 뽑아 베이지안 방법으로 모평균을 추정할 때, prior(conjugate prior)의 분산이 \\(\\tau^2\\)이라면 posterior의 분산은 \\(\\dfrac{1}{\\frac{1}{\\sigma^2/n}+\\frac{1}{\\tau^2}}\\)가 된다(Murphy 2007). 이 때 \\(\\tau\\)가 무한히 커지면 분산은 \\(\\sigma^2/n\\)이 되는데, \\(\\tau\\)가 커질수록 prior의 정보가 없어 data의 정보가 posterior로 그대로 반영되는 것으로 볼 수 있다. 반면 본 연구에서는 귀무가설의 uncertainty값인 \\(\\tau\\)가 penalty로 작용하여 이것이 커질수록 data의 정보가 희석된다고 생각할 수 있다.\n3개 이상의 그룹을 비교할 수 있는 ANOVA와 Chi-square test에 대해서는 uncertainty의 개념을 적용하기 힘들었다. ANOVA의 귀무가설은 \\(H_0: \\mu_1= \\mu_2 = \\cdots = \\mu_k\\)꼴로 여러 그룹들을 동시에 비교하기 때문에 본 연구의 uncertainty의 개념을 그대로 적용할 수 없지만 실용적으로 \\(F_c= \\dfrac{F}{1+nc^2}\\)로 정의하면 다른 test들과 일관성을 유지할 수 있을 것이다. 범주형 변수끼리의 연관성을 비교할 때 쓰이는 카이제곱검정의 경우에도 \\(\\chi_c= \\dfrac{\\chi}{1+nc^2}\\)가 실용적인 지표로 활용될 수 있으리라 생각한다.\nUncertainty의 개념은 multiple comparison에서도 활용될 수 있는데, 검정할 귀무가설의 갯수가 늘어나는 것을 각 가설에 대한 uncertainty가 늘어나는 것으로 생각할 수 있기 때문이다. 이 기준을 이용하면 sample size가 적을 때 상대적으로 유의한 결과를 많이 얻을 수 있어 반대의 특징을 갖는 FWER, FDR control의 한계를 보완할 수 있다. 다중비교가 흔히 쓰이는 GWAS의 경우 \\(p&lt;5 \\times 10^{-8}\\)라는 강한 기준이 gold standard로 사용되어 실제 효과가 있는 유전자는 통계적 유의성을 얻지 못하고, 아주 미미한 효과가 sample size의 힘만으로 의미있는 유전자가 되는 문제가 있었는데, uncertainty control을 같이 활용함으로서 이를 극복할 수 있으리라 생각한다.\n귀무가설에 uncertainty를 추가한 본 연구의 가설검정 방법이 과학연구에서 기존 가설 검정방법을 포괄하는 새로운 표준이 될 수 있으리라 확신한다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html",
    "href": "posts/2018-11-08-mdlm/index.html",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "",
    "text": "본 연구는 김진섭 대표가 박사학위 논문으로 계획했던 연구로, 결과적으로 학술지 게재와 심사통과에 실패했다는 것을 미리 알려드립니다. 계산법은 R package 로 만들었습니다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#abstract",
    "href": "posts/2018-11-08-mdlm/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Abstract",
    "text": "Abstract\n선형모형을 적용하기 어려운 \\(J,U\\)-shape같은 curved linear relationship을 비선형모형으로 분석하면 선형모형에 비해 해석이 어려워진다. 이에 본 연구에서는 선형관계의 컨셉은 유지하면서 \\(J,U\\)-shape 같은 curved linear relationship을 표현할 수 있는 모형을 제안한다. 이것은 선형모형의 무대를 1차원에서 휘어진 다차원 공간으로 확장함으로서 가능하며, curved linear relationship을 다차원공간에서의 선형관계로 재해석할 수 있다. 시뮬레이션 결과 선형관계는 기존의 선형모형과 동등한 성능으로 추정하면서 더 우수한 성능으로 curved relationship를 추정할 수 있었고, 실제 \\(U\\)-shape을 보이는 관계를 다차원공간에서의 선형관계로 쉽게 설명할 수 있었으며 \\(U\\)-shape의 cut-off값도 쉽게 계산할 수 있었다. 선형모형을 완벽히 포함하여 확장한 본 연구의 제안이 건강연구의 새로운 표준으로 자리잡을 수 있으리라 자신한다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#introduction",
    "href": "posts/2018-11-08-mdlm/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Introduction",
    "text": "Introduction\nMultivariable Linear Model은 분석결과의 해석이 간단하면서도 여러 독립변수들을 동시에 고려할 수 있는 장점이 있어 Health Science에서 널리 이용된다(Schneider, Hommel, and Blettner 2010). 그러나 모든 관계가 선형관계인 것은 아니며 흔한 non-linear relationship으로 \\(J,U\\)-shape같은 curved linear relationship이 있다(Calabrese and Baldwin 2001; Power, Rodgers, and Hope 1998; de Wit et al. 2009; Knutson and Turek 2006). 이런 관계를 단순히 선형모형으로 분석하게 되면 간단하긴 하나 정확한 추정을 할 수 없으며 exponential, Log나 제곱, 루트를 이용해 변수를 치환하여 선형모형을 이용할 수 있다(Jagodzinski and Weede 1981). 그러나 치환으로 선형관계를 만들 수 있는 경우는 극히 일부분에 지나지 않아 많은 경우에 비선형모형(non-linear model)을 활용하는데, 대표적인 방법으로는 독립변수의 고차항을 모형에 추가하거나(Polynomial Model) 비모수적인 방법으로 곡선을 추정하는 Additive Model, 그리고 Multi-layer를 이용한 neural network이 있다(Jagodzinski and Weede 1981; Buja, Hastie, and Tibshirani 1989; Hornik, Stinchcombe, and White 1989). 그러나 이런 비선형모형들은 휘어진 모양을 해석하기 때문에 직선으로 해석하는 선형모형에 비해 해석이 복잡할 수 밖에 없다.\n이와 비슷한 문제가 20세기 초 물리학에서도 있었는데 태양 주위에서 빛이 휘는 문제가 바로 그것이다. 이 현상은 뉴턴의 물리학으로 설명되지 않았었는데, 아인슈타인(Albert Einstein)은 빛이 휘는 것이 아니라 태양 근처의 4차원 시공간(spacetime)이 휘어진 것이라는 발상의 전환을 통해 이 문제를 설명하였다(Coles 2001). 이것이 유명한 일반상대성이론으로 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 의미한다(Verlinde 2011).\n이에 저자는 아인슈타인의 아이디어와 비슷하게 선형모형의 무대를 휘어진 다차원 공간으로 확장함으로서 \\(J,U\\)-shape을 선형관계로 해석할 수 있는 Multi-dimensional Linear Model(MDLM)을 제안한다. 이것은 기존의 선형모형에 차원(dimension)의 개념을 추가하여 일반화한 것으로 모든 독립변수(independent variable)들이 같은 dimension의 정보라면 기존의 Linear Model과 일치한다. 먼저 개념을 수식으로 정리한 후 계수들을 추정하는 방법을 설명할 것이며 다양한 시나리오를 시뮬레이션하여 MDLM의 유용성을 살펴보겠다. 마지막으로 실제 \\(U\\)-shape을 갖는 데이터에 본 모형을 적용하여 유용성을 평가할 것이다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#formula",
    "href": "posts/2018-11-08-mdlm/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Formula",
    "text": "Formula\n음이 아닌 실수 \\(Y\\)를 종속변수로 실수 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\)들을 독립변수라 하자.\n2 independent variables, 2 dimensions\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 선형관계를 2차원 벡터공간에서 표현하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\n\\end{aligned}\n\\] \\(\\boldsymbol{\\vec{g}}_i\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며 Figure @ref(fig:fig1)에 그림으로 표현되어 있다.\n\n\n\n\nMDLM with 2 variables, 2 dimensions\n\n\n\n이것은 방향의 개념을 제외하면 기존의 선형모형과 같으며, 만일 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)이 같은 방향이라면 아래와 같이 기존 선형모형과 일치하게 된다.\n\\[\n\\begin{aligned}\nY &= (\\beta_{01} + \\beta_1X_1) + (\\beta_{02} + \\beta_2X_2) \\\\\n  &= \\beta_{0} + \\beta_1X_1 + \\beta_2X_2\n\\end{aligned}\n\\] (\\(\\beta_0 = \\beta_{01}+\\beta_{02}\\))\n해석은 기존의 선형모형과 같이 변화량을 이용하며 \\(Y\\)와 \\(X_1\\), \\(X_2\\)의 변화량에 대해서 식을 재구성하면 아래와 같다.\n\\[\n\\begin{aligned}\nd\\boldsymbol{\\vec{Y}} &= \\beta_1dX_1\\boldsymbol{\\vec{g}}_1 +\\beta_2dX_2\\boldsymbol{\\vec{g}}_2\n= \\beta_1d\\boldsymbol{\\vec{X}_1} + \\beta_2d\\boldsymbol{\\vec{X}_2}\n\\end{aligned}\n\\] 즉, \\(X_2\\)가 고정되어 있을 때 \\(Y\\)는 \\(X_1\\)의 방향으로 \\(\\beta_1\\)만큼 증가한다고 할 수 있으며, \\(X_1\\)이 고정되어 있다면 \\(Y\\)는 \\(X_2\\)의 방향으로 \\(\\beta_2\\)만큼 증가한다고 볼 수 있다.\n벡터로 표현된 위 식을 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)의 내적값인 \\(g_{12}\\)를 이용해 스칼라로 표현하면 아래와 같다.\n\\[Y^2 = (\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)\\]\n만약 \\(g_{12}=0\\) 즉, \\(X_1, X_2\\)가 독립된 차원을 갖는다면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1=-\\frac{\\beta_{01}}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보이며 \\(X_2\\)와 \\(Y\\)의 관계도 마찬가지이다. 일반적으로 \\(Y^2 = (\\beta_{01} + \\beta_1X_1 + g_{12}(\\beta_{02} + \\beta_2X_2))^2+ (1-g_{12}^2)(\\beta_{02} + \\beta_2X_2)^2\\)로 식을 변형하면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1 = -\\frac{\\beta_{01}+g_{12}(\\beta_{02} + \\beta_2X_2)}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보임을 확인할 수 있다.\n\n\\(p\\) independent variable, 2 dimensions\n일반적으로 독립변수가 \\(p\\)개인 경우 \\(X_1, \\cdot, X_l\\)이 같은 차원, \\(X_{l+1}, \\cdot, X_p\\)가 같은 차원에 있다고 가정하면 다음과 같이 벡터식과 스칼라식을 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p )\\boldsymbol{\\vec{g}}_2 \\\\\\\\\nY^2 &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)^2 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)^2 \\\\\n+ & 2g_{12}(\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)(\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)\n\\end{aligned}\n\\]\n\n\\(p\\) independent variable, \\(p\\) dimensions\n마지막으로 독립변수들이 전부 다른 방향을 갖고 있다고 가정한다면 아래와 같은 벡터식과 스칼라식을 얻는다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{2}X_2 )\\boldsymbol{\\vec{g}}_2 + \\cdots (\\beta_{0p} + \\beta_pX_p)\\boldsymbol{\\vec{g}}_p \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\\\\\nY^2 &= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\cdot\n\\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)^2} + 2\\sum_{i &lt; j}{g_{ij}(\\beta_{0i} + \\beta_iX_i)(\\beta_{0j} + \\beta_jX_j)}\n\\end{aligned}\n\\]\n여기서 \\(g_{ij}\\)는 \\(X_i\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_i\\)와 \\(X_j\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_j\\)의 내적값으로 두 벡터의 dependency를 나타낸다. 위의 경우와 마찬가지로 모든 \\(g_{i}\\)들의 방향이 같다면 아래와 같이 기존의 선형모형과 같은 관계를 얻는다.\n\\[Y= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\] (단, (\\(\\beta_0 = \\beta_{01}+\\beta_{02}+\\cdots + \\beta_{0p}\\)) )"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#estimation",
    "href": "posts/2018-11-08-mdlm/index.html#estimation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Estimation",
    "text": "Estimation\n\\(\\beta = (\\beta_1, \\beta_2, \\cdots, \\beta_p, \\beta_{01}, \\beta_{02}, \\cdots, \\beta_{0p})\\) estimation을 위해 최소화해야 할 cost function은 아래와 같이 오차제곱의 합(Sum of Squared Error: SSE)으로 하면 자연스럽게 기존 선형모형의 최소제곱 추정을 일반화 할 수 있다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i&lt;j}g_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n위 식에서 \\(g_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\n\\(\\beta\\) Estimation\n기존 선형모형과 달리 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 직접 계산하기 어려워, optimization technique을 이용하며 Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다 (Nelder and Mead 1965; Fletcher 1964; Byrd et al. 1995). 위의 방법들을 활용하여 우리는 초기 \\(\\beta\\)값들부터 시작해서 반복적인 계산을 통해 수렴값을 얻게 된다. 한편 초기값이 바뀌면 \\(SSE(\\beta)\\)의 값은 같더라도 \\(\\beta\\)들의 부호가 다른 결과를 얻을 가능성이 있는데 이는 \\(SSE(\\beta)\\)가 \\(\\beta\\)들의 2차식으로만 구성되어 있기 때문이며, \\(\\beta\\)들의 부호가 바뀌더라도 \\(SSE(\\beta)\\)가 같다면 해석은 동일하다.\n추정된 \\(\\beta\\)값들의 standard error들은 \\(SSE(\\beta)\\)의 hessian matrix(\\(H\\))로 부터 구할 수 있다. 어떤 함수 \\(f(\\theta)\\)의 hessian matrix란 \\(f(\\theta)\\)를 2번 미분한 성분들로 이루어진 행렬인데, 일반적으로 \\(\\theta\\)의 variance와 반비례함이 알려져 있다(Dovi, Paladino, and Reverberi 1991). 여기에서는 1변수 함수의 예를 통해 직관적으로 이해해보도록 하자. \\(f(\\theta)\\)가 \\(\\theta_0\\)에서 최소값을 갖을 때 \\(f\\)를 1번 미분한 값은 0임을 이용, \\(f\\)를 \\(\\theta_0\\) 부근에서 2차항까지만 테일러 급수전개를 하면\n\\[\n\\begin{aligned}\nSSE(\\hat{\\theta} + d\\theta) &= SSE(\\hat{\\theta}) + H \\cdot\\dfrac{(d\\theta)^2}{2}\n\\end{aligned}\n\\]\n이고 \\(d\\theta\\)에 대해 정리하면\n\\[(d\\theta)^2 = 2\\cdot H^{-1}\\cdot (SSE(\\hat{\\theta}+d\\theta)-SSE(\\hat{\\theta}))\\]\n이 된다. 즉 hessian이 커질수록 \\(\\theta\\)의 분산에 해당하는 \\((d\\theta)^2\\) 이 감소함을 알 수 있다.\n일반적으로 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\hat{\\beta}\\)들의 variance-covariance matrix는 아래와 같이 표현되며, 대각성분에 루트를 취하면 \\(\\beta\\)들의 standard error 값이 되어 p-value와 confidence interval(CI)을 계산할 수 있다(Dovi, Paladino, and Reverberi 1991).\n\\[\\text{vcov}(\\hat{\\beta}) = 2\\cdot H^{-1}\\cdot MSE(\\hat{\\beta})\\]\n(\\(MSE\\): Mean Squared Error)\nEstimation of \\(g_{ij}\\)\n\n위에 설명한 추정은 \\(g_{ij}\\)가 고정되었을 때를 가정한 것인데, \\(g_{ij}\\)를 데이터에서 직접 구할 수도 있으며 이것은 Generalized Estimating Equation(GEE)에서 working correlation matrix를 직접 계산할 수 있는 것과 마찬가지이다(Pan and Connett 2002). 이 때는 \\(g_{ij}\\)들은 \\(\\beta\\)들과 달리 -1에서 1까지의 값을 갖는다는 제한조건이 있어 constrained optimization technique를 이용해야 하며 나머지는 앞서와 동일하다(Rios and Sahinidis 2013)."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#simulation",
    "href": "posts/2018-11-08-mdlm/index.html#simulation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Simulation",
    "text": "Simulation\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 여러가지 관계에 대한 Simulation을 이용해 MDLM의 유용성을 살펴볼 것이며 구체적으로 다음의 5개 모형을 비교하겠다.\n\nLinear Model (LM)\nMDLM with fixed \\(g_{12}=0\\) (MDLM 1)\nMDLM with non-fixed \\(g_{12}\\) (MDLM 2)\nQuadratic Model: Polynomial model with 2nd degree (Quadratic)\nGeneralized Additive Model (GAM)\n\n모형비교는 Root Mean Square Error(RMSE)와 Akaike Information Criterion(AIC)을 이용하였으며 GAM의 경우는 effective degree of freedom(edf)를 AIC 계산에 활용하였다(Wood 2001).\n편의상 \\(X_1\\)과 \\(X_2\\)는 각각 1부터 10까지의 자연수를 갖는 것으로 가정하였으며 따라서 샘플수는 100이다. 모든 계산은 R 3.5.1의 optim, constrOptim 함수를 이용하였다.\nScenario 1: \\(Y = X_1 + X_2\\)\n\n\\(Y \\sim N(X_1 + X_2, 1)\\) 로 샘플링해 데이터를 생성하였으며 100회의 시뮬레이션을 수행해 RMSE와 AIC를 비교하였다(Table 1).\n\n\n\nResults: \\(Y = X_1 + X_2\\)\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1 ± 0\n1.4 ± 0.1\n1 ± 0\n1 ± 0\n1 ± 0\n\n\nDF\n4\n5\n6\n6\n7.6 ± 2.2\n\n\nAIC\n296.4 ± 8\n358 ± 11.3\n300.4 ± 8\n299.6 ± 9.1\n289.8 ± 3\n\n\n\n\n\n비교 결과 선형모형이 적은 parameter로 효율적인 추정을 하고 있음을 알 수 있었으며 \\(g_{12}\\)를 고정하지 않은 MDLM(2)가 선형모형과 비슷한 성능을 보이는데 이는 MDLM이 선형모형을 포함한 개념임을 생각했을 때 자연스러운 결과이다.\nScenario 2: \\(Y^2 = X_1^2 + X_2^2\\)\n\n이번엔 \\(Y \\sim N(\\sqrt{X_1^2 + X_2^2}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 2).\n\n\n\nResults: \\(Y^2 = X_1^2 + X_2^2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.1 ± 0\n1 ± 0.1\n1 ± 0.1\n1.1 ± 0\n1.1 ± 0\n\n\nDF\n4\n5\n6\n6\n6.1 ± 0.5\n\n\nAIC\n317.4 ± 8.9\n290.3 ± 12.8\n292.1 ± 12.9\n313.9 ± 5.2\n313.2 ± 7.4\n\n\n\n\n\n이번엔 MDLM(1)이 선형모형보다 확실히 우수한 추정값을 보이는것을 확인할 수 있으며, \\(g_{12}\\)를 고정하지 않은 MDLM(2) 또한 MDLM(1)과 비슷한 성능을 보이는 것을 확인할 수 있다.\nScenario 3: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n마지막으로 Scenario2를 일반화한 경우를 시뮬레이션하였다. \\(\\beta\\)들은 -5~5, \\(g_{12}\\)는 -1~1의 값을 임의로 선택하여 \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)를 만족하는 \\(Y\\)를 샘플링하였다. 즉, \\(Y \\sim N(\\sqrt{(\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 3).\n\n\n\nResults: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.1 ± 0\n1.3 ± 0.2\n1 ± 0\n1.1 ± 0\n1.1 ± 0\n\n\nDF\n4\n5\n6\n6\n5.8 ± 0\n\n\nAIC\n318.7 ± 8.3\n341.3 ± 37.3\n300.4 ± 7.6\n317.3 ± 7.9\n315.8 ± 7.8\n\n\n\n\n\n시뮬레이션 결과 \\(g_{12}\\)를 추정할 수 있는 MDLM(2)가 다른 모형들보다 압도적으로 우수한 성능을 보이는 것을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Apply to Real data",
    "text": "Apply to Real data\n이번에는 실제 \\(U\\)-shape을 보이는 데이터를 MDLM으로 분석해보겠다. http://biostat.mc.vanderbilt.edu/dupontwd/wddtext/data/3.25.2.SUPPORT.csv의 데이터에는 응급실 내원 당시 평균 동맥압(mean arterial pressure, MAP)과 재실기간(length of stay,LOS)의 정보가 있는데 MAP와 LOS의 natural logarithm값의 관계가 \\(U\\)-shape이다. 이제 log(LOS)와 MAP의 관계를 아래와 같이 모델링 하였다.\n\\[\\boldsymbol{\\vec{\\text{log(LOS)}}} = \\beta_{00}\\boldsymbol{\\vec{g_{1}}} + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})\\boldsymbol{\\vec{g_{2}}}\\]\n즉 log(LOS)를 intercept와 MAP의 독립된 2차원으로 바라보는 관점으로 스칼라로 표현한 모형은 아래와 같다.\n\\[(\\text{log(LOS)})^2 = \\beta_{00}^2 + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})^2\\]\nMDLM을 이용하여 Intercept와 MAP의 2차원공간으로 log(LOS)를 \\(\\text{log(LOS)}^2 = 2.3669^2 + (-2.4276 + 0.0295\\cdot\\text{MAP})^2\\)의 모형으로 추정할 수 있었고 AIC값은 \\(2413\\)였다. 이것은 선형모형으로 추정한 \\(\\text{log(LOS)} = 2.2624 + 0.0027 \\cdot \\text{MAP}\\)(AIC \\(2434\\)), quadratic항을 추가한 \\(\\text{log(LOS)} = 3.3742 -0.0246 \\cdot \\text{MAP} + 2\\times 10^{-4} \\cdot \\text{MAP}^2\\)(AIC \\(2414\\))보다 좋은 추정 결과이다(Figure @ref(fig:fig2)). 또한 U shape의 cutoff값을 \\(\\dfrac{2.4276}{0.0295} = 82.29\\)로 간단히 계산할 수 있었다.\n\n\n\n\nRelation between mean arterial pressure(mmHg) and length of stay day(log scale)"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#discussion",
    "href": "posts/2018-11-08-mdlm/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Discussion",
    "text": "Discussion\nMDLM을 이용해서 기존의 선형모형을 완벽히 포괄하면서 선형모형의 개념을 휘어진 다차원 공간으로 확장할 수 있었고, 이를 통해 \\(J,U\\)-shape같은 curved linear relationship을 잘 추정하고 cut-off값도 쉽게 확인할 수 있는 장점을 확인할 수 있었다. 기존 선형모형의 틀은 유지하면서 그것만이 진실은 아닐 수 있다는 것을 보여줬다는 점, 이를 통해 선형관계가 아닌 것을 선형관계로 해석할 수 있는 방법을 제시하였다는 점에서 큰 의미가 있다. 향후 연구자들이 평면에서의 선형관계에 국한되지 않고 가설을 검증할 수 있을 것이며, 이를 토대로 Health science 연구에서 MDLM이 기존 선형모형을 포괄하는 새로운 표준으로 자리잡을 수 있으리라 예상한다.\nMDLM의 추정식은 제곱근이 포함되어 있어 \\(Y\\)가 음수값을 갖고 있는 경우에는 적용하기 어렵다는 한계가 있다. 그러나 Health Science 분야에서 음수값을 갖는 지표는 별로 많지 않아 큰 문제는 아닐 것으로 생각하며, 변수변환을 통해 (+)로만 이루어진 새로운 변수를 만들어 해결할 수도 있다. 이 문제는 물리학자 Paul Dirac이 특수상대성이론을 고려한 양자역학의 방정식을 만들 때 겪었던 문제와 비슷한데, 그는 방정식의 계수가 꼭 숫자일 필요가 없고 행렬일 수도 있다는 기발한 아이디어로 이를 해결했다(Dirac 1928). 예를 들어 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 }\\) 일 때, \\(\\beta_0, \\beta_1, \\beta_2\\)가 숫자일 필요가 없다는 것이다. \\(\\beta\\)들을 행렬로 간주한다면 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 } = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\) 꼴이 되어 제곱근을 없애는 것이 가능하다. 이 행렬들은 최소 \\(4\\times 4\\) 이상의 정방행렬이어야 함이 알려져 있으며, 대표적인 예로 Cliford Algebra를 만족하는 Dirac matrices 있는데 \\(\\beta\\)들의 예를 하나 들면 아래와 같다(Traubenberg 2009).\n\\[\n\\beta_0 = \\alpha_0 \\times \\begin{pmatrix}\n  1 & 0 &  0 &  0 \\\\\n  0 & 1 &  0 &  0 \\\\\n  0 & 0 &  -1 &  0 \\\\\n  0 & 0 &  0 & -1\n\\end{pmatrix}\n\\]\n\\[\n\\beta_1 = \\alpha_1 \\times \\begin{pmatrix}\n   0 &  0 & 0 & 1 \\\\\n   0 &  0 & 1 & 0 \\\\\n   0 & -1 & 0 & 0 \\\\\n  -1 &  0 & 0 & 0\n\\end{pmatrix}\n\\]\n\\[\n\\beta_2 = \\alpha_2 \\times \\begin{pmatrix}\n   0 & 0 & 1 &  0 \\\\\n   0 & 0 & 0 & -1 \\\\\n  -1 & 0 & 0 &  0 \\\\\n   0 & 1 & 0 &  0\n\\end{pmatrix}\n\\]\n(\\(\\alpha_0, \\alpha_1, \\alpha_2\\): 실수)\n회귀계수가 숫자가 아닌 행렬이 가능하다는 이 아이디어가 향후 본 연구의 제곱근 문제를 극복하는 열쇠가 될 수 있을 것이라 생각한다.\n\\(SSE(\\beta)\\)를 최소화하는\\(\\beta\\)를 구하기 위해 optimization tequnique을 활용한 것도 문제가 될 수 있는데, 얻은 \\(SSE(\\beta)\\)값이 진짜 최소값(global minimum)인지 보장할 수 없기 때문이다. 이를 local minima problem이라 한다. 그러나 machine learning의 유행과 더불어 optimization technique도 빠르게 발전되고 있어 조만간 이 문제가 해결되리라 예상한다. 게다가 최근 연구에서 high-dimensional space인 경우 local minima problem은 매우 희귀한 것으로 나타났는데, 모든 차원에서 local minima일 가능성은 매우 낮기 때문으로 여겨진다(Dauphin et al. 2014).\nIntroduction에서 언급했듯이 아인슈타인(Albert Einstein)은 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 설명하였는데, 수식으로 살펴보면 3차원 공간에서 기술된 뉴턴의 중력장 방정식 \\(\\nabla^2\\Phi = 4 \\pi G\\rho_0\\)를 휘어진 4차원에서의 방정식 \\(\\boldsymbol{R}_{uv}-\\dfrac{1}{2}\\boldsymbol{g}_{uv} = \\dfrac{8\\pi G}{c^4}\\boldsymbol{T}_{uv}\\)로 확장한 것이다(Verlinde 2011). 본 연구의 MDLM을 통해 선형공간의 무대를 다차원으로 확장하여 \\(U\\)-shape같은 curved linear relationship을 선형관계로 바라볼 수 있게 되었다는 점에서 물리학에서의 아인슈타인 방정식과 비슷한 의미를 가진다고 감히 주장해 본다. 실제로 \\(\\beta_i^2\\) 를 \\(g_{ii}\\), \\(g_{ij}\\beta_i\\beta_j\\)를 합쳐서 \\(g_{ij}\\)라 놓으면 \\(g_{ij}\\)는 아인슈타인 중력장 방정식을 표현하는데 쓰이는 계량텐서(metric tensor) \\(g_{uv}\\)와 같은 의미를 갖게 되는데, \\((dY)^2\\)를 표현하는 식은 \\(\\sum_{i,j}g_{ij}(dX_i)(dX_j)\\)로 휘어진 시공간에서 두 지점 사이의 거리를 나타내는 방법과 정확히 일치한다. 뉴턴의 방정식으로도 일상적인 운동을 잘 설명할 수 있으나 우주 공간같은 거시적인 스케일에서는 아인슈타인 방정식이 필요해지는데, 이와 마찬가지로 다차원 공간에서 기술된 본 연구의 MDLM이 population level에서 기존 선형모형보다 더 정확히 건강관련 현상을 설명할 수 있으리라 예상한다.\n한편 일반상대성이론은 원자 이하의 미시세계의 현상을 잘 설명하지 못한다는 문제점이 있으며,불확정성의 원리(uncertainty principle)와 슈뢰딩거 방정식(Schrödinger equation)으로 대표되는 양자역학(quantum mechanics)의 논리가 이곳을 지배한다. 슈뢰딩거 방정식은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용으로 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특이한 점이다. 복소수는 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현하게 되고 놀라운 정확도로 미시세계의 현상을 설명할 수 있다. 이것은 Health science에도 중요한 시사점이 될 수 있는데, Health science에서 가장 큰 문제점 중 하나가 population level의 연구결과가 개인의 건강상태를 잘 설명하지 못한다는 것이다. 상태를 확률로 기술한다는 점에서는 베이지안 접근법(bayesian approach)이 양자역학의 접근과 비슷하지만 복소수를 활용할 수 없다는 점에서 차이가 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 개인의 건강상태를 설명하는 새로운 방법이 될 것이라 과감히 추측해 본다."
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "차라투 블로그",
    "section": "",
    "text": "Below is a list of contributors to this blog.\n\n\n\n  \n\n\n\nJinseob Kim\n\n\nJinhwan Kim\n\n\nChangwoo Lim\n\n\nWon Kim\n\n\nSeoyoon Kim\n\n\nJunhyuk Ko\n\n\nYujin Lee\n\n\nJihee Han\n\n\nBeomsu Park\n\n\nYumin Kim\n\n\nSiyeol Jung\n\n\nJisoo Kim\n\n\nHyunjun Ko\n\n\nChaehee Lee\n\n\nHyunki Lee\n\n\nYeji Kang\n\n\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  url = {https://blog.zarathu.com/contributors.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d. https://blog.zarathu.com/contributors.html."
  },
  {
    "objectID": "posts/2018-09-25-welcome/index.html",
    "href": "posts/2018-09-25-welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Anpanman의 블로그를 만들었습니다.\nDiamond Prices\nCross reference 연습입니다.\nFootnote 연습입니다.1"
  },
  {
    "objectID": "posts/2018-09-25-welcome/index.html#footnotes",
    "href": "posts/2018-09-25-welcome/index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis will become a hover-able footnote↩︎"
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html",
    "href": "posts/2018-11-08-medianorratio/index.html",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#abstract",
    "href": "posts/2018-11-08-medianorratio/index.html#abstract",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Abstract",
    "text": "Abstract\nHeritability는 trait의 유전적인 측면을 정량적으로 설명하는 지표로 genetic study에서 흔히 쓰이는 지표이다. \\(Y\\)가 continuous variable일 때는 Intraclass Correlation Coeffcient(ICC) 혹은 Variance Partition Coefficients(VPC)의 형태로 heritability를 간단히 표시할 수 있으나, binary trait에는 이를 그대로 적용할 수 없다. 이에 liability threshold model 등의 다양한 approximation 방법이 이용되고 있으나 결과 해석의 어려움이 문제로 지적되고 있다. 한편 binary trait의 경우 sibling recurrence risk ratio(\\(\\lambda_s\\))가 유전적인 부분을 표현하는 직관적인 지표로 쓰이고 있으나 prevalence값이 필요하고 covariate 보정이 안되는 한계점이 있다. 이에 저자는 binary multilevel study에서 이용되는 Median Odds Ratio(MOR)을 이용하여 binary trait의 유전적인 정도를 평가하는 편하고 직관적인 지표인 Median OR Ratio(\\(\\text{MORR}_{SU}\\))을 제안하였으며 Healthy Twin Study, Korea의 hypertriglycemia trait에 적용하여 간단하며 직관적으로 유전적인 정도를 측정할 수 있었다. 이 지표가 binary trait에서 유전적인 정도를 표현하는 새로운 지표로서 heritability나 \\(\\lambda_s\\)를 보완할 수 있을 것으로 확신한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#introduction",
    "href": "posts/2018-11-08-medianorratio/index.html#introduction",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Introduction",
    "text": "Introduction\nHeritability는 continuous trait에서 polygenic effect가 설명할 수 있는 정도를 측정하는 지표로 전체 분산 중 polygenic effect가 설명하는 비율(the portion of phenotypic variance in a population attributable to additive genetic factors)로 정의되며, trait의 정규분포 가정이 깨지지 않는다면 mixed effect model 등을 이용하여 쉽게 구할 수 있다.(Manolio et al. 2009; Vattikuti 2012). Mixed effect model의 결과에서 polygenic effect의 분산과 residual의 분산이 추정되고 이를 통해 전체분산과 polygenic effect가 설명하는 분율을 계산하는 것인데 이는 Intraclass Correlation Coeffcient(ICC) 또는 Variance Partition Coefficients(VPC)의 형태이며, mixed effect model을 이용함으로서 covariate(age, sex, etc..)들을 보정할 수 있는 장점이 있다.\n그러나 binary trait일 때는 이 방법으로 heritability를 계산할 수가 없다. 보통 질병에 걸릴 확률 \\(p\\)가 Logistic distribution을 따른다고 가정하고 logistic regression model을 이용하게 되는데 이 때, Continuous variable에서 한 것처럼 추정된 variance of polygenic effect의 분산은 probability의 logit function의 scale에서 계산된 것이며, 전체분산은 probability scale에서 계산이 되기 때문에 이를 단순하게 분자, 분모로 하여 계산할 수 없다(Browne et al. 2005). binary trait에서 heritability를 구하는 방법으로는 first order Taylor series expansion을 이용하여 logit함수에 대한 회귀식을 \\(Y\\)에 대한 회귀식으로 approximation하여 linearisation 하는 방법, simulation을 이용하는 방법, latent variable을 이용하는 방법 등이 제시되어 있는데 이들 방법은 근본적으로 근사값의 계산이라는 한계점이 있으며, 해석을 어떻게 해야 되는지도 문제가 된다(Browne et al. 2005; Bonnet 2016; Vigre et al. 2004; Davies et al. 2015; Tenesa and Haley 2013).\nbinary trait에서 유전적인 정도를 표현하는 또다른 방법으로 recurrence risk ratio라는 것이 있는데, 이 중 흔히 이용되는 것은 sibling recurrence risk ratio(\\(\\lambda_s\\))로 일반 인구집단의 prevalence에 비해 affected people의 sibling 집단에서의 prevalence가 몇배나 더 높은지를 표현하며 직관적으로 이해할 수 있는 지표라는 장점이 있다(Rybicki and Elston 2000). 그러나 이 값은 일반 인구 집단에서의 prevalence를 정확히 측정해야 한다는 부담을 갖고 있으며 ascertainment bias에 민감하며, polygenic effect의 변화없이 common environmental factor의 영향에 의해서도 변화할 수 있다는 문제점이 있다(S.-W. Guo 1998, 2002; S.-W. Guo 2000).\n한편 binary trait의 multilevel analysis에서 group level의 효과를 쉽게 이해하기 위해 Median Odds Ratio(MOR)라는 개념이 제시되었는데, 이것의 정의는 “the median value of the odds ratio between the group at highest risk and the group at lowest risk when randomly picking out two groups”이다(Merlo et al. 2006). MOR은 group변수의 분산만을 이용하여 쉽게 계산될 수 있으며 OR scale이기 때문에 이해하기도 쉬운데, 1보다 클수록 group effect가 크고 1에 가까울수록 group effect가 없다고 해석할 수 있다(Larsen et al. 2000; Larsen and Merlo 2005; Merlo et al. 2006).\n이에 저자는 multilevel logistic regression에서 이용하였던 MOR의 개념을 이용하여 binary trait의 유전적인 정도를 측정하는 새로운 지표인 Median OR Ratio between Sibling Pairs and Unrelated Pairs(\\(\\text{MORR}_{SU}\\))을 소개할 것이다. 본 지표는 sibling pairs끼리의 OR에 비해 unrelated pairs끼리의 OR의 값이 대략적으로(median) 얼마나 높은가를 나타내며, 유전적인 부분이 전혀 없는 질병이라면 sibling pair에서나 unrelated pair에서의 OR이 같게 되어 \\(\\text{MORR}_{SU}\\)의 값은 1이 된다. 반면에 이 값이 크면 클수록 sibling pair끼리의 질병발생 양상이 unrelated pair끼리의 그것보다 비슷해지고 이는 유전적인 부분이 큰 질병임을 뜻한다. 본 지표는 hierarchial generalized linear model(HGLM)를 통하여 계산할 수 있으며, covariates를 보정할 수 있다는 heritability의 장점과 쉽게 해석되는 \\(\\lambda_s\\)의 장점을 동시에 갖고 있다. 반대로 말하면 직관적인 해석이 어려운 heritability의 단점과 prevalence 정보가 필요하며 covariate보정이 어려운 \\(\\lambda_s\\)의 단점을 보완하였다고도 할 수 있다. (Lee and Nelder 1996). 본 연구에서는 \\(\\text{MORR}_{SU}\\)의 개념을 소개한 후 실제 데이터인 Healthy twin study, Korea의 hypertriglycemia trait에 적용하여 유전적인 부분을 해석해 볼 것이다(SUNG et al. 2006).\nMethod\nBreif review of Meidan OR\n먼저 MOR에 대해 간략하게 요약하여 설명하겠다(Larsen et al. 2000).\n\\(Y_{ij}\\)를 \\(j\\)th group의 \\(i\\)th individual의 health status라 하고(case: 1, control: 0), \\(X_{ij}\\)를 vector of covariates, \\(G_j\\)를 \\(j\\)th group의 effect라 정의하자. 이 때 multilevel logistic regression의 formula를 mixed effect model로 표시하면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\beta_0+X_{ij}'\\beta_1+G_j\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_j\\): random intercept \\(G_j\\sim \\text{iid  } N(0,V_g)\\))\n이 때 Conditional Odds는\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\exp{(\\beta_0)}\\exp{(X_{ij}'\\beta_1)}\\exp{(G_j)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 X가 고정되어 있을 때 임의로 뽑은 \\(j\\)th Group과 \\(k\\)th Group의 Odds ratio는\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{ij}=1|X,G_j)]}{{\\text{Odds}[Pr(Y_{ik}=1|X,G_k)]}}=\\exp{(G_j-G_k)}\n\\end{aligned}\n\\]\n가 되고 Odds가 큰그룹을 Odds가 작은 그룹과 비교한다면 \\(\\text{OR}=\\exp{|G_j-G_k|}\\)이다. 이제 \\((G_j-G_k)\\sim N(0,2V_g)\\) 임을 이용하여 이것의 중앙값(median)을 계산하면\n\\[\n\\begin{aligned}\n\\text{MOR} = \\exp{(\\sqrt{2V_g}\\times \\Phi^{-1}{(0.75)})}\\simeq \\exp{(0.95\\sqrt{V_g})}\n\\end{aligned}\n\\]\n이 되고 이를 MOR로 정의한다. MOR은 VPC와 다르게 오직 group variable의 분산인 \\(V_g\\)만 가지고 계산될 수 있어 그 자체값과 95% 신뢰구간을 간단히 계산할 수 있으며, MOR\\(=1\\) 이라면 group variable의 effect가 없다고 해석할 수 있고 MOR이 커질수록 group variable의 effect가 크다고 해석할 수 있다.\nFormula’s of \\(\\text{MORR}_{SU}\\)\n\n이제 \\(\\text{MORR}_{SU}\\)의 수식을 유도하여 보자. \\(Y_{i}\\)를 \\(i\\)th individual의 health status라 하고(case: 1, control: 0, \\(1\\le i \\le n\\)), \\(X_{i}\\)를 vector of covariates, \\(G_i\\)를 \\(i\\)th individual의 polygenic effect라 정의하자. 이제 polygenic model을 수식으로 나타내면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{i}=1|X_{i},G_i)]=\\beta_0+X_{i}'\\beta_1+G_i\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_i\\): polygenic effect of \\(i\\)th individual)\n한편 \\(G_i\\)들의 vector를 \\(G=(G_1,G_2,\\cdots,G_n)'\\)이라 하면, \\(G\\sim N(0,V_p\\Sigma)\\) 이 된다(\\(\\Sigma\\): genetic relationship matrix, \\(V_p\\): variance of polygenic effect).\n이제 Conditional Odds를 계산해보면\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{i}=1|X_{i},G_i)]=\\exp{(\\beta_0)}\\exp{(X_{i}'\\beta_1)}\\exp{(G_i)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 임의로 뽑은 \\(i\\)th individual과 \\(j\\)th individual의 Odds ratio는 \\(X\\)가 같을 때 다음과 같이 표현된다.\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{i}=1|X,G_i)]}{{\\text{Odds}[Pr(Y_{j}=1|X,G_j)]}}=\\exp{(G_i-G_j)}\n\\end{aligned}\n\\]\n이제 두 가지 경우를 생각하자.\n\n\\(i\\)와 \\(j\\)를 unrelated individuals에서 뽑았을 경우이다. 이 때는 앞서 MOR과 마찬가지로 \\((G_i-G_j) \\sim N(0,2V_g)\\)가 된다.\n\\(i\\),\\(j\\)를 sibling pair에서 뽑았다면, 즉 \\(i\\)와 \\(j\\)가 항상 sibling이라면 \\(Cov(G_i,G_j)=\\frac{1}{2}V_g\\) 이므로, \\((G_i-G_j) \\sim N(0,V_g)\\)가 된다.\n\n이제 unrelated individual과 sibling을 한 쌍씩 뽑아 각각 \\(G_i, G_j\\)와 \\(G_k, G_l\\)이라고 하면 Odds Ratio의 비인 OR Ratio(ORR)를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n\\text{ORR}_{SU}=\\frac{\\text{OR}_{unrelated}}{\\text{OR}_{sibling}}=\\frac{\\exp{|G_i-G_j|}}{\\exp{|G_k-G_l|}} =  \\exp{(|G_i-G_j|-|G_k-G_l|)}\n\\end{aligned}\n\\]\n\\(|G_i-G_j|-|G_k-G_l|\\)는 \\(|N(0,2V_g)| - |N(0, V_g)|\\) 의 분포를 따르는 것을 이용하여 median값을 계산하면 약 \\(0.2453\\times \\sqrt{V_p}\\)이고 최종지표인 Median OR Ratio(MORR)은 아래와 같다(Appendix).\n\\[\n\\begin{aligned}\n\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\n\\end{aligned}\n\\]\n즉 sibling pair들에서의 polygenic effect의 OR과 unrelated에서의 polygenic effect의 그것을 비교한 지표이며 범위는 1부터 무한대까지이다. \\(\\text{MORR}_{SU}\\)가 1이면 unrelated pair에서의 질병발생 양상의 차이가 sibling pair에서의 그것과 같게 되어 유전적인 부분이 전혀 없다고 해석할 수 있다. 값이 커질수록 sibling pair끼리는 unrelated pair끼리보다 질병발생 양상이 비슷하다고 볼 수 있으며 이는 곧 유전적인 부분이 큰 것으로 이해할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Apply to Real Data",
    "text": "Apply to Real Data\nHealthy Twin Study, Korea의 데이터에 본 지표를 적용하였다(SUNG et al. 2006). 가족-쌍둥이 구조로 이루어진 3,461명의 사람 중 지질검사와 음주, 흡연 정보가 있는 2,729명을 대상으로 150이상을 case, 150미만을 control로 정의하였으며 아무것도 보정하지 않은 Null model(Model 1), 성별과 연령을 보정한 모형(Model 2), 그리고 성별, 연령, 음주, 흡연력을 보정한 모형(Model 3)에 대해서 각각의 \\(V_p\\)값과 그에 따른 \\(\\text{MORR}_{SU}\\)값을 제시하였다(Table 1). 분석은 R 3.5.1버전에서 hglm package를 이용하였다(Ronnegard, Shen, and Alam 2010).\n\n\n\nVariance parameter, heritablity and MORR of polygenic effect\n\n\n\n\n\n\n\n\nModel 1 (95%CI)\nModel 2 (95%CI)\nModel 3 (95%CI)\n\n\n\n\\(V_p\\)\n0.59 (0.49-0.71)\n0.68 (0.57-0.81)\n0.66 (0.55-0.79)\n\n\nHeritablity\n0.15 (0.13-0.18)\n0.17 (0.15-0.2)\n0.17 (0.14-0.19)\n\n\n\\(\\text{MORR}_{su}\\)\n1.16 (1.13-1.19)\n1.18 (1.15-1.22)\n1.18 (1.14-1.21)\n\n\n\n\n\n(Heritability: Intraclass correlation coefficients(\\(\\frac{V_p}{V_p+\\frac{\\pi^2}{3}}\\)), Model 1: no covariate, Model 2: age & sex as covariates, Model 3: age, sex and alcohol/smoking status as covariates)\nModel 2를 살펴보면 age와 sex의 effect를 보정하고 난 후의 \\(V_p\\)값은 0.59(95% CI: 0.49-0.71)이고 이에 해당하는 \\(\\text{MORR}_{su}\\)의 값은 1.18(95% CI: 1.15-1.22)였으며 이는 sibling pair의 OR에 비해 unrelated pair의 OR의 값이 대략적으로 18% 더 높다고 해석할 수 있다. 한편 Null model(Model 1)의 \\(\\text{MORR}_{su}\\)의 값은 1.16(95% CI: 1.13-1.19)이며 age, sex, smoling status를 보정했을 때(Model 3)는 1.18(95% CI: 1.14-1.21)로 세 Model의 결과는 비슷했다. 이는 age, sex, alcohol, smoking status 보정 여부에 크게 상관없이 hypertriglycemia에 일정한 유전적인 영향이 존재함을 의미한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "href": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Conclusion",
    "text": "Conclusion\n저자가 제시한 \\(\\text{MORR}_{su}\\)의 개념을 이용하여 binary trait의 유전적인 정도를 직관적으로 설명할 수 있었다. 이는 해석이 어려운 heritability의 단점을 극복하였다는 의미가 있다. 또한 age, sex 등 다양한 covariates의 효과를 보정한 후의 유전적인 부분을 설명할 수 있고, 인구집단의 prevalence를 측정할 필요가 없다는 점에서 \\(\\lambda_s\\)의 단점을 보완한 지표라 할 수 있다.\n향후 heritability와 \\(\\lambda_s\\) 각각의 장점은 살리고 단점은 보완한 이 지표가 binary trait의 polygenic effect를 직관적으로 설명하는 방법으로 널리 쓰이길 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#appendix",
    "href": "posts/2018-11-08-medianorratio/index.html#appendix",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Appendix",
    "text": "Appendix\nCalculate MORR\n\\(|N(0,2V_p)| - |N(0, V_p)|\\)의 median을 \\(M\\)이라 하자. R의 distr package를 이용하면 아래와 같이 \\(V_g\\)값들에 따른 \\(M\\)값을 쉽게 계산할 수 있다(Ruckdeschel and Kohl 2014).\n\n\n\n\n\\(V_p, \\sqrt{V_p}\\) vs Median value of \\(|N(0,2V_p)| - |N(0, V_p)|\\)\n\n\n\n이 그래프의 직선의 기울기가 바로 0.2453이고 따라서 \\(\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\\)가 된다.\nCompare to Other Measures\n\n\n\n\nRelationship between \\(h^2\\) and \\(\\text{MORR}_{SU}\\)"
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html",
    "href": "posts/2018-11-08-ruck2018/index.html",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "",
    "text": "김진섭 대표는 11월 26일(금) 서울특별시 시민청에서 열린 R User Conference in Korea 2018(RUCK 2018) 에서 맞춤형 의학연구 애플리케이션 개발 환경 구축 경험에 대해 발표하였습니다. 초록과 발표 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#abstract",
    "href": "posts/2018-11-08-ruck2018/index.html#abstract",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Abstract",
    "text": "Abstract\n맞춤형 의학통계 앱 제작을 위해\n\nDocker swarm 기반의 Rstudio & shiny server 를 구축하고\n의학통계 앱에 필요한 R 패키지와 Shiny Application 들을 만들었습니다.\n\n미리 Rstudio와 shiny server가 설치된 도커(docker) 이미지를 만들고 이것을 도커 스웜을 이용해 배포함으로써 서버의 종류와 갯수에 구애받지 않는 마이크로서비스 아키텍처(microservice architecture)를 구축하였으며, 동적 프록시 서버(dynamic proxy server) 프로그램인 Traefik 을 이용하여 서비스가 추가될 때 마다(ex: 홈페이지, Jupyter) 이에 맞추어 https 보안이 적용된 서브도메인(subdomain) 주소를 부여하였습니다. 흔히 이용되는 의학통계 방법들을 Shiny Application으로 만들어 위의 환경에 배포하였으며 DT, tableone, epiDisplay, svglite 등의 기존 패키지와 자체적으로 개발한 패키지를 이용, 데이터 라벨(label) 정보가 적용된 논문용 테이블과 그림을 보여줄 수 있었습니다. 이번 발표에서는 이러한 개발 환경 구축 경험을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#slide",
    "href": "posts/2018-11-08-ruck2018/index.html#slide",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/swarm-setting/RUCK2018_JSKIM 를 클릭하면 볼 수 있으며 PC 환경에 최적화되었다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html",
    "href": "posts/2019-01-03-rdatamanagement/index.html",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, tidyverse 생태계에서의 데이터 매니지먼트 방법을 강의할 예정입니다. 강의 내용을 미리 정리하였습니다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 두 번째의 기초에 해당하며\n\nreadr 패키지의 read_csv 함수로 데이터를 빠르게 읽은 후\nmagrittr 패키지의 %&gt;% 연산자와 dplyr 패키지의 select, mutate, filter, group_by, summarize 함수로 직관적인 코드를 작성하고\npurrr 패키지의 map 함수로 쉽게 반복문을 처리하는 것을 목표로 한다.\n\n각각의 패키지를 따로 설치할 수도 있고 install.packages(\"tidyverse\")로 tidyverse 생태계의 패키지를 모두 설치할 수도 있다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 읽기: readr\n",
    "text": "데이터 읽기: readr\n\nreadr 패키지에서 csv 파일을 읽는 함수는 read_csv이며, 구분자(ex: 공백, 탭)가 다를 때는 read_delim 함수를 이용하여 구분자를 설정할 수 있다. 예제 데이터를 읽어보자.\n\nlibrary(readr)\na &lt;- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\")\n\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Sex, STRESS_EXIST\ndbl (12): Age, Height, Weight, BMI, DM, HTN, Smoking, MACCE, Death, MACCE_da...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n데이터를 읽으면 각 변수들을 어떤 형태(숫자형, 문자형)로 읽었는지 표현되는데, 바꾸고 싶은 것이 있으면 아래와 같이 col_types 옵션을 이용하면 된다.\n\n## Character: col_character() or \"c\"\na &lt;- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\",col_types = cols(HTN = \"c\"))\n\n이제 데이터를 살펴보면 HTN 변수가 문자형이 된것을 볼 수 있다.\n\na\n\n\n\n\n  \n\n\n\n기본 함수인 read.csv 와 비교했을 때 아주 작은 데이터에서는 장점이 없으나, 그 크기가 커질수록 read_csv가 더 좋은 성능을 보임이 알려져 있다(Figure @ref(fig:readfunc)).\n\n\n\n\n파일 읽기 함수 비교3\n\n\n\nread_csv로 읽은 데이터는 tibble이라는 새로운 클래스로 저장된다. 직접 확인해보자.\n\nclass(a)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n기존의 data.frame 외에도 tbl_df, tbl 와 같은 것들이 추가되어 있다. tibble은 data.frame보다 좀 더 날 것(?)의 정보를 보여주는데, 범주형 변수를 factor가 아닌 character 그대로 저장하고 변수명을 그대로 유지하는 것(data.frame에서 변수명의 특수문자나 공백은 .으로 바뀜)이 가장 큰 특징이다. tibble의 추가 내용은 jumpingrivers 블로그4를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "href": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "직관적인 코드: %>% 연산자",
    "text": "직관적인 코드: %&gt;% 연산자\ntidyverse 생태계에서 가장 중요한 것을 하나만 고르라면 magrittr 패키지의 %&gt;% 연산자를 선택하겠다. %&gt;%은 Rstudio에서 단축키 Ctrl + Shift + M (OS X: Cmd + Shift + M)로 입력할 수 있는데, 이것을 이용하면 의식의 흐름대로 코딩을 할 수 있어 직관적인 코드를 작성할 수 있다. head함수를 통해 %&gt;%의 장점을 알아보자.\n\nlibrary(magrittr)\n\n## head(a)\na %&gt;% head\n\n\n\n\n  \n\n\n\n\na %&gt;% head(n = 10)\n\n\n\n\n  \n\n\n\nhead(a)와 a %&gt;% head는 같은 명령어 “a의 head를 보여줘”로, a %&gt;% head가 생각의 흐름이 반영된 코드임을 알 수 있다. 일반적으로 %&gt;% 연산자는 함수의 맨 처음 인자를 앞으로 빼오는 역할을 하고 f(x, y)는 x %&gt;% f(y)로 바꿀 수 있다. 맨 처음 인자가 아닐 때에는 y %&gt;% f(x, .)과 같이 앞으로 빼온 변수의 자리에 .를 넣으면 된다. 예를 들면 아래에 나오는 세 명령어는 모두 같다.\n\na %&gt;% head(n = 10)\n10 %&gt;% head(a, .)   \n10 %&gt;% head(a, n = .)\n\n%&gt;%의 진가는 여러 함수를 한 번에 사용할 때 나타나는데, head와 subset을 동시에 쓰는 경우를 예로 살펴보겠다.\n\n## head(subset(a, Sex == \"M\"))\na %&gt;% subset(Sex == \"M\") %&gt;% head\n\n\n\n\n  \n\n\n\n이 명령어는 a에서 남자만 뽑아서 head를 보여줘로 기존의 head(subset(a, Sex == \"M\"))보다 훨씬 직관적이며 함수가 3개, 4개로 늘어날수록 비교가 안될 정도의 가독성을 보여준다. 남자만 뽑아 회귀분석을 수행하고 그 계수와 p-value를 구하는 예를 살펴보자. 먼저 기존의 코딩 스타일대로 명령을 수행하면 아래와 같다.\n\nb &lt;- subset(a, Sex == \"M\")\nmodel &lt;- glm(DM ~ Age + Weight + BMI, data = b, family = binomial)\nsumm.model &lt;- summary(model)\nsumm.model$coefficients\n\n\n\n\n\n\n\n다음은 %&gt;% 를 이용한 코딩이다.\n\na %&gt;% \n  subset(Sex == \"M\") %&gt;% \n  glm(DM ~ Age + Weight + BMI, data = ., family = binomial) %&gt;% \n  summary %&gt;% \n  .$coefficients\n\n\n\n\n\n\n\n%&gt;% 연산자로 쓴 코드는 읽기 쉬운 것은 물론 불필요한 중간변수(b, model, summ.model)가 없어 깔끔하고 메모리의 낭비도 없다. 딱 하나 주의할 점은 코드를 내려쓸 때 각 줄은 반드시 %&gt;%로 끝나야 한다는 점이다. 예를 들어 아래의 코드를 실행하면 맨 윗줄만 실행되는 것을 확인할 수 있다.\n\na %&gt;% subset(Sex == \"M\")\n  %&gt;% head \n\n본 강의 후 다른 것은 까먹어도 %&gt;% 만 익숙해지면 성공이라고 생각한다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 정리: dplyr\n",
    "text": "데이터 정리: dplyr\n\ndplyr 는 데이터를 효과적으로 다룰 수 있는 일련의 함수들을 제공한다. 이중 group_by와 summarize는 쉽게 그룹 별 요약통계량을 보여줌으로서 기존 R 문법과 차별화된 가치를 제공하므로 꼭 익혀두자.\nfilter\nfilter는 subset 함수와 같은 기능으로 특정 조건으로 데이터를 필터링하는 데 이용된다. 아래는 데이터에서 남자만 추출하는 예시이다.\n\nlibrary(dplyr)\na %&gt;% filter(Sex == \"M\") \n\n\n\n\n  \n\n\n\nfilter에서는 & 외에 ,으로도 AND 조건을 쓸 수 있어 가독성이 좋다. between 함수를 이용하면 연속변수의 특정 범위를 선택할 수도 있는데, 이것 역시 기존의 &를 활용하는 것보다 직관적이다. 50~60세 사이를 필터링하는 예시를 살펴보자.\n\n## Age between 50 and 60.\na %&gt;% filter(between(Age, 50, 60))\n\n\n\n\n  \n\n\n\n아래의 &나 ,로 표현한 조건도 between을 이용한 것과 같은 결과를 보여준다.\n\na %&gt;% filter(Age &gt;= 50 & Age &lt;= 60)\na %&gt;% filter(Age &gt;= 50, Age &lt;= 60)\n\n\narrange: 정렬\narrange는 특정 순서에 따라 데이터를 정렬하는 함수로, 정렬 순서만 알려주는 order 함수와는 달리 정렬된 데이터를 보여주는 것이 특징이다.\n\n## a[order(a$Age), ]\na %&gt;% arrange(Age)\n\n\n\n\n  \n\n\n\n정렬 조건이 2개 이상이면 ,로 같이 적을 수 있으며 내림차순 정렬은 desc 명령어를 이용한다. 아래는 Age에 대해 오름차순, BMI에 대해 내림차순 정렬을 수행하는 예시이다.\n\n## a[order(a$Age, -a$BMI), ]\na %&gt;% arrange(Age, desc(BMI))\n\n\n\n\n  \n\n\n\n조건에 Age 대신 \"Age\"와 같이 문자열을 넣을 때는 언더바(_)가 붙은 arrange_ 함수를 이용하며, 이것은 나머지 함수들에서도 마찬가지이다.\n\na %&gt;% arrange_(\"Age\")\n\n\nselect: 변수 선택\nselect는 데이터에서 특정 변수들을 선택하는 함수로 기본 R 에는 없는 유용한 기능들을 제공한다.\n\n## a[, c(\"Sex\", \"Age\")]\na %&gt;% select(Sex, Age, Height)\n\n\n\n\n  \n\n\n\n변수명 대신에 열 번호를 넣어도 되며 Sex:Height을 이용해서 Sex와 Height 사이의 모든 변수를 선택할 수도 있다. arrange 함수와는 달리 \"Sex\"와 같은 문자열도 그대로 입력 가능하며, 아래의 방법들은 모두 a %&gt;% select(Sex, Age, Height)와 같은 코드이다.\n\na %&gt;% select(Sex:Height)\na %&gt;% select(\"Sex\":\"Height\")\na %&gt;% select(2, 3, 4)\na %&gt;% select(c(2, 3, 4))\na %&gt;% select(2:4)\n\n특정 변수들을 빼려면 -Sex나 -(Sex:Height)와 같이 적으면 된다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %&gt;% select(-Sex, -Age, -Height)\n\n\n\n\n  \n\n\n\n아래의 코드도 a %&gt;% select(-Sex, -Age, -Height)와 같은 결과를 준다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %&gt;% select(-2, -3, -4)\na %&gt;% select(-(2:4))\na %&gt;% select(-c(2, 3, 4))\n\na %&gt;% select(-(Sex:Height))\na %&gt;% select(-\"Sex\", -\"Age\", -\"Height\")\na %&gt;% select(-(\"Sex\":\"Height\"))\n\n만약 MACCE_date, Death_date와 같이 _date로 끝나는 변수들을 선택하고 싶다면 end_with 함수를 이용하면 된다.\n\n## a[, grep(\"_date\", names(a))]\na %&gt;% select(ends_with(\"date\"))\n\n\n\n\n  \n\n\n\n이외에 select와 함께 쓸 수 있는 유용한 함수들을 정리하면 아래와 같다.\n\nstart_with(\"abc\"): ’abc’로 시작하는 이름\nend_with(\"xyz\"): ’xyz’로 끝나는 이름\ncontains(\"ijk\"): ’ijk’를 포함하는 이름\none_of(c(\"a\", \"b\", \"c\")): 변수명이 a, b, c 중 하나\nmatches(\"(.)\\\\1\"): 정규표현식 조건\nnum_range(\"x\", 1:3): x1, x2, x3\n\nmutate: 변수 생성\nmutate는 새로운 변수를 만드는 함수이다. Age와 BMI 변수에서 고령과 비만을 뜻하는 Old, Overweight 변수를 만들어 보자.\n\n## a$old &lt;- as.integer(a$Age &gt;= 65); a$overweight &lt;- as.integer(a$BMI &gt;= 27)\na %&gt;% mutate(Old = as.integer(Age &gt;= 65),\n             Overweight = as.integer(BMI &gt;= 27)\n             )\n\n\n\n\n  \n\n\n\n새로운 변수만 보여주려면 mutate 대신 transmute를 사용한다.\n\na %&gt;% transmute(Old = as.integer(Age &gt;= 65),\n             Overweight = as.integer(BMI &gt;= 27)\n             )\n\n\n\n\n  \n\n\n\n\ngroup_by와 summarize\n\ngroup_by, summarize를 이용하여 원하는대로 그룹을 나누고 각 그룹의 요약통계량을 간단히 구할 수 있다. 기본 R에서는 aggregate함수가 같은 기능을 수행한다.\n\na %&gt;% \n  group_by(Sex, Smoking) %&gt;% \n  summarize(count = n(),\n            meanBMI = mean(BMI),\n            sdBMI = sd(BMI))\n\n\n\n\n  \n\n\n\ngroup_by에 \"Age\" 같은 문자열을 넣으려면 언더바(_)가 붙은 group_by_ 함수를 이용해야 한다.\n모든 변수에 같은 요약방법을 적용하려면 summarize_all 함수를 사용한다. 아래는 50세 이상을 대상으로 모든 변수에 그룹별 평균을 적용한 예시이다.\n\na %&gt;% \n  filter(Age &gt;= 50) %&gt;% \n  group_by(Sex, Smoking) %&gt;% \n  summarize_all(mean) \n\n\n\n\n  \n\n\n\n평균값을 계산할 수 없는 범주형 변수는 NA를, 그 외 변수들은 평균값을 보여줌을 확인할 수 있다. 여러 요약값을 동시에 보여주려면 funs 명령어로 여러 함수를 같이 적어주면 된다.\n\na %&gt;% \n  filter(Age &gt;= 50) %&gt;% \n  select(-Patient_ID, -STRESS_EXIST) %&gt;%       ## Except categorical variable\n  group_by(Sex, Smoking) %&gt;% \n  summarize_all(funs(mean = mean, sd = sd)) \n\n\n\n\n  \n\n\n\n\n%&gt;%와 기본함수만으로 똑같이 구현하기.\n마지막에 수행했던 작업을 %&gt;%와 R 기본함수만으로 구현하면서 본 단원을 마무리하겠다. filter 대신 subset, select 대신 [], group_by와 summarize_all 대신에 aggregate를 이용하면 된다.\n\na %&gt;% \n  subset(Age &gt;= 50) %&gt;%\n  .[, -c(1, 14)] %&gt;% \n  aggregate(list(Sex = .$Sex, Smoking = .$Smoking), \n            FUN = function(x){c(mean = mean(x), sd = sd(x))})\n\n위와 같이 기본 함수로도 %&gt;% 연산자를 이용하여 그럴듯하게 코드를 작성할 수 있으나, dplyr 와 비교했을 때 아무래도 가독성이 떨어진다는 점에서 dplyr가 유용한 패키지임을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "반복문 처리: purrr\n",
    "text": "반복문 처리: purrr\n\n본 내용에서는 for문이나 멀티코어의 사용은 언급하지 않는다. 해당 내용은 과거 강의5를 참고하기 바란다.\nR에서 반복문을 처리하는데 가장 많이 이용되는 함수는 lapply(또는 sapply)일 것이다. purrr의 대표 함수인 map은 이 lapply를 tidyverse 철학으로 구현한 것이다. 이제부터 차이점을 알아보자.\nmap\n데이터의 모든 변수들의 형태를 살펴보는 lapply 구문은 아래와 같다.\n\nlapply(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\n이것을 map으로 구현하면 lapply와 정확히 같은 형태가 된다.\n\nlibrary(purrr)\nmap(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\nmap은 list 형태로 결과를 반환하며 특정 형태를 지정하려면 아래와 같은 함수들을 이용한다.\n\nmap: 리스트\nmap_lgl: 논리값(T, F)\nmap_int: 정수\nmap_dbl: 실수\nmap_chr: 문자열\nmap_dfr: rbind된 data.frame\nmap_dfc: cbind된 data.frame\n\n앞의 예를 map_chr을 이용해 다시 실행하자.\n\nmap_chr(a, class)\n\n         Sex          Age       Height       Weight          BMI           DM \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \nSTRESS_EXIST Number_stent \n \"character\"    \"numeric\" \n\n\n이것은 sapply함수의 tidyverse 버전으로 기억하면 좋다. 아래의 명령어들은 모두 같은 결과를 나타낸다.\n\nmap_chr(a, class)\na %&gt;% map_chr(class)\nsapply(a, class)\nunlist(map(a, class))\nunlist(lapply(a, class))\n\n각 변수에서 첫 번째 값을 뽑는 아래의 코드들도 sapply와 map_*의 차이는 없다.\n\na %&gt;% sapply(function(x){x[1]})\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %&gt;% sapply(`[`, 1)\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %&gt;% map_chr(`[`, 1)\n\n          Sex           Age        Height        Weight           BMI \n          \"M\"   \"52.000000\"  \"160.000000\"   \"63.000000\"   \"24.609375\" \n           DM           HTN       Smoking         MACCE         Death \n   \"0.000000\"           \"1\"    \"1.000000\"    \"0.000000\"    \"0.000000\" \n   MACCE_date    Death_date  STRESS_EXIST  Number_stent \n\"1056.000000\" \"1056.000000\"     \"No test\"    \"3.000000\" \n\n\nclass나 mean, 그리고 `[` 등 간단한 함수들은 lapply를 쓰나 map을 쓰나 차이가 없다. map의 진가는 반복할 함수가 복잡할 때 드러난다. 성별로 같은 회귀분석을 반복하는 코드를 예로 들어 보자. 먼저 lapply를 이용한 코드는 아래와 같다.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  lapply(function(x){lm(Death ~ Age, data = x, family = binomial)})\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\n위 예시에서 알 수 있듯이 lapply에서 복잡한 함수를 반복하려면 function(x) 문이 꼭 필요하고 가독성을 저해하는 원인이 된다. 그러나 map에서는 ~로 간단히 function(x)를 대체할 수 있다. 아래 코드를 살펴보자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map(~lm(Death ~ Age, data = ., family = binomial))\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\nmap함수를 이용하여 function(x)를 ~로, 성별 데이터에 해당하는 x를 .으로 바꾸니 훨씬 읽기가 쉬워졌다.\n이번엔 같은 회귀분석을 수행한 후 Age의 p-value만 뽑는다고 하자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  sapply(function(x){\n    lm(Death ~ Age, data = x, family = binomial) %&gt;% \n      summary %&gt;% \n      .$coefficients %&gt;% \n      .[8]     ## p-value: 8th value\n    }) \n\n[1] 9.016998e-01 2.094342e-07\n\n\n%&gt;% 연산자를 이용해 나름대로 읽기 쉬운 코드가 되었다. 이것을 map_dbl로 다시 표현하면 아래와 같다.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map_dbl(~lm(Death ~ Age, data = ., family = binomial) %&gt;% \n        summary %&gt;% \n        .$coefficients %&gt;% \n        .[8]\n  )\n\n[1] 9.016998e-01 2.094342e-07\n\n\nfunction(x) 가 없어 더 잘 보이기는 하나 고작 이 정도의 장점이라면 map을 쓸 필요가 없을 것 같다. map을 좀 더 적극적으로 사용해 보자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map(~lm(Death ~ Age, data = ., family = binomial)) %&gt;% \n  map(summary) %&gt;% \n  map(\"coefficients\") %&gt;% \n  map_dbl(8)\n\n[1] 9.016998e-01 2.094342e-07\n\n\n기존 코드는 .$coefficient와 .[8] 같은 부분이 거슬렸는데, map(\"coefficients\"), map_dbl(8)으로 바꾸니 보기에 훨씬 깔끔하다. 참고로 map(\"coefficients\") 은 map(`[[`, \"coefficients\")의, map_dbl(8)은 map_dbl(`[`, 8)의 축약형 표현이다. 사실 아까 다루었던 첫 번째 값 뽑기의 경우도 아래와 같이 더 간단하게 쓸 수 있다.\n\na %&gt;% map_chr(`[`, 1)\na %&gt;% map_chr(1) \n\nmap을 통해 함수의 모든 중간과정을 따로따로 구현한 것도 장점이다. 에러가 발생했을 때 드래그로 중간과정까지만 실행할 수 있어 함수의 어느 부분에서 에러가 발생했는지를 쉽게 알아낼 수 있다.\n\nmap2, pmap: 입력 변수 2개 이상\n2개 이상의 입력값에 대해 반복문을 수행하는 함수로는 mapply가 있다. 이것의 tidyverse 버전이 map2와 pmap인데 전자는 2개의 조건에, 후자는 리스트 형태로 입력값의 갯수에 상관없이 반복문을 구현할 수 있다. 본 강의에서는 간단한 예만 다루어 보겠다. 먼저 mapply를 이용해서 여러 입력값의 합을 구하는 코드를 살펴보자.\n\nmapply(sum, 1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %&gt;% mapply(1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %&gt;% mapply(1:5, 1:5, 1:5)\n\n[1]  3  6  9 12 15\n\n\nmapply는 첫번째 인수에 함수를, 그 다음부터는 입력값들을 2개, 3개… 계속 받을 수 있다. 반면 map2와 pmap은 입력값을 먼저 받는데, 이 때문에 pmap에서는 입력값들을 리스트 형태로 받는다.\n\nmap2(1:5, 1:5, sum)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\npmap(list(1:5, 1:5, 1:5), sum)\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 6\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 12\n\n[[5]]\n[1] 15\n\n\n리턴 형태를 지정하려면 map과 마찬가지로 map2_*나 pmap_* 꼴의 함수를 이용하면 되며 아래의 코드들은 같은 결과를 나타낸다.\n\npmap_int(list(1:5, 1:5, 1:5), sum)\n\n[1]  3  6  9 12 15\n\nlist(1:5, 1:5, 1:5) %&gt;% pmap_int(sum)\n\n[1]  3  6  9 12 15\n\n\n마지막으로 paste함수로 문자열을 합치는 예를 살펴보겠다. 먼저 map2_chr로 두 문자열을 합쳐보자.\n\nname &lt;- c(\"Alice\", \"Bob\")\nplace &lt;- c(\"LA\", \"New york\")\nmap2_chr(name, place, ~paste(.x, \"was born at\", .y))\n\n[1] \"Alice was born at LA\"     \"Bob was born at New york\"\n\n\n첫 번째 입력값은 함수에서 .x로 두 번째 입력값은 .y로 표현할 수 있다. pmap 함수를 이용할 때는 ..1, ..2, ..3으로 바꿔 표현하면 된다.\n\nlife &lt;- c(\"born\", \"died\")\nlist(name, life, place) %&gt;% pmap_chr(~paste(..1, \"was\", ..2, \"at\", ..3))\n\n[1] \"Alice was born at LA\"     \"Bob was died at New york\""
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "href": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "마치며",
    "text": "마치며\n지금까지 tidyverse 생태계에서 몇 가지 패키지를 이용해 데이터를 다루는 방법을 살펴보았다. 앞서 말했듯이 이 생태계에서 가장 중요한 것은 %&gt;% 연산자를 이용하여 의식의 흐름대로 코딩을 수행하는 것이며, 이후 나머지 내용을 하나씩 적용해나가면 어느 순간 tidyverse 없이 살 수 없는(?) 자신을 발견하게 될 것이다. 본 글에서 다루지 않은 내용인 long, short 포맷을 다루는 tidyr, 문자열을 다루는 stringr, factor를 다루는 forcats, 날짜를 다루는 lubridate 그리고 모델을 다루는 modelr과 broom 등은 R for Data Science[^8]와 Rstudio cheetsheet[^9]를 참고하기 바란다. 다음번에는 빠른 속도가 장점인 data.table를 다시 한번 정리해 볼 생각이다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#footnotes",
    "href": "posts/2019-01-03-rdatamanagement/index.html#footnotes",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://jinseob2kim.github.io/rbasic.html↩︎\nhttps://jinseob2kim.github.io/radv1.html↩︎\nhttps://csgillespie.github.io/efficientR/5-3-importing-data.html↩︎\nhttps://www.jumpingrivers.com/blog/the-trouble-with-tibbles/↩︎\nhttps://jinseob2kim.github.io/radv1.html#faster_for-loop↩︎"
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html",
    "href": "posts/2019-01-09-doctorskku2019/index.html",
    "title": "진료실 밖 의사로서의 경험",
    "section": "",
    "text": "김진섭 대표는 2월 1일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "href": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "title": "진료실 밖 의사로서의 경험",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 1학기 수시 합격.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "href": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "title": "진료실 밖 의사로서의 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/doctorskku2019 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "",
    "text": "김진섭 대표는 2월 27일(수) Anpanman이 후원하는 Shinykorea 밋업에 참석, ShinyApps를 Rstudio Addins을 포함한 패키지로 만들어 CRAN에 배포신청한 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "요약",
    "text": "요약\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록\n\nShinyApps을 Rstudio Addins으로 만든 후, 이를 패키지로 만들어 github에 배포하였다.\ntestthat, covr로 코드 테스트를 수행하고 결과 리포트를 만들었으며, pkgdown으로 패키지를 소개하는 웹사이트를 만들었다.\nTravis CI와 appveyor로 2의 과정과 여러 운영체제에서의 테스트를 자동화하였다.\n최종적으로 CRAN에 패키지를 배포 신청했으나 거절되었다. 코멘트를 반영하여 재심사 중이다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Package",
    "text": "Package\nhttps://github.com/jinseob2kim/jsmodule"
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "",
    "text": "김진섭 대표는 5월 31일(금) 차라투(주)가 후원하는 Shinykorea 밋업에 참석, shiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "요약",
    "text": "요약\n\n\n의학연구자들에게 맞춤형 ShinyApps를 제공함.\n\n범용으로 쓰일만한 것들을 Shiny module로 만든 후, 웹과 RStudio Addins로 배포.\n\n\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 리포트를 제공 중(R Markdown 활용).\n심평원/보험공단 빅데이터 연구에서 R Markdown 리포트로 연구지원 중."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html",
    "href": "posts/2019-05-20-godelincompleteness/index.html",
    "title": "괴델의 불완전성 정리",
    "section": "",
    "text": "김진섭 대표는 5월 30일(목) 제주대학교 경영정보학과 산업·직무 특화 전문가 특강에 참석, 괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명할 예정입니다. 정리한 슬라이드와 강의록을 미리 공유합니다. 초청해주신 현정석 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "href": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "title": "괴델의 불완전성 정리",
    "section": "요약",
    "text": "요약\n\n18/19 세기 미적분학, 해석학의 발전으로, 수학은 점점 기존의 직관과 상식에서 벗어나 추상화되면서 많은 문제점들이 생겼다.\n특히 무한의 개념을 엄밀하게 다룰 필요성이 있었는데, 칸토어(Georg Cantor)는 집합론의 논법으로 무한을 엄밀하게 정의하고 그것들의 크기를 비교하였다.\n20세기 수학자들은 집합론을 이용, 수학의 기초를 구성하고 모순이 없는 수학체계를 만들 수 있다는 꿈으로 부풀어 있었으나, 러셀의 역설(Bertrand Russel’s paradox)로 대표되는 “자기언급의 역설”은 집합론의 기초를 위태롭게 하였다.\n힐베르트(David Hilbert)는 “자기언급의 역설” 은 수학의 명제와 메타수학(meta-mathematics)의 명제를 구분하지 않아 일어나는 것으로 판단하였으며, 이를 잘 구분하는 공리계를 세심하게 설계할 수만 있다면 모순없는 수학체계를 만들 수 있다고 보았다.\n그러나 괴델(Kurt Gödel)은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 즉, “자기언급의 역설” 을 피하기 위해 아무리 세심하게 공리계를 설계해도 그것을 피할 수는 없다는 것을 증명하였으며, 이것이 불완전성 정리이다.\n불완전성 정리로 인해 “참이지만 증명불가능한 명제가 존재” 하고 나아가 “수학의 무모순성을 수학 자체적으로는 증명할 수 없음” 이 증명되어 모순없는 완전한 수학체계의 꿈은 결국 산산조각이 난다.\n불완전성 정리를 인간 이성의 한계로만 해석하고 실의에 빠지지 말자. 어떤 기계보다도 더 복잡하고 정교한 인간의 정신구조와 능력을 긍정하며 창조적 이성의 힘을 인정할 때이다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "href": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "title": "괴델의 불완전성 정리",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureGodel/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "href": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "title": "괴델의 불완전성 정리",
    "section": "강의록",
    "text": "강의록\n본 강의록은 과거 https://jinseob2kim.github.io/godel.html 에 정리했던 내용을 약간만 수정하였습니다.\n서론\n괴델의 불완전성 정리에 대한 많은 책들이 시중에 출판되어 있다. 허나 대부분이 역사와 증명의 의미에 치중되어 있고 실제 증명의 아이디어를 설명한 책은 거의 없으며, 있다고 하더라도 외국 서적을 번역하면서 난해한 표현이 많이 나와 이해하기가 어렵다. 이에 본 글에서는 불완전성 정리 증명의 핵심적인 아이디어를 크게 수학(mathematics)과 메타수학(meta-mathematics), 괴델수(Gödel number), 메타수학의 수학화의 3가지로 나누어 설명하겠다. 본 글이 불완전성정리의 아름다움을 느끼는 데 도움이 되길 바란다.\n수학(mathematics)과 메타수학(meta-mathematics)\n보통 불완전성 정리에 대한 책들에서는 간략하게 증명의 개요만 언급하는데 그것은 아래와 같다. \\[G: G는 \\:증명불가능하다.\\]\n만약 \\(G\\)가 증명 가능하다면 그것은 참이고, 내용은 “\\(G\\)는 증명 불가능하다.” 므로 모순이다. 따라서 \\(G\\)는 증명 불가능하며, \\(G\\)의 내용은 “\\(G\\)가 증명 불가능하다”는 것이므로 참이다. 따라서 \\(G\\)는 참이지만 증명불가능한 명제라는 것이다. 이 설명을 보고 고개를 끄덕 인 후 증명이 끝난 것 아닌가? 라는 생각을 할 수도 있을 것이다. 그러나 명제 내부에 명제 자신을 언급한 순환논리부분을 해결하지 않으면 안되고 사실 불완전성정리의 핵심부분도 이 부분이며 수학의 명제와 메타수학의 명제를 구분하는 것이 이해의 시작이 된다.\n수학 명제와 메타수학 명제의 차이점\n아주 간단한 수학 명제 하나를 살펴보자. \\[1+1=2\\]\n이 표현은 수학에 속한 표현이다. 이제 다음 명제를 살펴보면 \\[\"1+1=2\"는 \\:수학\\: 명제이다.\\] 이 진술은 앞에 나온 수학명제에 대해 무엇인가를 주장하고 있으며 따라서 수학이 아니라 메타수학의 명제라고 할 수 있다. 비슷하게 “\\(x\\)는 변수이다”, “\\(x=1\\)은 방정식이다” 들도 수학 명제가 아니라 메타수학의 명제이다. 수학과 메타수학을 구별하는 것은 몇 번을 강조해도 지나치지 않은데 이 구별에 소홀한 것이 수많은 역설이 만들어지는 이유이기 때문이다. 힐베르트를 포함한 당시의 수학자들은 이를 잘 구별하면 역설을 제거할 수 있다고 보았다. 이제 다시 처음 식을 살펴보자. \\(G\\)가 수학명제라면 “\\(G\\)는 증명불가능하다” 는 수학의 명제가 아니라 메타수학의 명제이므로, 이것이 수학명제인 \\(G\\)가 될 수는 없다. 따라서 자기언급의 역설은 발생하지 않는다.\n그러나 괴델은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 괴델수부터 차근차근 알아보도록 하자.\n괴델수(Gödel number)\n괴델수는 모든 기호, 변수, 명제, 명제묶음(증명)들에 유일한 숫자를 부여하는 것인데 몇가지 예를 들어보면 다음과 같다.\n\n상항기호\n\n기호\n괴델수\n의미\n\n\n\n\\(\\sim\\)\n1\n아니다\n\n\n\\(\\vee\\)\n2\n또는\n\n\n\\(\\supset\\)\n3\n\n\\(\\cdots\\)라면 \\(\\cdots\\)다\n\n\n\\(\\exists\\)\n4\n\n\\(\\cdots\\)이 존재한다\n\n\n\\(=\\)\n5\n같다\n\n\n0\n6\n영(0)\n\n\ns\n7\n바로 다음 수\n\n\n(\n8\n왼쪽 괄호\n\n\n)\n9\n오른쪽 괄호\n\n\n,\n10\n쉼표\n\n\n\\(+\\)\n11\n더하기\n\n\n\\(\\times\\)\n12\n곱하기\n\n\n\n위의 표에 나와있는 기호들을 상항기호라고 하며, 이와 대응되는 개념은 변항(variable)기호인데 숫자 변항, 문장 변항, 술어 변항으로 나눌 수 있다. 숫자 변항에는 12보다 큰 소수, 문장 변항에는 12보다 큰 소수의 제곱수, 술어변항에는 12보다 큰 소수의 세제곱수를 부여하게 되며 예는 아래의 표와 같다.\n\n변항기호\n\n\n변항\n괴델수\n대입 예\n\n\n\n숫자변항\n\\(x\\)\n13\n0\n\n\n\n\\(y\\)\n17\n\\(s0\\)\n\n\n\n\\(z\\)\n19\n\\(y\\)\n\n\n문장변항\n\\(p\\)\n\\(13^2\\)\n\\(0=0\\)\n\n\n\n\\(q\\)\n\\(17^2\\)\n(\\(\\exists\\) x)(\\(x=sy\\)): \\(y\\)의 다음 수 \\(x\\)가 존재한다.\n\n\n\n\\(r\\)\n\\(19^2\\)\n\\(p\\supset q\\)\n\n\n술어변항\n\\(P\\)\n\\(13^3\\)\n\n\\(P(x)\\): \\(x\\)는 소수이다.\n\n\n\n\\(Q\\)\n\\(17^3\\)\n\n\n\n\n\\(R\\)\n\\(19^3\\)\n\n\n\n\n이제 문장 (\\(\\exists x\\))(\\(x=sy\\)) 를 살펴보자. 이것은 \\(y\\) 다음 수가 존재한다고 읽으며, 기본기호 하나하나의 괴델수를 살펴보면 8, 4, 13, 9, 8, 13, 5, 7, 17, 9이고 문장 전체의 괴델수는 다음과 같이 소수를 이용하여 표현한다. \\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n마찬가지로 증명에 대해서도 괴델수를 부여할 수 있는데 예를들어 증명이 두 서술로 되어 있고 각 서술의 괴델수가 \\(m\\)과 \\(n\\)이라면 증명의 괴델수는 \\(2^m \\times 3^n\\) 으로 표현한다. 이런식으로 모든 수학적 표현에 대해서 겹치지 않고 유일하게 괴델수를 부여할 수 있다. 한가지 예를 더 들면 괴델수 243,000,000은 \\(2^6 \\times 3^5 \\times 5^6\\)으로 소인수분해 되며 지수부분인 6,5,6은 각각 0, \\(=\\), 0에 대응되므로 결국 \\(0=0\\)이라는 수학명제를 나타내게 된다.\n메타수학의 수학화\n괴델은 괴델수를 이용하여 메타수학의 명제를 수학의 명제로 바꾸는 데 성공하였는데 간단한 예를 들어보겠다.\n\\[\\sim(0=0)\\]\n는 “0은 0이 아니다.” 라는 단순한 수학 명제인 반면\n\\[\\text{수학 명제} \"\\sim(0=0)\" \\text{의 첫 번째 기호는 틸드}(\\sim) \\text{이다}.\\]\n는 수학명제가 아니라 메타수학의 명제이다. 그런데 이 메타수학의 명제는 적당한 과정을 거치면 수학명제로 바꿀 수 있다. 편의상 \\(\"\\sim(0=0)\"\\)의 괴델수를 \\(a\\)라고 하고 위의 상항기호 표에서 \\(\"\\sim\"\\)의 괴델수가 1임을 기억하자. 그렇다면 첫 번째 기호가 \\(\"\\sim\"\\)이라는 것은 곧 \\(a\\)를 소인수분해했을 때 제일 작은 소수인 2의 지수가 1임을 의미하게 되고 메타수학의 명제는\n\\[2 \\text{는 } a \\text{의 인수이지만, } 2^2 \\text{ 은 } a \\text{의 인수가 아니다}.\\]\n와 같이 수학명제로 바뀌게 된다. 기호로 표현하기 위해 표현을 바꾸면 “\\(y=z\\times 2\\)인 \\(z\\)가 존재하고, \\(y=z\\times 2 \\times 2\\)인 \\(z\\)는 존재하지 않는다.” 가 되며 실제로 이를 기호로 표현하면 다음과 같다.\n\\[(\\exists z) (sss\\cdots sss0=z\\times ss0) \\cdot \\sim(\\exists z) (sss\\cdots sss0=z\\times (ss0 \\times ss0))\\] (\\(sss\\cdots sss0\\)에서 \\(s\\)는 정확히 \\(a\\)번 나타난다.) 이와 비슷한 방법으로 모든 메타수학의 명제를 괴델수를 이용하여 수학의 명제로 바꿀 수 있다.\n불완전성 정리\n이제 불완전성 정리를 천천히 이해해보자. Dem/dem과 Sub/sub의 개념을 정의한 후 증명의 핵심 아이디어로 들어가 보겠다.\nDem\ndem은 증명을 뜻하는 demonstration의 약자로 dem(\\(x\\), \\(z\\))은\n\\[\\text{괴델수 } x \\text{를 가진 문장묶음이 괴델수 } z \\text{를 가진 명제의 증명이다}.\\]\n의 축약표현으로 정의한다. 예를 들어 “피타고라스 정리 증명”의 괴델수가 \\(m\\)이고 “피타고라스 정리”의 괴델수가 \\(n\\)이라면 dem(\\(m\\), \\(n\\))이라고 쓸 수 있는 것이다. 한편, Dem(\\(x\\), \\(z\\))은 \\(x\\)와 \\(z\\)의 관계 dem을 형식적 표기법으로 표현하는 형식문으로 정의한다.\nSub\nSub은 치환 혹은 대입을 의미하며 소문자로 표현한 sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수 } x \\text{를 가진 문장에 등장하는} \\textbf{ 변수 } y \\textbf{들에 전부 숫자 } x \\textbf{를 대입} \\text{해서 만들어진 명제의 괴델수}\\]\n로 정의한다(17은 \\(y\\)의 괴델수임을 기억하자). 한 편, \\(s\\)를 대문자로 표시한 Sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수가 아닌 }\\textbf{명제 그 자체}\\]\n로 정의한다. 이해를 돕기 위해 \\(2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\) 를 다시 살펴보겠다. (\\(\\exists x\\))(\\(x=sy\\)) 는 앞서 설명했듯이 “\\(y\\) 다음 수가 존재한다” 고 읽을 수 있으며, 그 괴델수를 \\(k\\)라 하면\n\\[k=2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n가 된다. 이제 \\(y\\)대신 \\(k\\)를 대입한다면 수정된 명제는 (\\(\\exists x\\))(\\(x=sss \\cdots sss0\\))이 되고(\\(s\\)는 \\(k+1\\)번 연속됨), \\(s\\)의 괴델수가 7임을 고려하면 \\(k\\)를 대입한 명제의 괴델수는 \\(y\\)부분에 해당되는 \\(23^{17}\\)부터 \\(s\\)로 바뀌어 아래의 식과 같이 된다.\n\\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{7} \\times 29^7 \\times 31^7 \\times 37^7 \\times \\cdots (P_{k+10})^9\\] (\\(P_{k+10}\\): \\(k+10\\)번 째 소수)\n얼핏 보기에 문장의 괴델수를 문장 그 자체에 대입한다는 것이 순환논리같은 느낌을 주는데 이것이 바로 괴델의 핵심적인 아이디어 중 하나이다. 이제부터 본격적인 증명의 내용으로 들어가기로 하자.\n불완전성 정리\n불완전성 정리의 증명은 “괴델수 \\(z\\)를 가진 명제는 증명불가능하다”라는 메타수학의 명제를 수학의 명제로 바꾼 후, 이 명제의 특별한 경우가 증명될 수 없다는 것을 보이는것으로 이루어진다. 이제 다음의 형식문을 살펴보자.\n\\[\\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(y, 17, y))\\]\n이 수학의 형식문은 “Sub(\\(y\\) ,17, \\(y\\)) 의 증명은 존재하지 않는다, 즉 증명불가능하다”는 메타수학적 의미를 갖는다. 이 형식문의 괴델수를 \\(n\\)이라 하고, 형식문에 포함된 변수 \\(y\\)를 숫자 \\(n\\)으로 바꾼 형식문 \\(G\\)를 살펴보자.\n\\[\\text{G}: \\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(n, 17, n))\\]\n\\(G\\)는 변수가 포함되어 있지 않으므로 수학명제이며, 이것의 메타수학적 의미를 살펴보면 “괴델수 sub(\\(n\\), 17, \\(n\\))을 가진 명제는 증명 불가능하다.”이다. \\(G\\)의 괴델수를 \\(g\\)라고 하고 \\(g\\)는 어떤 숫자일지 생각해보자. 놀랍게도 \\(g=\\text{sub}(n, 17, n)\\)임을 알 수 있다. \\(G\\)는 괴델수 \\(n\\)을 가진 형식문에서 \\(y\\)에 숫자 \\(n\\)을 대입하여 만든 명제이므로, 이것의 괴델수 \\(g\\)는 정확히 sub(\\(n\\), 17, \\(n\\))의 정의와 일치하게 된다. 이제 \\(G\\)의 메타수학적 의미를 다시 살펴보면 “괴델 수 \\(g\\)를 가진 명제는 증명 불가능하다.”이고 즉, “\\(G\\)는 증명 불가능하다”는 의미가 된다. 맨 처음으로 돌아가면, \\(G\\)는 참이지만 증명 불가능한 명제가 되어 불완전성의 정리가 증명된다.\n마치며\n필자는 고등학생때부터 괴델의 불완전성정리를 이해해보려고 괴델의 논문원본도 찾아보고 여러 교양서적을 많이 읽어보았었다. 그러나 논문을 다 보는것은 이해하기가 어렵고 교양서적은 겉핥기식으로만 나와있어서 최근까지도 증명의 핵심을 이해하지 못했었는데 출판사 승산에서 나온 괴델의 증명을 읽으면서 한층 이해수준을 올릴 수 있었다. 이 책은 외국서적을 번역한 것으로 번역하면서 생소한 단어와 표현들이 많이나와 읽기가 어려운 부분이 있어 이번기회에 잘 풀어서 설명해볼 목적으로 글을 쓰게 되었다. 본 글이 논문과 교양서적 사이에서 가교 역할을 하여 불완전성 정리를 이해하는데 도움이 되길 기대한다.\n참고문헌\n\n괴델, 에셔, 바흐 : 영원한 황금 노끈 / 지은이: 더글러스 호프스태터 ; 옮긴이: 박여성, 안병서\n(호프스태터가 서문을 쓰고 개정한) 괴델의 증명 / 지은이: 어니스트 네이글, 제임스 뉴먼 ; 옮긴이: 곽강제, 고중숙\n괴델 불완전성 정리 / 지은이: 요시나가 요시마사 ; 옮긴이: 임승원"
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html",
    "href": "posts/2019-08-25-shinymanager/index.html",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 9월 Shinykorea 밋업에 참석, Shiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#요약",
    "href": "posts/2019-08-25-shinymanager/index.html#요약",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "요약",
    "text": "요약\n\nshinymanager 로 UI 종류에 상관없이, 간단하게 로그인기능을 추가한다.\nSQLite db 를 이용, 접속자와 그 log를 관리한다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#slide",
    "href": "posts/2019-08-25-shinymanager/index.html#slide",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/shinymanager 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html",
    "href": "posts/2019-10-25-ruck2019/index.html",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "",
    "text": "김진섭 대표는 10월 25일(금) 광화문 한국마이크로소프트 11층에서 열린 R User Conference in Korea 2019(RUCK 2019) 참석, 맞춤형 의학연구 앱을 만들고 그것을 패키지로 만들어 CRAN에 배포한 경험을 발표하였습니다. 초록과 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#abstract",
    "href": "posts/2019-10-25-ruck2019/index.html#abstract",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Abstract",
    "text": "Abstract\n의학연구자들에게 제공한 맞춤형 ShinyApps 중, 범용으로 쓰일만한 것들을 Shiny module 로 만들고 웹으로 공개하였습니다. 큰 용량의 데이터는 개인 PC에서 직접 다룰 수 있도록 Rstudio Addins 을 포함한 R 패키지 를 만들어 github 에 배포하였습니다. 패키지 관리를 위해 (1) testthat, covr 로 코드 테스트를 수행한 결과 리포트를, (2) pkgdown 으로 패키지를 소개하는 웹사이트를 만들었고, (3) Travis CI 와 appveyor 로 앞의 과정과 여러 운영체제에서의 테스트를 자동화하였습니다. 최종적으로 CRAN 에 패키지를 배포하였고, 1.01 버전까지 업데이트하였습니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#slide",
    "href": "posts/2019-10-25-ruck2019/index.html#slide",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RUCK2019/ 를 클릭하면 볼 수 있으며, Chrome 에 최적화되었습니다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "",
    "text": "김진섭 대표는 11월 7일 동국대학교 의생명공학과 세미나에 참석, 의료 데이터분석가가 되기까지의 경험을 학생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유하며, 초청해주신 김진식 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 의대 진학.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립.\n\nR 과 shiny 이용, 맞춤형 분석웹 제공\n범용 통계분석 웹: http://app.zarathu.com/\nR 패키지 개발"
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/dongguk_mbt 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html",
    "href": "posts/2020-01-25-pgconference2020/index.html",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "",
    "text": "김진섭 대표는 2월 15일(토) 디시인사이드 가 후원하는 프로그래밍 갤러리 컨퍼런스 2020에 참석, R과 shiny로 웹 애플리케이션을 만든 경험을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#요약",
    "href": "posts/2020-01-25-pgconference2020/index.html#요약",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\n\n모임 소개\n\nshinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#slide",
    "href": "posts/2020-01-25-pgconference2020/index.html#slide",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/pgconference2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 1주차 강의록을 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다. 본 블로그에 정리 내용이 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 첫 번째에 해당하며 2주차에 tidyverse 를 다룰 것이다. data.table 은 이번 교육에는 포함시키지 않았는데, R에 익숙해지면서 느린 속도가 점점 거슬린다면 data.table 을 시작할 때이다.\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nproject 생성\n\n\n\n개인 PC에서 실습을 원한다면 http://www.r-project.org 와 https://rstudio.com/products/rstudio/download/#download 에서 R과 RStudio 를 설치하자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "\nR 기초연산 : 벡터(vector)",
    "text": "R 기초연산 : 벡터(vector)\nR 의 기본 연산단위는 벡터이며, x &lt;- c(1, 2, 3) 은 1,2,3 으로 이루어진 길이 3인 벡터를 x 에 저장한다. 대입연산자는 = 와 &lt;- 둘 다 가능하지만 함수의 인자로도 쓰이는 = 와 구별하기 위해 &lt;- 를 권장한다. 자주 쓰는 연산을 실습하자.\n\nx &lt;- c(1, 2, 3, 4, 5, 6)            ## vector of variable\ny &lt;- c(7, 8, 9, 10, 11, 12)\nx + y                                  \n\n[1]  8 10 12 14 16 18\n\nx * y\n\n[1]  7 16 27 40 55 72\n\nsqrt(x)                            ## root\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490\n\nsum(x)                                \n\n[1] 21\n\ndiff(x)                            ## difference\n\n[1] 1 1 1 1 1\n\nmean(x)                            ## mean  \n\n[1] 3.5\n\nvar(x)                             ## variance\n\n[1] 3.5\n\nsd(x)                              ## standard deviation\n\n[1] 1.870829\n\nmedian(x)                          ## median\n\n[1] 3.5\n\nIQR(x)                             ## inter-quantile range\n\n[1] 2.5\n\nmax(x)                             ## max value\n\n[1] 6\n\nwhich.max(x)                       ## order of max value\n\n[1] 6\n\nmax(x, y)                          ## max value among x & y\n\n[1] 12\n\nlength(x)                          \n\n[1] 6\n\n\nmax(x, y) 는 x, y 각각의 최대값이 아닌, 전체에서 최대인 값 1개를 보여줌을 기억하자. 잠시 후 각각의 최대값 구하는 연습문제가 나온다.\n벡터에서 특정 항목을 골라내려면 그것의 위치 혹은 조건문을 이용한다.\n\nx[2]                               ## 2 번째\n\n[1] 2\n\nx[-2]                              ## 2 번째만 빼고\n\n[1] 1 3 4 5 6\n\nx[1:3]                             ## 1-3 번째\n\n[1] 1 2 3\n\nx[c(1, 2, 3)]                      ## 동일 \n\n[1] 1 2 3\n\nx[c(1, 3, 4, 5, 6)]                ## 1, 3, 4, 5, 6  번째\n\n[1] 1 3 4 5 6\n\nx &gt;= 4                             ## 각 항목이 4 이상인지 TRUE/FALSE\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\nsum(x &gt;= 4)                        ## TRUE 1, FALSE 0 인식 \n\n[1] 3\n\nx[x &gt;= 4]                          ## TRUE 인 것들만, 즉 4 이상인 것들         \n\n[1] 4 5 6\n\nsum(x[x &gt;= 4])                     ## 4 이상인 것들만 더하기. \n\n[1] 15\n\nx %in% c(1, 3, 5)                  ## 1, 3, 5 중 하나에 속하는지 TRUE/FALSE\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nx[x %in% c(1, 3, 5)]               \n\n[1] 1 3 5\n\n\n벡터만들기\nseq 로 일정 간격인, rep 로 항목들이 반복되는 벡터를 만들 수 있다.\n\nv1 &lt;- seq(-5, 5, by = .2); v1             ## Sequence\n\n [1] -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2\n[16] -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2  0.0  0.2  0.4  0.6  0.8\n[31]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[46]  4.0  4.2  4.4  4.6  4.8  5.0\n\nv2 &lt;- rep(1, 3); v2                       ## Repeat\n\n[1] 1 1 1\n\nv3 &lt;- rep(c(1, 2, 3), 2); v3              ## Repeat for vector\n\n[1] 1 2 3 1 2 3\n\nv4 &lt;- rep(c(1, 2, 3), each = 2); v4       ## Repeat for vector : each\n\n[1] 1 1 2 2 3 3\n\n\n\nfor, if/else, ifelse 문\nfor loop 는 같은 작업을 반복할 때 이용하며 while 도 비슷한 의미이다. 예시를 통해 배워보자.\n\nfor (i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\ni &lt;- 0\nfor (j in c(1, 2, 4, 5, 6)){\n  i &lt;- i + j\n}\ni\n\n[1] 18\n\n\nif 와 else 는 조건문을 다룬다. else 나 else if 문은 선행 조건문의 마지막과 같은 줄이어야 함을 기억하자.\n\nx &lt;- 5\nif (x &gt;= 3 ){\n  x &lt;- x + 3\n}\nx\n\n[1] 8\n\nx &lt;- 5\nif (x &gt;= 10){\n  print(\"High\")\n} else if (x &gt;= 5){\n  print(\"Medium\")\n} else {\n  print(\"Low\")\n}                                          ## else if, else 주의: 반드시 } 와 같은 줄에 위치하도록.\n\n[1] \"Medium\"\n\n\nifelse 는 벡터화된 if/else 문으로 벡터의 각 항목마다 조건문을 적용하는데, 엑셀의 if 문과 비슷하다.\n\nx &lt;- 1:6\ny &lt;- ifelse(x &gt;= 4, \"Yes\", \"No\")           ## ifelse (조건,참일때,거짓일때)\ny\n\n[1] \"No\"  \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\"\n\n\n함수 만들기\n막 R을 배우는 단계에서는 함수를 만들어 쓸 일이 거의 없겠지만, 결측치 포함된 데이터에서 평균이나 분산을 구할 때 귀찮을 수 있다. R은 결측치가 하나라도 포함되면 평균값, 분산값으로 NA를 출력하기 때문이다. 이를 해결하기 위해서라도 아래처럼 기초 함수 만드는 법은 알고 있는 것이 좋다.\n\nx &lt;- c(1:10, 12, 13, NA, NA, 15, 17)      ## 결측치가 포함되어 있다면..\nmean(x)                                           \n\n[1] NA\n\nmean0 &lt;- function(x){\n  mean(x, na.rm = T)\n}                                         ## mean함수의 na.rm 옵션을 TRUE로 바꿈. default는 F\n\nmean0 &lt;- function(x){mean(x, na.rm = T)}  ## 한줄에 쓸 수도 있다. \nmean0(x)\n\n[1] 8\n\n\n둘 이상의 변수를 포함한 함수도 다음과 같이 만들 수 있다.\n\ntwomean &lt;- function(x1, x2){\n  a &lt;- (x1 + x2)/2\n  a\n}\ntwomean(4, 6)\n\n[1] 5\n\n\nApply 문 : apply, sapply, lapply\n\n벡터를 다루는 연산을 잘 활용하면, 벡터의 각 항목에 대해 for loop 을 쓰는 것보다 간편하게 코드를 작성할 수 있다. 행렬에서 행마다 평균을 구하는 예를 살펴보자.\n\nmat &lt;- matrix(1:20, nrow = 4, byrow = T)   ## 4행 5열, byrow = T : 행부터 채운다. \nmat\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\n\n모든 행에 대해 for loop 을 이용, 평균을 구하여 저장하는 코드는 아래와 같다.\n\nout &lt;- NULL                                ## 빈 벡터, 여기에 하나씩 붙여넣는다.\nfor (i in 1:nrow(mat)){\n  out &lt;- c(out, mean(mat[i, ]))\n}\nout\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고 여기에 결과를 붙여가는 모습이 번거로워 보인다. sapply 또는 lapply 를 사용하면 행 또는 열 단위 연산을 간단히 수행할 수 있다.\n\nsapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return vector\n\n[1]  3  8 13 18\n\nlapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return list type\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 18\n\nunlist(lapply(1:nrow(mat), function(x){mean(mat[x, ])}))     ## Same to sapply\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고, 이어붙이는 과정이 생략되어 간단한 코드가 되었다. list 는 벡터보다 상위개념으로 모든 것을 담을 수 있는 큰 그릇에 비유할 수 있는데, 본 강의에서는 unlist 를 취하면 벡터나 행렬을 얻게 된다는 정도만 언급하고 넘어가겠다. 사실 행렬의 행/열 단위 연산은 apply 혹은 row***, col*** 시리즈의 함수가 따로 있어, 더 간단히 이용할 수 있다.\n\napply(mat, 1, mean)                                          ## 1: 행\n\n[1]  3  8 13 18\n\nrowMeans(mat)                                                ## 동일\n\n[1]  3  8 13 18\n\nrowSums(mat)                                                 ## 행별로 합\n\n[1] 15 40 65 90\n\napply(mat, 2, mean)                                          ## 2: 열\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\ncolMeans(mat)                                                ## 열별로 합\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\n연습문제 1\nsapply나 lapply를 이용하여, 아래 두 벡터의 최대값을 각각 구하여라.\n\nx &lt;- 1:6\ny &lt;- 7:12\n\n\n정답 보기\n\nlapply(list(x, y), max)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 12\n\n  sapply(list(x, y), max)\n\n[1]  6 12\n\n\n\n멀티코어 병렬연산으로 apply 를 빠르게 수행할 수도 있는데 본 강의에서는 생략한다. 궁금하신 분은 과거 정리 내용 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "데이터 불러와서 작업하기",
    "text": "데이터 불러와서 작업하기\n이제부터는 실제 데이터를 읽어서 그 데이터를 매니징 하는 방법을 배워보도록 하겠다.\n데이터 불러오기, 저장하기\n데이터를 불러오기 전에 미리 디렉토리를 지정하면 그 다음부터는 편하게 읽고 쓸 수 있다.\n\ngetwd()                                                     ## 현재 디렉토리 \nsetwd(\"data\")                                               ## 디렉토리 설정\n## 동일\nsetwd(\"/home/js/Homepage/blog/_posts/2020-02-16-rdatamanagement-basic/data\")\ngetwd()\n\n폴더 구분을 / 로 해야 한다는 점을 명심하자 (\\\\ 도 가능). R 은 유닉스 기반이기 때문이다. 이제 실습 데이터를 읽어볼텐데, 가급적이면 데이터 포맷은 csv로 만드는 것을 추천한다. 콤마로 분리된 가장 간단한 형태로, 용량도 작고 어떤 소프트웨어 에서도 읽을 수 있기 때문이다. 물론 Excel, SPSS, SAS 파일도 읽을 수 있는데, 변수명이나 값에 한글이 있으면 encoding 에러가 생길 수 있으므로 미리 처리하자.\n\nex &lt;- read.csv(\"example_g1e.csv\")\nhead(ex)\n\nURL 링크를 이용할 수도 있다.\n\nex &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n\n\n\n  \n\n\n\nExcel 파일은 readxl, SAS나 SPSS는 haven 패키지를 이용한다.\n\n#install.packages(c(\"readxl\", \"haven\"))                    ## install packages    \nlibrary(readxl)                                            ## for xlsx\nex.excel &lt;- read_excel(\"example_g1e.xlsx\", sheet = 1)      ## 1st sheet\n\nlibrary(haven)                                             ## for SAS/SPSS/STATA   \nex.sas &lt;- read_sas(\"example_g1e.sas7bdat\")                 ## SAS\nex.spss &lt;- read_sav(\"example_g1e.sav\")                     ## SPSS\nhead(ex.spss)\n\n아래와 같이 Excel, SAS, SPSS 데이터는 read.csv 와 형태가 좀 달라보인다. 이것은 최근 R에서 인기있는 tidyverse 스타일의 데이터인데, 자세한 내용은 다음 강의에서 다룰 예정이니 일단 넘어가자.\n\n\n\n  \n\n\n\n파일 저장은 write.csv 를 이용하며, 맨 왼쪽에 나타나는 행 넘버링을 빼려면 row.names = F 옵션을 추가한다.\n\nwrite.csv(ex, \"example_g1e_ex.csv\", row.names = F)\n\nhaven 패키지에서 write_sas 나 write_sav 도 가능하다.\n\nwrite_sas(ex.sas, \"example_g1e_ex.sas7bdat\")\nwrite_sav(ex.spss, \"example_g1e_ex.sav\")"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "읽은 데이터 살펴보기",
    "text": "읽은 데이터 살펴보기\n본격적으로 데이터를 살펴보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n데이터 살펴보기\nhead 로 처음 6줄, tail 로 마지막 6줄을 볼 수 있다. 데이터 간단히 확인하려고 쓰인다.\n\nhead(ex)                                                   ## 처음 6행\ntail(ex)                                                   ## 마지막 6행\nhead(ex, 10)                                               ## 처음 10행\n\n\n\n\n  \n\n\n\nstr 은 head 와는 다른 방식으로 데이터를 확인한다. int 는 정수, num 은 실수형을 의미한다.\n\nstr(ex)\n\n'data.frame':   1644 obs. of  32 variables:\n $ EXMD_BZ_YYYY  : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...\n $ RN_INDI       : int  562083 334536 911867 183321 942671 979358 554112 487160 793017 219397 ...\n $ HME_YYYYMM    : int  200909 200911 200903 200908 200909 200912 200911 200908 200906 200912 ...\n $ Q_PHX_DX_STK  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTDZ : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTN  : int  1 0 0 NA NA NA NA NA NA 1 ...\n $ Q_PHX_DX_DM   : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_DLD  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_PTB  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q_HBV_AG      : int  3 2 3 3 3 2 2 3 3 3 ...\n $ Q_SMK_YN      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Q_DRK_FRQ_V09N: int  0 0 0 0 0 0 0 0 0 0 ...\n $ HGHT          : int  144 162 163 152 159 157 160 159 156 146 ...\n $ WGHT          : int  61 51 65 51 50 55 56 54 53 48 ...\n $ WSTC          : int  90 63 82 70 73 73 67 66 67 78 ...\n $ BMI           : num  29.4 19.4 24.5 22.1 19.8 22.3 21.9 21.4 21.8 22.5 ...\n $ VA_LT         : num  0.7 0.8 0.7 0.8 0.7 1.5 1.5 1.2 1.2 1.5 ...\n $ VA_RT         : num  0.8 1 0.6 0.9 0.8 1.5 1.5 1.5 1 1.5 ...\n $ BP_SYS        : int  120 120 130 101 132 110 119 111 138 138 ...\n $ BP_DIA        : int  80 80 80 62 78 70 78 60 72 84 ...\n $ URN_PROT      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ HGB           : num  12.6 13.8 15 13.1 13 11.9 11.2 12.2 11 12.8 ...\n $ FBS           : int  117 96 118 90 92 100 84 88 74 107 ...\n $ TOT_CHOL      : int  264 169 216 199 162 192 152 166 155 178 ...\n $ TG            : int  128 92 132 100 58 109 38 42 86 87 ...\n $ HDL           : int  60 70 55 65 40 53 43 58 52 35 ...\n $ LDL           : int  179 80 134 114 111 117 101 99 85 125 ...\n $ CRTN          : num  0.9 0.9 0.8 0.9 0.9 0.7 0.8 1 0.6 0.7 ...\n $ SGOT          : int  25 18 26 18 24 15 8 16 15 21 ...\n $ SGPT          : int  20 15 30 14 23 12 6 11 13 21 ...\n $ GGT           : int  25 28 30 11 15 14 10 12 13 23 ...\n $ GFR           : int  59 74 79 61 49 83 97 65 96 70 ...\n\n\nnames 로 변수들 이름을 확인할 수 있다. 공백이나 특수문자는 “.” 로 바뀌고, 이름이 같은 변수들은 뒤에 숫자가 추가되어 구별된다. read.csv(..., check.names = F) 옵션으로 원래 이름을 유지할 수 있으나 에러의 원인이 되므로 추천하지 않는다.\n\nnames(ex)\n\n [1] \"EXMD_BZ_YYYY\"   \"RN_INDI\"        \"HME_YYYYMM\"     \"Q_PHX_DX_STK\"  \n [5] \"Q_PHX_DX_HTDZ\"  \"Q_PHX_DX_HTN\"   \"Q_PHX_DX_DM\"    \"Q_PHX_DX_DLD\"  \n [9] \"Q_PHX_DX_PTB\"   \"Q_HBV_AG\"       \"Q_SMK_YN\"       \"Q_DRK_FRQ_V09N\"\n[13] \"HGHT\"           \"WGHT\"           \"WSTC\"           \"BMI\"           \n[17] \"VA_LT\"          \"VA_RT\"          \"BP_SYS\"         \"BP_DIA\"        \n[21] \"URN_PROT\"       \"HGB\"            \"FBS\"            \"TOT_CHOL\"      \n[25] \"TG\"             \"HDL\"            \"LDL\"            \"CRTN\"          \n[29] \"SGOT\"           \"SGPT\"           \"GGT\"            \"GFR\"           \n\n\n샘플수, 변수 갯수는 dim, nrow, ncol 로 확인한다.\n\ndim(ex)                                                    ## row, column\n\n[1] 1644   32\n\nnrow(ex)                                                   ## row\n\n[1] 1644\n\nncol(ex)                                                   ## column\n\n[1] 32\n\n\n클래스는 class로 확인한다. read.csv 는 data.frame, Excel/SAS/SPSS 는 tibble & `data.frame 인데, data.frame 은 행렬이면서 데이터에 특화된 list, tibble 은 앞서 언급했던 tidyverse 스타일의 data.frame 인 정도만 알고 넘어가자.\n\nclass(ex)\n\n[1] \"data.frame\"\n\nclass(ex.spss)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsummary 로 모든 변수들의 평균, 중위수, 결측치 등을 한 번에 확인할 수 있다. R은 결측치를 NA 로 표시하며, 안타깝지만 분산은 나오지 않는다.\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI          HME_YYYYMM      Q_PHX_DX_STK   \n Min.   :2009   Min.   :   2270   Min.   :200901   Min.   :0.0000  \n 1st Qu.:2010   1st Qu.: 230726   1st Qu.:201011   1st Qu.:0.0000  \n Median :2012   Median : 487160   Median :201210   Median :0.0000  \n Mean   :2012   Mean   : 490782   Mean   :201216   Mean   :0.0112  \n 3rd Qu.:2014   3rd Qu.: 726101   3rd Qu.:201406   3rd Qu.:0.0000  \n Max.   :2015   Max.   :1010623   Max.   :201512   Max.   :1.0000  \n                                                   NA's   :573     \n Q_PHX_DX_HTDZ     Q_PHX_DX_HTN   Q_PHX_DX_DM      Q_PHX_DX_DLD   \n Min.   :0.0000   Min.   :0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.00   Median :0.0000   Median :0.0000  \n Mean   :0.0241   Mean   :0.25   Mean   :0.0693   Mean   :0.0399  \n 3rd Qu.:0.0000   3rd Qu.:0.25   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00   Max.   :1.0000   Max.   :1.0000  \n NA's   :566      NA's   :492    NA's   :547      NA's   :566     \n  Q_PHX_DX_PTB       Q_HBV_AG        Q_SMK_YN     Q_DRK_FRQ_V09N \n Min.   :0.0000   Min.   :1.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:0.000  \n Median :0.0000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :0.0276   Mean   :2.235   Mean   :1.632   Mean   :1.026  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :1.0000   Max.   :3.000   Max.   :3.000   Max.   :7.000  \n NA's   :703      NA's   :2       NA's   :2       NA's   :6      \n      HGHT            WGHT            WSTC             BMI       \n Min.   :134.0   Min.   : 31.0   Min.   : 57.00   Min.   :12.30  \n 1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00   1st Qu.:21.50  \n Median :165.0   Median : 64.0   Median : 81.00   Median :23.70  \n Mean   :164.5   Mean   : 65.1   Mean   : 80.69   Mean   :23.92  \n 3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00   3rd Qu.:26.20  \n Max.   :188.0   Max.   :118.0   Max.   :114.00   Max.   :37.20  \n                                                                 \n     VA_LT           VA_RT            BP_SYS          BP_DIA     \n Min.   :0.100   Min.   :0.1000   Min.   : 81.0   Min.   : 49.0  \n 1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0   1st Qu.: 70.0  \n Median :1.000   Median :1.0000   Median :120.0   Median : 78.0  \n Mean   :0.984   Mean   :0.9792   Mean   :122.3   Mean   : 76.6  \n 3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0   3rd Qu.: 82.0  \n Max.   :9.900   Max.   :9.9000   Max.   :180.0   Max.   :120.0  \n                                                                 \n    URN_PROT          HGB             FBS            TOT_CHOL    \n Min.   :1.000   Min.   : 5.90   Min.   : 61.00   Min.   : 68.0  \n 1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00   1st Qu.:170.0  \n Median :1.000   Median :14.10   Median : 94.00   Median :193.0  \n Mean   :1.078   Mean   :14.11   Mean   : 97.23   Mean   :194.9  \n 3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00   3rd Qu.:218.0  \n Max.   :5.000   Max.   :18.30   Max.   :290.00   Max.   :363.0  \n NA's   :4                                                       \n       TG              HDL             LDL              CRTN        \n Min.   :  13.0   Min.   : 23.0   Min.   :  19.0   Min.   : 0.4000  \n 1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0   1st Qu.: 0.8000  \n Median : 106.0   Median : 54.0   Median : 112.0   Median : 0.9000  \n Mean   : 134.9   Mean   : 55.9   Mean   : 118.7   Mean   : 0.9891  \n 3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0   3rd Qu.: 1.0000  \n Max.   :1210.0   Max.   :593.0   Max.   :8100.0   Max.   :16.5000  \n                                  NA's   :16                        \n      SGOT            SGPT             GGT              GFR        \n Min.   :  6.0   Min.   :  3.00   Min.   :  6.00   Min.   :  3.00  \n 1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00   1st Qu.: 76.00  \n Median : 23.0   Median : 20.00   Median : 24.50   Median : 87.00  \n Mean   : 25.6   Mean   : 25.98   Mean   : 36.34   Mean   : 89.74  \n 3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00   3rd Qu.:101.00  \n Max.   :459.0   Max.   :779.00   Max.   :408.00   Max.   :196.00  \n                                                   NA's   :467     \n\n\n특정 변수 보기\ndata.frame 에서 특정변수는 $ 를 이용, 데이터이름$변수이름 로 확인할 수 있다. 앞서 언급했듯이 data.frame 은 행렬과 list의 성질도 갖고 있어 해당 스타일로도 가능하다.\n\nex$EXMD_BZ_YYYY                                            ## data.frame style\nex[, \"EXMD_BZ_YYYY\"]                                       ## matrix style\nex[[\"EXMD_BZ_YYYY\"]]                                       ## list style\nex[, 1]                                                    ## matrix style with order\nex[[1]]                                                    ## list style with order\n\n2개 이상 변수선택은 행렬 스타일을 이용한다.\n\nex[, c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"BMI\")]                  ## matrix syle with names\nex[, c(1, 2, 16)]                                          ## matrix syle with names\nex[, names(ex)[c(1, 2, 16)]]                               ## same\n\n\n\n\n  \n\n\n\n특정 변수는 벡터형태로 나타나므로 처음에 다루었던 벡터다루기를 그대로 활용할 수 있다. 예를 들어 년도 변수인 EXMD_BZ_YYYY의 첫 50개만 확인하면 아래와 같다.\n\nex$EXMD_BZ_YYYY[1:50]                                      ## data.frame style\nex[1:50, 1]                                                ## matrix style\nex[[1]][1:50]                                              ## list style\n\n\n\n [1] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[16] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[31] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[46] 2009 2009 2009 2009 2009\n\n\nunique 로 변수가 어떤 값들로 이루어져 있는지, table 로 해당 값들이 몇개씩 있는지 확인한다.\n\nunique(ex$EXMD_BZ_YYYY)                                   ## unique value\n\n[1] 2009 2010 2011 2012 2013 2014 2015\n\nlength(unique(ex$EXMD_BZ_YYYY))                           ## number of unique value\n\n[1] 7\n\ntable(ex$EXMD_BZ_YYYY)                                    ## table\n\n\n2009 2010 2011 2012 2013 2014 2015 \n 214  236  223  234  243  254  240 \n\n\n새로운 변수 만들기\n연속형 변수인 BMI 에서 원하는 조건에 맞는 정보를 뽑아내는 연습을 해 보자.\n\nmean(ex$BMI)                                              ## mean\n\n[1] 23.92257\n\nBMI_cat &lt;- (ex$BMI &gt;= 25)                                 ## TRUE of FALSE\ntable(BMI_cat)                         \n\nBMI_cat\nFALSE  TRUE \n 1077   567 \n\nrows &lt;- which(ex$BMI &gt;= 25)                               ## row numbers\nhead(rows)                                      \n\n[1]  1 14 18 21 23 24\n\nvalues &lt;- ex$BMI[ex$BMI &gt;= 25]                            ## values\nhead(values)\n\n[1] 29.4 27.5 27.7 28.0 30.7 25.6\n\nlength(values)\n\n[1] 567\n\nBMI_HGHT_and &lt;- (ex$BMI &gt;= 25 & ex$HGHT &gt;= 175)              ## and\nBMI_HGHT_or &lt;- (ex$BMI &gt;= 25 | ex$HGHT &gt;= 175)               ## or\n\n데이터에 새로운 변수로 추가하는 방법은 간단하다.\n\nex$zero &lt;- 0                                              ## variable with 0\nex$BMI_cat &lt;- (ex$BMI &gt;= 25)                              ## T/F\nex$BMI_cat &lt;- as.integer(ex$BMI &gt;= 25)                    ## 0, 1\nex$BMI_cat &lt;- as.character(ex$BMI &gt;= 25)                  ## \"0\", \"1\"\nex$BMI_cat &lt;- ifelse(ex$BMI &gt;= 25, \"1\", \"0\")              ## same\ntable(ex$BMI_cat)\n\n\n   0    1 \n1077  567 \n\nex[, \"BMI_cat\"] &lt;- (ex$BMI &gt;= 25)                         ## matrix style\nex[[\"BMI_cat\"]] &lt;- (ex$BMI &gt;= 25)                         ## list style\n\n변수 클래스 설정: 데이터 읽은 후 가장 먼저 해야할 것.\n앞서 데이터의 클래스가 data.frame 임을 언급했었는데, 각 변수들도 자신의 클래스를 갖으며 대표적인 것이 숫자형(numeric), 문자형(character), 팩터(factor) 이다. 그 외 T/F 로 나타내는 논리(logical), 날짜를 나타내는 Date 클래스가 있다. 숫자는 integer(정수), numeric(실수) 이 있는데, 전부 실수형(numeric)으로 해도 상관없어 설명은 생략한다. 범주형은 character 와 factor 두 종류가 있는데, 전자는 단순 문자인 반면 후자는 레벨(level) 이 있어 reference 나 순서를 설정할 수 있다. read.csv 로 읽으면 숫자는 int/num, 문자는 전부 factor 가 기본값이므로, 숫자 변수 중 0/1 같은 것들은 직접 factor 로 바꿔줘야 한다. ID와 설문조사 변수를 범주형으로 바꿔보자.\n\nvars.cat &lt;- c(\"RN_INDI\", \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_PHX_DX_HTN\", \"Q_PHX_DX_DM\", \"Q_PHX_DX_DLD\", \"Q_PHX_DX_PTB\", \n              \"Q_HBV_AG\", \"Q_SMK_YN\", \"Q_DRK_FRQ_V09N\")\nvars.cat &lt;- names(ex)[c(2, 4:12)]                              ## same\nvars.cat &lt;- c(\"RN_INDI\", grep(\"Q_\", names(ex), value = T))     ## same: extract variables starting with \"Q_\"\n\nvars.conti &lt;- setdiff(names(ex), vars.cat)                     ## Exclude categorical variables\nvars.conti &lt;- names(ex)[!(names(ex) %in% vars.cat)]            ## same: !- not, %in%- including\n\nfor (vn in vars.cat){                                          ## for loop: as.factor\n  ex[, vn] &lt;- as.factor(ex[, vn])\n}\n\nfor (vn in vars.conti){                                        ## for loop: as.numeric\n  ex[, vn] &lt;- as.numeric(ex[, vn])\n}\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI       HME_YYYYMM     Q_PHX_DX_STK Q_PHX_DX_HTDZ\n Min.   :2009   4263   :   7   Min.   :200901   0   :1059    0   :1052    \n 1st Qu.:2010   38967  :   7   1st Qu.:201011   1   :  12    1   :  26    \n Median :2012   56250  :   7   Median :201210   NA's: 573    NA's: 566    \n Mean   :2012   84322  :   7   Mean   :201216                             \n 3rd Qu.:2014   99917  :   7   3rd Qu.:201406                             \n Max.   :2015   115809 :   7   Max.   :201512                             \n                (Other):1602                                              \n Q_PHX_DX_HTN Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG    Q_SMK_YN  \n 0   :864     0   :1021   0   :1035    0   :915     1   :  77   1   :995  \n 1   :288     1   :  76   1   :  43    1   : 26     2   :1102   2   :256  \n NA's:492     NA's: 547   NA's: 566    NA's:703     3   : 463   3   :391  \n                                                    NA's:   2   NA's:  2  \n                                                                          \n                                                                          \n                                                                          \n Q_DRK_FRQ_V09N      HGHT            WGHT            WSTC       \n 0      :805    Min.   :134.0   Min.   : 31.0   Min.   : 57.00  \n 1      :379    1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00  \n 2      :249    Median :165.0   Median : 64.0   Median : 81.00  \n 3      :121    Mean   :164.5   Mean   : 65.1   Mean   : 80.69  \n 4      : 28    3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00  \n (Other): 56    Max.   :188.0   Max.   :118.0   Max.   :114.00  \n NA's   :  6                                                    \n      BMI            VA_LT           VA_RT            BP_SYS     \n Min.   :12.30   Min.   :0.100   Min.   :0.1000   Min.   : 81.0  \n 1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0  \n Median :23.70   Median :1.000   Median :1.0000   Median :120.0  \n Mean   :23.92   Mean   :0.984   Mean   :0.9792   Mean   :122.3  \n 3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0  \n Max.   :37.20   Max.   :9.900   Max.   :9.9000   Max.   :180.0  \n                                                                 \n     BP_DIA         URN_PROT          HGB             FBS        \n Min.   : 49.0   Min.   :1.000   Min.   : 5.90   Min.   : 61.00  \n 1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00  \n Median : 78.0   Median :1.000   Median :14.10   Median : 94.00  \n Mean   : 76.6   Mean   :1.078   Mean   :14.11   Mean   : 97.23  \n 3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00  \n Max.   :120.0   Max.   :5.000   Max.   :18.30   Max.   :290.00  \n                 NA's   :4                                       \n    TOT_CHOL           TG              HDL             LDL        \n Min.   : 68.0   Min.   :  13.0   Min.   : 23.0   Min.   :  19.0  \n 1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0  \n Median :193.0   Median : 106.0   Median : 54.0   Median : 112.0  \n Mean   :194.9   Mean   : 134.9   Mean   : 55.9   Mean   : 118.7  \n 3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0  \n Max.   :363.0   Max.   :1210.0   Max.   :593.0   Max.   :8100.0  \n                                                  NA's   :16      \n      CRTN              SGOT            SGPT             GGT        \n Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00   Min.   :  6.00  \n 1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00  \n Median : 0.9000   Median : 23.0   Median : 20.00   Median : 24.50  \n Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98   Mean   : 36.34  \n 3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00  \n Max.   :16.5000   Max.   :459.0   Max.   :779.00   Max.   :408.00  \n                                                                    \n      GFR              zero      BMI_cat      \n Min.   :  3.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 76.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 87.00   Median :0   Median :0.0000  \n Mean   : 89.74   Mean   :0   Mean   :0.3449  \n 3rd Qu.:101.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :196.00   Max.   :0   Max.   :1.0000  \n NA's   :467                                  \n\n\nsummary 를 보면 설문조사 변수들이 처음과 달리 빈도로 요약됨을 알 수 있다. 한 가지 주의할 점은 factor 를 numeric 으로 바로 바꾸면 안된다는 것이다. 방금 factor 로 바꾼 Q_PHX_DX_STK 를 numeric 으로 바꿔서 테이블로 요약하면, 원래의 0/1 이 아닌 1/2로 바뀐다.\n\ntable(\n  as.numeric(ex$Q_PHX_DX_STK)\n  )\n\n\n   1    2 \n1059   12 \n\n\nfactor를 바로 바꾸면 원래 값이 아닌, factor에 내장된 레벨(순서값) 로 바뀌기 때문이다. 제대로 바꾸려면 아래처럼 character 로 먼저 바꿔준 후 숫자형을 적용해야 한다.\n\ntable(\n  as.numeric(as.character(ex$Q_PHX_DX_STK))\n  )\n\n\n   0    1 \n1059   12 \n\n\n마지막으로 Date 클래스를 살펴보자. 검진년월 변수인 HME_YYYYMM 를 Date 로 바꿔 볼텐데, Date는 년/월/일 이 모두 필요하므로 일은 1로 통일하고 paste 로 붙이겠다.\n\naddDate &lt;- paste(ex$HME_YYYYMM, \"01\", sep = \"\")                ## add day- use `paste`\nex$HME_YYYYMM &lt;- as.Date(addDate, format = \"%Y%m%d\")           ## set format                  \nhead(ex$HME_YYYYMM)\n\n[1] \"2009-09-01\" \"2009-11-01\" \"2009-03-01\" \"2009-08-01\" \"2009-09-01\"\n[6] \"2009-12-01\"\n\nclass(ex$HME_YYYYMM)\n\n[1] \"Date\"\n\n\n결측치 다루기\n변수 클래스만큼 중요한 것이 결측치 처리이다. 앞서 “함수만들기” 에서 봤듯이 결측치가 있으면 평균같은 기본적인 계산도 na.rm = T 옵션이 필요하다. 결측치가 있는 LDL 변수의 평균을 연도별로 구해보자. 그룹별 통계는 tapply 를 이용한다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, mean)                          ## measure/group/function\n\n\n\n\n\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n\n\n150.9486\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n2009년만 결측치가 없고, 나머지는 결측치가 있어 평균값이 NA 로 나온다.na.rm = T 옵션으로 결측치를 제외하면 원하는 결과를 얻는다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, \n       function(x){\n         mean(x, na.rm = T)\n         })    \n\n    2009     2010     2011     2012     2013     2014     2015 \n150.9486 112.9914 112.9450 117.5259 111.1577 116.5455 111.5294 \n\n\n더 큰 문제는, 대부분의 R 통계분석이 결측치를 갖는 샘플을 분석에서 제외한다는 점이다. 그래서 결측치를 신경쓰지 않고 분석하다보면, 원래 샘플 수와 분석에 이용된 샘플 수가 달라지는 문제가 생길 수 있다. LDL과 HDL 의 회귀분석 결과를 예로 살펴보자.\n\nsummary(lm(LDL ~ HDL, data = ex))\n\n\nCall:\nlm(formula = LDL ~ HDL, data = ex)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-103.8  -28.2   -6.6   15.4 7974.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 138.2747    15.2318   9.078   &lt;2e-16 ***\nHDL          -0.3499     0.2570  -1.362    0.174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201.9 on 1626 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.001139,  Adjusted R-squared:  0.0005244 \nF-statistic: 1.854 on 1 and 1626 DF,  p-value: 0.1735\n\n\n“16 observations deleted due to missingness” 라는 글자가 보일 것이다. LDL 이 결측인 16명은 분석에서 제외했다는 뜻이다.\n연습문제 2: 결측치 처리\n결측치를 처리하는 제일 간단한 방법은 “하나라도 결측치 있는 샘플은 제외” 로, na.omit 함수를 이용하면 된다.\n\nex.naomit &lt;- na.omit(ex)\nnrow(ex.naomit)\n\n[1] 620\n\n\n1644 명에서 620 명으로 샘플 수가 줄어든 것을 확인할 수 있다. 필자는 보통 결측치 처리에 다음의 3가지 원칙을 적용한다.\n\n결측치 너무 많으면(예: 10% 이상) 그 변수는 삭제\n연속변수는 중간값(median)\n범주형변수는 최빈값(mode)\n\n이제 문제이다. 아까 변수형을 정리한 ex 데이터에 위 3가지 원칙을 적용, 새로운 데이터 ex.impute 을 만들어 보아라. 단 최빈값 함수는 아래와 같이 getmode 로 주어진다.\n\ngetmode &lt;- function(v){\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ngetmode(ex$Q_PHX_DX_STK)\n\n[1] 0\nLevels: 0 1\n\n\n\n정답 보기\n\nvars.ok &lt;- sapply(names(ex), function(v){sum(is.na(ex[, v])) &lt; nrow(ex)/10})\nex.impute &lt;- ex[, vars.ok]                                     ## only missing &lt; 10%\n\nfor (v in names(ex.impute)){\n  if (is.factor(ex.impute[, v])){                              ## or class(ex[, v]) == \"factor\"\n    ex.impute[, v] &lt;- ifelse(is.na(ex.impute[, v]), \n                             getmode(ex.impute[, v]), \n                             ex.impute[, v])\n  } else if (is.numeric(ex[, v])){                             ## or class(ex[, v]) %in% c(\"integer\", \"numeric\")\n    ex.impute[, v] &lt;- ifelse(is.na(ex.impute[, v]), \n                             median(ex.impute[, v], na.rm = T), \n                             ex.impute[, v])\n  } else{                                                      ## when date\n    ex.impute[, v]\n  }\n}\n\nsummary(ex.impute)\n\n  EXMD_BZ_YYYY     RN_INDI        HME_YYYYMM            Q_HBV_AG    \n Min.   :2009   Min.   :  1.0   Min.   :2009-01-01   Min.   :1.000  \n 1st Qu.:2010   1st Qu.:133.8   1st Qu.:2010-11-01   1st Qu.:2.000  \n Median :2012   Median :275.0   Median :2012-10-01   Median :2.000  \n Mean   :2012   Mean   :272.7   Mean   :2012-08-31   Mean   :2.235  \n 3rd Qu.:2014   3rd Qu.:405.2   3rd Qu.:2014-06-01   3rd Qu.:3.000  \n Max.   :2015   Max.   :547.0   Max.   :2015-12-01   Max.   :3.000  \n    Q_SMK_YN     Q_DRK_FRQ_V09N       HGHT            WGHT      \n Min.   :1.000   Min.   :1.000   Min.   :134.0   Min.   : 31.0  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:158.0   1st Qu.: 56.0  \n Median :1.000   Median :2.000   Median :165.0   Median : 64.0  \n Mean   :1.631   Mean   :2.023   Mean   :164.5   Mean   : 65.1  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:171.0   3rd Qu.: 73.0  \n Max.   :3.000   Max.   :8.000   Max.   :188.0   Max.   :118.0  \n      WSTC             BMI            VA_LT           VA_RT       \n Min.   : 57.00   Min.   :12.30   Min.   :0.100   Min.   :0.1000  \n 1st Qu.: 74.00   1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000  \n Median : 81.00   Median :23.70   Median :1.000   Median :1.0000  \n Mean   : 80.69   Mean   :23.92   Mean   :0.984   Mean   :0.9792  \n 3rd Qu.: 87.00   3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000  \n Max.   :114.00   Max.   :37.20   Max.   :9.900   Max.   :9.9000  \n     BP_SYS          BP_DIA         URN_PROT          HGB       \n Min.   : 81.0   Min.   : 49.0   Min.   :1.000   Min.   : 5.90  \n 1st Qu.:110.0   1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90  \n Median :120.0   Median : 78.0   Median :1.000   Median :14.10  \n Mean   :122.3   Mean   : 76.6   Mean   :1.078   Mean   :14.11  \n 3rd Qu.:130.0   3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40  \n Max.   :180.0   Max.   :120.0   Max.   :5.000   Max.   :18.30  \n      FBS            TOT_CHOL           TG              HDL       \n Min.   : 61.00   Min.   : 68.0   Min.   :  13.0   Min.   : 23.0  \n 1st Qu.: 86.00   1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0  \n Median : 94.00   Median :193.0   Median : 106.0   Median : 54.0  \n Mean   : 97.23   Mean   :194.9   Mean   : 134.9   Mean   : 55.9  \n 3rd Qu.:103.00   3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0  \n Max.   :290.00   Max.   :363.0   Max.   :1210.0   Max.   :593.0  \n      LDL              CRTN              SGOT            SGPT       \n Min.   :  19.0   Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00  \n 1st Qu.:  90.0   1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00  \n Median : 112.0   Median : 0.9000   Median : 23.0   Median : 20.00  \n Mean   : 118.6   Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98  \n 3rd Qu.: 134.0   3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00  \n Max.   :8100.0   Max.   :16.5000   Max.   :459.0   Max.   :779.00  \n      GGT              zero      BMI_cat      \n Min.   :  6.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 16.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 24.50   Median :0   Median :0.0000  \n Mean   : 36.34   Mean   :0   Mean   :0.3449  \n 3rd Qu.: 41.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :408.00   Max.   :0   Max.   :1.0000  \n\n\n\nSubset\n특정 조건을 만족하는 서브데이터는 지금까지 배웠던 것을 응용해 만들 수도 있지만, subset 함수가 편하다. 아래는 2012 이후의 자료만 뽑는 예시이다. 이제부터는 결측치를 전부 제외한 ex.naomit 데이터를 이용하겠다.\n\nex1 &lt;- ex.naomit                                               ## simple name\nex1.2012 &lt;- ex1[ex1$EXMD_BZ_YYYY &gt;= 2012, ]\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\nex1.2012 &lt;- subset(ex1, EXMD_BZ_YYYY &gt;= 2012)                  ## subset\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\n\n그룹별 통계\n결측치 다루기에서 그룹별 통계를 구할 때 tapply 를 이용했었다. tapply 를 여러 변수, 여러 그룹을 동시에 고려도록 확장할 수 있는 함수가 aggregate 로, 허리둘레와 BMI의 평균을 고혈압 또는 당뇨 여부에 따라 살펴보자.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN), mean)\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex1, mean)   ## same\n\n\n\n\n\nGroup.1\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n결측치가 있어도 잘 적용된다는 장점이 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.23958\n23.70961\n\n\n1\n83.87847\n24.99861\n\n\n\n\n\n당뇨여부도 그룹으로 다루려면 list 에 추가하면 된다.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN, ex1$Q_PHX_DX_DM), mean)\n\n\n\n\n\nGroup.1\nGroup.2\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\nGroup.1 이 첫번째 그룹은 고혈압 여부, Group.2 가 두번째 그룹인 당뇨 여부이다. 위와 마찬가지로 formula 형태를 이용할 수도 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\n표준편차를 같이 보려면 function(x){c(mean = mean(x), sd = sd(x))} 와 같이 원하는 함수들을 벡터로 모으면 된다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})\n\n\n\nWarning in `[&lt;-.data.frame`(`*tmp*`, , isn, value = structure(list(WSTC.mean =\nc(\"80.231068\", : provided 4 variables to replace 2 variables\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.231068\n9.546884\n\n\n1\n0\n83.939759\n9.124277\n\n\n0\n1\n87.555556\n7.551674\n\n\n1\n1\n88.000000\n6.177918\n\n\n\n\n\n아예 데이터의 모든 변수의 평균을 다 볼순 없을까? 아래처럼 “.” 으로 전체 데이터를 지정할 수 있다.\n\naggregate(. ~ Q_PHX_DX_HTN  + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})    \n\n  Q_PHX_DX_HTN Q_PHX_DX_DM EXMD_BZ_YYYY.mean EXMD_BZ_YYYY.sd RN_INDI.mean\n1            0           0       2013.493204        1.109498    269.30680\n2            1           0       2013.578313        1.105645    251.78313\n3            0           1       2013.333333        1.414214    269.77778\n4            1           1       2013.307692        1.031553    303.53846\n  RN_INDI.sd HME_YYYYMM.mean HME_YYYYMM.sd Q_PHX_DX_STK.mean Q_PHX_DX_STK.sd\n1  159.12594      16102.3184      422.8574        1.00776699      0.08787296\n2  154.03951      16121.8072      413.1641        1.01204819      0.10976426\n3   92.88807      16036.3333      551.2248        1.00000000      0.00000000\n4  142.18686      16018.6923      417.4666        1.07692308      0.27735010\n  Q_PHX_DX_HTDZ.mean Q_PHX_DX_HTDZ.sd Q_PHX_DX_DLD.mean Q_PHX_DX_DLD.sd\n1         1.00194175       0.04406526         1.0174757       0.1311630\n2         1.06024096       0.23937916         1.0722892       0.2605404\n3         1.00000000       0.00000000         1.0000000       0.0000000\n4         1.07692308       0.27735010         1.0769231       0.2773501\n  Q_PHX_DX_PTB.mean Q_PHX_DX_PTB.sd Q_HBV_AG.mean Q_HBV_AG.sd Q_SMK_YN.mean\n1         1.0271845       0.1627787     2.2291262   0.5236863     1.6970874\n2         1.0000000       0.0000000     2.1927711   0.5512255     1.3855422\n3         1.0000000       0.0000000     2.0000000   0.0000000     1.6666667\n4         1.0769231       0.2773501     2.2307692   0.4385290     1.5384615\n  Q_SMK_YN.sd Q_DRK_FRQ_V09N.mean Q_DRK_FRQ_V09N.sd  HGHT.mean    HGHT.sd\n1   0.8674234           2.0388350         1.3329287 166.613592   9.116636\n2   0.6777172           1.9759036         1.3612754 160.506024   9.254364\n3   0.8660254           1.8888889         0.3333333 168.333333  10.185774\n4   0.6602253           1.9230769         1.1151636 162.384615   9.639662\n  WGHT.mean   WGHT.sd WSTC.mean   WSTC.sd  BMI.mean    BMI.sd VA_LT.mean\n1 66.582524 13.211630 80.231068  9.546884 23.829903  3.276315  1.0190291\n2 65.313253 13.155661 83.939759  9.124277 25.179518  3.693922  0.8469880\n3 71.777778  8.913161 87.555556  7.551674 25.344444  2.711140  0.9111111\n4 65.076923  6.211032 88.000000  6.177918 24.723077  2.057164  0.7769231\n   VA_LT.sd VA_RT.mean  VA_RT.sd BP_SYS.mean  BP_SYS.sd BP_DIA.mean BP_DIA.sd\n1 0.5248189  1.0079612 0.3503677  119.889320  13.378266   75.452427  9.464616\n2 0.3201895  0.8638554 0.3444962  132.879518  14.344539   81.481928 11.015910\n3 0.1691482  0.8111111 0.2368778  128.555556   8.647415   83.333333 11.842719\n4 0.2350668  0.9000000 0.1080123  129.461538  12.149180   79.307692  7.846280\n  URN_PROT.mean URN_PROT.sd   HGB.mean     HGB.sd  FBS.mean    FBS.sd\n1     1.0543689   0.3430173 14.3749515  1.5952305  94.75534  12.71807\n2     1.2168675   0.6634756 14.1048193  1.6682036 103.60241  14.34330\n3     1.0000000   0.0000000 15.2555556  1.0284832 131.11111  19.62425\n4     1.0769231   0.2773501 13.4153846  0.9711215 125.30769  34.11838\n  TOT_CHOL.mean TOT_CHOL.sd   TG.mean     TG.sd  HDL.mean    HDL.sd  LDL.mean\n1     196.96505    34.20684 132.80777 107.56421 54.943689 12.881333 118.49903\n2     191.30120    32.64769 138.09639  81.93106 55.903614 16.123468 108.22892\n3     169.77778    47.79325 164.66667  68.40870 46.333333  9.394147  92.77778\n4     179.61538    39.92397 154.76923 139.23072 48.769231 10.288779 102.30769\n     LDL.sd CRTN.mean   CRTN.sd SGOT.mean   SGOT.sd SGPT.mean   SGPT.sd\n1  50.86475 0.8871845 0.1867988 24.151456  9.426161 24.609709 16.616090\n2  29.09167 0.9168675 0.2483208 25.289157  6.400324 22.963855  9.671993\n3  38.88694 0.9666667 0.2549510 35.777778 25.849457 47.666667 44.235167\n4  28.39420 0.9153846 0.1675617 31.769231 19.689904 36.307692 27.417709\n  GGT.mean   GGT.sd GFR.mean   GFR.sd zero.mean zero.sd BMI_cat.mean BMI_cat.sd\n1 34.47573 31.35216 92.01359 19.14246         0       0    0.3223301  0.4678230\n2 35.77108 31.18799 81.16867 17.75430         0       0    0.4337349  0.4986022\n3 44.22222 21.01058 87.55556 22.20423         0       0    0.5555556  0.5270463\n4 48.46154 66.83265 80.69231 14.34332         0       0    0.1538462  0.3755338\n\n\nSort\n정렬은 순위함수인 order 를 이용한다. 기본은 오름차순이며, 내림차순을 원한다면 (-) 붙인 값의 순위를 구하면 된다.\n\nord &lt;- order(ex1$HGHT)                                        ## 작은 순서대로 순위\nhead(ord)\n\n[1] 500 168   3 328 473 177\n\nhead(ex1$HGHT[ord])                                           ## Sort\n\n[1] 138 139 140 140 141 143\n\nord.desc &lt;- order(-ex1$HGHT)                                  ## descending\nhead(ex1$HGHT[ord.desc])\n\n[1] 188 186 185 185 184 183\n\n\n\nex1.sort &lt;- ex1[ord, ]\nhead(ex1.sort)\n\n\n\n\n  \n\n\n\nWide to long, long to wide format\n받은 데이터가 원하는 형태가 아닌 경우가 있다. 수축기 혈압을 10번 측정해서 각각 SBP1, SBP2, …, SBP10 변수에 기록된 데이터를 본다면, 이것들을 쫙 아래로 내려 측정시기, 측정값 2개의 변수로 정리하고 싶다는 마음이 들 것이다. 이럴 때 쓰는 함수가 melt, 반대로 데이터를 옆으로 늘릴 때 쓰는 함수가 dcast 이다(Figure @ref(fig:melt)3).\n\n\n\n\nmelt and dcast\n\n\n\n실습으로 수축기/이완기 혈압 변수를 합쳐서 아래로 내려보자.\n\nlibrary(reshape2)\nlong &lt;- melt(ex1, id = c(\"EXMD_BZ_YYYY\", \"RN_INDI\"), measure.vars = c(\"BP_SYS\", \"BP_DIA\"), variable.name = \"BP_type\", value.name = \"BP\")\nlong\n\n\n\n\n  \n\n\n\nid 는 유지할 변수, measure.vars 는 내릴 변수를 의미하고, variable.name, value.name 은 각각 그룹, 값의 변수이름을 의미한다. 이를 원래대로 되돌리려면 dcast 를 이용하는데, “유지할 변수 ~ 펼칠 변수” 형태로 formula 를 입력한다.\n\nwide &lt;- dcast(long, EXMD_BZ_YYYY + RN_INDI ~ BP_type, value.var = \"BP\")\nhead(wide)\n\n\n\n\n  \n\n\n\nMerge\nmerge 함수를 이용한다. “by” 옵션으로 기준이 되는 공통 컬럼을 설정하며, 기준 컬럼의 이름이 두 데이터 셋에서 다른 경우는 “by.x” 와 “by.y” 로 따로 설정한다. 실습을 위해 ex1 데이터를 2개로 나눈 후 merge 를 적용하겠다.\n\nex1.Q &lt;- ex1[, c(1:3, 4:12)]\nex1.measure &lt;- ex1[, c(1:3, 13:ncol(ex1))]\nhead(ex1.Q)\nhead(ex1.measure)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n전자는 설문조사 결과를, 후자는 측정값을 포함했고 “년도, ID, 검진년월” 은 공통변수이다. 이 공통변수로 merge 를 적용하면\n\nex1.merge &lt;- merge(ex1.Q, ex1.measure, by = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all = T)\nhead(ex1.merge)\n\n\n\n\n  \n\n\n\n합쳐진 원래 데이터를 얻을 수 있다. all = T 는 한 쪽에만 있는 샘플을 유지하는 옵션이며 빈 변수는 NA 로 채워진다. 공통인 샘플만 취하려면 all = F 로 바꾸자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "마치며",
    "text": "마치며\n이번 강의를 정리하자.\n\nRStudio cloud 로 클라우드 환경에서 실습을 진행했으며\n기초 벡터연산과 for, if, ifelse, 함수만들기, apply 문을 통해 기본 문법을 익혔고\n\n공단 검진 데이터를 실습자료를 읽어와 데이터를 살펴보는 법을 배웠다.\n\n변수 생성, 클래스 설정, 결측치 처리, 서브데이터, 그룹별 통계, 정렬\n\n\n마지막으로 Long/wide type 데이터 변환과 merge 를 다루었다.\n\n기타 기본적으로 알아야 할 R 명령어는 아래의 Base R Cheat Sheet 에서 확인할 수 있다.\n 다음 강의에서는 쉬운 문법으로 R 의 대세가 된 tidyverse 를 다룰 예정인데, 오늘 배운 기본 문법과 많은 비교가 될 것이다. 미리 알아보고 싶은 분은 본 블로그의 이전 글4 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#footnotes",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#footnotes",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://jinseob2kim.github.io/rbasic.html↩︎\nhttps://jinseob2kim.github.io/radv1.html↩︎\nhttps://t1.daumcdn.net/cfile/tistory/2433F13D55E1163907↩︎\nhttps://blog.zarathu.com/posts/2019-01-03-rdatamanagement↩︎"
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html",
    "href": "posts/2020-06-20-shinymed2020/index.html",
    "title": "2020년 만들었던 ShinyApps",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 6월 Shinykorea 밋업에 참석, 올해 만들었던 ShinyApps 를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#요약",
    "href": "posts/2020-06-20-shinymed2020/index.html#요약",
    "title": "2020년 만들었던 ShinyApps",
    "section": "요약",
    "text": "요약\n\n대한심혈관중재학회 COBIS III 레지스트리 분석: 추가계약\n서울성모병원 COREA-AMI II 레지스트리 분석: 10개 연구 계약\n삼성서울병원 공통데이터모델(CDM) 분석: 심평원 코로나데이터 분석 중\n강동성심병원 위암 위험인자 분석: 공단표본데이터 분석 중\n경기도감염병관리지원단 코로나 대시보드 with Shinykorea: 최종보고\n삼성서울병원 이식외과 육종(sarcoma) 데이터 분석: 5개 연구 계약\n해운대백병원 정신질환 네트워크분석: 논문 4편 게재\n성균관의대 환경역학연구실 미세먼지 대시보드\nShiny로 연구용 환자정보 입력웹(Electronic Case Report Forms, eCRF) 만들어 분석모듈 앞에 붙이고 싶습니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#slide",
    "href": "posts/2020-06-20-shinymed2020/index.html#slide",
    "title": "2020년 만들었던 ShinyApps",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/shinymed2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html",
    "href": "posts/2020-07-22-regressionbasic/index.html",
    "title": "회귀분석 in 의학연구",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 2주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#요약",
    "href": "posts/2020-07-22-regressionbasic/index.html#요약",
    "title": "회귀분석 in 의학연구",
    "section": "요약",
    "text": "요약\n\n\n\n\n\n\n\n\n\nDafault\nRepeated measure\nSurvey\n\n\n\nContinuous\nlinear regression\nGEE\nSurvey GLM\n\n\nEvent\nGLM (logistic)\nGEE\nSurvey GLM\n\n\nTime & Event\nCox\nmarginal Cox\nSurvey Cox\n\n\n0,1,2,3 (rare event)\nGLM (poisson)\nGEE\nSurvey GLM"
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#slide",
    "href": "posts/2020-07-22-regressionbasic/index.html#slide",
    "title": "회귀분석 in 의학연구",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/regression 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html",
    "href": "posts/2020-10-05-docker-rshiny/index.html",
    "title": "RStudio & Shiny Docker 소개",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 자체 제작한 RStudio & Shiny-server Docker image 를 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "href": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "title": "RStudio & Shiny Docker 소개",
    "section": "요약",
    "text": "요약\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앤다.\n공식 image 참고하여 자체개발. https://github.com/jinseob2kim/docker-rshiny"
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "href": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "title": "RStudio & Shiny Docker 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/docker-rshiny 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html",
    "href": "posts/2020-10-29-survivalpractice/index.html",
    "title": "생존분석 실습",
    "section": "",
    "text": "김진섭 대표는 성균관의대 사회의학교실 김종헌 교수님 수업에 참가, Kaplan-meier curve, 비례위험가정 및 모형적합도, Time-dependent covariate 그리고 모수적 생존분석을 중심으로 R 코드를 실습할 예정입니다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#요약",
    "href": "posts/2020-10-29-survivalpractice/index.html#요약",
    "title": "생존분석 실습",
    "section": "요약",
    "text": "요약\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "href": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "title": "생존분석 실습",
    "section": "Kaplan-meier plot",
    "text": "Kaplan-meier plot\nKaplan-meier plot 은 R 기본 plot에서도 제공하지만, survminer 패키지의 ggsurvplot 함수에서 다양한 옵션을 제공한다. 본 실습에서는 본사가 개발한 jskm 패키지의 jskm 함수를 survival 패키지 내장 데이터 veteran 에 적용하겠다. 우선 패키지를 불러온 후 survfit 으로 구간별 생존율을 구하자.\n\nlibrary(DT);library(survival);library(jskm)\ndatatable(veteran, rownames = F, caption = \"Example data\", options = list(scrollX = T))\n\n\n\n\nsfit &lt;- survfit(Surv(time, status) ~ trt, data = veteran)\nsummary(sfit, times = c(100, 200, 300, 365), extend = T)\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = veteran)\n\n                trt=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     34      34   0.5020  0.0606       0.3962        0.636\n  200     12      19   0.1947  0.0501       0.1176        0.322\n  300      5       6   0.0885  0.0371       0.0390        0.201\n  365      4       1   0.0708  0.0336       0.0279        0.180\n\n                trt=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     21      45    0.333  0.0578       0.2367        0.467\n  200     13       7    0.216  0.0517       0.1354        0.345\n  300      8       4    0.146  0.0454       0.0797        0.269\n  365      6       2    0.110  0.0407       0.0530        0.227\n\n\ntrt 1 은 “Standard”, 2 는 “Test” 이며 jskm 을 적용하면 아래와 같다.\n\njskm(sfit)\n\n\n\n\n\n\n\n라벨을 수정하고, risk table 과 log-rank p-value 를 추가하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T)\n\n\n\n\n\n\n\n십자가 무늬는 실제 censoring 이 발생한 부분이며 mark = F 로 숨길 수 있다. 생존율이 아닌 누적발생률을 %로 보는 코드는 아래와 같다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cumhaz = T, surv.scale = \"percent\" )\n\n\n\n\n\n\n\np-value 위치는 pval.coord legend 위치는 legendposition 옵션을 이용한다. 선을 흑백으로 바꾸려면 linecols = \"black\" 을 추가한다. legendposition 은 x,y 값 모두 0~1 scale 임을 주의하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, pval.coord = c(100, 0.1), legendposition = c(0.85, 0.6), linecols = \"black\")\n\n\n\n\n\n\n\n마지막으로 특정 시간을 기준으로 나누어보는 landmark analysis 옵션을 소개한다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cut.landmark = 365)"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "href": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "title": "생존분석 실습",
    "section": "연속변수의 최적 cut-off 구하기",
    "text": "연속변수의 최적 cut-off 구하기\nmaxstat 패키지를 이용한다.\n\nlibrary(maxstat)\nmtest &lt;- maxstat.test(Surv(time, status) ~ karno, data = veteran, smethod = \"LogRank\")\nmtest\n\n\nMaximally selected LogRank statistics using none\n\ndata:  Surv(time, status) by karno\nM = 4.6181, p-value = NA\nsample estimates:\nestimated cutpoint \n                40 \n\ncut &lt;- mtest$estimate\nveteran$karno_cat &lt;- factor(as.integer(veteran$karno &gt;= cut))\n\nsfit2 &lt;- survfit(Surv(time, status) ~ karno_cat, data = veteran)\njskm(sfit2, ystrataname = \"Karno\", ystratalabs = paste(c(\"&lt;\", \"≥\"), cut), table = T, pval = T)"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "href": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "title": "생존분석 실습",
    "section": "비례위험가정 확인",
    "text": "비례위험가정 확인\nLogrank test, Cox model 로 추정할 때 비례위험을 가정하므로 이것이 깨지면 큰일이다. 본 글에서는 비례위험가정을 확인하는 그림 2개와 테스트를 소개한다. 자세한 내용은 https://3months.tistory.com/357?category=743476 를 참고하기 바란다.\nLog-log plot\n\\(\\log(t)\\) 와 \\(\\log(-\\log(S(t)))\\) 관계를 그림으로 보는 방법이다. 왜 로그를 이용하는지는 모수적 생존분석에서 이야기하겠다.\n\nplot(sfit, fun=\"cloglog\", lty=1:2, col=c(\"Black\", \"Grey50\"), lwd=2, font.lab=2, main=\"Log-log KM curves by Treat\", \n     ylab=\"log-log survival\", xlab=\"Time (log scale)\")\nlegend(\"bottomright\",lty=1:2,legend=c(\"Standard\", \"Test\"), bty=\"n\", lwd=2, col=c(\"Black\", \"Grey50\"))\n\n\n\n\n\n\n\n두 선이 평행한지 확인하면 되고 직선인지 곡선인지는 상관없다. 모수적 생존분석에서 다룰 weibull 모형에서는 직선인지도 확인해야 한다.\nObserved-expected plot\n비례위험을 가정하는 cox model 예상과 비교하는 방법이다.\n\nplot(sfit, lty=\"dashed\", col=c(\"Black\", \"Grey50\"), lwd=2, font=2, font.lab=2, main=\"Observed Versus Expected Plots by Treat\", \n     ylab=\"Survival probability\", xlab=\"Time\")\npar(new = T)\n\n#expected\nexp &lt;- coxph(Surv(time, status) ~ trt, data = veteran)\nnew_df &lt;- data.frame(trt = c(1, 2))\nkmfit.exp &lt;- survfit(exp, newdata = new_df)\nplot(kmfit.exp, lty = \"solid\", col=c(\"Blue\", \"Red\"), lwd=2, font.lab=2)\n\n\n\n\n\n\n\nGoodness of fit\ncox.zph 함수로 통계검정을 수행한다.\n\ncox.zph(exp)\n\n       chisq df    p\ntrt     3.54  1 0.06\nGLOBAL  3.54  1 0.06\n\nplot(cox.zph(exp), var = \"trt\")\nabline(h = 0, lty = 3)\n\n\n\n\n\n\n\n선이 시간 상관없이 일정할수록, 즉 x축과 평행할수록 비례위험가정을 만족한다고 판단한다. 위 그림은 x축과 평행은 아니지만 경향성이 있다고 볼수도 없는 애매한 느낌이며 p 는 0.06 이다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "href": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "title": "생존분석 실습",
    "section": "모형 비교",
    "text": "모형 비교\nCox 모형에서 얻은 log-likelihood 값으로 여러 모형을 비교할 수 있다. 모형들은 n수가 전부 동일 해야 비교 가능하므로, 에러 나올땐 먼저 결측치를 확인하자.\n\nexp$loglik\n\n[1] -505.4491 -505.4442\n\nexp2 &lt;- coxph(Surv(time, status) ~ trt + age, data = veteran)\nexp3 &lt;- coxph(Surv(time, status) ~ trt + age + celltype, data = veteran)\n\nanova(exp, exp2, exp3)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(time, status)\n Model 1: ~ trt\n Model 2: ~ trt + age\n Model 3: ~ trt + age + celltype\n   loglik   Chisq Df Pr(&gt;|Chi|)    \n1 -505.44                          \n2 -505.14  0.6162  1     0.4325    \n3 -492.43 25.4161  3  1.264e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nstep 함수를 이용, AIC 기반 최적 모형을 고를 수 있다. scope 옵션으로 빠지면 안 될 변수를 미리 정한다.\n\nstep(exp3, scope = list(lower = ~ 1))\n\nStart:  AIC=994.86\nSurv(time, status) ~ trt + age + celltype\n\n           Df     AIC\n- age       1  993.04\n- trt       1  993.65\n&lt;none&gt;         994.86\n- celltype  3 1014.27\n\nStep:  AIC=993.04\nSurv(time, status) ~ trt + celltype\n\n           Df     AIC\n- trt       1  992.05\n&lt;none&gt;         993.04\n- celltype  3 1012.89\n\nStep:  AIC=992.05\nSurv(time, status) ~ celltype\n\n           Df     AIC\n&lt;none&gt;         992.05\n- celltype  3 1010.90\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ celltype, data = veteran)\n\n                    coef exp(coef) se(coef)     z        p\ncelltypesmallcell 1.0013    2.7217   0.2535 3.950 7.83e-05\ncelltypeadeno     1.1477    3.1510   0.2929 3.919 8.90e-05\ncelltypelarge     0.2301    1.2588   0.2773 0.830    0.407\n\nLikelihood ratio test=24.85  on 3 df, p=1.661e-05\nn= 137, number of events= 128"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "href": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "title": "생존분석 실습",
    "section": "Time-dependent analysis",
    "text": "Time-dependent analysis\n자세한 내용은 https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf 를 참고하기 바란다.\n비례위험가정 깨졌을 때 (time-dependent coefficients)\n어떤 공변량이 비례위험가정을 만족하지 않을 경우, 먼저 survSplit 으로 time 을 쪼개 몇 개의 그룹으로 나눈다.\n\nvet2 &lt;- survSplit(Surv(time, status) ~ ., data = veteran, cut=c(90, 180), episode = \"tgroup\", id = \"id\")\ndatatable(vet2, rownames = F, caption = \"Time split data\", options = list(scrollX = T))\n\n\n\n\n\n이제 공변량의 계수를 시간그룹 별로 따로 구한다.\n\nvfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data=vet2)\nsummary(vfit2)\n\nCall:\ncoxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n    data = vet2)\n\n  n= 225, number of events= 128 \n\n                                  coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \ntrt                          -0.011025  0.989035  0.189062 -0.058    0.953    \nprior                        -0.006107  0.993912  0.020355 -0.300    0.764    \nkarno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15 ***\nkarno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530    \nkarno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                             exp(coef) exp(-coef) lower .95 upper .95\ntrt                             0.9890      1.011    0.6828    1.4327\nprior                           0.9939      1.006    0.9550    1.0344\nkarno:strata(tgroup)tgroup=1    0.9524      1.050    0.9409    0.9641\nkarno:strata(tgroup)tgroup=2    1.0081      0.992    0.9831    1.0337\nkarno:strata(tgroup)tgroup=3    0.9917      1.008    0.9637    1.0205\n\nConcordance= 0.725  (se = 0.024 )\nLikelihood ratio test= 63.04  on 5 df,   p=3e-12\nWald test            = 63.7  on 5 df,   p=2e-12\nScore (logrank) test = 71.33  on 5 df,   p=5e-14\n\n\n반복측정 공변량이 있을 때\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6015946/pdf/atm-06-07-121.pdf 예제를 이용하였다.\n\nlibrary(survsim)\n\nLoading required package: eha\n\n\nLoading required package: statmod\n\nN=100 #number of patients\nset.seed(123)\ndf.tf&lt;-simple.surv.sim(#baseline time fixed\n n=N, foltime=500,\n dist.ev=c('llogistic'),\n anc.ev=c(0.68), beta0.ev=c(5.8),\n anc.cens=1.2,\n beta0.cens=7.4,\n z=list(c(\"unif\", 0.8, 1.2)),\n beta=list(c(-0.4),c(0)),\n x=list(c(\"bern\", 0.5),\n c(\"normal\", 70, 13)))\n\nfor (v in 4:7){\n  df.tf[[v]] &lt;- round(df.tf[[v]])\n}\n\nnames(df.tf)[c(1,4,6,7)]&lt;-c(\"id\", \"time\", \"grp\",\"age\")\ndf.tf &lt;- df.tf[, -3]\n\ndatatable(df.tf, rownames = F, caption = \"df.tf: Original data\", options = list(scrollX = T))\n\n\n\n\n nft&lt;-sample(1:10,\n N,replace=T)#number of follow up time points\ncrp&lt;-round(abs(rnorm(sum(nft)+N,\n mean=100,sd=40)),1)\ntime&lt;-NA\nid&lt;-NA\ni=0\nfor(n in nft){\ni=i+1\ntime.n&lt;-sample(1:500,n)\ntime.n&lt;-c(0,sort(time.n))\ntime&lt;-c(time,time.n)\nid.n&lt;-rep(i,n+1)\nid&lt;-c(id,id.n)\n}\ndf.td &lt;- cbind(data.frame(id,time)[-1,],crp)\ndatatable(df.td, rownames = F, caption = \"df.td: Time dependent CRP\", options = list(scrollX = T))\n\n\n\n\n\ndf.tf 는 기본정보가 담긴 데이터, df.td 는 time-dependent covariate 가 담긴 데이터이다. tmerge 함수를 2번 실행하면 두 정보를 합칠 수 있다. 먼저 df.tf 만 이용해서 tstart, tstop 변수를 만들자.\n\ndf &lt;- tmerge(df.tf, df.tf, id = id, status1 = event(time, status))\n\ndatatable(df, rownames = F, caption = \"df: add tstart/tstop\", options = list(scrollX = T))\n\n\n\n\n\ntmerge 함수의 첫번째는 baseline data, 둘째는 time-dependent covariate 가 담긴 데이터가 들어가지만, tstart, tstop 를 만들기 위해 모두 df.tf 를 넣었다. status1 이라는 변수를 event(time, status) 로 지정함으로서 tstart, tstop 을 인식할 수 있다. status1 변수 자체는 status 와 동일하다. 이렇게 만든 df 에 time-dependent 정보가 담긴 df.td 를 결합하면 원하는 데이터를 얻을 수 있다. tmerge 의 자세한 내용은 https://ww2.amstat.org/meetings/sdss/2018/onlineprogram/ViewPresentation.cfm?file=304494.pdf 를 참고하기 바란다.\n\ndf2 &lt;- tmerge(df, df.td, id = id, crp = tdc(time, crp))\n\ndatatable(df2, rownames = F, caption = \"df2: final\", options = list(scrollX = T))\n\n\n\n\n\ncrp 변수를 tdc(time, crp) 로 만들었다. 이제 cox model 을 실행할 수 있는데, 반복측정정보를 cluster 옵션에 넣는 것을 잊지 말자.\n\nmodel.td &lt;- coxph(Surv(tstart, tstop, status1) ~ grp + age + crp, data = df2, cluster = id)\nsummary(model.td)\n\nCall:\ncoxph(formula = Surv(tstart, tstop, status1) ~ grp + age + crp, \n    data = df2, cluster = id)\n\n  n= 376, number of events= 67 \n\n         coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)  \ngrp 0.5022750 1.6524764 0.2525914 0.2555150 1.966   0.0493 *\nage 0.0005535 1.0005536 0.0081077 0.0072342 0.077   0.9390  \ncrp 0.0007922 1.0007925 0.0027391 0.0023373 0.339   0.7347  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ngrp     1.652     0.6052    1.0015     2.727\nage     1.001     0.9994    0.9865     1.015\ncrp     1.001     0.9992    0.9962     1.005\n\nConcordance= 0.554  (se = 0.04 )\nLikelihood ratio test= 4.21  on 3 df,   p=0.2\nWald test            = 4.34  on 3 df,   p=0.2\nScore (logrank) test = 4.18  on 3 df,   p=0.2,   Robust = 4.55  p=0.2\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "href": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "title": "생존분석 실습",
    "section": "모수적(parametric) 생존분석",
    "text": "모수적(parametric) 생존분석\nCox model 은 baseline hazard 없이도 HR 을 구할 수 있는 장점이 있다. 아래 식\n\\[h(t) = h_0(t) \\cdot \\exp(\\sum \\beta_i x_i)\\]\n에서 \\(h_0(t)\\) 를 몰라도 \\(\\beta\\) 들을 구할 수 있다는 뜻이고, cox model 이 준모수적(semi-parametric) 모형으로 불리는 이유이기도 하다. 그러나 Cox model 로 예측모형을 만들 때 이것은 단점이 된다. \\(t\\) 년 생존율을 구할 수 없기 때문이다. 생존함수 \\(S(t)\\) 는 아래처럼 계산하는데\n\\[S(t) = \\int_{0}^{t} h(u) \\,du\\] baseline hazard 를 모르므로 \\(h(t)\\) 도 알 수 없고 따라서 \\(S(t)\\) 도 수식으로 표현할 수 없다. Cox model 로 예측모형을 만든 연구는 (1) 데이터에서 시간 \\(t\\) 마다 \\(S(t)\\) 의 값을 직접 구해 이용하거나, (2) 인구집단통계에서 \\(S(t)\\) 를 얻어온다.\n그러면 baseline hazard 가 어떤 형태라고 가정하면 어떨까? 이것이 모수적 생존분석이며 cox model 과 장단점을 비교하면 아래와 같다.\nCox model\n– distribution of survival time unkonwn\n– Less consistent with theoretical \\(S(t)\\) (typically step function)\n+ Does not rely on distributional assumptions\n+ Baseline hazard not necessary for estimation of hazard ratio\nParametric Survival Model\n+ Completely specified \\(h(t)\\) and \\(S(t)\\)\n+ More consistent with theoretical \\(S(t)\\)\n+ time-quantile prediction possible\n– Assumption on underlying distribution\n아래는 대표적인 분포들이며 본 글에서는 흔히 쓰는 weibull 을 다루려 한다.\n\n\n\n\n\n\n\n\n아까 비례위험가정 얘기할 때 weibull 모형은 log-log 그래프가 직선인지도 확인해야 한다고 했는데, 그 이유는 아래 식에 나와있듯이 \\(\\log(-\\log(S(t)))\\) 와 \\(\\log(t)\\) 가 정비례관계이기 때문이다.\n\\[\n\\begin{align}\nS(t) &= \\exp(-\\lambda t^p) \\\\\n-\\log(S(t)) &= \\lambda t^p \\\\\n\\log(-\\log(S(t))) &= \\log(\\lambda) + p\\log(t) \\\\\n\\log(-\\log(S(t))) &\\propto \\log(t)\n\\end{align}\n\\]\n\\(p\\) 를 scale parameter 라 하며 \\(p = 1\\) 이면 baseline hazard 가 시간에 따라 일정함을 의미하며, 자세한 내용은 https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf 를 참고하자. R의 survreg 함수를 이용하며, 결과해석은 cox model 과 동일한데 scale parameter 값이 추가로 나온다(scale parameter를 미리 정할 수도 있다).\n\nmodel.weibull &lt;- survreg(Surv(time, status) ~ trt, data = veteran)\nsummary(model.weibull)\n\n\nCall:\nsurvreg(formula = Surv(time, status) ~ trt, data = veteran)\n             Value Std. Error     z      p\n(Intercept) 4.7218     0.3275 14.42 &lt;2e-16\ntrt         0.0478     0.2079  0.23  0.818\nLog(scale)  0.1585     0.0673  2.35  0.019\n\nScale= 1.17 \n\nWeibull distribution\nLoglik(model)= -748.1   Loglik(intercept only)= -748.1\n    Chisq= 0.05 on 1 degrees of freedom, p= 0.82 \nNumber of Newton-Raphson Iterations: 5 \nn= 137 \n\n\nScale = 1.17 임을 확인할 수 있고, trt 그룹별 \\(S(t)\\) 를 그려보면 아래와 같다.\n\npcut &lt;- seq(0.01, 1, by = 0.01)  ## 1%-99%\nptime &lt;- predict(model.weibull, newdata = data.frame(trt = 1), type = \"quantile\", p = pcut, se = T)\nmatplot(cbind(ptime$fit, ptime$fit + 1.96*ptime$se.fit, ptime$fit - 1.96*ptime$se.fit), 1 - pcut,\n        xlab = \"Days\", ylab = \"Survival\", type = 'l', lty = c(1, 2, 2), col=1)\n\n\n\n\n\n\n\n\\(S(t)\\) 를 구할 수 없는 cox model 의 그림과 비교해보자.\n\nmodel.cox &lt;- exp\nkmfit.exp &lt;- survfit(exp, newdata = data.frame(trt = 1))\nplot(kmfit.exp, lty = c(1, 2, 2), col=1, lwd=2, xlab = \"Days\", ylab = \"Survival\")\n\n\n\n\n\n\n\n지금까지 생존분석 때 고려할 내용을 다루었으며 처음의 요약을 반복하면 아래와 같다.\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다.\n\n자세한 내용은 중간중간 링크한 자료들을 참고하기 바란다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html",
    "href": "posts/2021-04-02-shinyecrf/index.html",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 4월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#요약",
    "href": "posts/2021-04-02-shinyecrf/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "요약",
    "text": "요약\n삼성서울병원 심혈관중재실 의뢰: 환자데이터 입력웹(eCRF).\n\nTychobra의 Shiny CRUD 참고해 용병 1인과 개발 중.\nshinymanager 로 로그인 모듈: 어떤 ID가 생성, 수정했는지 기록.\nDB: RSQLite 이용, 파일로 관리.\nDT 사용: proxy 기능으로 빠른 업데이트 가능. 테이블 안에 클릭(수정)버튼 삽입.\n버튼 1개 당 shiny module 1개.\n의료데이터 입력/관리/분석 통합서비스 목표."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#slide",
    "href": "posts/2021-04-02-shinyecrf/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html",
    "href": "posts/2021-07-11-kstartup/index.html",
    "title": "창업지원사업 도전기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 7월 Shinykorea 밋업에 참석, 창업지원사업 도전경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#요약",
    "href": "posts/2021-07-11-kstartup/index.html#요약",
    "title": "창업지원사업 도전기",
    "section": "요약",
    "text": "요약\n4전 5기만에 창업지원사업(비대면스타트업육성사업) 선정(1.4억)\n\n비대면 의료 분야: 블록체인 기반 의료데이터 입력, 관리, 분석 플랫폼\n\n18년초 창업전, 지도교수, 대학원동료와 함께 창업선도대학 선정(6천만)\n\n서울대학교: 블록체인 기반 유전체 빅데이터 플랫폼\n\n심평원 공모전 선정: 맞춤형 의학연구웹\n창업 후 주요사업 광탈, 작은 사업 선정\n\n20년 초기창업패키지, 추경(비대면), 추경2차, 21년 초기창업패키지 4연속 서류탈락\n비대면바우처(400만원), 클라우드 지원사업(720만원) 선정\n벤처기업인증: 혁신성장유형\n\n21년 공개소프트웨어 기반 창업기업 선정, 선릉역 오피스 6개월 지원."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#slide",
    "href": "posts/2021-07-11-kstartup/index.html#slide",
    "title": "창업지원사업 도전기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/kstartup 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html",
    "href": "posts/2021-08-19-beadatascientist/index.html",
    "title": "R 활용 의학연구지원",
    "section": "",
    "text": "김진섭 대표는 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에 참석, “Be a data scientist - major actor in the future research” 라는 제목으로 R 활용 의학연구지원경험을 발표할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#요약",
    "href": "posts/2021-08-19-beadatascientist/index.html#요약",
    "title": "R 활용 의학연구지원",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\nShinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#slide",
    "href": "posts/2021-08-19-beadatascientist/index.html#slide",
    "title": "R 활용 의학연구지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/microbiome 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html",
    "href": "posts/2021-09-11-googlelogin/index.html",
    "title": "Google Login",
    "section": "",
    "text": "Django allauth와 google OAuth를 이용해 zarathu 앱에 구글 로그인을 구현합니다.\nDjango admin 페이지에서 회원별 접근 권한을 관리합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "href": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "title": "Google Login",
    "section": "allauth 설치",
    "text": "allauth 설치\npython Django에서는 쉽게 사용할 수 있는 로그인 모듈을 제공합니다. 로그인 모듈 사용을 위해 Django allauth를 설치합니다.\n\npip install Django  \npip install django-allauth \n\nDjango allauth 사용을 위해서 settings.py에 다음과 같이 추가합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "href": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "title": "Google Login",
    "section": "구글 클라이언트 발급",
    "text": "구글 클라이언트 발급\nGoogle cloud platform (https://console.cloud.google.com)에서 웹 애플리케이션의 클라이언트를 발급합니다.\n자세한 방법은 https://cloud.google.com/endpoints/docs/frameworks/java/creating-client-ids?hl=ko#web-client 를 참고하여 주시기 바랍니다.\n클라이언트를 발급하면 클라이언트 ID와 보안 비밀번호를 얻게 되는데,\n이 ID와 비밀번호를 장고 admin 페이지의 소셜 어플리케이션 탭에 입력합니다.\n장고 admin 페이지는 장고 사이트 주소 뒤에 /admin 을 입력하여 접근할 수 있습니다.\n\n이제 장고 사이트에서 구글 로그인 링크를 누르면 400 오류:redirect_uri_mismatch 메시지가 뜹니다.\nGoogle cloud platform의 웹 애플리케이션 클라이언트에서 ’승인된 리디렉션 URI’에 https://domain.name/accounts/google/login/callback/ 을 입력해 주면 오류를 해결할 수 있습니다.\n\n리디렉션 uri에는 반드시 도메인 네임을 포함하여야 합니다. IP주소만으로는 실행할 수 없습니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "href": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "title": "Google Login",
    "section": "접근 권한 관리",
    "text": "접근 권한 관리\n장고로 배포한 페이지에서 shinyproxy로 shiny app을 배포한 주소로 링크를 연결할 것입니다.\n저희의 목표는 회원분들 개인마다 접근할 수 있는 앱을 지정해 주는 것입니다.\n이는 장고 admin 페이지와 html 파일에서 쉽게 구현할 수 있습니다.\n먼저, 장고 admin에서 다음과 같이 개인 사용자에게 그룹을 지정해줍니다.\n\n그리고 다음과 같이 링크를 연결할 페이지에서 해당 유저가 링크를 보기 위한 그룹에 속해있는지 확인하는 코드를 입력합니다.\n제 코드에서는 main.html에 입력하였습니다.\nmain.html 파일의 if 다음의 class가 해당 그룹에 속한 유저들에게만 노출됩니다.\n\n{% if request.user | has_group:\"&lt;group&gt;\" %}\n\n\nhas_group 함수의 내용은 다음과 같습니다.\n\nhas_group 함수는 제 코드에서는 goologin/templatetags/user_tags에 정의되어 있고 main.html에 다음 코드를 입력하여 불러옵니다.\n\n{% load user_tags %}"
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "href": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "title": "Google Login",
    "section": "참고사항",
    "text": "참고사항\n포트 포워딩\n장고의 기본 포트가 8000이기 때문에 브라우저 도메인 뒤에 8000포트가 붙게 됩니다.\n다양한 해결 방법이 있지만, 여기서는 프로그램 iptables를 이용하여 포트 포워딩을 수행하는 방법을 소개합니다.\n다음 명령어를 이용하면 80포트로 들어온 신호를 8000포트로 리다이렉트할 수 있습니다.\n\niptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8000 \n\nshinyproxy 배포\n장고 사이트에서 링크로 연결할 샤이니 앱을 배포하기 위해 shinyproxy를 이용합니다.\n먼저, rstudio server에서 도커 컨테이너를 빌드합니다.\n\nsudo docker build -t &lt;container image&gt;\n\napplication.yml 파일의 specs: 에 컨테이너 이미지를 입력하고 shinyproxy를 구동합니다.\n\njava -jar shinyproxy-2.5.0.jar\n\napplication.yml 파일에서 port, authentication, template-path, landing-page를 건드리면 배포할 포트를 결정하거나, 사용자 인증을 만들거나, 템플릿으로 이용할 html 파일을 지정하거나, 기본 페이지를 설정할 수 있습니다.\nshinyproxy에 관한 자세한 내용은 https://shinyproxy.io 를 참고하여 주시기 바랍니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html",
    "href": "posts/2021-10-01-notionoopy/index.html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "",
    "text": "기존 Hugo로 작성하고 Netlify로 배포하던 홈페이지의 한계점로, 새롭게 홈페이지를 구축하기로 결정.\n당사 홈페이지가 정적 웹이고, 최근 트렌드를 고려하여, Notion으로 홈페이지를 구축하기로 결정.\nNotion을 홈페이지화 해 주는 배포 서비스를 oopy로 결정.\n구축 후 oopy를 통해 커스텀 URL, 추가 JS, CSS 등 설정."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "href": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Jekyll 프레임워크와 GitHub Pages",
    "text": "Jekyll 프레임워크와 GitHub Pages\n정적 웹 프레임워크에서 가장 유명한 두 개의 프레임워크 중 하나는 Jekyll입니다. 당사는 소스코드를 GitHub로 관리하기에, GitHub Pages로 바로 배포가 가능한 Jekyll도 고려하였으나, 근본적으로 Hugo와 큰 차이가 없어 기존의 문제를 다 해결할 수는 없을 것이라고 생각해 포기하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "href": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "새롭게 웹 작성",
    "text": "새롭게 웹 작성\n새롭게 홈페이지을 작성하고 당사 서버의 남은 자원으로 호스팅 하는 방법도 있었습니다. 다만 빠른 제작이 필요했던 탓에 단기간에 만들기 힘든 부분이 있어 배제하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "href": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포",
    "text": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포\n최근 많은 기업체에서 채용 등 일부 페이지를 Notion으로 구축하는 사례가 늘고 있습니다. 예를 들어 왓챠나 클라썸같은 유명 스타트업이 있습니다. 다만 이들 또한 주 홈페이지까지 Notion을 사용하지는 않았습니다. 각 회사에서 서비스하는 제품이 정적 웹으로는 해결되지 않았기 때문입니다. 그러나 당사의 경우 주 홈페이지는 정적 웹으로 충분하고, 실제 서비스는 Zarathu App을 사용하기 때문에 큰 문제가 되지 않습니다.\n또한 노션의 기본 기능 뿐 아니라, oopy에서 간단한 SEO, 스타일(테마), HTML(CSS, JS포함), 클린 URL, Google Analytics나 channel.io 등 플러그인까지 지원하기에 자유도와 편리함이 적절하다 판단했습니다. oopy와 비슷한 서비스 - 대표적으로 Super - 도 존재하나, 지원의 편리성과 가격을 고려하여 Notion과 oopy를 사용하기로 최종 결정하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML",
    "text": "커스텀 HTML\n저희가 Notion과 oopy조합을 사용하게 된 가장 큰 이유는 커스텀 HTML(이하 CSS, JS 포함)입니다. 당사 홈페이지 하단의 ‘연구지원 신청하기’ 버튼은 oopy의 페이지별 HTML기능을 사용한 것입니다. oopy는 모든 페이지에 커스텀 HTML을 적용시킬 수도 있고, 페이지마다 개별적으로 적용할 수도 있습니다.  차라투는 위 사진처럼 media쿼리를 사용해 사용자의 환경에 따라 컨텐츠를 다르게 적용하기도 하였으며,\n\n\n\n전역적으로 적용하는 HTML\n\n\n위 사진처럼 모든 페이지 하단에 연구지원 신청하기 버튼을 만들기도 하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "href": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "쉽게 적용하는 플러그인",
    "text": "쉽게 적용하는 플러그인\n\n\n\n플러그인\n\n\n대부분 JS를 통해 직접 설치하는 플러그인(Google Analytics나 channel.io)또한 위 사진처럼 쉽게 추가가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "href": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "클린 URL",
    "text": "클린 URL\n 노션 주소를 공유할 때에는 위 사진의 복잡한 부분처럼 uuid형식으로 된 값을 사용합니다. 이는 효율적인 방법일 수 있으나, 심미적인 관점에서는 그렇지 않습니다. 또 링크를 누군가에게 공유할 때에도 링크만을 보고 대략적 내용을 유추할 수 없어 비 효율적인 면이 있습니다. oopy는 이러한 Notion의 URL을 쉽게 바꿀 수 있게 해 줍니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML의 편의성 부재",
    "text": "커스텀 HTML의 편의성 부재\nMicrosoft의 Visual Studio Code(VSCode)나 JetBrains사의 WebStorm같은 전문 프로그램은 물론이고, 최근에는 가벼원 편집기들 또한 자동 완성이나 괄호 자동 닫음 기능을 지원합니다. oopy의 커스텀 html 편집의 경우 아래의 사진과 같은 창에서 수행하게 되는데, 외부 편집기를 이용할 수 없어 Intellisence같은 기능을 사용할 수 없는 등의 단점은 있습니다. 다행히, 기초적인 오류는 경고 표시로 탐지가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "href": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "부족한 SEO",
    "text": "부족한 SEO\n우피는 자체적으로 SEO를 지원한다고 밝혔으나, 실제로는 Favicon과 og:image 설정, 메타태그 설정 등만이 가능하고, robots.txt는 일괄 변경만이 가능해 보여 해당 기능들의 추가가 필요합니다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html",
    "href": "posts/2022-01-20-dbbackup/index.html",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 3주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html#목차",
    "href": "posts/2022-01-20-dbbackup/index.html#목차",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "목차",
    "text": "목차\n\nSSH-key 생성 및 GitHub 등록\nDockerfile 작성\nCronFile 및 Shell File 작성\n\n\n\nSSH-Key 생성 및 GitHub 등록\nGitHub Repository에 SSH를 통한 접근 인증을 위해서는 SSH Public Key 생성 및 등록 과정을 걸쳐야 합니다.\n\nSSH-Key 생성\nssh-keygen -t rsa \n\n\n\n생성된 key는 인증키(private key)의 경우 ‘/.ssh/id_rsa’에 저장되어 있고 공개 키(public key)는’/.ssh/id_rsa.pub’로 저장되어 있습니다. 저희는 공개키를 사용하기에 공개키를 복사해주세요.\ncat ~/.ssh/id_rsa.pub\n위 명령어를 실행하면 공개키가 출력되는 모습을 확인하실 수 있습니다. 이제 GitHub로 이동하겠습니다.\n\n\n\nGitHub 등록\n\n\n\n\n\n\n\n\n본인 GitHub계정에 로그인한 이후 Settings → SSH and GPG keys → new SSH key에 복사한 SSH Key를 붙여주면 됩니다. Title은 임의로 정하셔도 상관없습니다. SSH-Key 등록 이후 본격적인 진행에 앞서 앞으로의 코드에 이해를 돕고자 현재 작업하고 있는 로컬 디렉토리의 계층도를 안내하겠습니다.\n\n\n\n\n\n\n\n\nDockerFile 작성\nDockerContainer를 DockerFile을 통해 만들도록 하겠습니다. 코드의 전문은 아래와 같습니다.\n\n\n\nFROM mysql:8.0.27\n생성되는 컨테이너는 MYSQL:8.0.27 이미지를 바탕으로 한다는 내용의 코드입니다\n\nRUN apt-get update\nRUN apt-get install cron -y\nRUN apt-get install git-all -y \n생성되는 컨테이너에 주기적인 백업을 위한 Cron, Git 명령어 수행을 위한 Git을 설치하는 코드입니다.\n\nRUN mkdir ~/.ssh \nCOPY /rsa/id_rsa /root/.ssh/id_rsa\nCOPY /rsa/id_rsa.pub /root/.ssh/id_rsa.pub\n앞서 생성한 SSH-key를 컨테이너에서 활용 가능하도록 컨테이너 생성시 .ssh폴더를 생성한 뒤 해당 키 값들을 복사하여 생성하는 코드입니다.\n\nCOPY cron /etc/cron.d/cron\nCOPY backup.sh /etc/cron.d/backup.sh \nCOPY cronstart.sh /etc/cron.d/cronstart.sh \n로컬에 있는 cron, backup.sh, cronstart.sh 파일들을 복사하여 컨테이너 생성시 해당 디렉토리에 넣는 코드입니다.해당 파일의 내용은 아래에서 살펴보도록 하겠습니다.\n\n\n\nCronFile 및 Shell File작성\n\n\nCronFile 작성\n\n\n\n앞서 컨테이너의 /etc/cron.d/cron에 복사한 cron파일입니다.\nCron을 작동시키는 시간을 맞추기 위해서 TZ = Asia/Seoul를 설정하여 현재 서울의 시간대와 일치시켰습니다.\n* * * * * /etc/cron.d/backup.sh &gt;&gt; /var/log/cron.log 2&gt;&1\n매분 마다 /etc/cron.d/backup.sh 파일을 작동시키고, 해당 파일의 작동 결과를 /var/log/cron.log 파일에 기록할 수 있도록 설정하였습니다. Backup.sh의 내용은 아래에서 살펴보겠습니다.\n(Cron 설정에 관련해서는 임의로 설정하셔도 상관없습니다.)\n\n\n\nShell 파일 작성\n\n\n\n앞서 Cron 설정으로 일정주기 마다 실행되는 Backup.sh 파일입니다. mysqldump를 활용해서 test db를 백업한 내용을 “$FileDir/$YmdH”.sql에 저장하고 이후 Git으로 Push를 진행해준다.\n여기까지cron을 활용하여 GitHub에 주기적으로 백업을 진행하는 내용의 코드는 완성되었습니다. 이에 더하여 컨테이너를 실행하면 cron이 자동으로 시작되는 기능을 추가하겠습니다.\n앞선 DockerFile에서 다음과 같은 코드를 확인할 수 있습니다.\nENTRYPOINT [\"/etc/cron.d/cronstart.sh\"]\nENTRYPOINT는 컨테이너가 시작되었을때 스크립트 혹은 명령을 실행합니다. 위 코드에서는 /etc/cron.d/cronstart.sh 파일을 실행하는 것입니다. /etc/cron.d/cronstart.sh 파일의 내용은 아래에서 확인하겠습니다.\n\n\n\ncronstart.sh\n#!/bin/bash\nservice cron start \nbash /usr/local/bin/docker-entrypoint.sh mysqld\n컨테이너가 시작되면 service cron start로 cron을 시작합니다. 이후 bash /usr/local/bin/docker-entrypoint.sh 파일에 mysqld 인자를 넘기며 실행시켜 해당 컨테이너에 mysqld를 작동시킵니다.\n\ndockerfile을 활용하여 컨테이너를 구축하고 실제로 작동이 잘되는지 확인해보도록 하겠습니다.\ndocker build -t test .\ndocker run -i --name test test \n\n\n\n컨테이너 생성과 동시에 정상적으로 DB 백업이 이루어지는 것을 확인할 수 있습니다.\n\n\n\n\n결론\nDB의 내용을 GitHub에 주기적으로 백업하는 방식에 대해 알아보았습니다. DB의 내용을 주기적으로 자동으로 백업이 가능하다는 유의미한 결과를 보이기는 하지만 코드상 DB의 암호가 노출된다는 점 등을 미루어 보았을 때 취약점이 존재하는것 같습니다. 개선된 방향으로의 학습이 필요할것 같습니다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html",
    "href": "posts/2022-02-07-gtsummary/index.html",
    "title": "gtsummary 패키지 소개",
    "section": "",
    "text": "본 자료에서는 데이터 셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. gtsummary 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다. gtsummary 패키지에 관한 기본 개념 및 함수들을 예제를 통해 다루어 보자."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#setup",
    "href": "posts/2022-02-07-gtsummary/index.html#setup",
    "title": "gtsummary 패키지 소개",
    "section": "Setup",
    "text": "Setup\n\n## Setup\n\n# install.packages(\"tidyverse\")\n# install.packages(\"data.table\")\n# install.packages(\"gtsummary\")\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(gtsummary)"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#road-file",
    "href": "posts/2022-02-07-gtsummary/index.html#road-file",
    "title": "gtsummary 패키지 소개",
    "section": "Road file",
    "text": "Road file\n예제에 사용할 데이터를 fread함수를 통해 불러오자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n\n## Load file\n\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url, header=T)\ndt"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "href": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "title": "gtsummary 패키지 소개",
    "section": "tbl_summary",
    "text": "tbl_summary\ntbl_summary 함수를 사용하여 기본 테이블을 작성할 수 있다. 출력값으로 데이터 셋의 각 열에 대한 기술 통계량을 반환한다. 데이터 셋에 섞인 범주형 변수와 연속형 변수를 자동적으로 인식해 그에 맞는 값을 반환하며, 범주형 변수의 기본 출력값은 n(%)이고 연속형 변수의 기본 출력값은 median(IQR)이다. 결측값은 테이블에 Unknown으로 출력된다.\nfread를 통해 불러온 데이터에서 몇 개의 변수를 추출해 요약 테이블을 만들어보자.\n\n# select variables\n\ndt2 &lt;- dt %&gt;% select(\"EXMD_BZ_YYYY\", \"Q_PHX_DX_STK\", \"Q_SMK_YN\",\n                     \"HGHT\", \"WGHT\" ,\"TOT_CHOL\", \"TG\")\ndt2\n\n\n\n\n  \n\n\n\ntbl_summary 함수를 통해 간단한 요약 테이블을 만들어보자.\n\n# create table\n\ndt2 %&gt;% tbl_summary()\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 1,6441\n\n\n\n\nEXMD_BZ_YYYY\n\n\n\n    2009\n214 (13%)\n\n\n    2010\n236 (14%)\n\n\n    2011\n223 (14%)\n\n\n    2012\n234 (14%)\n\n\n    2013\n243 (15%)\n\n\n    2014\n254 (15%)\n\n\n    2015\n240 (15%)\n\n\nQ_PHX_DX_STK\n12 (1.1%)\n\n\n    Unknown\n573\n\n\nQ_SMK_YN\n\n\n\n    1\n995 (61%)\n\n\n    2\n256 (16%)\n\n\n    3\n391 (24%)\n\n\n    Unknown\n2\n\n\nHGHT\n165 (158, 171)\n\n\nWGHT\n64 (56, 73)\n\n\nTOT_CHOL\n193 (170, 218)\n\n\nTG\n106 (72, 163)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n변수 유형이 자동으로 구분되어 연속형 변수는 median(IQR), 범주형 변수는 n(%)의 형태로 출력된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "href": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "title": "gtsummary 패키지 소개",
    "section": "그룹별 통계",
    "text": "그룹별 통계\nby\ntbl_summary 함수에는 다양한 옵션이 존재한다. by 옵션을 이용하여 그룹별 통계량을 계산할 수 있다. by 옵션에 그룹별 통계를 수행할 변수를 지정하여 사용 가능하다. 다음 예시에서 연도 변수인 EXMD_BZ_YYYY를 기준으로 그룹별 통계량을 출력해보자.\n\ndt2 %&gt;% tbl_summary(by = EXMD_BZ_YYYY)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n2009, N = 2141\n\n\n2010, N = 2361\n\n\n2011, N = 2231\n\n\n2012, N = 2341\n\n\n2013, N = 2431\n\n\n2014, N = 2541\n\n\n2015, N = 2401\n\n\n\n\nQ_PHX_DX_STK\n2 (1.5%)\n2 (1.2%)\n1 (0.7%)\n2 (1.3%)\n2 (1.2%)\n2 (1.3%)\n1 (0.6%)\n\n\n    Unknown\n82\n74\n85\n78\n75\n95\n84\n\n\nQ_SMK_YN\n\n\n\n\n\n\n\n\n\n    1\n125 (59%)\n132 (56%)\n140 (63%)\n146 (62%)\n141 (58%)\n157 (62%)\n154 (64%)\n\n\n    2\n34 (16%)\n42 (18%)\n35 (16%)\n36 (15%)\n35 (14%)\n38 (15%)\n36 (15%)\n\n\n    3\n53 (25%)\n62 (26%)\n48 (22%)\n52 (22%)\n67 (28%)\n59 (23%)\n50 (21%)\n\n\n    Unknown\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n165 (159, 171)\n165 (159, 171)\n165 (157, 171)\n165 (159, 172)\n165 (159, 171)\n164 (158, 172)\n164 (158, 172)\n\n\nWGHT\n64 (55, 72)\n64 (56, 73)\n63 (56, 72)\n64 (57, 74)\n64 (57, 73)\n63 (56, 72)\n64 (57, 74)\n\n\nTOT_CHOL\n193 (170, 216)\n193 (168, 220)\n190 (168, 215)\n196 (173, 224)\n190 (168, 218)\n193 (171, 216)\n194 (171, 217)\n\n\nTG\n105 (71, 149)\n107 (70, 159)\n104 (75, 165)\n108 (69, 164)\n107 (76, 160)\n108 (75, 163)\n111 (71, 167)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\ntbl_strata\nby 옵션을 통해 그룹별 통계량을 계산한 것처럼 tbl_strata 함수를 이용하면 여러 계층으로 그룹을 묶을 수 있다. tbl_strata(data, strata, .tbl_fun, …) 형식을 사용하며 strata에 그룹화할 칼럼, .tbl_fun 인자에는 출력할 tbl_summary formula를 지정한다.\n\ntbl_strata(data = dt2,\n           strata = EXMD_BZ_YYYY,\n           .tbl_fun =\n             ~ .x %&gt;%\n             tbl_summary(by = Q_SMK_YN) %&gt;%\n             add_p() %&gt;%\n             add_n(),\n           .header = \"**{strata}**, N={n}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n2009, N=214\n2010, N=236\n2011, N=223\n2012, N=234\n2013, N=243\n2014, N=254\n2015, N=240\n\n\nN\n\n1, N = 1251\n\n\n2, N = 341\n\n\n3, N = 531\n\n\np-value2\n\nN\n\n1, N = 1321\n\n\n2, N = 421\n\n\n3, N = 621\n\n\np-value2\n\nN\n\n1, N = 1401\n\n\n2, N = 351\n\n\n3, N = 481\n\n\np-value2\n\nN\n\n1, N = 1461\n\n\n2, N = 361\n\n\n3, N = 521\n\n\np-value2\n\nN\n\n1, N = 1411\n\n\n2, N = 351\n\n\n3, N = 671\n\n\np-value2\n\nN\n\n1, N = 1571\n\n\n2, N = 381\n\n\n3, N = 591\n\n\np-value2\n\nN\n\n1, N = 1541\n\n\n2, N = 361\n\n\n3, N = 501\n\n\np-value2\n\n\n\n\n\nQ_PHX_DX_STK\n130\n1 (1.5%)\n1 (3.6%)\n0 (0%)\n0.5\n162\n2 (2.3%)\n0 (0%)\n0 (0%)\n&gt;0.9\n138\n1 (1.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n156\n2 (2.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n168\n2 (2.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n159\n2 (2.2%)\n0 (0%)\n0 (0%)\n&gt;0.9\n156\n1 (1.0%)\n0 (0%)\n0 (0%)\n&gt;0.9\n\n\n    Unknown\n\n58\n6\n18\n\n\n44\n10\n20\n\n\n53\n14\n18\n\n\n52\n12\n14\n\n\n44\n7\n24\n\n\n67\n8\n20\n\n\n58\n12\n14\n\n\n\nHGHT\n212\n160 (155, 166)\n167 (164, 174)\n170 (165, 175)\n&lt;0.001\n236\n160 (156, 166)\n168 (164, 173)\n172 (165, 176)\n&lt;0.001\n223\n159 (155, 166)\n170 (166, 174)\n172 (169, 177)\n&lt;0.001\n234\n161 (156, 167)\n169 (165, 172)\n172 (166, 177)\n&lt;0.001\n243\n160 (156, 165)\n170 (166, 172)\n171 (167, 174)\n&lt;0.001\n254\n160 (155, 165)\n169 (165, 174)\n173 (168, 177)\n&lt;0.001\n240\n161 (155, 166)\n170 (165, 173)\n173 (169, 177)\n&lt;0.001\n\n\nWGHT\n212\n59 (52, 67)\n69 (61, 74)\n71 (62, 77)\n&lt;0.001\n236\n60 (54, 67)\n71 (66, 77)\n70 (62, 78)\n&lt;0.001\n223\n60 (53, 67)\n69 (64, 75)\n72 (64, 79)\n&lt;0.001\n234\n61 (54, 69)\n70 (63, 77)\n72 (64, 80)\n&lt;0.001\n243\n59 (53, 69)\n68 (63, 73)\n69 (62, 75)\n&lt;0.001\n254\n59 (53, 66)\n69 (62, 77)\n72 (65, 79)\n&lt;0.001\n240\n61 (54, 69)\n70 (63, 79)\n74 (62, 84)\n&lt;0.001\n\n\nTOT_CHOL\n212\n192 (166, 215)\n195 (177, 224)\n198 (174, 217)\n0.6\n236\n193 (166, 220)\n200 (181, 219)\n187 (168, 218)\n0.5\n223\n186 (165, 212)\n195 (172, 222)\n198 (176, 228)\n0.2\n234\n198 (173, 224)\n200 (171, 238)\n192 (175, 214)\n0.8\n243\n187 (165, 214)\n191 (177, 227)\n196 (170, 220)\n0.3\n254\n193 (170, 216)\n194 (174, 220)\n193 (175, 214)\n&gt;0.9\n240\n189 (170, 216)\n194 (171, 216)\n204 (180, 222)\n0.12\n\n\nTG\n212\n87 (59, 125)\n130 (79, 189)\n139 (100, 173)\n&lt;0.001\n236\n86 (64, 133)\n113 (78, 177)\n142 (101, 231)\n&lt;0.001\n223\n93 (67, 129)\n129 (78, 194)\n148 (91, 200)\n&lt;0.001\n234\n94 (65, 133)\n131 (91, 188)\n132 (94, 201)\n0.001\n243\n96 (72, 140)\n105 (75, 162)\n127 (85, 217)\n0.009\n254\n98 (73, 140)\n119 (87, 165)\n134 (83, 202)\n0.005\n240\n98 (64, 145)\n125 (97, 255)\n151 (90, 248)\n&lt;0.001\n\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n2 Fisher’s exact test; Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "href": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "title": "gtsummary 패키지 소개",
    "section": "Modifying function arguments",
    "text": "Modifying function arguments\ntbl_summary 함수에는 다양한 옵션이 존재하며, 이러한 옵션 조정을 통해 원하는 테이블을 작성할 수 있다. 다음은 tbl_summary 함수의 주요 옵션에 대한 설명이다.\n\nlabel : 테이블에 출력되는 변수명 지정\ntype : 변수 유형 지정 (ex. 연속형, 범주형)\nstatistic : 요약 통계량 지정\ndigits : 자릿수 지정\nmissing : 결측값이 있는 행을 표시할지 여부\nmissing_text : 결측행의 변수명 지정\nsort : 빈도에 따라 범주형 변수의 level 정렬\npercent : 열/행의 백분율 출력\ninclude : 테이블에 포함할 변수 지정\n\n\n\n\n\n\n\n\n\n다음은 옵션을 활용한 예시이다. 연도 변수 EXMD_BZ_YYYY의 그룹별 통계량을 출력하고, Q_SMK_YN 변수를 “smoking y/n”로 바꾸어보자. 이때 연속형 변수의 출력값을 {mean}({sd})으로, 범주형 변수의 출력값을 {n}/{N} ({p}%) 형태로 바꾸어보자. 결측값의 변수명은 “Missing”으로 수정한다.\n\ndt2 %&gt;%\n  tbl_summary(\n    by = EXMD_BZ_YYYY,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),\n    label = Q_SMK_YN ~ \"smoking y/n\",\n    missing_text = \"Missing\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\n2009, N = 2141\n\n\n2010, N = 2361\n\n\n2011, N = 2231\n\n\n2012, N = 2341\n\n\n2013, N = 2431\n\n\n2014, N = 2541\n\n\n2015, N = 2401\n\n\n\n\nQ_PHX_DX_STK\n2 / 132 (1.5%)\n2 / 162 (1.2%)\n1 / 138 (0.7%)\n2 / 156 (1.3%)\n2 / 168 (1.2%)\n2 / 159 (1.3%)\n1 / 156 (0.6%)\n\n\n    Missing\n82\n74\n85\n78\n75\n95\n84\n\n\nsmoking y/n\n\n\n\n\n\n\n\n\n\n    1\n125 / 212 (59%)\n132 / 236 (56%)\n140 / 223 (63%)\n146 / 234 (62%)\n141 / 243 (58%)\n157 / 254 (62%)\n154 / 240 (64%)\n\n\n    2\n34 / 212 (16%)\n42 / 236 (18%)\n35 / 223 (16%)\n36 / 234 (15%)\n35 / 243 (14%)\n38 / 254 (15%)\n36 / 240 (15%)\n\n\n    3\n53 / 212 (25%)\n62 / 236 (26%)\n48 / 223 (22%)\n52 / 234 (22%)\n67 / 243 (28%)\n59 / 254 (23%)\n50 / 240 (21%)\n\n\n    Missing\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n164 (9)\n165 (9)\n164 (10)\n165 (9)\n165 (9)\n164 (9)\n164 (9)\n\n\nWGHT\n64 (13)\n65 (12)\n65 (13)\n66 (12)\n65 (12)\n64 (12)\n66 (13)\n\n\nTOT_CHOL\n195 (37)\n195 (39)\n194 (38)\n199 (35)\n192 (36)\n195 (36)\n195 (36)\n\n\nTG\n129 (90)\n136 (101)\n138 (108)\n129 (89)\n132 (98)\n138 (127)\n141 (113)\n\n\n\n\n1 n / N (%); Mean (SD)"
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 5주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "목차",
    "text": "목차\n\nTraefik이란?\ndocker-compose.yml 파일 작성\nrules.yml 파일 작성\n실행결과\n결론\n\n\nTraefik이란?\n nginx와 같이 reverse프록시의 종류로서 별도의 제어 없이 실행중에 실시간으로 통신되는 요소끼리 찾아서 연결해주는 기능이 특징입니다. 또한 기본적으로 제공하는 대시보드 기능을 통하여 실시간으로 연결되어 있는 서비스들을 확인할 수 있고 또한 어떤 서버와 연결되어 있는지 파악이 가능합니다.\n해당 게시글은 Docker와 Traefik version2.2를 활용한 서비스에서 Reverse-Proxy를 구현하는 방법에 대해 설명 하도록 하겠습니다. 위 이미지에서 보이는 것처럼 Traefik은 들어오는 요청을 각각의 Docker Container에 배정해주는 역할을 수행합니다.\n아래 이미지는 Traefik 대시보드 페이지 입니다. \n\n\n\ndocker-compose.yml 파일 작성\n\ntraefik을 image로 가지고 있는 proxy 컨테이너와, whoami를 image로 가지고 있는 website 컨테이너를 생성하기 위한 docker-compose.yml에 작성된 코드입니다. 해당 코드를 활용하여 traefik으로 각 컨테이너를 어떻게 제어할 수 있는지에 대해 알아보도록 하겠습니다. 먼저 proxy 컨테이너 속 주요 코드를 살펴 보도록 하겠습니다.\n\n\nProxy 컨테이너\nimage: traefik:v2.2\ncommand:\n  - --entrypoints.web.address=:80\n  - --entrypoints.websecure.address=:443\n컨테이너의 이미지로서 traefik:v2.2를 사용하며 entrypoints로서 80번 포트로 들어오는 요청들은 web, 443번 포트로 들어오는 요청들은 websecure로 각각 명명하는 코드입니다\n\n- --certificatesresolvers.re.acme.email=*****@naver.com\n- --certificatesresolvers.re.acme.storage=./acme.json\n- --certificatesresolvers.re.acme.httpchallenge.entryPoint=websecure\n웹사이트에 Https를 적용하기 위해 Let’s Encrypt로부터 Certificate를 발급 받는 과정을 ACME protocol을 활용해서 자동으로 발급 받고 적용시킬 수 있도록 해주는 코드입니다.\n\n위 코드에서 ‘re’, ‘email 주소’, ‘acme.json’ 파일의 경로 혹은 파일명’은 사용자가 편하게 수정해도 괜찮습니다. 물론 entryPoint 값 또한 앞서 선언한 entrypoints 중에 본인이 희망하는 포트로 변경해도 괜찮습니다.\n\n해당 코드를 작성하기에 앞서 해당 경로에 acme.json파일을 생성해야 합니다. 이후 코드를 실행시키면 acme.json 파일에 certificate에 대한 내용이 담기게 됩니다.\n\n(아래 이미지는 acme.json 파일의 일부입니다.)\n\n\nports:\n  - 80:80\n  - 443:443\n  - 8080:8080\n80번 포트, 443번 포트, 8080포트로 들어오는 요청에 대한 포트 연결입니다.\n\n8080포트는 Traefik dashboard의 기본 포트입니다. 8080포트를 통하여 Traefik dashboard에 접속이 가능합니다.\n\nvolumes:\n  - /var/run/docker.sock:/var/run/docker.sock\n  - ./rules.yml:/etc/traefik/rules.yml:ro\n  - ./acme.json:/acme.json \ntraefik이 /var/run/docker.sock를 사용 가능하도록하여 docker container들의 정보를 사용이 가능합니다. 또한 앞서 작성한 acme.json 파일과 앞으로 작성할 rules.yml 파일 또한 사용이 가능하도록하는 코드입니다. rule.yml파일을 통하여 저희는 동작을 제어할것입니다.\n\n\n\nWebsite 컨테이너\nwebsite:\n  image: containous/whoami\n  labels:\n    - traefik.http.routers.website.rule=Host('food.dogdog.cf')\n    - traefik.http.routers.website.tls=true\n    - traefik.http.routers.website.tls.certresolver=re\n    - traefik.http.routers.website.entrypoints=websecure\nwebsite 컨테이너의 image를 containous/whoami로 설정합니다. 또한 http 서비스 제공시 routers 규칙을 traefik.http.routers.website.()를 통해 설정 합니다.’food.dogdog.cf’로 요청이 들어오면 ‘website’컨테이너로 라우팅이 되도록 설정합니다. 또한 https가 가능하도록 ’tls=true’로 설정하며, ’tls/certresolver=re’ 앞서 설정한 re 값을 certresolver값으로 설정합니다. 해당 컨테이너의 entrypoints는 앞서 설정한 :443포트로 들어오는 요청인 ’websecure’로 설정합니다. traefik 동작의 전반적인 이해를 돕고자 이미지를 첨부합니다.\n\n\n- traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https\n- traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\n- traefik.http.routers.redirs.entrypoints=web\n- traefik.http.routers.redirs.middlewares=redirect-to-https\nhttp로 ’food.dogdog.cf’를 통해 들어오는 요청들에 대해서 https로 redirect가 가능하도록 도와주는 코드입니다.\n\n\n\n\nrules.yml 파일 작성\n앞서 작성한 docker-compose.yml 파일을 통해서 서비스를 제공하는 기본적인 컨테이너 구축은 완료했습니다. 이제는 특정 요청에 대해서는 외부 서버에 있는 서비스를 이용하도록 하는 rules.yml 파일을 작성하도록 하겠습니다. \n\nroute1: \n  entryPoints:\n    -websecure\n  rule: Host('food.dogdog.cf')&&PathPrefix('/toy')\n  service: reverse-proxy\n  tls: {}\nentrypoints가 websecure로 들어오는 요청(:443포트로 들어오는 요청)에 대해서 만약 ‘food.dogdog.cf/toy’ 요청이면 reverse-proxy라는 서비스로 넘기며 해당 서비스의 내용을 수행하며. tls(https 서비스)를 사용하겠다는 코드입니다. 해당 코드형식만 유지하면 변수명 및 요청명에 대해서는 수정하셔도 됩니다.\n\n\n\n실행결과\n\n\n\n\n\n\n결론\nTraefik을 활용한 reverse-proxy를 구현하는 방법에 대해 알아봤습니다. 실습을 진행하며 파악한 것과 같이 Traefik에서는 let’s Encrypt를 통해 자동으로 https업로드, 대시 보드 제공을 통한 router, service, Middleware 상태 확인등의 기능을 확인할 수 있었습니다. Traefik은 이번 실습에서 다룬 기능보다 많은 기능이 제공되는는 유용한 오픈소스입니다"
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html",
    "href": "posts/2022-03-19-statreview/index.html",
    "title": "Reviewer들을 위한 의학통계",
    "section": "",
    "text": "김진섭 대표는 4월 8일(금) 제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 주제로 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#요약",
    "href": "posts/2022-03-19-statreview/index.html#요약",
    "title": "Reviewer들을 위한 의학통계",
    "section": "요약",
    "text": "요약\nTable 1\n\n연속변수 2그룹: 정규분포 t-test, 아니면 Wilcox-test\n연속변수 3그룹이상: 정규분포 ANOVA, 아니면 Kruskal–Wallis ANOVA\n범주형 변수: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n\n회귀분석\n\nUnivariate, multivariate 같이 보여주기, Subgroup 분석 추천\nStepwise selection 비추천: 예측모형 목적 아님, 임상맥락 고려X\n\n생존분석\n\nKaplan-meier 그림선 겹치면 안됨: Time stratification 필요\n보정할 변수가 Index time 이후면 안됨: Time-dependent covariate 필요\nPropensity score 매칭 후 pair 고려한 stratified cox 는 필수아님\n\n국민건강영양조사\n\n표본추출정보를 고려한 통계분석: Survey table1/GLM/Cox"
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#slide",
    "href": "posts/2022-03-19-statreview/index.html#slide",
    "title": "Reviewer들을 위한 의학통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/statreview 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html",
    "href": "posts/2022-04-12-status2022/index.html",
    "title": "22년 지원사업 후기",
    "section": "",
    "text": "김진섭 대표는 4월 20일(수) Shinykorea 밋업에서 “22년 지원사업 후기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#요약",
    "href": "posts/2022-04-12-status2022/index.html#요약",
    "title": "22년 지원사업 후기",
    "section": "요약",
    "text": "요약\n사무실\n\n송파ICT청년창업지원센터 입주, 기업부설연구소 설립\n공개SW 창업기업 6개월 연장(선릉 저스트코타워)\n\nR&D 과제\n\n창업성장기술개발사업(디딤돌 첫걸음) 서류탈락\n정보통신·방송 기술개발사업 앤틀러과 같이 지원, 6:1 경쟁률 결과 기다리는중\n\n창업지원사업\n\n혁신분야 창업패키지(BIG3), SW고성장기업 서류탈락\n창업도약패키지 심사중, 클라우드서비스 이용지원, Datastars 심사중\n\n특허지원사업\n\n국제 지재권분쟁 대응전략 지원사업(특허) 심사중\n\n멘토링\n\nSW마에스트로 기술멘토 선정, 한이음멘토링\n오픈소스 컨트리뷰톤 심사중\n\n고용지원, 인턴십, 기타\n\n`21 청년디지털일자리, 미래청년인재육성사업 TO 5명\n’22년 일경험프로그램 TO 1명, 송파구 중소기업 청년취업인턴제 TO 1명\n숭실대 스타트업 인턴십 2명(6주), ICT 학점연계 프로젝트 인턴십 TO 1명\n인공지능고성능컴퓨팅지원 3년 연속 선정"
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#slide",
    "href": "posts/2022-04-12-status2022/index.html#slide",
    "title": "22년 지원사업 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/status2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html",
    "href": "posts/2022-07-13-r-datatable2/index.html",
    "title": "data.table 패키지 기초",
    "section": "",
    "text": "data.table은 빠른 속도와 메모리 효율성에 가장 적합한 패키지입니다.\n대용량의 데이터를 분산처리 시스템 없이 처리할 수 있습니다.\n데이터 프레임(data.frame)을 대신하여 더 빠르고 편리하게 사용할 수 있는 데이터 타입입니다.\n\n장점으로는, 상대적으로 메모리 요구량이 적고, 속도가 매우 빠르다는 특징이 있습니다.\n단점으로는, 다소 난해한 문법으로 널리 사용되지 못하고 있다는 특징이 있습니다.\n\n본격적으로 data.table에 대해서 알아보기 전에, Setup 과정에 대해서 먼저 소개하려고 합니다. data.table은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\nlibrary(data.table)\nlibrary(curl)\n\n위의 과정을 통해 pacakge 설치 및 불러오기를 실행합니다.\ndata.table을 생성하는 데는 두 가지 방법이 있습니다.\n첫번째는, data.table() 함수를 통해 직접 생성하는 방식입니다. 다음의 예시로 살펴보겠습니다.\n\nEX=data.table(\n  ID=c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  MATH=c(100,96,94,88,92),\n  ENGLISH=c(96,86,97,92,93),\n  HISTORY=c(85,92,87,92,94))\n\n\n\n\n\nID\nMATH\nENGLISH\nHISTORY\n\n\n\nA\n100\n96\n85\n\n\nB\n96\n86\n92\n\n\nC\n94\n97\n87\n\n\nD\n88\n92\n92\n\n\nE\n92\n93\n94\n\n\n\n\n\nID, MATH, ENGLISH, HISTORY를 변수로 한, data.table이 형성된 것을 확인할 수 있습니다.\n두번째는, 기존의 데이터를 불러오는 방법이 있습니다.\nfread 함수는 대용량 파일을 빠르게 가져올 수 있는 함수입니다. 파일을 읽어와서 data.table 형식의 자료로 만들 때, 로컬 file path를 입력하거나, http://로 시작하는 URL을 입력하는 방법을 사용할 수 있습니다.\n\nlibrary(data.table) ; library(magrittr)\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하여,\ndf에는 data.frame 형식으로 데이터를 불러왔고, dt에는 fread 함수를 이용하여 data.table의 형식으로 데이터를 불러온 것을 확인할 수 있습니다.\nfread 함수로 파일을 불러오면 그 class는 data.frame에 data.table이 추가되며, 문법이 원래의 data.frame과 달라지는 점을 유의해야 합니다.\nclass 함수를 통해 df와 dt의 속성을 확인해보겠습니다.\n\nprint(class(df)) ; print(class(dt))\n\n[1] \"data.frame\"\n\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 확인할 수 있습니다.\n지금까지 data.table을 생성하는 두 가지 방법에 대해서 알아보았습니다.\n다음으로 data.table이 data.frame과 다른 점은, 행(Row)의 이름을 받지 않는 것을 기본값으로 한다는 것입니다.\n예시로 알아보도록 하겠습니다.\nR에 기본적으로 저장되어 있는 mtcars 데이터를 이용하도록 하겠습니다.\n\n# mtcars\nEX1&lt;-as.data.frame(mtcars)\nEX2&lt;-as.data.table(mtcars)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n실행하였을 때, EX1과 EX2의 행의 이름에서 차이점이 있음을 확인할 수 있습니다.\n만약, data.table에서도 행의 이름을 남겨 놓고 싶을 때는 다음과 같이 실행하면 됩니다.\n\nEX3&lt;-as.data.table(mtcars,keep.rownames=T)\n\n\n\n\n  \n\n\n\ndata 값 뒤에 keep.rownames=T로 설정하였을 때,\n각 행의 이름이 rn 컬럼에 남아 있는 것을 확인할 수 있습니다.\n\ndata.table의 기본 문법은 DT[i, j, by] 형태입니다.\n\ni는 행(row)과 관련되어, 행에 대해서 subset 하는 역할을 수행합니다.\nj는 열(column)을 선택하거나, 열 또는 테이블 전체(.SD)에 함수를 적용합니다.\nby는 집단을 나눕니다. j에서 지정한 열과 함수에 대한 실행을 그룹 별로(by group) 수행합니다.\n맨 마지막에 [order]를 붙여 오름차순이나 내림차순으로 정렬할 수 있습니다.\n\ndata.table에서만 확인할 수 있는 특수기호들이 있습니다.\n각 특수기호의 자세한 기능과 사용법은 이하에서 설명하기로 하고, 여기에서는 간단히 개념정도만 다뤄보려고 합니다.\n\n.SD : Subset of Data(by=로 나눠진 부분 데이터). 특수 기호를 사용하여 그룹 칼럼(by grouping columns)을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용합니다.\n.SDcols : 특수 기호를 사용하여 특정 다수의 칼럼을 지정하여 처리할 때 사용합니다.\n.N : 부분 데이터의 행의 수를 나타낼 때 사용합니다. 특정한 열을 잡아서 length() 함수를 이용해도 되지만 좀 더 간편하게 구할 수 있습니다.\n:= : DT[i, j, by]에서 칼럼 j를 추가/갱신/삭제할 때 특수기호 := 연산자를 사용하여 수행할 수 있습니다.\n\n이상에서 data.table을 이용하면서 가장 많이 쓰이는 특수기호들에 대해서 알아보았습니다. 각각의 특수기호들이 어떻게 실제로 쓰이는지에 대해서는 이하에서 등장할 때마다 자세하게 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#data.-table",
    "href": "posts/2022-07-13-r-datatable2/index.html#data.-table",
    "title": "data.table 패키지 기초",
    "section": "",
    "text": "data.table은 빠른 속도와 메모리 효율성에 가장 적합한 패키지입니다.\n대용량의 데이터를 분산처리 시스템 없이 처리할 수 있습니다.\n데이터 프레임(data.frame)을 대신하여 더 빠르고 편리하게 사용할 수 있는 데이터 타입입니다.\n\n장점으로는, 상대적으로 메모리 요구량이 적고, 속도가 매우 빠르다는 특징이 있습니다.\n단점으로는, 다소 난해한 문법으로 널리 사용되지 못하고 있다는 특징이 있습니다.\n\n본격적으로 data.table에 대해서 알아보기 전에, Setup 과정에 대해서 먼저 소개하려고 합니다. data.table은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\nlibrary(data.table)\nlibrary(curl)\n\n위의 과정을 통해 pacakge 설치 및 불러오기를 실행합니다.\ndata.table을 생성하는 데는 두 가지 방법이 있습니다.\n첫번째는, data.table() 함수를 통해 직접 생성하는 방식입니다. 다음의 예시로 살펴보겠습니다.\n\nEX=data.table(\n  ID=c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  MATH=c(100,96,94,88,92),\n  ENGLISH=c(96,86,97,92,93),\n  HISTORY=c(85,92,87,92,94))\n\n\n\n\n\nID\nMATH\nENGLISH\nHISTORY\n\n\n\nA\n100\n96\n85\n\n\nB\n96\n86\n92\n\n\nC\n94\n97\n87\n\n\nD\n88\n92\n92\n\n\nE\n92\n93\n94\n\n\n\n\n\nID, MATH, ENGLISH, HISTORY를 변수로 한, data.table이 형성된 것을 확인할 수 있습니다.\n두번째는, 기존의 데이터를 불러오는 방법이 있습니다.\nfread 함수는 대용량 파일을 빠르게 가져올 수 있는 함수입니다. 파일을 읽어와서 data.table 형식의 자료로 만들 때, 로컬 file path를 입력하거나, http://로 시작하는 URL을 입력하는 방법을 사용할 수 있습니다.\n\nlibrary(data.table) ; library(magrittr)\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하여,\ndf에는 data.frame 형식으로 데이터를 불러왔고, dt에는 fread 함수를 이용하여 data.table의 형식으로 데이터를 불러온 것을 확인할 수 있습니다.\nfread 함수로 파일을 불러오면 그 class는 data.frame에 data.table이 추가되며, 문법이 원래의 data.frame과 달라지는 점을 유의해야 합니다.\nclass 함수를 통해 df와 dt의 속성을 확인해보겠습니다.\n\nprint(class(df)) ; print(class(dt))\n\n[1] \"data.frame\"\n\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 확인할 수 있습니다.\n지금까지 data.table을 생성하는 두 가지 방법에 대해서 알아보았습니다.\n다음으로 data.table이 data.frame과 다른 점은, 행(Row)의 이름을 받지 않는 것을 기본값으로 한다는 것입니다.\n예시로 알아보도록 하겠습니다.\nR에 기본적으로 저장되어 있는 mtcars 데이터를 이용하도록 하겠습니다.\n\n# mtcars\nEX1&lt;-as.data.frame(mtcars)\nEX2&lt;-as.data.table(mtcars)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n실행하였을 때, EX1과 EX2의 행의 이름에서 차이점이 있음을 확인할 수 있습니다.\n만약, data.table에서도 행의 이름을 남겨 놓고 싶을 때는 다음과 같이 실행하면 됩니다.\n\nEX3&lt;-as.data.table(mtcars,keep.rownames=T)\n\n\n\n\n  \n\n\n\ndata 값 뒤에 keep.rownames=T로 설정하였을 때,\n각 행의 이름이 rn 컬럼에 남아 있는 것을 확인할 수 있습니다.\n\ndata.table의 기본 문법은 DT[i, j, by] 형태입니다.\n\ni는 행(row)과 관련되어, 행에 대해서 subset 하는 역할을 수행합니다.\nj는 열(column)을 선택하거나, 열 또는 테이블 전체(.SD)에 함수를 적용합니다.\nby는 집단을 나눕니다. j에서 지정한 열과 함수에 대한 실행을 그룹 별로(by group) 수행합니다.\n맨 마지막에 [order]를 붙여 오름차순이나 내림차순으로 정렬할 수 있습니다.\n\ndata.table에서만 확인할 수 있는 특수기호들이 있습니다.\n각 특수기호의 자세한 기능과 사용법은 이하에서 설명하기로 하고, 여기에서는 간단히 개념정도만 다뤄보려고 합니다.\n\n.SD : Subset of Data(by=로 나눠진 부분 데이터). 특수 기호를 사용하여 그룹 칼럼(by grouping columns)을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용합니다.\n.SDcols : 특수 기호를 사용하여 특정 다수의 칼럼을 지정하여 처리할 때 사용합니다.\n.N : 부분 데이터의 행의 수를 나타낼 때 사용합니다. 특정한 열을 잡아서 length() 함수를 이용해도 되지만 좀 더 간편하게 구할 수 있습니다.\n:= : DT[i, j, by]에서 칼럼 j를 추가/갱신/삭제할 때 특수기호 := 연산자를 사용하여 수행할 수 있습니다.\n\n이상에서 data.table을 이용하면서 가장 많이 쓰이는 특수기호들에 대해서 알아보았습니다. 각각의 특수기호들이 어떻게 실제로 쓰이는지에 대해서는 이하에서 등장할 때마다 자세하게 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "href": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "title": "data.table 패키지 기초",
    "section": "2. data.table에 접근하기",
    "text": "2. data.table에 접근하기\n이하에서는 위에서 불러온 dt(=09-15년 공단 건강검진 데이터)와 EX3(=mtcars) 데이터를 이용해서 실습하려고 합니다.\n2-1. 행(Row)에 접근하기\ndata.table에서 행(Row)에 접근하는 방법은 DT[i, j, by]에서 i 자리에 값을 넣는 것입니다. 즉, DT[c(row1, row2, …)]의 방식입니다.\nmtcars 데이터로 예시를 들어보겠습니다.\n여러 개의 자동차 종류 중, Datsun 710과 Hornet Sprotabout에 대해서만 알아보고 싶을 때는 다음과 같이 작성하면 됩니다.\n\nEX3[c(3,5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n만약 Mazda RX4 부터 Hornet Sportabout까지 알아보고 싶다면, 범위로 지정할 수도 있습니다.\n\nEX3[1:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n여기서 중요한 것은 DT[i, j, by]라는 기본적인 형식에서 i의 자리에 지금 내용을 채워 넣는 것인데, i 자리에 내용을 작성한 후 꼭 콤마(,)를 찍지 않아도 된다는 것입니다. 꼭 콤마(,)를 찍지 않아도 뒤에 특정한 열 을 선택하지 않으면 모든 열에 대해서 알아서 필터링을 하기 때문입니다.\n다음으로는 특정 조건을 만족하는 행(row)을 선택하는 방법에 대해서 알아보려고 합니다.\nDT[조건]의 형식을 이용하면 됩니다.\nmtcars 데이터에서 cyl&gt;=6이면서, carb==4인 조건을 만족하는 데이터를 찾고 싶은 경우, 다음과 같이 작성하면 됩니다.\n\nEX3[cyl&gt;=6 & carb==4]\n\n\n\n\n  \n\n\n\nKEY를 미리 설정해놓으면 더 빠르게 검색할 수 있는데, 이 내용에 대해서는 뒤에서 자세하게 다루도록 하겠습니다.\n다음으로는 특정 행(row)을 제외하는 방법에 대해서 알아보려고 합니다. 제외하려는 행 혹은 행의 범위 앞에 - 나 ! 를 붙여주면 됩니다.\n\nEX3[!1:5]\nEX3[-2]\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n위와 같이 실행했을 때, 제외하려는 데이터가 사라진 것을 확인할 수 있습니다.\n2-2. 열(Column)에 접근하기\n행(Row)을 선택할 때와 유사합니다. 기본적인 형식은 DT[i, j, by]의 j 자리에 넣는 것입니다.\nmtcars 데이터에서 ‘cyl’ 열(Column)을 가져오고 싶을 때는 다음과 같이 가져올 수 있습니다.\n\nEX3[,3] ; EX3[,.(cyl)] ; EX3[,\"cyl\"]\n\n\n\n\n\ncyl\n\n\n\n6\n\n\n6\n\n\n4\n\n\n6\n\n\n8\n\n\n6\n\n\n\n\n\n열(column)의 숫자로 불러와도 되고, 변수의 이름으로 불러오는 것도 가능합니다. 그런데, 여기서 중요하게 봐야할 점은 변수의 이름으로 가져올 때 앞에 .()의 형식을 이용했다는 점입니다.\n.()은 list()와 동일한 기능을 하는데, 조금 더 간편하게 쓸 수 있는 형식이라 생각하면 됩니다.\ndata.table에서는 변수의 이름만 넣었을 경우, 벡터의 형식으로 값을 불러옵니다. 그렇기 때문에 data.table의 형식을 유지하면서 데이터를 불러오려면 .() 혹은 list() 형식을 이용해야 합니다. 혹은 변수를 따옴표를 이용하여 작성하는 것도 동일한 결과를 가져옵니다.\n열(column)을 선택할 때, DT[,.(new_col_name=col)] 형식을 사용하여 새로운 열 이름을 지정해서 출력할 수도 있습니다.\n\nEX3[,.(MPG=mpg, CYL=cyl)]\n\n\n\n\n  \n\n\n\n위와 같이 mpg와 cyl에 대해서 변수 이름을 대문자로 바꿔준 것을 확인할 수 있습니다.\n다음으로는 변수로 열을 선택하는 방법에 대해서 알아보려고 합니다.\n예시를 위해 mpg, cyl, disp 세 변수를 묶는 VARS라는 새로운 변수를 임의로 설정하겠습니다.\n\nVARS&lt;-c(\"mpg\",\"cyl\",\"disp\")\n\n\nEX3[,..VARS]\n\n\n\n\n  \n\n\n\nVARS 라는 변수를 넣었을 때, 위에서 설정한 것처럼 mpg, cyl, disp에 대한 값들만 추출한 것을 확인할 수 있습니다. 여기서 중요한 것은, 변수 앞에 .. 을 넣어줬다는 것입니다.\ndata.table의 약속이라고 보면 되는데, 같은 결과를 도출하는 다른 형식들에 대해서 소개하려고 합니다.\n우선은, with = F가 있습니다.\n\nEX3[,VARS,with=F]\n\n\n\n\n  \n\n\n\nVARS 앞에 ..을 붙이지 않아도, with=F를 추가한다면 같은 결과를 도출하는 것을 확인할 수 있습니다.\n다음으로는 앞서 배운 .SD 과 .SDcols를 이용하는 방법에 대해 알아보겠습니다.\n\nEX3[,.SD,.SDcols=VARS]\n\n\n\n\n  \n\n\n\n.SD를 통해 전체 변수를 대상으로 하되, .SDcols로 특정 변수만을 설정하는 메커니즘입니다.\n또한 특정조건을 만족하는 값들에 대해 VARS의 변수 값을 알고 싶으면 다음과 같이 실행하면 됩니다.\n\nEX3[hp&gt;=130 & gear&gt;=4, ..VARS]\n\n\n\n\n  \n\n\n\nhp가 130을 넘고, gear가 4를 넘는 값들 중 VARS(mpg,cyl,disp) 변수에서 해당하는 값들을 보여주는 것을 확인할 수 있습니다.\n다음으로는 열을 제거하는 방법에 대해서 알아보려고 합니다. 행(row)을 제거할 때와 유사하게 - , ! 을 통해서 실행하면 됩니다. 그리고 같은 결과를 도출하는 다른 형식들에 대해서도 소개하려고 합니다.\n\nEX3[,-..VARS] ; EX3[,!..VARS] ; EX3[,.SD,.SDcols=-VARS]\n\n\n\n\n  \n\n\n\n마지막으로 열(column)의 값에 대해서 함수들을 이용해 값들을 가공하는 방법입니다.\nmpg와 hp의 평균에 대해서 구해보겠습니다.\n\nEX3[,.(mean(mpg), mean(hp))]\n\n\n\n\n  \n\n\n\n값을 실행할 경우, V1, V2라는 변수 아래에 값이 도출되는 것을 확인할 수 있습니다.\n위에서 배웠던, 변수에 새로운 이름을 부여하는 방식을 이용해보겠습니다.\n\nEX3[,.(MEAN_mpg=mean(mpg), MEAN_hp=mean(hp))]\n\n\n\n\n  \n\n\n\n위와 동일한 값에 변수의 이름이 생긴 것을 확인할 수 있습니다.\n.SD, .SDcols를 이용해서도 도출할 수 있습니다.\n\nEX3[,lapply(.SD,mean), .SDcols=(c(\"mpg\", \"hp\"))]\n\n\n\n\n\nmpg\nhp\n\n\n20.09062\n146.6875\n\n\n\n\n또한 행(row)의 자리에 특정 조건을 입력하여, 특정 조건을 만족하는 변수들에 대해서만 특정 함수를 적용할 수도 있습니다.\n\nEX3[gear==4, lapply(.SD,mean), .SDcols=c(\"mpg\",\"hp\")]\n\n\n\n\n\nmpg\nhp\n\n\n24.53333\n89.5\n\n\n\n\n2-3. by에 접근하기\nby는 집단을 나눕니다. 정확히는 옵션을 이용하여 그룹별로 함수를 적용할 수 있습니다.\nby = (그룹1, 그룹2, …)의 형식으로 두 개 이상의 그룹별로 함수를 적용할 수도 있는데, 이 때 괄호 앞에 있는 점(.)은 list를 의미하므로 꼭 포함시켜야 합니다. (ex. by=.(EXMD_BZ_YYYY, Q_SMK_YN) 와 같이 두 개 이상의 그룹으로 묶을 때는 .()의 형식을 이용해야 합니다.)\n어떻게 쓰이는지 바로 알아보도록 하겠습니다.\n여기에서는 dt(=09-15 공단 건강검진 데이터)를 이용해서 실습해보려고 합니다. EXMD_BZ_YYYY을 기준으로 집단을 분리한 후, 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같습니다.\n\ndt[,.(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by= EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY에 따라 각 연도를 기준으로, HGHT, WGHT, BMI가 정렬이 되었고, 그 값들의 평균을 그룹별로 구하여 나타낸 데이터 값입니다.\n만약 특정한 변수가 아닌, 모든 변수에 대해서 평균을 구하고 싶다면 .SD를 이용하면 됩니다.\n\ndt[,lapply(.SD,mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n위의 값은 평균을 낼 수 없는 변수들에 대해서도 일괄적으로 평균을 돌렸기 때문에 NA 값이 도출되었습니다. 값에 집중하기보다, 전체에 대한 함수를 적용하는 방식에 대해서 알아두면 좋을 것 같습니다. 만일 전체가 아닌 특정 변수에만 함수를 적용하고 싶다면 .SDcols 을 이용하면 됩니다.\n다음으로는 두 개 이상의 그룹 변수를 지정해 행(row)의 개수를 구해보겠습니다.\n키가 175cm 이상인 사람들에 대해서, 연도(EXMD_BZ_YYYY)와 흡연 여부(Q_SMK_YN)로 구분해보려고 합니다.\n\ndt[HGHT&gt;=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\n위에서 잠깐 언급한 .N 을 이용하여 특정조건에 부합하며 각 변수값에 해당되는 행(row)의 수를 구해보았습니다. 그러나, 여기에서 도출된 결과값의 문제는 Q_SMK_YN의 값이 섞여 있다는 것입니다.\n조금 더 정렬된 결과값으로 나타내고 싶을 때는, by 대신에 keyby를 이용하면 됩니다. keyby는 기존의 by에 오름차순/내림차순 기능이 포함되었다고 생각하면 됩니다. 만약 by를 이용하면서 정렬을 시키고 싶다면 마지막에 [order(정렬기준)]를 붙이면 됩니다.\n\ndt[HGHT&gt;=175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\ndt[HGHT&gt;=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)][order(EXMD_BZ_YYYY)]\n\n\n\n\n  \n\n\n\n연도를 기준으로 정렬을 하고 싶은 경우, 위와 같이 뒤에 [order(EXMD_BZ_YYYY)]를 붙여주면, 위의 값과 다르게 정렬된 것을 확인할 수 있습니다.\n다음으로, 특정 조건(HGHT&gt;=175)를 만족시키면서, 하나의 기준을 더 추가하여 분류하고 싶을 때는 다음과 같은 방식을 이용하면 됩니다.\n\ndt[HGHT&gt;=175, .N, keyby=.(Y2015 = ifelse(EXMD_BZ_YYYY&gt;=2015, \"&gt;=2015\", \"&lt;2015\"))]\n\n\n\n\n\nY2015\nN\n\n\n\n&lt;2015\n206\n\n\n&gt;=2015\n36\n\n\n\n\n\nHGHT가 175 이상인 사람들을 우선으로 뽑아놓고, 거기에서 Y=2015를 기준으로 행(row)의 갯수를 확인하였습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "href": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "title": "data.table 패키지 기초",
    "section": "3. 다른 기능들",
    "text": "3. 다른 기능들\n3-1. setkey()\n키를 설정합니다. 키를 활용하는 이유는 자료를 찾을 때, 그 탐색 및 처리 시간을 단축시키기 위함입니다.\nsetkey(DT,col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용합니다.\n만일 설정된 키를 제거할 경우, setkey(DT, NULL)를 활용합니다.\ndt 데이터를 이용하여, key를 설정하고 활용해보겠습니다.\n\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\ndt의 키로 EXMD_BZ_YYYY가 설정된 것을 확인할 수 있습니다. 다른 변수들도 setkey 함수에 추가로 입력하면, 그 변수들이 key로 저장된 것을 확인할 수 있습니다.\n다음으로는 키를 활용한 행(row) 선택에 대해서 알아보려고 합니다. dt[.(a)], dt[J(a)], dt[list(a)], dt[col==a] 중에서 아무거나 사용하여 행을 선택할 수 있습니다. 위에서 EXMD_BZ_YYYY를 key로 설정하였기 때문에, dt[J(a)]에서 a의 자리에 EXMD_BZ_YYYY에 포함되어 있는 값을 넣으면 그 값을 기준으로 데이터를 정리합니다.\n예시로,\n\ndt[J(2013)]\n\n\n\n\n  \n\n\n\n이 값의 경우, EXMD_BZ_YYYY가 2013인 값에 대하여 정리한 것을 확인할 수 있습니다.\n만약, key를 두 개 이상 설정해놓은 경우, a의 자리에 다른 조건을 연결하면 그 조건도 포함하고 있는 값이 도출됩니다.\n\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\ndt[J(2013,2)]\n\n\n\n\n  \n\n\n\n위의 과정은 key에 Q_HBV_AG를 추가한 뒤, 2013년도에 Q_HBV_AG가 2인 값들에 대해서 정리한 것입니다.\n3-2. Merge : data.table 병합\n다음으로는 두 개의 data.table에 대해서 공통된 column을 기준으로,\n하나의 data.table로 만드는 방법에 대해서 소개하려고 합니다.\ndt 파일의 설문조사에 관한 데이터(Q_)들을 하나의 변수 colvars로 편의를 위해 설정하였습니다.\n\ncolvars&lt;-grep(\"Q_\", names(dt), value=T)\n\n다음으로는 dt 데이터를 임의로 분리하여 dt1, dt2를 설정하겠습니다.\n\ndt1&lt;-dt[1:10, .SD, .SDcols=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\", colvars)]\ndt2&lt;-dt[6:15, -..colvars]\n\n본격적인 분석을 하기에 앞서, dt1 과 dt2에 대해서 간단히 살펴보겠습니다.\n행(row)을 기준으로는 6:10행까지가 겹치고,\n열(column)을 기준으로는 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM” 이 공통입니다.\ndt1, dt2 데이터를 이용해 merge 함수에 대해서 알아보도록 하겠습니다.\nmerge 에는 inner_join, full_join, left_join, right_join, anti_join 등이 있습니다.\n하나하나씩 예시와 함께 알아보도록 하겠습니다.\n처음으로 알아볼 것은 inner_join 입니다. 집합의 교집합 개념과 유사하지만, 약간의 차이점은 존재합니다.\n\ninner_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=F)\n\n\n\n\n  \n\n\n\n우선 inner_join을 실행함에 있어 단순하게 inner_join 함수를 이용해도 되지만, merge 함수에서 공통 변수인 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”을 직접 merge의 매개체로 설정할 수도 있습니다. 그리고 inner_join의 경우, merge 함수의 뒤에 all=F가 들어간다는 것을 유의하시면 될 것 같습니다. (뒤에 full_join과 비교)\ninner_join을 실행하였습니다. dt1과 dt2의 공통 행(row)에 속하는 6:10행까지를 기준으로 정렬하되, 각 값들이 dt1, dt2에서 가지고 있던 변수 값들도 그대로 가져온 것을 확인할 수 있습니다.\n결과값의 RN_INDI가 714509인 값을 살펴보겠습니다.\n이 변수값은 원래 dt1에서는 HGHT와 WGHT 등의 값을 가지고 있지 않았습니다. 그러나, inner_join을 하면서 dt2의 값을 그대로 받아와, HGHT, WGHT 등의 값을 부여받은 것을 확인할 수 있습니다.\n다음으로는 full_join 입니다. 집합의 합집합 개념과 유사합니다.\n\nfull_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=T)\n\n\n\n\n  \n\n\n\nfull_join을 실행하였습니다. dt1과 dt2의 모든 행이 나열된 것을 확인할 수 있습니다. (공통된 행(row)은 한번만 표시되었습니다. 또한 inner_join과 다르게 all=T 임을 확인할 수 있습니다.)\n여기서 유심히 봐야할 것은 1:5, 11:15행입니다.\n1:5행의 경우에는 dt에는 속해 있지만, dt2에는 속해있지 않습니다. 그렇기 때문에 1:5행은 HGHT, WGHT 등 dt2에만 있는 값들에 대해서는 받을 값이 존재하지 않아, NA로 표시된 것을 확인할 수 있습니다.\n반면, 11:15행의 경우에는 dt2에는 속해 있지만, dt1에는 속해있지 않습니다. 그렇기 때문에 Q_로 시작하는 변수값에 대해서 받을 수가 없어서 NA로 나온 것을 확인할 수 있습니다.\n다음으로는 left_join과 right_join 입니다.\n\nleft_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.x=T)\ndt2[dt1, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\nleft_join을 실행하였습니다. 변수의 값이 dt1의 row를 기준으로 설정된 것을 확인할 수 있습니다. 그러나, dt2의 column이 추가되어, HGHT, WGHT 등 기존의 dt1에 없던 변수들이 생긴 것을 확인할 수 있습니다. inner_join과 유사하게, dt2에 있는 변수들에 대해서는 left_join을 했을 때도, 원래 dt1에는 없었던 HGHT, WGHT 등의 값이 새로 생긴 것을 확인할 수 있습니다.\nright_join으로도 직접 실습하여 차이를 확인하시기 바랍니다.\n하나의 차이가 있다면, left_join을 했을 때는 all.x = T 였지만, right_join의 경우에는 all.y =T 를 사용합니다.\n\nright_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.y=T)\ndt1[dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\n마지막으로 anti_join이 있습니다. 예시부터 보이고 설명하도록 하겠습니다.\n\nanti_join(dt1,dt2)\ndt1[!dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\n직관적으로 확인할 수 있듯, dt2와 겹치지 않는 dt1의 값들에 대해서만 나타낸 것을 확인할 수 있습니다. 또한 다른 join들이 dt2의 column(variable)을 가져왔던 것과 다르게, anti_join은 오직 dt1의 변수들로만 구성된 것을 확인할 수 있습니다.\n만약에 anti_join(dt2,dt1) (또는, dt2[!dt1, on = c(“EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”))은 이라고 작성한다면 위와 반대로 dt2를 기준으로 데이터의 값이 도출됩니다.\n3-3. 수정 연산자 :=\ndata.table에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용합니다.\n수정 또는 생성하는 column이 두 개 이상이라면, DT[,c(“cola”, “colb”) := list(valA,valB)] 또는, DT[, “:=”(cola, colb)]의 형식을 사용합니다.\n즉, := 는 새로운 data.table을 생성하지 않고 기존의 데이터 테이블에 덮어씌우거나(수정), 새로운 컬럼을 추가합니다.\n다음 예시로 알아보겠습니다.\nBMI2 라는 새로운 변수를 data.table에 추가하려고 합니다.\n(BMI2 = WGHT/(HGHT/100)^2 를 하고, 소수점 첫째자리까지 반영)\n\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\n\n\n\n  \n\n\n\n열(column)의 맨 마지막에 BMI2가 새롭게 생긴 것을 확인할 수 있습니다.\n다음으로는 특정 조건을 충족하는 값들에 대해서 새로운 변수를 만들어 확인하는 것에 대해서 알아보려고 합니다.\n두 가지 조건을 설정하겠습니다. 하나는, BP_SYS가 140이 넘는지 그리고 BMI가 25가 넘는지에 대해서, factor로 바꾸어 0과 1로 나타내는 column에 대해서 추가하려고 합니다.\n\ndt[, ':=' (BP_SYS140 = factor(as.integer(BP_SYS&gt;=140)), BMI25 = factor(as.integer(BMI&gt;=25)))]\n\n\n\n\n  \n\n\n\ndt의 column에 새롭게 BP_SYS140과 BMI25 컬럼이 생성되어, 0과 1로 TRUE/FALSE를 보여주고 있습니다.\n그리고 := 을 이용해서 column을 삭제할 수도 있습니다. 위에서 만들어본 BMI25 column에 대해서 제거해보려고 합니다.\n\ndt[,BMI25 := NULL]\n\n\n\n\n  \n\n\n\n3-4. 데이터 재구조화\n마지막으로, data의 형태를 바꾸는 melt(wide to long), dcast(long to wide) 함수에 대해서 알아보겠습니다.\n우선 melt 함수입니다.\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있습니다. melt 함수의 기본 구조는 다음과 같습니다.\nmelt(data, id.vars, measure.vars, variable.name, value.name)의 구조를 가지고 있습니다.\n\nid.vars에는 data에서 그대로 유지할, 고정될 column에 대해 작성하면 됩니다.\nmeasure.vars에는 새로운 변수에 포함될 기존의 데이터 값들에 대해서 넣으면 됩니다.\nvariable.name에는 measure.vars에서 추출한 데이터들을 모은 변수에 대한 이름을 설정하면 됩니다.\nvalue.name에는 그 variable.name의 값들이 적히는 곳의 이름을 설정한다고 보면 됩니다.\n\n예시로 쉽게 설명해보겠습니다.\n\nmelt_EX&lt;-melt(dt,\n              id.vars=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n              measure.vars = c(\"TOT_CHOL\", \"TG\", \"HDL\", \"LDL\"),\n              variable.name = \"Lipie\",\n              value.name = \"Value\")\nmelt_EX\n\n\n\n\n  \n\n\n\n위의 예시에서 EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM 세 변수는 고정되어 있는 것을 확인할 수 있습니다. 그리고 TOT_CHOL, TG, HDL, LDL 값들이 Lipid라는 새로운 변수에 묶여있고, 그것들의 값이 Value에 나타나는 것을 확인할 수 있습니다.\nmelt 함수는 또한 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있습니다. melt 함수에 meausre = list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣습니다. 이 때 공통의 value.name을 지정할 수 있습니다.\n다음의 예시를 보겠습니다.\n\ncol1&lt;-c(\"BP_SYS\", \"BP_DIA\")\ncol2&lt;-c(\"VA_LT\", \"VA_RT\")\nmelt_EX2 &lt;- melt(dt,\n                 id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n                 measure.vars = list(col1, col2),\n                 value.name = c(\"BP\", \"VA\"))\nmelt_EX2\n\n\n\n\n  \n\n\n\n예시가 직관적이진 않지만 간단하게 설명을 하자면, col1(BP_SYS, BP_DIA)과 col2(VA_LT, VA_RT)의 내용이 매칭 되는 것입니다. 그래서 BP_SYS와 VA_LT일 때, variable에서 1로 나타나고, BP_DIA와 VA_RT일 때, 2로 나타나는 것입니다.\n(따라서, BP_SYS와 VA_RT의 값이 매칭되는 경우는 없습니다.)\n(또한 만약에 col1과 col2에 각각 새로운 변수가 하나씩 추가되었다면 그 값은 variable에 3으로 표시될 것입니다.)\n다음은 dcast 입니다.\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용합니다. 쉽게 설명하면, 방금 melt에서 행한 작업을 정확히 반대로 수행한다고 보면 됩니다.\ndcast의 기본 구조는 다음과 같습니다.\ndcast(data, formula, value.var, fun.aggregate)\n조금 더 실용적으로 작성하면 dcast(data, ID1+ID2+ID3+…~ variable, value.var = “val”)의 구조입니다.\n구조에 대해서 설명하자면,\n\ndata에는 dcast 함수를 실행할 데이터를 의미하고\nID1+ID2+ID3+…는 기존의 data부터 dcast 이후에도 고정적으로 유지될 변수들을 의미합니다.\nvariable은 melt 함수로 모아진 변수들을 다시 long to wide 하게 하기 위해 해당 변수들을 작성하는 곳입니다.\nvalue.var = ‘val’ 은 dcast 함수로 long to wide 하게 될 변수 값을 작성하는 공간입니다.\n그리고 variable과 value.var = 칸에는 melt를 하면서 지정한 변수 이름을 넣어주면 됩니다.\n\n예시로 알아보도록 하겠습니다.\n\ndcast_EX &lt;- dcast(melt_EX, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ Lipid, value.var=\"Value\")\ndcast_EX\n\n\n\n\n  \n\n\n\nLipid 변수에 하나로 묶어뒀던 TOT_CHOL, TG, HDL, LDL 변수가 다시 각각의 변수로 바뀐 것을 확인할 수 있습니다.\n다음은 dcast 함수를 조금 더 응용해서, 재구조화를 할 때 sum, mean 등의 집계함수를 이용해서 그룹별 요약 통계량을 나타내는 과정을 소개하겠습니다.\n\ndcast_EX2 &lt;- dcast(melt_EX, EXMD_BZ_YYYY ~ Lipid, value.var = \"Value\", fun.aggregate = mean, na.rm = T)\ndcast_EX2\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY, 즉 연도 변수를 기준으로 Lipid에 묶여있던 TOT_CHOL, TG, HDL, LDL 변수들의 평균에 대해서 (결측치를 제거하고) 나타낸 것을 확인할 수 있습니다.\n조금 더 구체적으로 fun.aggregate 뒤에는 기존에 존재하는 함수가 아닌, fun.aggregate = function(x){}의 형식으로 어떠한 함수도 이용할 수 있습니다.\n다음으로는 여러 개의 칼럼들을 묶어서 melt 한 data.table에 대해서도 dcast를 하는 것에 대해 설명하겠습니다. 기본적으로 dcast 구조와 동일하지만 약간의 차이가 있습니다. variable 칸에는 그대로 long to wide 하려는 변수 이름만을 작성하면 되지만, value.var = 칸에는 각각의 데이터 값의 이름을 다 적어야 한다는 차이점이 존재합니다.\n예시로 보이겠습니다.\n\ndcast_EX3 &lt;- dcast(melt_EX2, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ variable, value.var = c(\"BP\", \"VA\"))\ndcast_EX3"
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "href": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "title": "data.table 패키지 기초",
    "section": "4. 마치며",
    "text": "4. 마치며\n이상에서 R에서 데이터를 쉽고 빠르게 가공할 수 있는 data.table에 대하여 알아보았습니다.\n배운 내용을 가볍게 정리하고 마무리하도록 하겠습니다.\n1) 생성하기 : data.table은 기본적으로 설치되어 있는 프로그램이 아니기 때문에, package 설치가 필요하다.\n2) 기본구조 : data.table의 기본 문법은 DT [ i, j, by ] 형태이며 각각의 쓰임새는 다음과 같다.\n\ni : 행(row)을 선택\nj : 열(column)을 선택, data.table 전반에 함수를 적용\nby: 그룹을 구성, j에서 행한 함수를 by 그룹 별로 수행시킬 수 있음.\n\n3) 특수기호: data.table 에서만 확인할 수 있는 특수기호들이 있다. (.SD, .SDcols, .N, :=)\n4) KEY를 설정하여, 데이터에 조금 더 빠르게 접근할 수 있다.\n5) merge, melt, dcast 등의 함수를 통해 데이터를 보기 쉽게 가공할 수 있다.\n감사합니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html",
    "href": "posts/2022-08-26-shinyforpython/index.html",
    "title": "Shiny for Python",
    "section": "",
    "text": "이제 파이썬에서도 Dash 이외에도 shiny를 사용하여 반응형 웹앱을 쉽게 구현할 수 있게 되었습니다.\n아직 초기 단계라 많은 것이 구현되지는 않았고 Alpha단계이므로 API가 변경되거나 아래의 설명과 다른 점이 생길 수 있습니다. 글의 내용은 Shiny for Python를 참고했습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "title": "Shiny for Python",
    "section": "\n2.1 python과 Shiny 설치하기",
    "text": "2.1 python과 Shiny 설치하기\n우선 Shiny app을 저장할 디렉토리를 만듭니다.\n\n# make new directory for Shiny App\nmkdir myapp\n\n# change directory\ncd myapp\n\npython은 Python.org Anaconda 에서 설치할 수 있습니다.\npython이 설치되었다면 사용하는 디렉토리의 폴더에 python 가상 환경을 만들고 이를 활성화합니다.\n\n# Create a virtual environment in the .venv subdirectory\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\nshiny 를 설치합니다.\n\n# install shiny\npip install shiny"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "title": "Shiny for Python",
    "section": "\n2.2 Shiny Server에서 이용하기",
    "text": "2.2 Shiny Server에서 이용하기\nShiny for python은 Shinyapps.io, Shiny server, shinylive와 같은 다양한 방법으로 배포될 수 있습니다. 그중 shiny server에 배포하는 방법을 소개합니다.\nShiny Server는 v1.5.19 이상이 필요합니다. 만약 이전 버전을 사용하고 있다면 업데이트 합니다.\n\n#shiny-server v1.4.19\nwget https://download3.rstudio.org/ubuntu-18.04/x86_64/shiny-server-1.5.19.995-amd64.deb\ngdebi shiny-server-1.5.19.995-amd64.deb\n\nshiny server에서 python 파일을 실행하기 위해 config file을 수정해야 합니다.\nconfig file 은 /etc/shiny-server/shiny-server.conf 에 위치해있습니다.\n\n# Edit the file /etc/shiny-server/shiny-server.conf\nsudo vim /etc/shiny-server/shiny-server.conf\n\nconfig file 을 열어 코드 상단에 python 경로를 추가합니다.\n예를 들어 /home/js/myapp/venv/bin/python3를 사용하고 싶다면 코드는 아래와 같습니다.\n\n# Use system python3 to run Shiny apps\npython /home/js/myapp/venv/bin/python3;\n\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /srv/shiny-server;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n  }\n}"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "title": "Shiny for Python",
    "section": "\n3.1 Shiny App",
    "text": "3.1 Shiny App\n\n\npython\nR\n\n\n\n\n# import shiny\nfrom shiny import ui, render, App\n\n# ui\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"N\", 0, 100, 40),\n    ui.output_text_verbatim(\"txt\"),\n)\n\n# server\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"n*2 is {input.n() * 2}\"\n\n# This is a shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\n\n\n\nlibrary(shiny)\n\n# ui\nui = fluidPage(\n    sliderInput(\"n\", \"N\", 0, 100, 40),\n    textOutput(\"txt\")\n  )\n\nserver = function(input, output,session) {\n     output$txt = renderText(\"n*2 is\",input$n * 2,\"}\")\n  }\n\nshinyApp(ui, server)\n\n\n\n\n위는 ui에서 입력 받은 값을 server에서 2를 곱해주어 계산하고 이를 ui부분에서 출력하여 보여주는 예시입니다.\n\n먼저 from shiny import *를 통해 필요한 shiny 모듈을 불러옵니다.\nui의 ui.input_slider() 함수를 통해 입력값을 받습니다. 이처럼 인풋에 해당하는 부분은 ui.input_*() 함수를 통해 만들 수 있습니다. “n”은 해당 input의 이름에 해당하는 부분이며, N은 label, 0,100,40은 각각 min, max,value에 해당하는 인자입니다.\n입력된 인풋값을 서버에 전송하면 이를 토대로 계산하여 값을 return하고 @render.text 라는 decorator를 통해 텍스트 형태로 렌더링합니다.\n다시 ui 부분에서는 ui.output_text_verbatim() 함수를 통해 텍스트를 출력합니다. 이처럼 ui부분에서 출력할 때는 ui.output_* 함수가 사용됩니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "title": "Shiny for Python",
    "section": "\n3.2 Shiny for Python의 문법",
    "text": "3.2 Shiny for Python의 문법\n위의 예시를 이용하여 R과 Python의 문법을 간단하게 비교해보면 다음과 같습니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\n\nimport shiny\nfrom shiny import *\nlibrary(shiny)\n\n\nUI\napp_ui = ui.page_fluid(\nui = fluidPage(\n\n\ninput\n  ui.input_slider(“n”,“N”,0,100,40),   ui.output_text_verbatim(“txt”)   )\n  sliderInput(“n”,“N”,0,100,40)   textOutput(“txt”))\n\n\nServer\ndef server(in, out, session):\nserver=function(in,out,session){\n\n\ndecorator\n\n@output @render.text\n\n\n\noutput\n  def txt:     return input.x()\n output$txt = renderText (input $ x)   }\n\n\napp\napp = App(app_ui, server)\nshinyApp(ui, server)\n\n\n\n인풋에 해당하는 부분은 ui.input_*(), ui 부분에서 출력할 때는 ui.output_*()함수들을 사용합니다.\n서버 부분에서 def server(input, output, session): def txt(): 와 같이 파이썬의 함수 문법을 사용합니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\ndecorater\n\n@render.text … @reactive.event() @reactive.Calc() …@output\nrenderText() …  reactive({}) output$id\n\n\n예시의 server부분에서 @render.text @output이 사용된 것을 볼 수 있습니다. 이것은 함수를 인자로 받아 함수를 출력하는 decorator 라는 함수입니다. 보통 python 에서 여러 함수들이 부분적으로 중복될때 코드의 재사용을 용이하게 하기 위해 사용됩니다. 여기서는 그냥 함수의 일종이라고 생각하면 될 것 같습니다.\nR에서는 렌더링을 위해 renderPlot, renderText 같은 함수를 사용하지만 Python에서는 @render.text 같은 decorator를 사용합니다. 위의 예시에서는 txt() 라는 텍스트를 반환하는 함수에 @render.text라는 decorator가 적용됩니다. 이는 텍스트를 반환하는 함수를 리턴하는 함수로 텍스트를 렌더링해 주는 함수라고 생각할 수 있습니다.\ndecorator는 output, module, reactivity, rendering 등 많은 부분에서 사용됩니다. 하나의 함수에 여러개가 적용될 수 있으며 @render.text 는 @output 보다 먼저 적용되어야 하는 것과 같이 순서나 parameter가 정해져 있습니다.\n\n\n\npython\nR\n\n\nHTML\nui.div(), ui.a()\ntags$div, tags$a\n\n\nHTML 태그의 경우는 ui.tags.*() 를 통해서 사용할 수 있습니다. 예를 들어 li 태그의 경우 ui.tags.li()로 사용 가능합니다.\n일반적으로 많이 사용되는 div, span, a 같은 태그들은 ui.div()와 같이 ui 서브모듈에서 직접 사용할 수 있습니다.\n\n\n\npython\nR\n\n\nmutability\nobjects can be modified\nobjects cannot be modified\n\n\npython으로 shiny앱을 작성할때 가변성 처리(Handling mutability)도 고려해야합니다. Python에서 문자,숫자열,튜플 같은 간단한 객체들은 변경할 수 없지만 딕셔너리,리스트 같은 대부분의 객체들은 수정할 수 있습니다.\n이로 인해 반응형 프로그래밍에서 문제가 발생할 수 있습니다. 즉, 프로그램의 한 부분에서 객체를 수정하면 프로그램의 다른 부분과 값이 다른 문제가 발생할 수 있습니다.\n이러한 문제를 해결하기 위한 몇가지 방법들이 있습니다.\n첫번째는 두 값이 객체를 copy해서 동일한 객체를 먼저 가리키는 것을 피하는 것입니다. a = [1,2], b = a.copy와 같이 사용하게 되면 a의 값이 바뀌어도 b는 바뀌기 이전의 값을 가리키게 됩니다. 두번째는 객체를 변경하는 연산자나 매서드를 사용하는 것입니다. 예를 들어 a = [1,2]를 a = [1,2,3]로 만들고 싶을 때 a = a+[3]보다는 a.append(3)을 사용하는것이 바람직합니다. 마지막은 변경 불가능한 객체를 사용하는 것입니다. 리스트 대신 튜플을 사용하거나 pyrsistent 패키지의 리스트나 딕셔너리를 사용할 수 있습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "href": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "title": "Shiny for Python",
    "section": "\n4.1 shinylive 배포",
    "text": "4.1 shinylive 배포\nshinylive editor을 통해 간단하게 shinylive를 통해 배포해볼 수 있습니다 .\n이외에도 Netlify, GITHUB gist를 통해서도 가능하며 Sharing Shinylive applications를 참고할 수 있습니다."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html",
    "href": "posts/2022-10-17-medical-scientist/index.html",
    "title": "의료데이터분석가 성장기",
    "section": "",
    "text": "김진섭 대표는 10월 19일(토) 융합형 의사과학자 심포지움 에서 “의료데이터분석가 성장기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#요약",
    "href": "posts/2022-10-17-medical-scientist/index.html#요약",
    "title": "의료데이터분석가 성장기",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n의학연구지원 \\(\\rightarrow\\) 임상시험분석시장 목표\n천하3분지계: 법학, 의학, 종교\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n김옥균, 맹상군, 유비\n문제해결 \\(\\rightarrow\\) 문제정의 \\(\\rightarrow\\) 아름다움 \\(\\rightarrow\\) 신내림"
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#slide",
    "href": "posts/2022-10-17-medical-scientist/index.html#slide",
    "title": "의료데이터분석가 성장기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/medical-scientist 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-02-07-bambooforest-with-slack-api/index.html",
    "href": "posts/2023-02-07-bambooforest-with-slack-api/index.html",
    "title": "Python과 Slack API를 이용한 대나무숲 앱 제작",
    "section": "",
    "text": "직장 동료가 추천해준 이 글을 보고, 우리 회사 Slack에도 대나무숲을 만들어 보았습니다. 원 글은 Node.js를 이용했지만, Javascript에 익숙하지 않아 Slack에서 제공하는 Python용 Bolt API를 사용했습니다.\n본 글을 읽기에 앞서, Bolt Documentation을 읽고 오시면 이해해 큰 도움이 됩니다.\n\n\n\n먼저, Slack API에 접속하여, Create New App을 선택합니다. From Scratch를 선택한 후, App Name과 워크스페이스를 선택합니다. 저는 대나무숲 봇, Zarathu 워크스페이스를 선택했습니다.\n\n먼저, Incoming Webhooks에서, Incoming Webhooks를 활성화합니다.\n다음으로, Interactivity & Shortcuts에 들어가 Interactivity를 활성화시키고, Request URL을 본인의 ngrok주소 또는 실제 주소 + “/slack/events”(바뀌면 작동하지 않습니다.)로 설정합니다.\n아래의 Shortcuts에서, 두 가지를 추가합니다. 한 가지는 Global로(게시글 포스팅), 다른 한 가지는 On Messages로(댓글 포스팅) 합니다. Name, Short Description, Callback ID는 적당히 작성합니다.\n다음으로, OAuth & Permission에 들어가 아래의 Scopes에서, Bot Token Scopes에 필요한 권한을 추가합니다.\n\nchannels:history, read\nchat: write, write.customize, write.public\ncommands\nincoming-webhook\n\n그 후, 같은 페이지 상단에 보이는 Bot User OAuth Token이 자신의 Bot Token입니다.\n\n\n\n파일 구조는 아래와 같습니다.\nproject\n│   dockerfile\n│   requirements.txt    \n│   .env\n│   .gitignore\n│ \n└───src\n│   │   app.py\n│   │   name.py\n│   \n└───.venv\n    │   ...\n먼저, Python 가상 환경을 만들기 위해 다음 명령어를 실행합니다. python -m venv .venv\n그리고, venv를 활성화 시킵니다. source .venv/bin/activate (macOS)\n이제 Python 가상 환경에 접속했습니다.\n먼저, Slack API 및 dotenv를 사용하기 위해 다음 명령어로 bolt와 dotenv를 설치합니다.\npip install python-dotenv\npip freeze로 현재 설치된 pip 패키지를 조회 해 보면,\npython-dotenv==0.21.0\nslack-bolt==1.16.1\nslack-sdk==3.19.5\n위와 같습니다. 버전은 설치 한 시점에 따라 다를 수 있습니다.\n시작하기에 앞서, 환경변수를 설정하도록 하겠습니다.\n프로젝트 루트의 .env 파일에\nSLACK_SIGNING_SECRET=&lt;자신의 Signing Secret&gt;\nSLACK_BOT_TOKEN=&lt;자신의 Bot Token&gt;\nCHANNEL_NAME=대나무숲\n를 추가합니다.\nSigning Secret은 api.slack.com에서 자신의 앱을 선택한 후 Basic Information에 있는 Signing Secret 입니다.\n이제 실제로 코드를 만들어 보겠습니다. src/app,py를 생성합니다.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv(verbose=True)\n\nSLACK_SIGNING_SECRET = os.getenv('SLACK_SIGNING_SECRET')\nSLACK_BOT_TOKEN = os.getenv('SLACK_BOT_TOKEN')\nCHANNEL_NAME=os.getenv(\"CHANNEL_NAME\")\n\n로 dotenv 환경을 불러오고, 변수에 .env의 내용을 불러옵니다.\n다음으로,\n\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\nfrom slack_bolt import App\nfrom slack_sdk.errors import SlackApiError\n\n로 logging 기능과 Slack API를 불러옵니다. SlackApiError는 추후 Error Handling에 사용 될 에정입니다.\n\napp = App(\n    token=SLACK_BOT_TOKEN,\n    signing_secret=SLACK_SIGNING_SECRET\n)\n\n위 코드로, Slack API를 초기화합니다.\nSlack API는 Decorator를 통해 각각의 Action에 따른 기능을 수행합니다. FastAPI를 사용 한 경험에 비추어 설명 해 보면, 예를 들어 @app.post(\"/\")의 경우 “/”로 post 요청을 보낸 경우에 작동하는 것 처럼, @app.shortcut(\"post\")는 post라는 ID의 shortcut을 실행 한 경우에 작동합니다.\n우리는 Modal을 이용해 게시글과 답글을 작성할 수 있도록 할 것입니다. Slack에서는 Modal을 쉽게 생성할 수 있는 도구를 제공하고 있습니다. 그 도구를 통해 Modal을 생성한 후, action_id와 text, callback_id, private_metadata 등을 수정하면 쉽게 우리가 원하는 Modal 창을 얻을 수 있습니다.\n저는 아래와 같이 했습니다.\n\n우리는 - “reply” action - “port” shortcut - “reply” shortcut 을 받은 경우 modal이 필요하므로 아래와 같은 Decorator를 작성합니다.\n\n@app.action(\"reply\")\n@app.shortcut(\"post\")\n@app.shortcut(\"reply\")\ndef open_modal(ack, body, client, logger):\n  ...\n\nSlack API는 요청을 받는 경우 ack()를 실행하도록 요청하고 있습니다. 그러므로, open_modal(…)이 호출되는 경우 {python} ack()가 실행되어야 합니다.\n\ndef open_modal(ack, body, client, logger):\n  ack()\n\n다음으로, open_modal이 끝난 경우, 즉 유저가 Modal에 무엇인가를 입력하여 반환 된 경우를 보겠습니다. Modal이 닫히면, 그 내용을 대나무숲 봇이 #bambooforest 채널에 포스팅/댓글 작성 해야합니다. 이를 위해, Modal에 callback_id를 설정해야 합니다. 우리는 “view_post”라고 하겠습니다.\n\ndef open_modal(ack, body, shortcut, client, logger, block_actions):\n    ack()\n    try:\n        try:\n            try:\n                message_ts=body['message_ts']\n            except:\n                message_ts=body['container']['message_ts']\n        except:\n            message_ts=\"\" # Global의 경우 message_ts가 없음\n        result = client.views_open(\n            trigger_id=body[\"trigger_id\"],\n            view={\n                # View 위 내용 생략\n                \"callback_id\": \"view_post\",\n                \"private_metadata\": message_ts\n            }\n        )\n        logger.info(result)\n    except SlackApiError as e:\n        logger.error(\"Error creating conversation: {}\".format(e))\n\n그러면 위 “view_post” 콜백를 처리할 수 있는 함수를 만들어 보겠습니다. 마찬가지로 Decorator을 사용합니다.\n\n@app.view(\"view_post\")\ndef handle_submission(ack, body, client, view, logger):\n  ack()\n  logger.info(body)\n  message_ts=view['private_metadata']\n  content=view['state']['values']['post_input_block']['post_content_input']['value']\n  username=view[\"state\"][\"values\"][\"name_input_block\"][\"name_input\"]['value']\n\n“view_post” 콜백이 실행된 경우, ack() 하고 body를 logging 합니다. 이후, 복잡한 경로에 있는 값을 접근하기 쉽도록 값을 변수에 대입합니다. 우리는 message_ts를 private_metadata로 전달했습니다. 또, content와 username의 값은 view[“state”][“values”]의 [‘post_input_block’][‘post_content_input’][‘value’] 및 [“name_input_block”][“name_input”][‘value’]에 존재합니다.\n익명 게시판이므로, Username을 작성하지 않은 경우가 많을 것입니다. 이 경우, 아래에서 만들 username() 함수를 통해 랜덤한 username을 생성합니다. 또한, 게시글의 경우 message_ts 값이 없습니다(위에서 ““를 대입합니다). 경우에 따라 type=”게시글” 또는 type=“댓글”을 저장합니다.\n\nif username is None:\n  username=randname()\n\nif message_ts == \"\":\n  type=\"게시글\"\nelse:\n  type=\"댓글\"\n\n위에서 정리된 내용을 바탕으로 메시지를 발송합니다. message_ts가 있으면(즉 댓글이면) 메시지를 해당 thread에, 아니면 .env에서 지정한 채널에 발송합니다.\n\nsend_message=[\n  {\n    \"type\": \"header\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": \":bamboo: 익명 메시지 :bamboo:\"\n      }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": f\"*{username}님의 {type}입니다.*\"\n      }\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": f\"{content}\"\n  },\n  \"accessory\": {\n      \"type\": \"button\",\n      \"action_id\": \"reply\",\n      \"text\": {\n        \"type\": \"plain_text\",\n        \"text\": \"댓글 달기\",\n        \"emoji\": True\n      }\n    }\n  }\n]\n\nif message_ts == \"\":\n    client.chat_postMessage(channel=CHANNEL_NAME, blocks=send_message)\nelse: \n    client.chat_postMessage(channel=CHANNEL_NAME, thread_ts=message_ts, blocks=send_message)\n\nsrc/name.py는 다음과 같습니다.\n\nimport random\n\nfirstNames = [\n  \"깔끔한\",\n  \"근면한\",\n#  ...\n]\n\nlastNames = [\n  \"토끼\",\n  \"오리\",\n#  ...\n]\n\ndef randname():\n    return firstNames[random.randint(0, len(firstNames)-1)] + \" \" + lastNames[random.randint(0, len(lastNames)-1)]\n\nrandname()은 firstNames와 lastNames의 원소를 각각 랜덤으로 추출하여 합쳐 반환하는 함수입니다.\n\n위 firstNames, lastNames는 이 글의 GitHub 저장소를 참조하였습니다.\n\nsrc/app.py 상단에\n\nfrom name import randname\n\n을 추가합니다.\n마지막으로, src/app.py 마지막에\n\nif __name__ == \"__main__\":\n  app.start(3000)\n\n을 추가하고, 서버에서 python3 src/app.py를 실행하면 정상적으로 작동하는 것을 알 수 있습니다.\n\n\n\n생각보다 Slack API Documentation이 충실하고, 회사에서 서버도 자유롭게 사용하도록 허용해주고 있어 쉽게 구현할 수 있었습니다. 실제로는 Docker를 사용하여 회사 서버에서 사용하고 있습니다. 테스트 목적으로는 https://ngrok.com을 사용하면 쉽게 테스트 할 수 있습니다.\n다만, 한 달 정도 대나무숲을 사용 해 본 결과 실질적으로 수십 명 정도의 조직이어야 제대로 사용할 수 있을 것이라는 생각이 들었습니다.\n전체 코드는 링크에서 확인하실 수 있습니다.(글과 일부 바뀌었을 수 있음.)"
  },
  {
    "objectID": "posts/2023-02-07-bambooforest-with-slack-api/index.html#slack-apibolt를-사용해-대나무숲-앱을-제작한-후기",
    "href": "posts/2023-02-07-bambooforest-with-slack-api/index.html#slack-apibolt를-사용해-대나무숲-앱을-제작한-후기",
    "title": "Python과 Slack API를 이용한 대나무숲 앱 제작",
    "section": "",
    "text": "직장 동료가 추천해준 이 글을 보고, 우리 회사 Slack에도 대나무숲을 만들어 보았습니다. 원 글은 Node.js를 이용했지만, Javascript에 익숙하지 않아 Slack에서 제공하는 Python용 Bolt API를 사용했습니다.\n본 글을 읽기에 앞서, Bolt Documentation을 읽고 오시면 이해해 큰 도움이 됩니다.\n\n\n\n먼저, Slack API에 접속하여, Create New App을 선택합니다. From Scratch를 선택한 후, App Name과 워크스페이스를 선택합니다. 저는 대나무숲 봇, Zarathu 워크스페이스를 선택했습니다.\n\n먼저, Incoming Webhooks에서, Incoming Webhooks를 활성화합니다.\n다음으로, Interactivity & Shortcuts에 들어가 Interactivity를 활성화시키고, Request URL을 본인의 ngrok주소 또는 실제 주소 + “/slack/events”(바뀌면 작동하지 않습니다.)로 설정합니다.\n아래의 Shortcuts에서, 두 가지를 추가합니다. 한 가지는 Global로(게시글 포스팅), 다른 한 가지는 On Messages로(댓글 포스팅) 합니다. Name, Short Description, Callback ID는 적당히 작성합니다.\n다음으로, OAuth & Permission에 들어가 아래의 Scopes에서, Bot Token Scopes에 필요한 권한을 추가합니다.\n\nchannels:history, read\nchat: write, write.customize, write.public\ncommands\nincoming-webhook\n\n그 후, 같은 페이지 상단에 보이는 Bot User OAuth Token이 자신의 Bot Token입니다.\n\n\n\n파일 구조는 아래와 같습니다.\nproject\n│   dockerfile\n│   requirements.txt    \n│   .env\n│   .gitignore\n│ \n└───src\n│   │   app.py\n│   │   name.py\n│   \n└───.venv\n    │   ...\n먼저, Python 가상 환경을 만들기 위해 다음 명령어를 실행합니다. python -m venv .venv\n그리고, venv를 활성화 시킵니다. source .venv/bin/activate (macOS)\n이제 Python 가상 환경에 접속했습니다.\n먼저, Slack API 및 dotenv를 사용하기 위해 다음 명령어로 bolt와 dotenv를 설치합니다.\npip install python-dotenv\npip freeze로 현재 설치된 pip 패키지를 조회 해 보면,\npython-dotenv==0.21.0\nslack-bolt==1.16.1\nslack-sdk==3.19.5\n위와 같습니다. 버전은 설치 한 시점에 따라 다를 수 있습니다.\n시작하기에 앞서, 환경변수를 설정하도록 하겠습니다.\n프로젝트 루트의 .env 파일에\nSLACK_SIGNING_SECRET=&lt;자신의 Signing Secret&gt;\nSLACK_BOT_TOKEN=&lt;자신의 Bot Token&gt;\nCHANNEL_NAME=대나무숲\n를 추가합니다.\nSigning Secret은 api.slack.com에서 자신의 앱을 선택한 후 Basic Information에 있는 Signing Secret 입니다.\n이제 실제로 코드를 만들어 보겠습니다. src/app,py를 생성합니다.\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv(verbose=True)\n\nSLACK_SIGNING_SECRET = os.getenv('SLACK_SIGNING_SECRET')\nSLACK_BOT_TOKEN = os.getenv('SLACK_BOT_TOKEN')\nCHANNEL_NAME=os.getenv(\"CHANNEL_NAME\")\n\n로 dotenv 환경을 불러오고, 변수에 .env의 내용을 불러옵니다.\n다음으로,\n\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\nfrom slack_bolt import App\nfrom slack_sdk.errors import SlackApiError\n\n로 logging 기능과 Slack API를 불러옵니다. SlackApiError는 추후 Error Handling에 사용 될 에정입니다.\n\napp = App(\n    token=SLACK_BOT_TOKEN,\n    signing_secret=SLACK_SIGNING_SECRET\n)\n\n위 코드로, Slack API를 초기화합니다.\nSlack API는 Decorator를 통해 각각의 Action에 따른 기능을 수행합니다. FastAPI를 사용 한 경험에 비추어 설명 해 보면, 예를 들어 @app.post(\"/\")의 경우 “/”로 post 요청을 보낸 경우에 작동하는 것 처럼, @app.shortcut(\"post\")는 post라는 ID의 shortcut을 실행 한 경우에 작동합니다.\n우리는 Modal을 이용해 게시글과 답글을 작성할 수 있도록 할 것입니다. Slack에서는 Modal을 쉽게 생성할 수 있는 도구를 제공하고 있습니다. 그 도구를 통해 Modal을 생성한 후, action_id와 text, callback_id, private_metadata 등을 수정하면 쉽게 우리가 원하는 Modal 창을 얻을 수 있습니다.\n저는 아래와 같이 했습니다.\n\n우리는 - “reply” action - “port” shortcut - “reply” shortcut 을 받은 경우 modal이 필요하므로 아래와 같은 Decorator를 작성합니다.\n\n@app.action(\"reply\")\n@app.shortcut(\"post\")\n@app.shortcut(\"reply\")\ndef open_modal(ack, body, client, logger):\n  ...\n\nSlack API는 요청을 받는 경우 ack()를 실행하도록 요청하고 있습니다. 그러므로, open_modal(…)이 호출되는 경우 {python} ack()가 실행되어야 합니다.\n\ndef open_modal(ack, body, client, logger):\n  ack()\n\n다음으로, open_modal이 끝난 경우, 즉 유저가 Modal에 무엇인가를 입력하여 반환 된 경우를 보겠습니다. Modal이 닫히면, 그 내용을 대나무숲 봇이 #bambooforest 채널에 포스팅/댓글 작성 해야합니다. 이를 위해, Modal에 callback_id를 설정해야 합니다. 우리는 “view_post”라고 하겠습니다.\n\ndef open_modal(ack, body, shortcut, client, logger, block_actions):\n    ack()\n    try:\n        try:\n            try:\n                message_ts=body['message_ts']\n            except:\n                message_ts=body['container']['message_ts']\n        except:\n            message_ts=\"\" # Global의 경우 message_ts가 없음\n        result = client.views_open(\n            trigger_id=body[\"trigger_id\"],\n            view={\n                # View 위 내용 생략\n                \"callback_id\": \"view_post\",\n                \"private_metadata\": message_ts\n            }\n        )\n        logger.info(result)\n    except SlackApiError as e:\n        logger.error(\"Error creating conversation: {}\".format(e))\n\n그러면 위 “view_post” 콜백를 처리할 수 있는 함수를 만들어 보겠습니다. 마찬가지로 Decorator을 사용합니다.\n\n@app.view(\"view_post\")\ndef handle_submission(ack, body, client, view, logger):\n  ack()\n  logger.info(body)\n  message_ts=view['private_metadata']\n  content=view['state']['values']['post_input_block']['post_content_input']['value']\n  username=view[\"state\"][\"values\"][\"name_input_block\"][\"name_input\"]['value']\n\n“view_post” 콜백이 실행된 경우, ack() 하고 body를 logging 합니다. 이후, 복잡한 경로에 있는 값을 접근하기 쉽도록 값을 변수에 대입합니다. 우리는 message_ts를 private_metadata로 전달했습니다. 또, content와 username의 값은 view[“state”][“values”]의 [‘post_input_block’][‘post_content_input’][‘value’] 및 [“name_input_block”][“name_input”][‘value’]에 존재합니다.\n익명 게시판이므로, Username을 작성하지 않은 경우가 많을 것입니다. 이 경우, 아래에서 만들 username() 함수를 통해 랜덤한 username을 생성합니다. 또한, 게시글의 경우 message_ts 값이 없습니다(위에서 ““를 대입합니다). 경우에 따라 type=”게시글” 또는 type=“댓글”을 저장합니다.\n\nif username is None:\n  username=randname()\n\nif message_ts == \"\":\n  type=\"게시글\"\nelse:\n  type=\"댓글\"\n\n위에서 정리된 내용을 바탕으로 메시지를 발송합니다. message_ts가 있으면(즉 댓글이면) 메시지를 해당 thread에, 아니면 .env에서 지정한 채널에 발송합니다.\n\nsend_message=[\n  {\n    \"type\": \"header\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": \":bamboo: 익명 메시지 :bamboo:\"\n      }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": f\"*{username}님의 {type}입니다.*\"\n      }\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"plain_text\",\n      \"text\": f\"{content}\"\n  },\n  \"accessory\": {\n      \"type\": \"button\",\n      \"action_id\": \"reply\",\n      \"text\": {\n        \"type\": \"plain_text\",\n        \"text\": \"댓글 달기\",\n        \"emoji\": True\n      }\n    }\n  }\n]\n\nif message_ts == \"\":\n    client.chat_postMessage(channel=CHANNEL_NAME, blocks=send_message)\nelse: \n    client.chat_postMessage(channel=CHANNEL_NAME, thread_ts=message_ts, blocks=send_message)\n\nsrc/name.py는 다음과 같습니다.\n\nimport random\n\nfirstNames = [\n  \"깔끔한\",\n  \"근면한\",\n#  ...\n]\n\nlastNames = [\n  \"토끼\",\n  \"오리\",\n#  ...\n]\n\ndef randname():\n    return firstNames[random.randint(0, len(firstNames)-1)] + \" \" + lastNames[random.randint(0, len(lastNames)-1)]\n\nrandname()은 firstNames와 lastNames의 원소를 각각 랜덤으로 추출하여 합쳐 반환하는 함수입니다.\n\n위 firstNames, lastNames는 이 글의 GitHub 저장소를 참조하였습니다.\n\nsrc/app.py 상단에\n\nfrom name import randname\n\n을 추가합니다.\n마지막으로, src/app.py 마지막에\n\nif __name__ == \"__main__\":\n  app.start(3000)\n\n을 추가하고, 서버에서 python3 src/app.py를 실행하면 정상적으로 작동하는 것을 알 수 있습니다.\n\n\n\n생각보다 Slack API Documentation이 충실하고, 회사에서 서버도 자유롭게 사용하도록 허용해주고 있어 쉽게 구현할 수 있었습니다. 실제로는 Docker를 사용하여 회사 서버에서 사용하고 있습니다. 테스트 목적으로는 https://ngrok.com을 사용하면 쉽게 테스트 할 수 있습니다.\n다만, 한 달 정도 대나무숲을 사용 해 본 결과 실질적으로 수십 명 정도의 조직이어야 제대로 사용할 수 있을 것이라는 생각이 들었습니다.\n전체 코드는 링크에서 확인하실 수 있습니다.(글과 일부 바뀌었을 수 있음.)"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "이번 태스크의 목적은 바로 구글, 네이버, 다음 API를 모두 이용하여 검색결과를 DB화 시키는 작업입니다.\n사용자로 부터 query와 크롤링 원하는 검색 글 수를 인풋으로 입력받아서, API로 부터 제공받은 데이터를 가공하여 새로운 데이터프레임으로 만드는 작업을 수행합니다. 데이터 프레임의 컬럼은\n[Title] - 글 제목, [Link] - 글 링크 , [Description] - 글 미리보기 내용, [Search engine] - 검색결과를 제공한 포털 (naver,google,daumkakao), [search_date] -글을 검색한 날짜\n입니다.\n전체 JupyterNoteBook Source Code\nhttps://github.com/zarathucorp/blog/blob/master/source_code/Custom_Search_Zarathu.ipynb\n\n\n\n흔히 동적 크롤러로 Selenium을 많이 활용합니다. 하지만 이런 크롤링에는 기업의 자산인 데이터 자산을 침해한다는 문제점이 있습니다. 이와 관련하여 부동산 정보 플랫폼 직방의 데이터를 크롤링한 경위로 스타트업 방픽은 소송에서 얻은 데이터를 폐기하고, 직방에 2000만원을 지급하도록 2월 3일에 판결을 내려졌습니다. 따라서 홈페이지를 통해 정보가 공개됐다고 하더라도, 이런 타 사이트에 대한 데이터 베이스를 수집하는 행위는 위험하다고 생각합니다. 따라서 크롤링을 해야한다면 해도 되는지 확인을 하고 하거나, 개발자를 위한 도구로 제공된 API가 먼저 있는지 없는지 검토를 하는 것이 안전해 보입니다. (특히 얻은 데이터로 수익을 창출하는 경우는 더 더욱)\n\nhttps://www.joongang.co.kr/article/25139935#home\n\n\n\nAPI란 무엇인지 위키백과의 설명을 통하면, “API(application programming interface 애플리케이션 프로그래밍 인터페이스[*], 응용 프로그램 프로그래밍 인터페이스)는 컴퓨터나 컴퓨터 프로그램 사이의 연결이다. 일종의 소프트웨어 인터페이스이며 다른 종류의 소프트웨어에 서비스를 제공한다”라고 설명하고 있습니다. 즉 컴퓨터끼리 연결하고 소통하는 방식인데, 주로 우리가 API를 연결해서 쓸 때는 클라이언트와 서버를 연결하여 원하는 요청을 처리하는 것이 보통입니다. 오늘 이 글에서 사용할 API는 REST API라고 합니다. API와 구별되는 REST API의 특징으로는 API를 이용할 때 규칙이 정해져 있다는 특징이 있습니다.\nURL을 통해 소통할 방법을 서버와 클라이언트 사이에 주고 받는다. 그때 어떤 자원에 대해 어떤 행위를 요청받을지 URL에 명시를 해주는 것이 원칙입니다.\n\nGET : 리소스 생성\nPOST: 조회\nPUT : 수정\nDELETE: 삭제\n\n등이 있고, 서버는 요청에 대해 상태코드로 응답합니다\n상태코드에 따라 정상과 비정상적으로 처리했는지 response 개체를 받아옵니다.\n\n200: 정상\n400: 비정상"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#왜-크롤링을-하지-않았는지",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#왜-크롤링을-하지-않았는지",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "흔히 동적 크롤러로 Selenium을 많이 활용합니다. 하지만 이런 크롤링에는 기업의 자산인 데이터 자산을 침해한다는 문제점이 있습니다. 이와 관련하여 부동산 정보 플랫폼 직방의 데이터를 크롤링한 경위로 스타트업 방픽은 소송에서 얻은 데이터를 폐기하고, 직방에 2000만원을 지급하도록 2월 3일에 판결을 내려졌습니다. 따라서 홈페이지를 통해 정보가 공개됐다고 하더라도, 이런 타 사이트에 대한 데이터 베이스를 수집하는 행위는 위험하다고 생각합니다. 따라서 크롤링을 해야한다면 해도 되는지 확인을 하고 하거나, 개발자를 위한 도구로 제공된 API가 먼저 있는지 없는지 검토를 하는 것이 안전해 보입니다. (특히 얻은 데이터로 수익을 창출하는 경우는 더 더욱)\n\nhttps://www.joongang.co.kr/article/25139935#home"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#api란",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#api란",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "API란 무엇인지 위키백과의 설명을 통하면, “API(application programming interface 애플리케이션 프로그래밍 인터페이스[*], 응용 프로그램 프로그래밍 인터페이스)는 컴퓨터나 컴퓨터 프로그램 사이의 연결이다. 일종의 소프트웨어 인터페이스이며 다른 종류의 소프트웨어에 서비스를 제공한다”라고 설명하고 있습니다. 즉 컴퓨터끼리 연결하고 소통하는 방식인데, 주로 우리가 API를 연결해서 쓸 때는 클라이언트와 서버를 연결하여 원하는 요청을 처리하는 것이 보통입니다. 오늘 이 글에서 사용할 API는 REST API라고 합니다. API와 구별되는 REST API의 특징으로는 API를 이용할 때 규칙이 정해져 있다는 특징이 있습니다.\nURL을 통해 소통할 방법을 서버와 클라이언트 사이에 주고 받는다. 그때 어떤 자원에 대해 어떤 행위를 요청받을지 URL에 명시를 해주는 것이 원칙입니다.\n\nGET : 리소스 생성\nPOST: 조회\nPUT : 수정\nDELETE: 삭제\n\n등이 있고, 서버는 요청에 대해 상태코드로 응답합니다\n상태코드에 따라 정상과 비정상적으로 처리했는지 response 개체를 받아옵니다.\n\n200: 정상\n400: 비정상"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#구글-크롤링-코드-전체",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#구글-크롤링-코드-전체",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "2.1 구글 크롤링 코드 전체",
    "text": "2.1 구글 크롤링 코드 전체\n응답받은 response 이 코드에선 data로 변수명을 저장했습니다. 이 객체에 접근해서 우리의 목표에 맞게 데이터에 접근해서 한 줄 한 줄 데이터 프레임에 입력하여 저장하는 작업을 하겠습니다.\n\n2.1.1 url에 내용의 신뢰도가 낮은 사이트의 검색결과는 저장하지 않도록 예외처리를 합니다. (선택사항)\n\nTrash_Link = [\"tistory\", \"kin\", \"youtube\", \"blog\", \"book\", \"news\", \"dcinside\", \"fmkorea\", \"ruliweb\", \"theqoo\", \"clien\", \"mlbpark\", \"instiz\", \"todayhumor\"] \n\nurl 링크에, 티스토리, 지식인,유튜브, 블로그,책,뉴스,디시인사이드,에펨코리아,루리엡,더쿠,클리앙,엠엘비파크,인스티즈,오늘의유머의 검색결과는 제거하도록 했습니다.\n\ndef Google_API(query, wanted_row):\n    \"\"\"\n    input : \n        query : str  검색하고 싶은 검색어 \n        wanted_row : str 검색 결과를 몇 행 저장할 것인지 \n    output : \n        df_google : dataframe / column = title, link,description  \n        사용자로 부터 입력받은 쿼리문을 통해 나온 검색 결과를 wanted_row만큼 (100행을 입력받았으면) 100행이 저장된 데이터 프레임을 return합니다.\n    \"\"\"\n\n    query= query.replace(\"|\",\"OR\") #쿼리에서 입력받은 | 기호를 OR 로 바꿉니다 \n    query += \"-filetype:pdf\" # 검색식을 사용하여 file type이 pdf가 아닌 것을 제외시켰습니다 \n    start_pages=[] # start_pages 라는 리스트를 생성합니다. \n\n    df_google= pd.DataFrame(columns=['Title','Link','Description']) # df_Google이라는 데이터 프레임에 컬럼명은 Title, Link, Description으로 설정했습니다.\n\n    row_count =0 # dataframe에 정보가 입력되는 것을 카운트 하기 위해 만든 변수입니다. \n\n\n    for i in range(1,wanted_row+1000,10):\n        start_pages.append(i) #구글 api는 1페이지당 10개의 결과물을 보여줘서 1,11,21순으로 로드한 페이지를 리스트에 담았습니다. \n\n    for start_page in start_pages:\n      # 1페이지, 11페이지,21페이지 마다, \n        url = f\"https://www.googleapis.com/customsearch/v1?key={Google_API_KEY}&cx={Google_SEARCH_ENGINE_ID}&q={query}&start={start_page}\"\n        # 요청할 URL에 사용자 정보인 API key, CSE ID를 저장합니다. \n        data = requests.get(url).json()\n        # request를 requests 라이브러리를 통해서 요청하고, 결과를 json을 호출하여 데이터에 담습니다.\n        search_items = data.get(\"items\")\n        # data의 하위에 items키로 저장돼있는 값을 불러옵니다. \n        # search_items엔 검색결과 [1~ 10]개의 아이템들이 담겨있다.  start_page = 11 ~ [11~20] \n        \n        try:\n          #try 구문을 하는 이유: 검색 결과가 null인 경우 link를 가져올 수가 없어서 없으면 없는대로 예외처리\n            for i, search_item in enumerate(search_items, start=1):\n              # link 가져오기 \n                link = search_item.get(\"link\")\n                if any(trash in link for trash in Trash_Link):\n                  # 링크에 dcinside, News 등을 포함하고 있으면 데이터를 데이터프레임에 담지 않고, 다음 검색결과로 \n                    pass\n                else: \n                    # 제목저장\n                    title = search_item.get(\"title\")\n                    # 설명 저장 \n                    descripiton = search_item.get(\"snippet\")\n                    # df_google에 한줄한줄 append \n                    df_google.loc[start_page + i] = [title,link,descripiton] \n                    # 저장하면 행 갯수 카운트 \n                    row_count+=1\n                    if (row_count &gt;= wanted_row) or (row_count == 300) :\n                      #원하는 갯수만큼 저장끝나면\n                        return df_google\n        except:\n          # 더 이상 검색결과가 없으면 df_google 리턴 후 종료 \n            return df_google\n\n    \n    return df_google\n\nlink를 입력 받을 때 try except구문을 활용하여 예외처리를 하였습니다.이를 자세히 설명하게 되면, 특정 키워드로 검색했을 때 나오는 검색결과가 충분하지 않는 경우 response객체에 items 값이 null이 되게 됩니다. 따라서 이후의 작업들이 문제가 생기게 되는데, 이를 try except 구문을 통해 처리했습니다."
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#네이버-크롤링-코드",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#네이버-크롤링-코드",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "3.1 네이버 크롤링 코드",
    "text": "3.1 네이버 크롤링 코드\n\n# Naver_client_id = '~~~~' # 발급받은 ID를 입력해주세요 \n# Naver_client_secret = '~~~' \n\ndef Naver_API(query,wanted_row):\n    query = urllib.parse.quote(query)\n\n    display=100 \n    #네이버 검색 API는 한 페이지를 요청했을 때 몇개의 건수를 보여줄 것인지 인자로 표시할 수 있습니다. \n    start=1\n    # start page를 1로 설정합니다.\n    end=wanted_row+10000\n    # 끝내는 페이지를 원하는 행의 갯수보다 더 많이 설정했는데 이유는 , trashlink 보다 많은 데이터를 저장합니다.  \n    sort='sim'\n    # 네이버 API 검색 결과는 검색결과 데이터를 정렬하는 순서의 기준을 정합니다. \n\n    df= pd.DataFrame(columns=['Title','Link','Description'])\n    # 마찬가지로 title,link,description의 컬럼을 가진 데이터프레임을 생성합니다. \n    row_count= 0 \n    # dataframe에 정보가 입력되는 것을 카운트 하기 위해 만든 변수입니다. \n    \n    for start_index in range(start,end,display):\n        url = \"https://openapi.naver.com/v1/search/webkr?query=\"+ query +\\\n            \"&display=\" + str(display)+ \\\n            \"&start=\" + str(start_index) + \\\n            \"&sort=\" + sort\n\n        #url에 요청할 정보에 대한 내용을 담아 변수 선언하고, \n        request = urllib.request.Request(url)\n        #urllib.request 모듈을 통해 요청을만들고,  \n        request.add_header(\"X-Naver-Client-Id\",Naver_client_id)\n        request.add_header(\"X-Naver-Client-Secret\",Naver_client_secret)\n        # 그 요청에 헤더를 만들어서 클라이언트 아이디와, 비밀번호를 헤더에 입력합니다. \n        try:\n            response = urllib.request.urlopen(request)\n            # 요청하여 받은 내용을 response로 저장합니다.\n            rescode = response.getcode()\n            # response 객체에 담긴 응답 코드를 받아옵니다\n            if(rescode==200):\n                response_body = response.read()\n                # response 내용을 읽어들여 response_body에 저장합니다. \n                items= json.loads(response_body.decode('utf-8'))['items']\n                # 전체 response를 json화 한 뒤 key값이 items로 되어있는 값에 저장을 합니다. \n                remove_tag = re.compile('&lt;.*?&gt;')\n                # html문법의 태그들을 제거하는 컴파일러를 정규식을 패키지를 통해 생성합니다.\n                for item_index in range(0,len(items)):\n                    link = items[item_index]['link']\n                    # 아이템에 링크에 접근합니다\n                    if any(trash in link for trash in Trash_Link):\n                      # link url에 출처가 신뢰도가 낮은 사이트의 정보라면 데이터프레임에 저장하지 않고 넘어갑니다. \n                        pass\n                    else:\n                        title = re.sub(remove_tag, '', items[item_index]['title'])\n                        description = re.sub(remove_tag, '', items[item_index]['description'])\n                        # html 태그를 제거한 후, 제목 설명,링크 저장 \n                        df.loc[row_count] =[title,link,description]\n                        row_count+=1\n                        if (row_count &gt;= wanted_row) or (row_count == 300):\n                            return df\n                        \n        except:\n            return df\n\n큰 흐름은 구글에서 작성한 코드와 유사합니다.\n먼저 네이버 클라이언트 ID와 KEY값을 변수로 넣어줍니다. 경우에 따라 자기 혼자 쓰는 건 상관없겠지만 , 보통 키를 저렇게 코드에 보이게 노출하는 것 보다는 INPUT을 통해 보이지 않게 입력하는 것이 좋을 것 같습니다. 그리고 구글에서는 URL에 서비스 요청할 정보와 요청자의 신원을 확인할 수 있는 정보들을 모두 입력했습니다. 네이버에서는 조금 다르게 URL에 넣는 인자로는 어떤 검색결과를 받을지 표시하고, 몇 건이 출력될지 등 파라미터를 줘서 코드를 작성할 수 있습니다. 또 URL에 통해 사용자를 키값 정보를 같이 담지 않고 요청에 헤더를 추가하여 그곳에 담아 요청합니다.\n이후는 이전과 동일하게 네이버 웹문서 검색 API가 제공하는 JSON 데이터 형식을 보고 필요한 키-밸류값에 접근하여 데이터를 수집한 뒤 데이터프레임을 반환합니다.\n구글에서는 요청에 대한 응답(response) items에 데이터가 불용어와 html문법의 태그로 돼있는 것들이 나와있지 않았지만, 네이버에서 보낸 응답에는 불용어가 많이 섞여있어서 그것을 제거하기 위해 re 패키지의 정규식을 이용하여 글자만 남기도록 했습니다."
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#다음-api-활용-파이썬-코드",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#다음-api-활용-파이썬-코드",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "4.1 다음 API 활용 파이썬 코드",
    "text": "4.1 다음 API 활용 파이썬 코드\n\n# Kakao_API_key = '~~~~' # 발급받은 api키를 입력해주세요 \n\ndef Daum_API(query,wanted_row):\n    pages= wanted_row//10 \n    # 검색해야할 페이지를 10으로 나눈 몫으로 구합니다 \n    # 예: 100 행 검색이면 10페이지\n\n    method = \"GET\"\n    url = \"https://dapi.kakao.com/v2/search/web\"\n    header = {'authorization': f'KakaoAK {Kakao_API_key}'}\n    # 다음 카카오 API를 호출할 때는, header 딕셔너리로 생성하여 정해진 형식으로  API키를 넘겨줘야합니다. 형식은 위와 같습니다. \n\n    df= pd.DataFrame(columns=['Title','Link','Description'])\n    #데이터프레임을 생성합니다 \n    row_count=0\n    # row_count 변수를 생성합니다\n\n    for page in range(1,pages+10):\n      #여유있게 10페이지더 검색합니다 이유는 Link가 버려지는 경우를 위해서입니다. \n        params = {'query' : query, 'page' : page}\n        # 다른 API 호출방식과 달리 url string에 담아서 넘겨주는 형식이 아니라, 딕셔너리형태로 \n        # params와 header에 담아서 리퀘스트를 요청합니다. \n        request = requests.get( url, params= params, headers=header)\n        #요청한 내용을 받은 것을 request에 저장합니다. 변수명 request이지만 사실은 response\n        for i, item in enumerate(request.json()[\"documents\"], start=1):\n          #아이템 객체에서 url을 받아옵니다.\n            link = item['url']\n            try:\n              # date time이 null인 경우가 많아서 예외처리를 해서 경우를 나눴습니다. 앞의 4글자 년도 YYYY를 저장합니다. \n                written_year=int(item['datetime'][:4])\n            except:\n              # 작성일자가 null인 경우, 2023년으로 저장합니다.  \n                written_year = 2023\n\n            if (any(trash in link for trash in Trash_Link) or (written_year &lt;2020)):\n              # 출처가 신뢰도가 낮은 사이트거나, 작성된지 오래된 글의 경우\n                pass\n            else:\n                title= item[\"title\"]\n                description = item[\"contents\"]\n                df.loc[10*page+i] =[title,link,description]\n                #title과  본문 설명을 담아서 데이터프레임에 한줄한줄 append합니다. \n                row_count+=1\n                if (row_count &gt;= wanted_row) or (row_count == 300):\n                  # 행수가 원하는 로우만큼 채워지거나, Maximum_row개수인 300에 도달하면\n                  # html태그들을 제거한 후 반환합니다.\n                    remove_tag = re.compile('&lt;.*?&gt;')\n                    df['Title'] =df['Title'].apply(lambda x :re.sub(remove_tag, '',x))\n                    df['Description'] =df['Description'].apply(lambda x :re.sub(remove_tag, '',x))\n\n                    return df\n\n                \n\n    remove_tag = re.compile('&lt;.*?&gt;')\n    df['Title'] =df['Title'].apply(lambda x :re.sub(remove_tag, '',x))\n    df['Description'] =df['Description'].apply(lambda x :re.sub(remove_tag, '',x))\n    \n    return df \n\n카카오 API는 요청을 하면 보내주는 response 중 우리가 원하는 내용을 구글과 네이버는 items에 내용들이 담겨있었습니다. 카카오 다음검색 api는 보낸 응답에 documents라는 키값에 저장돼있음을 문서를 통해 확인할 수 있습니다. 문서를 통해 확인할 수 있고, 보통 api를 제공하는 사이트에서 테스트해볼 수 있어 입력물과 결과물의 형식을 요청해보지 않아도 확인가능합니다. 조금 다른 점이라면 이번에는 카카오 api에서 담겨있는 내용 중 작성일자를 Document-datetime에 저장돼있어서 검색 기간을 설정할 수 있는 기능을 구현했습니다.\n이렇게 나온 결과물에서 한번 더 필터링하는 것도 있을 수 있지만, 애초에 검색식을 이용하여 검색결과로 반환해야할 데이터를 줄일 수 있는데, 기간검색은 검색식으로 제어가 되지 않았습니다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html",
    "href": "posts/2023-04-21-sunburst/index.html",
    "title": "sunburstr 패키지 소개",
    "section": "",
    "text": "R에서 선버스트 차트를 그리는 방법은 다양하다. 그중에서 sunburstR 패키지가 도출 결과가 매우 깔끔하고, shiny에서도 실행이 가능하며 plotly나 ggplot을 사용하는 것보다 편리하다 느껴 이 글에서 소개하려고 한다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#선버스트-차트란",
    "href": "posts/2023-04-21-sunburst/index.html#선버스트-차트란",
    "title": "sunburstr 패키지 소개",
    "section": "1. 선버스트 차트란?",
    "text": "1. 선버스트 차트란?\n계층형 데이터 구조를 시각화할 때 이상적인 선버스트 차트는 계층 구조를 갖는 외부원과 내부원의 관계를 쉽게 보여주는 차트이다.\n\n\n\n\n\n\n\n\nsunburst chart\n\n\n\n\n\nhierarchical data\n\n\n\n\nimage from syncfusion*image from earo\n\n하나의 고리 또는 원이 계층 구조의 각 수준을 나타내며, 가장 안쪽에 있는 원이 계층 구조의 가장 높은 수준을 나타낸다. 복잡한 계층적 데이터를 명확하고 매력적인 방식으로 이해할 수 있는 능력에 있다.\n언제 사용하면 될까?\n\n전체 항목 중에서 개별 항목이 차지하는 비중을 파악하고 싶을 때\n\n\n트리맵 차트와 유사하게 데이터가 차지하는 비율을 면적으로 표현한 그래프이기 때문에 비중을 파악하기 용이하다.\n\n데이터의 패턴 및 관계를 나타낸다.\n\n일례로, 가장 많은 하위 범주를 가진 범주나 가장 인기 있는 하위 범주를 빠르게 파악할 수 있다.\n\n\n\n\n위계 구조를 가지는 데이터들의 패턴을 빠르게 확인하고 싶을 때\n\n\n데이터의 개별 요소들이 소속된 계층이 있는 경우 패턴 파악이 용이하다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#sunburstr-패키지",
    "href": "posts/2023-04-21-sunburst/index.html#sunburstr-패키지",
    "title": "sunburstr 패키지 소개",
    "section": "2. SunburstR 패키지",
    "text": "2. SunburstR 패키지\nR에 있는 sunburstr 패키지를 사용해 직접 계층적 데이터의 선버스트 차트를 만들어 보자.\n2.1. Setup\nsunburstr은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n\n# install.packages(\"data.table\")\n# install.packages(\"sunburstr\")\n\nlibrary(sunburstR);library(data.table);library(htmltools);library(magrittr)\n\n위의 과정을 통해 package 설치 및 불러오기를 실행합니다.\n2.2. Load data\n예제 데이터로는 2 단계에 걸쳐서 약을 복용한 149명 환자의 데이터를 사용한다.\n\n#load data\ndata &lt;- fread(\"https://raw.githubusercontent.com/seodaegal/blog-sy/main/blog_example_data/sunburst_example.csv\")\n\nrmarkdown::paged_table(data)\n\n\n  \n\n\n\n\n\ndata에서 Step.1은 첫 단계에서 복용한 약의 종류, Step.2는 첫 단계 이후에 두 번째로 복용한 약의 종류이다.\npersonCount는 특정 약을 복용한 사람의 수를 의미한다.\n2.3. Fix data\n데이터를 불러온 후, sunburstr 패키지를 적용하기 위해서는 sunburstr에 맞게 데이터를 재정렬해야 한다.\n\n\ndata 에는 단계마다 (Step.1, Step.2) 복용한 약이 한 개가 아니라 여러 개인 경우도 있어 각 단계 안에서도 복용한 약을 따로 분리해 주자.\n\n\n# size는 personCount로 지정\n\ndata2 &lt;-data[, names(data) := lapply(.SD, function(x) gsub(\"\\\\+\", \"-\", x)), .SDcols = names(data)]%&gt;%\n  .[, .(size= personCount, Step.1=Step.1, Step.2=Step.2)]%&gt;% \n  .[, c(\"st1level2\", \"st1level1\") := tstrsplit(Step.1, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;% #Step.1에서 복용한 약 종류는 2개라서, 2개의 다른 열로 나눈다\n  .[is.na(st1level1), st1level1 := st1level2]%&gt;% # 복용한 약의 종류가 하나라면 두번째 열에 똑같은 약을 입력하도록 한다다\n  .[, Step.1:= NULL]%&gt;%\n  .[, c(\"st2level1\", \"st2level2\", \"st2level3\") := tstrsplit(Step.2, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;% #Step.2에서 복용한 약 종류는 3개라서, 3개의 다른 열로 나눈다\n  .[is.na(st2level2), st2level2 := st2level1]%&gt;% \n  .[is.na(st2level3), st2level3 := st2level2] %&gt;% # 복용한 약이 3개가 아닐 경우 그전에 복용했던 약을 입력\n  .[,Step.2:=NULL]%&gt;%\n  setcolorder(., c(\"st1level1\", \"st1level2\", \"st2level1\", \"st2level2\",\"st2level3\", \"size\"))%&gt;% .[]\n\n# setcolorder( ) 함수로 열의 순서를 정한다\n\n\nrmarkdown::paged_table(data2)\n\n\n  \n\n\n\ndata2 에서\n\nst1 은 Step.1, st2 는 Step.2 에서 복용한 약을 의미한다.\n\nsize 는 해당 약을 복용한 personCount 이다.\n 열의 순서가 hierarchy에 영향을 미치기 때문에, 열의 순서를 ordering 하는데 주의가 필요하다!\n\n계층 구조가 높을수록 열의 순서가 왼쪽으로 오게 설정해야 한다.\n\n\n2.4. Building hierarchy on data\nd3r 패키지를 사용해 sunburstr에 적합한 환자들의 약 복용 데이터의 계층 구조를 구축한다. d3_nest 함수로 높은 순서대로 정리된 data2의 변수들을 그룹화한 뒤 데이터 갯수 변수 (count)를 지정한다.\n\n#datatable을 hierarchy로 convert\nlibrary(d3r)\nhierarchy &lt;- d3_nest(data2, value_cols = \"size\")\n\nhierarchy\n\n{\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"8\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Triflusal\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"61\",\"colname\":\"st1level2\"},{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"7\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"size\":\"1\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"3\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"36\",\"colname\":\"st1level2\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"4\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"5\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"cilostazol\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"11\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"cilostazol\",\"children\":[],\"size\":\"6\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"Triflusal\",\"children\":[{\"name\":\"Triflusal\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Triflusal\",\"size\":\"1\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"}],\"name\":\"root\"} \n\n\n2.5. Sunburstr 패키지 사용\nsunburstr 패키지를 사용해 차트의 색깔, 크기 등등을 바꿀 수 있다.\n\n#colors\ncolors&lt;- c('#FFAA00', '#2D5F91','#91D4D2', '#E8655F')\nlabels &lt;- c(\"KYR_Aspirin_20230414\", \"KYR_Clopidogrel_20230414\", \"KYR_cilostazol_20230414\",  \"KYR_Triflusal_20230414\")\n\n# setting data as 'hierarchy', then setting legend, width, height, count\n\nsb&lt;-sunburst(\n  hierarchy,\n  legend = TRUE,\n  width = \"100%\",\n  colors= list(range = colors, domain = labels),\n  height = 400,\n  count= TRUE\n)\n\nsb\n\n\n\n\n\n\n\n\n\n\nLegend\n\n\n\n\n\n\n\n차트의 오른쪽 위에 Legend를 누르면 색깔별 약 종류에 대해서 볼 수 있다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#shiny에-sunburstr-사용하기",
    "href": "posts/2023-04-21-sunburst/index.html#shiny에-sunburstr-사용하기",
    "title": "sunburstr 패키지 소개",
    "section": "3. Shiny에 sunburstr 사용하기",
    "text": "3. Shiny에 sunburstr 사용하기\nShiny에 sunburstr를 적용하는 방법은 여기에 잘 설명 되어있다. Shiny의 기초는 여기를 참고해주세요.\n Shiny의 목적은 유저가 R을 사용하지 않고도 제공된 UI를 통해 통계 분석을 (지금의 경우 선버스트 차트와 데이터 테이블을 보여주는) 진행할 수 있도록 하는 것이다.\n위에서 배운 sunburstr을 이용해 shiny를 만들어 보자.\n3.1. Shiny setup\n우선 R studio에서 New file에서 Shiny Web app을 선택한 뒤, Single File (app.R)을 생성한다. 필요한 패키지들을 불러오고 위에서 정리한 데이터들을 가져오기 위해, 코드를 global 파일에 저장해 두자.\n새 Shiny 파일 생성\n\n\napp.R에 필요한 library와 global R code 불러오기\n\nlibrary(shiny);library(sunburstR);library(data.table);library(DT)\nsource(\"global.R\")\n\n\nglobal.R 에는 아래 코드 작성:\n\n#in global.R\n\ndata &lt;- fread(\"https://raw.githubusercontent.com/seodaegal/blog-sy/main/blog_example_data/sunburst_example.csv\")\ndata2 &lt;-data[, names(data) := lapply(.SD, function(x) gsub(\"\\\\+\", \"-\", x)), .SDcols = names(data)]%&gt;%\n  .[, .(size= personCount, Step.1=Step.1, Step.2=Step.2)]%&gt;%\n  .[, c(\"st1level2\", \"st1level1\") := tstrsplit(Step.1, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;%\n  .[is.na(st1level1), st1level1 := st1level2]%&gt;%\n  .[, Step.1:= NULL]%&gt;%\n  .[, c(\"st2level1\", \"st2level2\", \"st2level3\") := tstrsplit(Step.2, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;%\n  .[is.na(st2level2), st2level2 := st2level1]%&gt;%\n  .[is.na(st2level3), st2level3 := st2level2] %&gt;%\n  .[,Step.2:=NULL]%&gt;%\n  setcolorder(., c(\"st1level1\", \"st1level2\", \"st2level1\", \"st2level2\",\"st2level3\", \"size\"))%&gt;% .[]\n\n\nlibrary(d3r)\nhierarchy &lt;- d3_nest(data2, value_cols = \"size\")\n\n#colors\ncolors&lt;- c('#FFAA00', '#2D5F91','#91D4D2', '#E8655F')\nlabels &lt;- c(\"KYR_Aspirin_20230414\", \"KYR_Clopidogrel_20230414\", \"KYR_cilostazol_20230414\",  \"KYR_Triflusal_20230414\")\n\n3.2. Table panel\n우리가 보여주고 싶은 Web App은 Table Panel이 2 개이다: 데이터 테이블과 선버스트.\n\n우선 navbar을 이용해 두 개의 table panel을 생성해보자.\n\n# using navbar to create table panel\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\"),\n                 tabPanel(\"Sunburst\")\n)\n\n\nserver &lt;- function(input, output) {\n\n}     \n\nnavbarPage( ) 함수를 통해 Title name과 table panel 이름들을 지정해준다.\n▶️Run App을 실행하면 아래와 같이 탭 3 개가 생긴 것을 확인할 수 있다.\n\n3.3. Data Panel 설정\n\n생성한 Data Panel에서는 메인 패널에 기존 약을 복용한 환자들의 예제 data set을 보여주면 된다.\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\",\n                          mainPanel(\n                            DTOutput(\"data\")\n                          )\n                 ),\n                 tabPanel(\"Plot\")\n)\n\nserver &lt;- function(input, output) {\n  \n  output$data &lt;- renderDT({\n    DT::datatable(data, rownames = F,  caption = \"Pathway 리포트\")\n  })\n}\n\n▶️Run App을 실행하면 데이터 테이블이 나오는 것을 확인할 수 있다.\n\n3.4. Plot Panel 설정\nPlot panel에서는 선버스트 차트를 보여주면 된다. 마우스를 차트 위에 올렸을 때, 약의 종류, 퍼센트 그리고 count가 나오도록 설정해야 한다.\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\",\n                          mainPanel(\n                            DTOutput(\"data\")\n                          )\n                 ),\n                 tabPanel(\"Plot\",\n                          mainPanel(\n                            sunburstOutput(\"sunburst\"),\n                            textOutput(\"selection\")\n                          )\n                 )\n)\n\n선버스트 차트를 output으로 보여줄 것이기 때문에 sunburstOutput( ) 함수를 사용한다. textOutput( )은 selection을 문자열 인수로 설정한다.\n\nserver &lt;- function(input, output) {\n  \n  output$data &lt;- renderDT({\n    DT::datatable(a, rownames = F,  caption = \"Pathway 리포트\")\n  })\n  \n  output$sunburst &lt;- renderSunburst({\n    add_shiny(sunburst(hierarchy, legend = FALSE, width = \"100%\", colors= list(range = colors, domain = labels), height = 400, count= TRUE))\n  })\n  \n  selection &lt;- reactive({\n    input$sunburst_mouseover\n  })\n  output$selection &lt;- renderText(selection())\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nadd_shiny( ) 함수를 통해 sunburst 차트를 shiny 앱에 추가한다. selection은 input$sunburst_mouseover 값에 반응하여 업데이트되도록 설정되어 있다. output$selection 은 sunburst 차트에서 유저가 선택한 항목의 이름을 출력한다.\n▶️Run App을 실행하면 선버스트 차트가 잘 나오는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#마치며",
    "href": "posts/2023-04-21-sunburst/index.html#마치며",
    "title": "sunburstr 패키지 소개",
    "section": "마치며",
    "text": "마치며\n지금까지 sunburstr 패키지 사용법을 알아보고 Shiny에서 직접 작동시켜 Web App을 제작해 보았다. 선버스트 차트는 계층적 데이터를 시각화하는데 강력한 도구이지만, 사용할 때 고려해야 할 부분들도 있다. 일례로, 선버스트의 세그먼트가 많아지거나 레이블이 길어지는 경우 가독성이 떨어질 수 있다. 또, 각도를 이용해 각 그룹의 크기를 나타내는 특성상, 작은 각도 차이도 큰 크기 차이를 나타낼 수 있어 그룹 간 비교가 다소 부정확할 수 있다. 선버스트 차트의 이러한 사항들을 고려하고 필요에 따라 대안적인 시각화 방법을 탐색하는 것도 필요 할 것 같다.\n\nReferences\n\n\n“Sunburst 2.0.0.” Sunburst 2.0.0 • sunburstR, 5 Feb. 2023, timelyportfolio.github.io/sunburstR/articles/sunburst-2-0-0.html.\n“Add Shiny Events — Add_Shiny.” Add Shiny Events — Add_Shiny • sunburstR, timelyportfolio.github.io/sunburstR/reference/add_shiny.html."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html",
    "href": "posts/2023-05-24-surveyDashboardR/index.html",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "",
    "text": "R Shiny의 활용성은 무궁무진합니다. 다양한 Shiny 전용 패키지와 함수, Javascript, CSS와의 연계성 등 다양한 기능을 활용해 반응형 웹페이지 제작이 가능합니다.\n본 글에서는 이렇게 무궁무진한 R Shiny로 웹 기반 방역관리 위험도 평가 대시보드를 제작한 경험을 공유하며 사용한 여러 기능을 소개하려고 합니다.\nUI의 전반적인 레이아웃은 Shinydashboard를 사용했으며 본문에선 Shinydashboard와 Shiny의 기본 함수에 대한 설명은 생략합니다. 두 패키지에 대한 기본적인 함수 및 구조 설명은 아래 링크를 참조하세요.\nShinydashboard package : Shiny Dashboard\nShiny package : Mastering Shiny"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#db-dbi-rsqlite",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#db-dbi-rsqlite",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "1. DB : DBI & RSQLite\n",
    "text": "1. DB : DBI & RSQLite\n\n본 대시보드는 사용자별로 데이터 무결성이 유지되어야 했고 지속적인 데이터 수집이 필요했기에 DB를 따로 운영하기로 했습니다.\nDB를 다룰 수 있는 DBI 패키지와 간단한 DBMS인 SQLite를 사용할 수 있는 RSQLite를 사용하여 DB 설계 및 유지 보수를 진행했습니다.\n다음과 같이 DB connect / disconnect 함수를 따로 지정하여 초기 설정을 해줍니다.\n\nlibrary(DBI);library(RSQLite)\n\n# connection\ncon &lt;- function() {\n  DBI::dbConnect(SQLite(),\n            dbname = \"[path]/[DBname].sqlite\")    \n  \n  #기존에 SQLite DB가 존재한다면 Connected, 아니면 create DB\n}\n\n# disconnection\ndiscon &lt;- function(){\n  dbDisconnect(con())\n}\n\nDBI 패키지의 대부분의 함수는 parameter로 DBIconnection object를 요구합니다. 따라서 다음과 같이 con() 이라는 함수를 parameter로 호출하여 DBI의 함수를 사용할 수 있습니다.\n\n# Example :  DBname : database.sqlite\n\nDBI::dbExecute(con(), \"CREATE TABLE table1 (\n                                 keyvalue INTEGER(10) PRIMARY KEY,\n                                 value1 DATE,\n                                 value2 VARCHAR(20) CHECK(value2 IN ('a', 'b', 'c'))\n                                 )\")\ndiscon()\n\nDBI::dbExecute(con(), \"INSERT INTO table1 VALUES (?, ?, ?)\", c(1, \"2023-05-24\", \"a\"))\nDBI::dbGetQuery(con(), \"SELECT * FROM table1\")\n\ndbExecute 함수를 통해 실행하고자 하는 SQL문을 실행시킵니다. 이전에 생성한 con() 함수를 통해 DB에 연결한 뒤 SQL문을 문자열 형태로 입력하여 실행합니다. 또한 SQL문에 동적으로 R의 변수를 넣어야하는 경우, ?를 통해 SQL문을 입력한 뒤에 ?에 넣고자 하는 순서에 맞게 뒤에 vector 형태로 입력하면 됩니다.\ndbGetQuery의 경우SELECT문같이 SQL Query의 결과를 갖고 오고 싶은 경우 사용합니다. 추출 결과를 dataframe 형태로 가져옵니다.\n본 대시보드 제작 과정에선 RSQLite를 이용하여 Table 4개를 운용하였고, SQLite에서도 당연히 참조 무결성 제약조건(PK-FK)을 생성할 수 있기때문에 직접 R에서 코드를 작성하지 않고 SQL문을 통해 미리 제약조건을 생성할 수 있습니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#log-in-register-shinyauthr-customizing",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#log-in-register-shinyauthr-customizing",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "2. Log in / Register : shinyauthr Customizing",
    "text": "2. Log in / Register : shinyauthr Customizing\nShiny 패키지 중 Log in/Log out UI를 제공하는 패키지는 대표적으로 shinymanager와 shinyauthr가 있습니다. 그러나 Shinymanager에는 회원가입 기능을 추가하기가 어려운 부분이 있어 shinyauthr를 사용하되, 살짝 코드를 수정하여 Log in/Log out/Register option을 Web에 추가하였습니다.\nshinyauthr package의 주요 함수는 다음과 같습니다.\n\n\nloginUI : log in UI를 보여주는 함수로 화면에 나타나는 메시지를 수정가능합니다.\n\nloginServer : log in의 base가 되는 database를 설정하고 id, password 설정, 쿠키 로그인 설정이 가능합니다.\n\nlogoutUI : log out UI를 보여주는 함수로, 화면에 나타나는 메시지 및 css style을 수정할 수 있습니다.\n\nlogoutServer : log out시 실행되는 함수로, 사용자의 권한(user_auth)를 boolean reactive 형태로 관리합니다.\n\nlogin logic : loginUI에 ID/PW 입력 -&gt; loginServer에서 확인 후 권한 부여\nlogout logic : logoutUI의 logout button 클릭 -&gt; logoutServer에서 reactive하게 user_auth 상태 변경\n (자세한 내용은 shinyauthr package 참조) \nUI의 일반적인 구성은 다음과 같습니다.\n\nui &lt;- dashboardPage(\n  \n  skin = \"black\",\n  header = dashboardHeader(\n    title = (\"방역관리 위험도 평가\"),\n    tags$li(class = \"dropdown\", style = \"padding: 8px;\", shinyauthr::logoutUI(\"logout\"))\n  ),\n  \n  sidebar = dashboardSidebar(\n    # menu \n  ),\n  \n  body = dashboardBody(\n    shinyauthr::loginUI(\"login\",\n                        title = h4(HTML(\"&lt;center&gt; 이름과 전화번호를 입력해주세요 &lt;/center&gt;\")), \n                        user_title = \"이름\", \n                        pass_title = \"전화번호\", \n                        login_title = \"로그인\",\n                        error_message = h6(\"유효하지 않은 정보입니다. 처음이시라면 회원가입을 눌러주세요.\", style = \"color : red\"),\n                        additional_ui = tags$a(\n                          actionBttn(\n                            inputId = \"register\",\n                            label = \"회원가입\",\n                            style = \"fill\", \n                            color = \"danger\",\n                            size = \"xs\"\n                          )\n                        )\n    ),\n    # menu 별 UI\n  )\n)\n\n본 UI는 shinydashboard를 기반으로 구성되어있습니다. 따라서 header에 logout 버튼을 생성하려 했고, dashboradBody에 login UI를 배치해 메인 화면에 바로 로그인 화면이 나오도록 하였습니다. 또한 loginUI의 parameter를 원하는 텍스트로 설정하였고, 회원가입은 additional_ui parameter를 이용하여 추가적으로 shinyWidgets::actionBttn 에 따른 UI가 생성되도록 설계하였습니다. 아래는 로그인 화면과 회원가입 버튼 클릭 시의 화면입니다.\n\n\n\n\n로그인 화면\n\n\n\n\n회원가입 화면\n\n\n\n그러나 loginServer 함수의 경우, 기본 ID/PW 저장 DB 세팅이 tidyverse 패키지의 tibble형태로 작성되어 있기 때문에 동적 형태의 DB를 지원하고 있지 않습니다.\n예를 들어 회원 가입 후 즉시 로그인하기 위해선 새로운 사용자의 정보가 반영된 DB를 reactive하게 Call 해야하는데 이 부분에 제약사항이 있었습니다.\n따라서 reactive하게 DB 정보를 받아올 수 있도록 함수를 수정해야 했습니다. (다음 링크 참조 : Shinyauthr loginServer Customizing)\n\nserver &lt;- function(input, output, session) {\n  \n#login/logout function--------------------------------------\n\n  credentials &lt;- Myloginserver(\n      id = \"login\",\n      log_out = reactive(logout_init()),\n      reload_on_logout = TRUE\n  )\n  \n  logout_init &lt;- shinyauthr::logoutServer( \n    \"logout\", \n    reactive(credentials()$user_auth)\n  )\n  \n  userdata &lt;- reactive({\n    credentials()$info\n  })\n  \n}\n\nMyloginserver는 customizing 된 shinyauthr::loginServer 함수이며 미리 생성된 사용자 정보 DB table을 바탕으로 로그인이 되도록 설정되어 있습니다. 기본적으로 shinyauthr::loginServer 함수는 info와 user_auth라는 변수를 담고 있습니다.\n\ninfo의 경우, 로그인의 기반이 되는 DB 내에서 사용자의 ID/PW에 해당하는 row의 컬럼 값들을 table 형태로 저장하고 있습니다.\nuser_auth의 경우, 권한이 있느냐 없느냐를 나타내며 로그인이 완료될 시 TRUE, 아닐 시 FALSE 값을 가지게 됩니다.\n\n따라서 credentials이라는 변수에 loginServer 함수를 저장하면 reactive한 user_auth값과 info값을 지니게 됩니다.\nreload_on_logout = TRUE로 설정하면 로그 아웃 시 세션이 초기화되어 자동으로 로그인 화면으로 돌아갑니다. 기본적으로 세션이 초기화되면 credentials()$user_auth == FALSE가 되어 로그인이 취소되고 권한이 사라지므로, 좀 더 확실한 로그아웃을 위해 다음과 같은 옵션을 설정하였습니다.\nlogout_init은 logoutServer 함수로 logoutUI의 ID를 받아온 뒤, reactive 함수에 따라 logout 버튼을 보여줄 지 숨길 지 반응형으로 설정할 수 있습니다. 기본적으로 권한이 있느냐 없느냐에 따라 logout 버튼을 보여주거나 숨겨야하기때문에 위와 같이 설정해줍니다.\n또한 shinyauthr 패키지는 로그인 / 로그아웃 UI만 보여주고 로그인 상태에 따른 UI 변환 기능은 없기 때문에, 수동으로 로그인 완료시에만 보여주고 싶은 UI에는 옵션을 추가해야 합니다.\nreq(credential()$user_auth)\n다음과 같은 옵션을 uiOutput을 이용하여 renderUI에 추가하거나, 아래와 같이 MenuOutput 과 renderMenu를 이용하여 shinydashboard의 Menu 자체를 숨길 수 있습니다.\n\n#UI\n\nui &lt;- dashboardPage(\n  \n  # others\n  \n  sidebar = dashboardSidebar(\n    sidebarMenu(\n      id = \"tabs\",\n      menuItemOutput(\"check\")\n    )\n  ),\n  \n  body = dashboardBody(\n    tabItems(\n      tabItem(tabName = \"check\",\n              # UI contents\n      )\n    )\n  )\n)\n\n#Server\nserver &lt;- function(input, output, session) {\n  \n  output$check &lt;- renderMenu({\n    req(credentials()$user_auth)\n    menuItem(\"방역관리자 업무 점검\", icon = icon(\"check\", lib =\"glyphicon\"), tabName = \"check\")\n  })\n}"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#필수-응답-설문-설정-shinyvalidate",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#필수-응답-설문-설정-shinyvalidate",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "3. 필수 응답 설문 설정 : shinyvalidate\n",
    "text": "3. 필수 응답 설문 설정 : shinyvalidate\n\nshinyvalidate package는 selectInput, numericInput 등 Input function에 대해 사용자의 입력값에 대한 제약조건을 걸 수 있는 package입니다.\n기본적으로 필수적으로 응답해야는 부분에 대해 설정할 수 있으며, ‘&lt;’ 등의 연산자를 사용한 제약조건, ‘email’ 형식 제약조건 등 여러가지 option이 있습니다.\n본 개발에선 필수 응답 설문 항목에 대한 설정을 위해 다음과 같이 shinyvalidate package를 사용하였습니다.\n\n[validation name] &lt;- shinyvalidate::InputValidator$new()\n[validation name]$add_rule(\"input$[input variable]\", sv_required(\"[Warning message]\"))\n\n기본적인 Logic은 다음과 같습니다.\n\n\nInputValidator$new()를 통한 validation 변수 선언\n\nadd_rule을 통한 input variable별 Warning Message 작성\n\n\n\n회원가입 시 shinyvalidate 사용 예시\n\n다음과 같이 필수 입력 항목의 경우, 응답하지 않을 시 기본적으로 빨간색 테두리와 작성한 경고 메시지가 뜨게 됩니다.\n추가적으로 이러한 필수 응답 항목에 답하지 않을 시 다음단계로 지나가지 못하게 제약조건을 추가할 수 있습니다.\n    1. req([validation name]$is_valid())\n\n    2. if([validation name]$is_valid())\n다음과 같은 옵션을 추가하여 renderUI 혹은 actionButton click시의 전제 조건으로 추가하여 Web 설계가 가능합니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#reactable-onclick-활용-ui에서-동적으로-db-table-update하기",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#reactable-onclick-활용-ui에서-동적으로-db-table-update하기",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "4. Reactable onClick 활용 : UI에서 동적으로 DB Table Update하기",
    "text": "4. Reactable onClick 활용 : UI에서 동적으로 DB Table Update하기\n본 대시보드의 관리자 버전에선 관리자가 컨설팅 완료 여부를 입력해야하는 기능이 필요했습니다.\n또한 DB의 값이 관리자의 Web 내의 완료 여부 입력에 따라 같이 변경되어야 했습니다.\n따라서 reactable 형태로 DB의 테이블을 보여준 뒤 cell 마다 onClick 옵션을 추가하여 binary 형태로 값을 자유롭게 변경할 수 있도록 기능을 추가했습니다.\n아래는 reactable에 구현한 예시 코드입니다.\n\n consult_rt &lt;- function(table){\n    rt &lt;-reactable(\n      data = table, # DB에서 받아온 테이블\n      onClick = JS(\"function(rowInfo, colInfo) {\n                      var tb_index = {'colId': colInfo.id, 'rowId': rowInfo.id };\n                      Shiny.setInputValue('consult_index', tb_index, { priority: 'event' })}\"),\n      \n        #...이외 내용 생략\n        \n\n    )\n}\n\n먼저 onClick parameter를 통해 각 cell을 클릭할 시의 reactive action을 설정해줍니다.\nJS 함수를 통해 JavaScript 코드를 호출하였고, tb_index라는 variable을 선언하여 row Id와 column Id를 저장한 뒤 Shiny 내 consult_index라는 input 변수에 tb_index의 값이 저장되도록 하였습니다.\n따라서 이 input 변수를 다음과 같이 활용하였습니다.\n\nobserveEvent(input$consult_index,  {\n    # table : reactable에 사용한 table로 똑같이 DB에서 받아옴\n\n    rowid &lt;- as.integer(input$consult_index$rowId) + 1\n    colid &lt;- input$consult_index$colId \n    colname &lt;- consult_list[match(colid, consult_list_name)] #원래 DB 컬럼명\n    userKey &lt;- table[rowid][[\"KeyName\"]] #해당 row의 DB Key\n    value &lt;- table[rowid][[colname]] #실제 DB에서의 값\n    \n    colname &lt;- consult_list[match(colid, consult_list_name)]\n    date &lt;- as.character(Sys.Date())\n    if(value %&gt;% is.na()){\n      message &lt;- h5(\"컨설팅 완료 상태로 변경되었습니다.\")\n      query &lt;- paste0(\"UPDATE consult SET \", colname, \" = ? WHERE PKcolumn = ?\")\n      dbExecute(con(), query, c(date, userKey))\n      discon()\n    }else{\n      message &lt;- h5(\"컨설팅 미완료 상태로 변경되었습니다.\")\n      query &lt;- paste0(\"UPDATE consult SET \", colname, \" = ? WHERE PKcolumn = ?\")\n      dbExecute(con(), query, c(NA, userKey))\n      discon()\n    }\n    \n})\n\nconsult_incex에 저장된 row Id와 column Id는 index 형태로, input$consult_index$rowId 형식으로 값을 받아올 수 있습니다.\n받아온 index는 0부터 시작하고, DB table이 저장된 data.table 형태의 table은 index가 1부터 시작하기 때문에 +1 해주어 row의 index를 받아왔습니다.\n이렇게 row의 index와 column의 index를 rowid, colid에 저장한 다음 table[rowid][[\"Key Name\"]]을 통해 현재 row의 DB 내 key 값을 받아왔습니다.\n현재 DB는 컨설팅 완료 시에는 컨설팅 완료 날짜를, 미완료 시에는 NA로 저장되어 있기 때문에 이를 ifelse 구문을 활용하여 각각의 경우에 맞게 코드를 작성하였습니다.\n위에서 받아온 row의 DB Key 값을 이용해 SQL문으로 DB table에 접근하여 값을 update 해주었습니다.\n (이렇게 변경된 DB table이 반영된 reactable을 사용자에게 동적으로 보여주기 위해선 UI 함수와 reactable을 재호출해야하는데, 이 부분에 대해선 생략하겠습니다.) \n또한 shinyalert를 사용하여 값이 변경되었음을 팝업 메시지로 띄워주었습니다.\n아래는 실제 UI에서 구현된 예시입니다.\n\n\n\n\n변경 전\n\n\n\n\n변경 후"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#이-외",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#이-외",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "5. 이 외",
    "text": "5. 이 외\n(1) reactable : reactive download to csv\nadmin을 위한 web page 제작 과정에서, DB table 들로부터 받아온 사용자 정보를 reactable을 이용하여 다음과 같이 제작하였습니다.\n\n\n사용자 정보\n\n이러한 table을 csv로 다운로드 받을 수 있게 하면서도, 만약 사용자가 일부 사용자의 정보를 확인하고 싶어 검색 기능을 사용했을 때 보여지는 subset table을 reactive하게 csv로 다운로드 받을 수 있게 하려했습니다.\n이 부분은 Reactable 공식 문서의 JavaScript API 이용 부분을 차용하여 작성하였습니다.\n\nbox(width = 12, title = \"전체 사용자 정보\" %&gt;% h5c,\n                    reactableOutput(\"infotable\"), \n                    br(),\n                    htmltools::browsable(\n                      tagList(\n                        tags$button(\n                          tagList(fontawesome::fa(\"download\"), \"Download CSV\"),\n                          onclick = \"Reactable.downloadDataCSV('user_info', '[filename].csv')\"\n                        )\n                      )\n                    )\n    )\n\nUI 작성 Part에서 box를 통해 reactable을 이용한 사용자 정보를 보여주려고 한다면 위와 같이 filename 부분을 수정하여 작성하면 현재 Web에 보여지는 reactable을 Download to csv가 가능합니다.\n(2) lapply를 이용한 설문 UI 간단하게 만들기\n만약 설문의 스타일이 간단하거나 (Ex: 예/아니오 유형, 체크 유형, 점수 유형) 반복되는 경우 lapply함수를 통해 좀 더 간단하고 정갈하게 UI 및 Server 코드를 작성할 수 있습니다.\n예를 들어, 해당하는 항목에 체크하는 형식의 설문이라면, 아래와 같이 미리 설문내용만 list 형태로 만들어 놓을 수 있습니다.\n\n# example 5개 Question\n\nQ_list &lt;- c(\n    \" 시설 위험도 평가 후 시설 별 맞춤형 방역관리 지침을 마련하였는가?\", \n    \" 정기적으로 종사자들에게 방역 수칙 교육·안내 하였는가?\",\n    \" 발열 및 호흡기 증상 유무를 확인하고 증상이 있는 경우 즉시 검사받도록 안내하였는가?\",\n    \" 감염병 예방수칙 홍보 안내문을 잘 보이는 곳에 배치 하였는가?\",\n    \" 환기 대장 및 소독 대장을 배치 하였는가?\"\n)\n\n# Shiny Input 함수의 ID를 각 설문 항목별 name으로 지정 (이 때 DB를 사용하실 거라면 DB의 컬럼명으로 ID를 지정하면 간편합니다)\nnames(Q_list) &lt;- paste0(\"q\", 1:5)\n\n다음과 같이 설문 내용을 정리한 변수를 기반으로 Shiny 코드를 작성할 수 있습니다.\n\n# UI code \n  lapply(names(Q_list)[1:5], function(x){\n                              checkboxInput(x, Q_list[[x]], value = F)\n                             })\n\n# Server code\n  \n  ## 답변 여부에 따라 Yes or No 형식으로 저장 \n  inputlist &lt;- lapply(names(Q_list), function(x){\n      if(input[[x]]){\n        \"Yes\"\n      }else{\n        \"No\"\n      }\n  })\n  \n  ## DB에Insert (DB 컬럼 순서에 맞게) \n  DBI::dbExecute(con(), paste0(\"INSERT INTO [table name] values ('\", paste(unlist(inputlist), collapse = \"', '\"), \"')\") )\n  discon()\n\nShiny의 UI 내에서도 간편하게 위와 같이 lapply 함수를 사용하여 Input 함수를 나타내는 것이 가능합니다.\nInput[[x]]로 사용자의 입력 내용에 접근 가능하므로 다음과 같이 inputlist로 Input 값에 접근에 DB에 저장할 수 있습니다.\n설문 내용을 value로, Input ID 및 DB columnname을 name으로 할 시 좀더 간편하게 코드 작성이 가능합니다.\n답변 유형(Yes or No, 점수 등등)에 따라 설문 datatable을 만들어 놓은 뒤 이에 맞게 datatable 컬럼 별로 lapply 함수를 잘 사용하여 UI 코드를 작성하면 보다 간편하게 Shiny를 이용하실 수 있습니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#마치며",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#마치며",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "마치며",
    "text": "마치며\nR만을 이용하여 Server와 UI를 동시에 제작하고 배포할 수 있다는 것이 R Shiny 의 가장 큰 장점입니다. 또한 오픈 소스가 활성화되어 있어 다양한 사용자 개발 library를 이용할 수 있으므로 사용하고 싶은 기능은 웬만하면 사용이 가능하며, 기존 library의 함수를 원하는 방식으로 변형하여 사용할 수도 있습니다. JavaScript나 CSS와도 호환이 잘 되기 때문에 UI 디자인까지 R이라는 한 Tool로 작업이 가능하기 때문에 상당히 편리합니다.\n본 글에선 이러한 R Shiny 웹 App의 기반이 되는 로그인 시스템, DB 운용 등 잘 알려지지 않은(?) 여러가지 세세한 기능과 가능성에 대해 소개해보았습니다.\n이외에 여러 package를 공부해가며 나만의 Shiny Web App을 제작해보는 과정 역시 재미있으니, 많은 분들께서 R Shiny를 이용하여 멋진 웹 페이지 제작에 참여해보시길 바라겠습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html",
    "href": "posts/2023-09-09-wasm/index.html",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "",
    "text": "이번 글에서는 R 패키지 webR를 사용하여 별도의 서버 기능을 제공하지 않는 정적 웹페이지(이 블로그 같은!) 에서 R을 사용할 수 있게 하는 과정에 대해 소개합니다.\n단 이 글에서는 wasm에 대한 이론적 배경 내용보다는 wasm을 활용하는 방법을 위주로 소개합니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#개요",
    "href": "posts/2023-09-09-wasm/index.html#개요",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "",
    "text": "이번 글에서는 R 패키지 webR를 사용하여 별도의 서버 기능을 제공하지 않는 정적 웹페이지(이 블로그 같은!) 에서 R을 사용할 수 있게 하는 과정에 대해 소개합니다.\n단 이 글에서는 wasm에 대한 이론적 배경 내용보다는 wasm을 활용하는 방법을 위주로 소개합니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#정적-페이지",
    "href": "posts/2023-09-09-wasm/index.html#정적-페이지",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "정적 페이지",
    "text": "정적 페이지\nwebR의 이해를 돕기 위해서 먼저 정적 페이지 (Static Page)를 설명하겠습니다.\n위키피디아의 설명을 인용하면, 정적 페이지는 모든 상황에서 모든 사용자에게 동일한 정보를 표시하는 페이지로, 처음 만들어 놓은 콘텐츠만이 사용자에게 전달되는 페이지(HTML 문서)라고 볼 수 있습니다.\n서버에 저장되어 있는 데이터가 변경되지 않는한, 사용자와의 상호작용이나 요청에 관계 없이 동일한 정보를 보여주기 때문에 회사의 홈페이지나 개인의  이력서 페이지, 기술 문서나 설명서, 블로그 등이 이에 해당됩니다.\n\n\n반대로 동적 페이지는 서버에서 사용자의 입력을 기반으로 추가 연산을 거쳐 결과물을 만들어내는 페이지로, 댓글이나 날씨, 잔여 재고수 , 연산 프로그램 등 정보의 업데이트가 있어야 하는 곳 등 대부분의 웹페이지에 사용됩니다.\n\n\n정적 페이지의 주요 특징 중 하나는 동적 페이지에 비해 배포가 쉽고, 비용이 거의 들지 않는 다는 것입니다.\n가령 동적 페이지의 예시중 하나인 shiny 어플리케이션을 제공하려면 별도의 서버를 준비하고 shiny server를 설치하여 배포 해야하지만 정적 페이지인 블로그의 경우 단순히 markdown을 github 에 올리는 것만으로 배포가 가능합니다.\n(연산 로직이 필요하지 않아 사실상 콘텐츠를 print로 출력하는 것과 큰 차이가 없습니다.)"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#웹과-r",
    "href": "posts/2023-09-09-wasm/index.html#웹과-r",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "웹과 R",
    "text": "웹과 R\n차라투에서는 openstat이라는 사용자의 PC에 R을 설치하거나, 프로그래밍을 하지 않고도 웹에서 R의 기능을 이용할 수 있게 하는 의학통계 앱을 서비스로 제공하고 있습니다.\n\n\n이 서비스는 간단하게 표현하면 다음과 같은 구조로 이루어져 있습니다.\n\n\n즉, openstat은 R의 연산 기능을 웹에서 활용하기 위해 동적 페이지를 구성했기 때문에 다음과 같은 문제점이 발생할 수 있습니다.\n\n사용량이 많아지면 서버가 뻗을 가능성 존재. 특히나 R은 모든 작업들을 메모리에서 하기 때문에 데이터에 따라 사용자 당 GB 단위 이상을 필요로 하기도 합니다.\n서버에 데이터가 왔다갔다하는 과정에서 (네트워크에서의) 보안 문제가 발생할 수도 있습니다.\n모든 연산 결과를 네트워크를 통해 전달해야하기 때문에 사용자의 PC 성능이 아닌, 네트워크 연결 상태와 서버의 PC 성능에 어플리케이션의 퍼포먼스가 영향을 받습니다.\n\n물론 openstat에서 제공하는 (의학 연구용) 분석은, 스케일이나 컴퓨팅 퍼포먼스가 필요하지 않은 간단한 통계 분석 작업이 많기 때문에 별다른 문제가 발생하지는 않습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#wasm-webr",
    "href": "posts/2023-09-09-wasm/index.html#wasm-webr",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "wasm & webR",
    "text": "wasm & webR\nwasm은 Web Assembly의 줄임말로, 웹 브라우저 (크롬)에서 실행할 수 있는 프로그래밍 언어 정도로 이해해도 충분합니다.\n2017년에 처음 등장한 개념으로, 각 프로그래밍 언어를 대상으로 작업되고 있으며 R에서는 webR이라는 이름으로 2022년 1월부터 작업이 진행되고 있습니다.\n이 webR의 정확한 원리는 복잡하지만, 간단하게는 아래 그림과 같이 자바스크립트 코드를 통해 사용자 PC에 백그라운드에서 실행되는 별도의 브라우저(web worker)를 만들어 서버 역할을 하게 한다 정도로 생각하셔도 충분합니다.\n\n\nwebR을 사용하기 위해서는 웹페이지에 web worker를 실행하기 위한 자바스크립트 코드를 추가해야 하는데, quarto를 사용해서 간단한 웹페이지를 만들어 보겠습니다. 이후 배포는 github (github page)를 사용하며, 이 글에서는 이를 다루지 않습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#quarto-page의-구성",
    "href": "posts/2023-09-09-wasm/index.html#quarto-page의-구성",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "Quarto page의 구성",
    "text": "Quarto page의 구성\nwebR을 정적 페이지에서 활용하기 위해 1. 코드를 입력할 공간과 2. 실행 결과를 보여줄 공간 (에디터), 3. 입력된 코드를 실행하게 할 버튼, 4. 마지막으로 앞서 언급했던 자바스크립트 코드를 추가해야합니다. (단, 자바스크립트가 에디터보다 먼저 로드되어야 하므로 앞서 선언해야함)\nquarto에서 html을 실행하게 하기 위해서 아래의 내용을 ```{=html} … ```로 감싸야 합니다.\n&lt;!-- 4. scripts --&gt;\n&lt;link rel=\"stylesheet\" href=\"codemirror.min.css\"&gt;\n&lt;script src=\"codemirror.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"r.js\"&gt;&lt;/script&gt;\n&lt;script src='webr-worker.js'&gt;&lt;/script&gt; \n&lt;script src='webr-serviceworker.js'&gt;&lt;/script&gt; \n&lt;script type=\"module\" src='editor.js'&gt;&lt;/script&gt;\n\n&lt;!-- editor --&gt;\n\n&lt;!-- 1. code editor --&gt;\n&lt;h4&gt;Editor&lt;/h4&gt;\n&lt;div id=\"editor\"&gt;&lt;/div&gt;\n\n&lt;!-- 3. run button --&gt;\n&lt;p style=\"text-align: right;\"&gt;\n  &lt;button class=\"btn btn-success btn-sm\" disabled type=\"button\" id=\"runButton\"&gt;\n    Loading webR...\n  &lt;/button&gt;\n&lt;/p&gt;\n\n&lt;!-- 2. code result --&gt;\n&lt;h4&gt;Result&lt;/h4&gt;\n&lt;pre&gt;&lt;code id=\"out\"&gt;&lt;/code&gt;&lt;/pre&gt;\n먼저 4. scripts는 (웹페이지에 보여지는 내용은 아니지만) 아래와 같이 크게 3종류로 구분할 수 있습니다.\n\n에디터를 위한 script: codemirror.min.css, codemirror.min.js, r.js\n\n백그라운드 프로세스 (사용자의 PC에서의 서버)를 위한 script: webr-worker.js, webr-serviceworker.js\n\n에디터와 백그라운드 프로세스를 연결하는 script\n\n두번째로 1. code editor와 3. run button 부분은 이미지처럼 UI에 코드를 넣는 에디터(껍데기)와 이 코드를 실행하는 버튼을 만드는데 사용됩니다.\n\n\n마지막으로 2. code result 부분은 아래처럼 처음에는 없지만 계산 결과를 보여주는 공간을 만드는데 사용됩니다.\n\n\n추가로 script 파일의 경우는 아래처럼 CDN(웹)에서 불러와도 되지만 해당 파일도 다운로드 받아 미리 정적 페이지에서 제공하게 실행할 수도 있습니다.\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.css\"&gt; (웹에서 사용자가 실시간으로 다운로드)\n&lt;link rel=\"stylesheet\" href=\"codemirror.min.css\"&gt; (페이지에서 미리 다운로드 후 제공)"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#실제-webr-결과",
    "href": "posts/2023-09-09-wasm/index.html#실제-webr-결과",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "실제 webR 결과",
    "text": "실제 webR 결과\n아래는 실제 위 코드를 차라투 블로그 (정적 페이지)에 추가해 빌드한 결과물로 Run code 버튼을 누를때마다 랜덤한 값을 생성하고 평균과 표준편차를 계산하는 webR 예시입니다.\n특별히 설치가 필요한 R 패키지와 함수를 사용하지 않는 이상\n\nprint(head(iris))\n\n위의 내용처럼 코드를 바꿔 작성하면 그 결과도 바뀌는 것을 확인할 수 있습니다.\n\nEditor\n\n\n\n  \n    Loading webR...\n  \n\n\nResult"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#정리",
    "href": "posts/2023-09-09-wasm/index.html#정리",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 webR을 이용하여 정적 페이지에서 사용자가 R을 실행할 수 있는 방법을 정리했습니다.\n이러한 방법의 장단점은 다음과 같습니다.\n장점\n\nR의 간단한 개념을 설명할때 효과적\n\n\nex) head()를 사용하면 데이터의 처음 6개 행을 보여준다\n\n코드와 실행 결과를 이미지로 첨부할 수도 있지만, head(iris) 같은 코드를 webR로 심어두는 것으로 내용을 더욱 효과적으로 전달할 수 있습니다.\n만약 learnr패키지를 함께 사용한다면 더욱 효과적으로 활용할 수 있습니다.\n\n동적 페이지를 위한 서버 비용을 지출 없음\n\n정적페이지는 무료로 제공할 수 있는 서비스가 많이 있고, 모든 R 연산은 사용자의 PC에서 이루어지기 때문에 별도의 서버 비용을 신경쓰지 않아도 좋습니다. (파일만 미리 서버에 넣어두면)\n\n데이터가 오고 가지 않기 때문에 보안상의 문제가 전혀 발생하지 않습니다. (이론상으로는 페이지의 모든 내용을 사용자가 저장해두면 오프라인 상태에서도 webR 활용 가능)\n단점\n\nwebR은 아직 초기 단계이기 때문에 지원되지 않는 기능이나, 활용할 수 있는 자료가 거의 없습니다.\n파일을 업로드/다운로드 하는 것 같이 단순하지 않은 작업은 webR에서 할 수 없습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#번외",
    "href": "posts/2023-09-09-wasm/index.html#번외",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "번외",
    "text": "번외\n\n\n\n\n\n\nquarto\n\n\n\n추가로, 위처럼 별도의 스크립트 파일 준비 없이 quarto에서 바로 webR을 사용할 수 있게 하는 템플릿도 있어  링크를 첨부합니다.\n\n\n\n\n\n\n\n\nShiny\n\n\n\nshiny application 또한 webR을 이용해서 정적페이지에서 제공할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html",
    "href": "posts/2023-09-18-shinyexe/index.html",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "",
    "text": "이번 글에서는 R Shiny 앱을 별도의 설치나 외부 연결 없이 폐쇄 환경에서도 실행할 수 있는 (Standalone) exe 파일로 패키징하는 과정을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#개요",
    "href": "posts/2023-09-18-shinyexe/index.html#개요",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "",
    "text": "이번 글에서는 R Shiny 앱을 별도의 설치나 외부 연결 없이 폐쇄 환경에서도 실행할 수 있는 (Standalone) exe 파일로 패키징하는 과정을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#standalone-app",
    "href": "posts/2023-09-18-shinyexe/index.html#standalone-app",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Standalone App",
    "text": "Standalone App\n먼저 글의 맥락을 더 효과적으로 전달하기 위해 Standalone App에 대해 간단히 정의해보겠습니다.\nAn app that can run independently without any external help.\n즉, 외부에 의존하지 않고 독립적으로 실행 가능한 앱으로 정의하고 싶은데요.\n여기서 외부에는 보통 Shiny를 실행하기 위해 쓰이는 웹 브라우저(크롬)가 포함될 수 있습니다.\n한편, R과 Rstudio 또한 Shiny를 실행하기 위해서 (로컬에서) 필요한 외부로 볼 수 있습니다.\n이러한 외부의 도움을 받지 않고, 다운로드 이후 압축만 풀어서 바로 실행할 수 있는 것을 Standalone App이라고 하며, 이러한 예시에는 (아는 사람은 아는 ㅎㅎ) 피카츄 배구.exe가 있습니다.\n\n\n\n출처: https://gbworld.tistory.com/1362\n이후 내용에서 소개되는 Shiny를 standalone app으로 만드는 것은 일반적인 R Shiny 개발과는 상당히 다르고 동시에 복잡합니다.\n그렇다면 Standalone app으로 만드는 것은 Shiny를 사용자에게 제공하는 다른 방법들과 어떤 차이점이 있을까요?\n\n\n\n이에는 여러가지를 생각해 볼 수 있지만 크게 2가지의 차이점이 있습니다.\n\n사용자 경험\n\nelectron으로 만들어진 Standalone App은 Shiny를 제공하는 서버와의 네트워크 연결이 불필요합니다. Shiny를 사용하기 위해 브라우저를 열고 특정 URL에 접속하는 대신 설치된 프로그램을 실행하는 것으로 충분합니다.\n이로 인해 네트워크와의 데이터를 주고 받는 과정에 리소스가 쓰이지 않고, 사용자의 (로컬 PC) 자원을 활용하기 때문에 살짝 더 좋은 퍼포먼스를 보일 수 있습니다.\n\n폐쇄성 환경\n\n또한 네트워크가 연결되지 않는다는 점은 shiny에 입력하는 값이 사용자의 PC 외부로 나가지 않고, 동시에 외부의 리소스가 PC에 들어오지 않는다는 이야기이기도 합니다.\n그렇기 때문에 금융이나 병원등 망분리 / 폐쇄 되어 있는 개발 환경에서도 Shiny를 실행할 수 있고 더 뛰어난 보안성을 가지게 됩니다. (단, Shiny가 계산을 위해 외부의 API 같은 자원을 사용하려면 네트워크 연결이 필요합니다)"
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#electron",
    "href": "posts/2023-09-18-shinyexe/index.html#electron",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Electron",
    "text": "Electron\nElectron (정확히는 electron.js) 은 크로미움 (크롬)과 node.js를 활용하여 html과 css, js 같은 웹 개발 결과물 (shiny가 이에 포함됩니다) 을 임베디드 형태로 만들 수 있는 프레임워크입니다.\n\n\n\nStandalone App을 만들기 위해서 electron의 기술적인 원리를 이해할 필요는 없지만, 흐름을 표현하면 아래와 같습니다.\n\n\n\n이 기술을 활용하여 shiny로 standalone app을 만들려는 시도는 꽤 오래 전부터 있었고, 2020년에 공유된 Turn a shiny application into a tablet or desktop app 아티클도 있지만, 관련된 자료들이 2022년을 마지막으로 아카이브되어 업데이트 되지 않았기 때문에 최신의 내용을 반영한 업데이트가 필요했습니다.\n차라투에서는 연구를 통해 기존의 내용 중 일부를 최적화하고, 최근 내용들을 반영한 뒤, Windows와 M1 Mac 2개의 OS에서 Standalone App을 개발하여 분리 환경에서 Shiny를 사용해야 하는 (공공기관을 포함한) 고객에게 제공하였고, 이후 개발에 활용할 수 있는 템플릿과 가이드를 제공하고 있습니다.\n국내에는 윈도우 사용자가 더 많기 때문에, 이번 글에서는 윈도우를 기준으로 방법을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#개발-준비",
    "href": "posts/2023-09-18-shinyexe/index.html#개발-준비",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "개발 준비",
    "text": "개발 준비\nStandalone shiny app을 개발하기 위해 shiny 개발에 필요한 R과 Rstudio 외에 추가 설치가 필요합니다.\n\n1. node.js 설치\n글이 작성되는 23년 9월을 기준으로, LTS인 18.17.1 버전을 설치합니다.\n\n\n\n이제 Rstudio를 관리자 권한으로 실행합니다. (아이콘을 오른쪽 클릭 후 선택)\n정상적으로 설치가 되었다면 Rstudio의 터미널에서 node -v, npm -v를 실행하여 설치 버전을 확인할 수 있습니다.\n\n\n2. electron-forge 설치\nelectron-forge는 electron을 조금 더 쉽게 사용할 수 있게 하는 패키지라고 생각하셔도 좋습니다.\n이는 npm을 사용해 (R의 install.packages와 유사) 설치할 수 있으며, 마찬가지로 Rstudio의 터미널에서 아래의 명령어를 입력하여 설치합니다.\nnpm i -g @electron-forge/cli\n\n\n3. 템플릿 포크 / 클론\n차라투 github에서 제공하는 템플릿을 자신의 계정에 포크 후, 클론하여 로컬 PC에 다운로드 받습니다.\nhttps://github.com/zarathucorp/shiny-electron-template-windows-2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. R project 열기\n템플릿 폴더의 shiny-elecgtron-template-windows-2023.Rproj를 Rstudio에서 실행합니다.\n이제 Rstudio 터미널의 작업 디렉토리가 해당 프로젝트의 위치로 변경됩니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#electron-app-만들기",
    "href": "posts/2023-09-18-shinyexe/index.html#electron-app-만들기",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Electron App 만들기",
    "text": "Electron App 만들기\n\n1. electron app 템플릿을 설치\nRstudio의 터미널에서 npx create-electron-app myApp을 실행하여 템플릿을 설치합니다.\n이때 myApp이 Standalone App의 이름이 되며 app을 제외한 다른 이름으로 변경할 수 있습니다.\n정상적으로 실행되었다면 디렉토리에 myApp 폴더가 새롭게 생기는 것을 확인할 수 있습니다.\n\n\n\n\n\n2. github 템플릿의 파일을 myApp으로 이동\nelectron app 에서 기본으로 제공하는 템플릿은 shiny를 개발하기에는 약간 다른 내용들이 있어서 차라투 github에 제공된 파일로 교체합니다.\n이때 만들어진 myApp으로 이동해야 하는 파일은 아래의 5개입니다.\n\nshiny (폴더)\nsrc (폴더)\nadd-cran-binary-pkgs.R\nget-r-win.sh\nstart-shiny.R\n\n\n\n\n이후 Rstudio의 터미널에서 cd myApp으로 디렉토리를 이동합니다.\n\n\n3. Standalone R 설치\nelectron 에 포함 시킬 local R을 현재 프로젝트에 설치합니다.\n단, 이때 기존에 사용중인 R의 버전과 동일한 버전을 설치해야 하며, 23년 9월에 최신 버전인 4.3.1을 기준으로 사용합니다.\nlocal R은 Rstudio의 터미널에서 sh ./get-r-win.sh를 실행하는 것으로 설치할 수 있습니다.\n정상적으로 실행되었다면 Done. 메세지와 함께 폴더에 r-win 이라는 폴더가 새롭게 만들어 진 것을 확인할 수 있습니다. r-win 내부의 구조는 아래와 같습니다.\n\n\n\n\n\n4. Shiny 패키지 설치\n예시에서 사용하는 shiny는 shiny 폴더의 app.R 코드를 사용합니다. (shiny 외의 다른 패키지를 사용하지 않는다면 해당 코드로 바꿔도 작동합니다) 이를 실행하기 위해 기본 R 에서 제공하는 패키지외에 (shiny를 포함한) 추가 CRAN 패키지를 설치합니다.\nRstudio의 터미널에서 Rscript add-cran-binary-pkgs.R을 입력하여 패키지를 설치합니다.\n실행전 (기본 R 패키지)\n\n\n\n실행후 (shiny를 포함한 패키지)\n\n\n\n\n\n5. node 패키지 설치\npackage.json의 내용을 다음과 같이 [fix] package-json의 내용으로 복사 붙여넣기합니다.\n이때 author와 repository는 본인의 내용에 맞게 수정해야합니다.\n\n\n\n이후 Rstudio 터미널에서 npm install을 입력하여 패키지를 설치할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#shiny-실행-및-패키징",
    "href": "posts/2023-09-18-shinyexe/index.html#shiny-실행-및-패키징",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Shiny 실행 및 패키징",
    "text": "Shiny 실행 및 패키징\napp.R은 개발 의도대로, 정상적으로 실행된다는 가정하에 Electron으로 shiny 를 실행하기 위해 Rstudio의 터미널에 electron-forge start를 입력합니다.\n\n\n\n큰 문제 없이 실행이 되었다면 이제 electron-forge make로 패키지를 만들 차례입니다.(zip)\n패키지 빌드를 위한 약간의 시간이 지난 후 out 디렉토리에서 앱과 zip 파일을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#정리",
    "href": "posts/2023-09-18-shinyexe/index.html#정리",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 Standalone의 특징과 electron을 활용하여 shiny를 standalone app으로 만드는 방법을 다뤄봤습니다.\nElectron을 활용하여 Standalone Shiny app을 만드는 것은 소개된 것처럼 R외의 개발 지식을 필요로 하기 때문에 경험이 없다면 다소 복잡하게 느껴질 수 있습니다. (글에서 다루지 않은 내용들도 있습니다)\n특히 electron 내부에서 local R과 electron을 위한 node 패키지들을 이미 포함해야 하기 때문에 간단한 shiny app도 용량이 200 메가바이트 정도부터 시작한다는 치명적인 단점도 존재합니다.\n그럼에도 불구하고 이는 Standalone 특유의 몇가지 특징들이 있어 사용자의 환경에 따라 적합한 shiny 개발 방법으로 고려해볼 수 있는 선택지 중 하나입니다.\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html",
    "href": "posts/2023-10-19-quarto-manuscript/index.html",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "",
    "text": "지난 9월 R Studio 2023.09.0 버전이 공개되었습니다. 새롭게 추가된 여러 기능 중에서 .qmd 파일을 이용해 학술 논문을 작성하고 웹페이지로 발행할 수 있는 Quarto Manuscript Project를 소개합니다. 본 게시글은 Quarto manuscript 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#개요",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#개요",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "",
    "text": "지난 9월 R Studio 2023.09.0 버전이 공개되었습니다. 새롭게 추가된 여러 기능 중에서 .qmd 파일을 이용해 학술 논문을 작성하고 웹페이지로 발행할 수 있는 Quarto Manuscript Project를 소개합니다. 본 게시글은 Quarto manuscript 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript-소개",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript-소개",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "Quarto Manuscript 소개",
    "text": "Quarto Manuscript 소개\nQuarto Manuscript는 .qmd 파일을 통해 학술 논문을 작성할 수 있는 프로젝트로, 다음과 같은 장점을 갖습니다.\n\n그림, 표, 수식, 인용 정보, 코드 블록 등 학술 논문에 필요한 모든 요소를 포함한 원고를 작성할 수 있습니다.\n작성한 원고를 웹사이트로 발행할 수 있어 공유가 쉽습니다.\n원고를 PDF, docx, 압축 파일 등 다양한 형식으로 손쉽게 다운로드할 수 있습니다.\n\nQuarto Manuscript를 활용해 작성한 학술 논문 예시는 여기에서 확인할 수 있습니다. 이 예시를 참고해 Quarto manuscript 원고를 작성하고 웹사이트로 발행해보겠습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript로-학술-논문-작성하기",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript로-학술-논문-작성하기",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "Quarto Manuscript로 학술 논문 작성하기",
    "text": "Quarto Manuscript로 학술 논문 작성하기\n1. 준비하기\nQuarto Manuscript를 사용하기에 앞서 (1) 2023.09 버전 이상의 RStudio와 (2) 1.4 버전 이상의 Quarto를 설치해야 합니다.\n2. 원고 작성하기\nRStudio에서 New project &gt; New Directory &gt; Quarto Manuscript를 선택해 프로젝트를 생성합니다.\n학술 논문 원고는 index.qmd 파일에서 작성합니다. 우선 작성할 논문의 제목, 저자, 키워드, 초록과 같은 정보를 아래 예시와 같이 YAML header에 입력합니다.\n\n\n\nindex.qmd\n\n# YAML head 작성 예시\n\n---\ntitle: La Palma Earthquakes\nauthor:\n  - name: Steve Purves\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: steve@curvenote.com\n    roles:\n      - Investigation\n      - Project administration\n      - Software\n      - Visualization\n    affiliations:\n      - Curvenote\n  - name: Rowan Cockett\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n      - Curvenote\nkeywords:\n  - La Palma\n  - Earthquakes\nabstract: |\n  In September 2021, a significant jump in seismic activity on the island of La Palma (Canary Islands, Spain) signaled the start of a volcanic crisis that still continues at the time of writing. Earthquake data is continually collected and published by the Instituto Geográphico Nacional (IGN). ...\nplain-language-summary: |\n  Earthquake data for the island of La Palma from the September 2021 eruption is found ...\nkey-points:\n  - A web scraping script was developed to pull data from the Instituto Geogràphico Nacional into a machine-readable form for analysis\n  - Earthquake events on La Palma are consistent with the presence of both mantle and crustal reservoirs.\ndate: last-modified\nbibliography: references.bib\ncitation:\n  container-title: Earth and Space Science\nnumber-sections: true\n---\n\n\nYAML header 아래에 본문을 작성합니다. 본문은 Quarto markdown 형식으로 작성하며, 기본적인 사용법은 Quarto Markdown Basics 공식 문서 혹은 R markdown 기초를 다루었던 이전 게시글에서 참고할 수 있습니다. 본 게시글에서는 논문 작성에 필요한 몇 가지 기능을 중점적으로 살펴보겠습니다.\n2.1. figure 삽입하기\nr 코드 블록을 활용해 figure를 삽입할 수 있습니다. 아래 예시와 같이 figure을 생성하는 코드와 함께 figure의 label, caption, alt text, width, height 등을 입력합니다. 본문에는 figure와 캡션만 표시되고 코드는 Article Notebook에서 모아 볼 수 있습니다. 본문에서 @fig-timeline와 같이 @ 기호와 figure의 label을 입력하면 figure을 인용할 수 있습니다.\n\n\n\nindex.qmd\n\n# figure 작성 예시\n\n# ```{r}\neruptions &lt;- c(1492, 1585, 1646, 1677, 1712, 1949, 1971, 2021)\nn_eruptions &lt;- length(eruptions)\n# ```\n\n# ```{r}\n#| label: fig-timeline\n#| fig-cap: Timeline of recent earthquakes on La Palma\n#| fig-alt: An event plot of the years of the last 8 eruptions on La Palma.\n#| fig-height: 1.5\n#| fig-width: 6\npar(mar = c(3, 1, 1, 1) + 0.1)\nplot(eruptions, rep(0, n_eruptions), \n  pch = \"|\", axes = FALSE)\naxis(1)\nbox()\n# ```\n\n# ```{r}\n#| output: false\navg_years_between_eruptions &lt;- mean(diff(eruptions[-n_eruptions]))\navg_years_between_eruptions\n#```\n\n\n2.2. 다른 .qmd 파일에 작성한 plot 삽입하기\nSection 3.2.1 에서와 같이 간단한 그림이 아닌, 연구에서 사용된 데이터를 통해 plot을 그리고 이를 본문에 삽입하기 위해서는 디렉토리에 notebooks 폴더를 생성해야 합니다. 생성한 notebooks 폴더 안에 데이터 파일을 넣고 새로운 .qmd 파일을 만들어 plot을 그리는 r 코드 블록을 작성합니다.\n이후 index.qmd 파일로 돌아와 아래와 같은 코드를 작성하면 plot이 삽입됩니다.\n\n\n\nindex.qmd\n\n# explore-earthquakes.qmd 파일에 작성한 fig-spatial-plot 삽입 예시\n\n{{&lt; embed notebooks/explore-earthquakes.qmd#fig-spatial-plot &gt;}}\n\n\nnotebooks 폴더의 .qmd 파일에서 작성한 코드는 마찬가지로 Article Notebook에서 모아 볼 수 있습니다.\n2.3. 참고문헌 작성하기\n참고문헌은 아래 예시와 같이 references.bib 파일에 BibTeX 형태로 작성합니다. index.qmd 파일의 본문에서 @marrero2019와 같이 @ 기호와 참고문헌의 label을 입력하면 참고문헌을 인용할 수 있습니다.\n\n\n\nreferences.bib\n\n# 참고문헌 작성 예시\n\n@article{marrero2019,\n  author = {Marrero, Jos{\\' e} and Garc{\\' i}a, Alicia and Berrocoso, Manuel and Llinares, {\\' A}ngeles and Rodr{\\' i}guez-Losada, Antonio and Ortiz, R.},\n  journal = {Journal of Applied Volcanology},\n  year = {2019},\n  month = {7},\n  pages = {},\n  title = {Strategies for the development of volcanic hazard maps in monogenetic volcanic fields: the example of {La} {Palma} ({Canary} {Islands})},\n  volume = {8},\n  doi = {10.1186/s13617-019-0085-5},\n}\n\n\n2.4. journal template 적용하기\njournal template은 PDF에 적용됩니다. .qmd 파일을 PDF 파일로 내보내려면 tinytex 패키지가 설치되어 있어야 합니다. 아래 코드를 통해 패키지를 설치하겠습니다.\n\n\n\nconsole\n\n# tinytex 설치 후 로드\n\ninstalled.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nlibrary(tinytex)\n\n\n\n\n\nconsole\n\n# 패키지 설치 확인(True가 출력되면 성공적으로 설치된 것임)\n\ntinytex:::is_tinytex()\n\n\njournal format을 추가하기 위해 Quarto Extensions: Journal Articles를 참고해 터미널에 아래 코드를 실행시켜 익스텐션을 설치합니다.\n\n\n\nterminal\n\n# acs format 설치 예시\n\nquarto install extension quarto-journals/acs\n\n\n이후 _quarto.yml 파일의 format:에 acs-pdf: default를 추가하면 acs format이 적용됩니다.\n웹페이지를 발행한 뒤 PDF 파일을 다운로드하면 아래와 같이 journal format이 적용된 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(좌) acs format, (우) elsevier format 적용 예시\n3. 깃허브 페이지를 통해 웹으로 발행하기\n원고를 깃허브 페이지를 통해 웹으로 발행하기 위해 _quarto.yml파일의 project:에 output-dir: docs 설정을 추가합니다. 이후 index.qmd 파일으로 돌아와 터미널에 quarto render을 입력합니다. 모든 변경사항을 깃허브에 commit 후 push 하면, 원고가 깃허브 페이지를 통해 웹으로 발행됩니다. 깃허브의 해당 레포지토리의 settings &gt; pages에서 웹페이지 링크를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#마치며",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#마치며",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "마치며",
    "text": "마치며\n이번 게시글에서는 새롭게 공개된 Quarto Manuscript를 통해 학술 논문 원고를 작성하고 깃허브 페이지를 통해 웹으로 발행하는 방법을 알아보았습니다. Quarto Manuscript를 통해 논문 작성과 공유가 더욱 편리하게 이루어지기를 기대하며 글을 마칩니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html",
    "href": "posts/2023-11-21-copilot/index.html",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Github Copilot은 OpenAI의 GPT-3를 기반으로 만들어진 AI 코딩 도우미로, Github에 있는 수많은 “Public Repository”의 코드들을 학습하여, 자동 완성 형태의 제안을 통해 사용자의 코드 제작을 돕습니다.\nGithub에는 코드도 있지만, README와 같은 여러 종류의 설명 글 또한 있기 때문에 Copilot에서는 아래 이미지처럼 (Quarto로 블로그 작성) 다양한 종류의 자동 완성을 제공합니다.\n\n\n\nMicrosoft의 설명에 따르면 chatGPT는 자연어 처리 기술로, Microsoft 365 Copilot은 코드 생성 기술로 설명하고 있습니다. (Microsoft 365 copilot과 github copilot은 살짝 다르긴합니다.)\n즉, chatGPT는 자연어 처리 기술을 통해 사용자의 질문에 대한 답변을 생성하는 것이 주 역할입니다.\n따라서 chatGPT는 아래 작업 용도로 활용되기도 합니다.\n\n에세이, 이메일 및 커버 레터 작성\n목록 만들기\n예술에 대해 자세히 설명\n코드 작성\n콘텐츠 요약\n시와 노래 가사 만들기\n이력서 작성\n\n한편 Copilot은 Github에서의 학습을 바탕으로 코드 스니펫을 제공하는 것에 더 큰 장점을 가지고 있습니다.\n코드 스니펫이란 아래 이미지처럼 자주 사용되는 조각 코드를 미리 작성해 놓은 것을 말합니다.\n\n\n얼핏보면 큰 차이가 없어보이지만, chatGPT는 최근의 기술은 잘 반영하지 못한다는 단점이 있습니다. 또한 Copilot은 IDE (Rstudio)에서 바로 사용할 수 있다는 장점도 있습니다.\n어느 것이 좋다/나쁘다 라기보단 서로 다른 특징을 가지고 있기에, 두 방법 모두를 필요한 목적에 따라 적절히 활용하는 것을 권장합니다.\n\nCopilot을 사용하기 위해서는 몇가지 준비가 필요합니다.\n\n\n\n\n\n\n이 글에서는 Github 에서의 Copilot 결제를 비롯한 요금제에 대해서는 설명하지 않습니다.\n\n\n\n\nGithub 개인 계정 (무료)\nIDE (코드 에디터) - VS Code, Rstudio 등. 글에서는 Rstudio를 기준으로 설명\nGithub Copilot 가입 (첫 한달은 무료, 이후 월 10달러의 유료,  Pricing  참조)\n\nCopilot의 공식 홈페이지에서 설명이 제공하는 개발 도구는 Azure Data studio, JetBrains IDEs, Vim/NeoVim, Visual Studio, Visual Studio Code가 있지만 다행히 Rstudio에서도 사용가능합니다.\n\n\n\n\n\n\n\n\n현재는 Rstudio Desktop 2023.09.0 이상 버전에서만 사용 가능하며 Rstudio Server나 Posit Workbench에서는 관리자 설정 이후 사용 가능합니다.\n버전을 확인하기 위해서는 Rstudio에서 Help &gt; About Rstudio를 클릭하면 됩니다.\n\n\n\n\n이후 Copilot 설정 과정은 다음과 같습니다.\n\nRstudio에서 Tools &gt; Global Options &gt; copilot을 클릭합니다.\n\n\n\n\n“Enable Github Copilot”을 체크합니다.\n\n\n\n\n“Sign in”을 클릭합니다.\n\n\n\n\n이후 나타나는 Device Activation에 Rstudio에서 보여지는 코드를 입력합니다.\n\n\n\n\n이제 Github Copilot을 사용할 수 있습니다.\n\n\n\n\nCopilot은 \"Ghost text\"라고 불리는 방법으로 사용자의 코드를 자동 완성합니다.\n앞서 본 gif 이미지나, 아래의 예시처럼 코드의 일정 부분을 작성하면 나머지 부분을 회색으로 보여주어 탭 키를 누르는 것으로 완성할 수 있습니다.\n\n\n\nCopilot이 인지할 코드: 이때 꼭 주석으로 하지 않아도 이전 코드를 기반으로 copilot이 자동으로 제안합니다.\nCopilot이 제안하는 코드: 회색으로 보여지는 부분이 제안되는 부분입니다.\nCopilot 상태바: Waiting for Completions(대기), Completion response received(코드 제안 완료), No completions available(제안 없음) 등의 상태를 보여줍니다.\n\n한편 상태바 옆의 언어 설정을 통해 어떤 코드를 자동생성할지 설정할 수 있습니다.\n\n\n추가로 Copilot 옵션에서 Index project files… 를 선택하여 현재 Rstudio 프로젝트의 파일을 코드제안에 반영할 수도 있습니다.\n\nCopilot을 사용하는 가장 기본적인 방법은 코드를 자동완성하는 것입니다.\n\n\n위의 예시처럼, 함수의 기능을 잘 설명하는 이름을 작성하는 것으로 Copilot은 함수의 목적을 이해하고, 함수의 기능에 맞는 코드를 제안합니다.\n\n\n한편 함수 이름에 기능을 명시하지 않고 적절한 한글 주석을 통해서도 함수를 자동으로 완성할 수 있습니다.\n물론 이를 위해서는 (이름으로나 주석으로나) 함수의 목적을 명확하게 알아야만 합니다.\n\nCopilot은 코드를 자동완성하는 것 외에도, 코드를 작성하는데 도움을 주는 질문을 제안합니다.\n\n\n이때 질문을 위해서는 코드와는 다르게 주석에 q:와 a:형식을 맞춰야만 합니다.\n\n# q: QUESTION\n# a: \n\n개인적으로 이러한 방법의 활용을 위해서는 코드를 위주로 학습한 Copilot보다는, chatGPT를 바로 쓰거나 gptStudio, chattr 패키지를 사용해 LLM 모델을 사용하는 것도 좋다고 생각합니다.\n\n\n\n\n\n\n gptStudio 설명글 \n chattr 패키지 웹페이지\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 주석을 작성하는데에도 쓰일 수 있습니다.\n예를 들면, 아래의 표준 편차를 계산하는 함수에 대해 주석을 작성하게 할 수도 있습니다.\n\ncalc_se &lt;- function(x, na.rm = TRUE) {\n  if (!is.numeric(x)) {\n    stop(\"x must be numeric\")\n  }\n  if (na.rm) {\n    x &lt;- x[!is.na(x)]\n  }\n  sqrt(var(x) / length(x))\n}\n\ncalc_se(1:10)\n\n[1] 0.9574271\n\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 코드의 품질을 올리기 위한 목적의 테스트 코드를 작성하기 위해서도(!) 쓰일 수 있습니다.\n\n\n\n당연한 이야기지만, Copilot은 유용한 코드를 생성하는 경우가 많지만 항상 유효하거나 의도한 문제를 정확하게 해결하지 않을 수도 있습니다.\n또한 Github의 다양한 수준의 코드를 학습한 만큼 안전하지 않은 코딩 패턴이나, 버그, 비효율적인 관행등을 포함한 코드를 만들 수 도 있기 때문에 완전히 신뢰할 수는 없습니다.\n그러나 대부분의 R 사용자에게는 크게 체감될만한 문제가 없을 것으로 보이며, 특히 데이터 매니지먼트의 목적으로는 매우 유용하게 사용할 수 있을 것으로 보입니다.\n꼭 Rstudio가 아니더라도 다른 IDE에서 SQL, SASS 등의 다른 언어를 목적으로도 사용할 수 있기에 Copilot은 대체로 코드 작업에 아주 아주 효과적인 방법입니다.\n그러나 개인 기준 월 10달러의 비용이 들기 때문에, 코딩 작업이 많이 필요하지 않은 사람에게는 다소 부담스러울 수도 있으니 무료 기간동안 활용해보고 결정하는 것도 좋을 것 같습니다.\n비교를 위한 넷플릭스의 요금제\n\n\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot이란",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot이란",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Github Copilot은 OpenAI의 GPT-3를 기반으로 만들어진 AI 코딩 도우미로, Github에 있는 수많은 “Public Repository”의 코드들을 학습하여, 자동 완성 형태의 제안을 통해 사용자의 코드 제작을 돕습니다.\nGithub에는 코드도 있지만, README와 같은 여러 종류의 설명 글 또한 있기 때문에 Copilot에서는 아래 이미지처럼 (Quarto로 블로그 작성) 다양한 종류의 자동 완성을 제공합니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#chatgpt와의-차이점",
    "href": "posts/2023-11-21-copilot/index.html#chatgpt와의-차이점",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Microsoft의 설명에 따르면 chatGPT는 자연어 처리 기술로, Microsoft 365 Copilot은 코드 생성 기술로 설명하고 있습니다. (Microsoft 365 copilot과 github copilot은 살짝 다르긴합니다.)\n즉, chatGPT는 자연어 처리 기술을 통해 사용자의 질문에 대한 답변을 생성하는 것이 주 역할입니다.\n따라서 chatGPT는 아래 작업 용도로 활용되기도 합니다.\n\n에세이, 이메일 및 커버 레터 작성\n목록 만들기\n예술에 대해 자세히 설명\n코드 작성\n콘텐츠 요약\n시와 노래 가사 만들기\n이력서 작성\n\n한편 Copilot은 Github에서의 학습을 바탕으로 코드 스니펫을 제공하는 것에 더 큰 장점을 가지고 있습니다.\n코드 스니펫이란 아래 이미지처럼 자주 사용되는 조각 코드를 미리 작성해 놓은 것을 말합니다.\n\n\n얼핏보면 큰 차이가 없어보이지만, chatGPT는 최근의 기술은 잘 반영하지 못한다는 단점이 있습니다. 또한 Copilot은 IDE (Rstudio)에서 바로 사용할 수 있다는 장점도 있습니다.\n어느 것이 좋다/나쁘다 라기보단 서로 다른 특징을 가지고 있기에, 두 방법 모두를 필요한 목적에 따라 적절히 활용하는 것을 권장합니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot을-사용하기-위한-준비",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot을-사용하기-위한-준비",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Copilot을 사용하기 위해서는 몇가지 준비가 필요합니다.\n\n\n\n\n\n\n이 글에서는 Github 에서의 Copilot 결제를 비롯한 요금제에 대해서는 설명하지 않습니다.\n\n\n\n\nGithub 개인 계정 (무료)\nIDE (코드 에디터) - VS Code, Rstudio 등. 글에서는 Rstudio를 기준으로 설명\nGithub Copilot 가입 (첫 한달은 무료, 이후 월 10달러의 유료,  Pricing  참조)\n\nCopilot의 공식 홈페이지에서 설명이 제공하는 개발 도구는 Azure Data studio, JetBrains IDEs, Vim/NeoVim, Visual Studio, Visual Studio Code가 있지만 다행히 Rstudio에서도 사용가능합니다.\n\n\n\n\n\n\n\n\n현재는 Rstudio Desktop 2023.09.0 이상 버전에서만 사용 가능하며 Rstudio Server나 Posit Workbench에서는 관리자 설정 이후 사용 가능합니다.\n버전을 확인하기 위해서는 Rstudio에서 Help &gt; About Rstudio를 클릭하면 됩니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#rstudio에서-github-copilot-사용-설정",
    "href": "posts/2023-11-21-copilot/index.html#rstudio에서-github-copilot-사용-설정",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "이후 Copilot 설정 과정은 다음과 같습니다.\n\nRstudio에서 Tools &gt; Global Options &gt; copilot을 클릭합니다.\n\n\n\n\n“Enable Github Copilot”을 체크합니다.\n\n\n\n\n“Sign in”을 클릭합니다.\n\n\n\n\n이후 나타나는 Device Activation에 Rstudio에서 보여지는 코드를 입력합니다.\n\n\n\n\n이제 Github Copilot을 사용할 수 있습니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot-사용하기",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot-사용하기",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Copilot은 \"Ghost text\"라고 불리는 방법으로 사용자의 코드를 자동 완성합니다.\n앞서 본 gif 이미지나, 아래의 예시처럼 코드의 일정 부분을 작성하면 나머지 부분을 회색으로 보여주어 탭 키를 누르는 것으로 완성할 수 있습니다.\n\n\n\nCopilot이 인지할 코드: 이때 꼭 주석으로 하지 않아도 이전 코드를 기반으로 copilot이 자동으로 제안합니다.\nCopilot이 제안하는 코드: 회색으로 보여지는 부분이 제안되는 부분입니다.\nCopilot 상태바: Waiting for Completions(대기), Completion response received(코드 제안 완료), No completions available(제안 없음) 등의 상태를 보여줍니다.\n\n한편 상태바 옆의 언어 설정을 통해 어떤 코드를 자동생성할지 설정할 수 있습니다.\n\n\n추가로 Copilot 옵션에서 Index project files… 를 선택하여 현재 Rstudio 프로젝트의 파일을 코드제안에 반영할 수도 있습니다.\n\nCopilot을 사용하는 가장 기본적인 방법은 코드를 자동완성하는 것입니다.\n\n\n위의 예시처럼, 함수의 기능을 잘 설명하는 이름을 작성하는 것으로 Copilot은 함수의 목적을 이해하고, 함수의 기능에 맞는 코드를 제안합니다.\n\n\n한편 함수 이름에 기능을 명시하지 않고 적절한 한글 주석을 통해서도 함수를 자동으로 완성할 수 있습니다.\n물론 이를 위해서는 (이름으로나 주석으로나) 함수의 목적을 명확하게 알아야만 합니다.\n\nCopilot은 코드를 자동완성하는 것 외에도, 코드를 작성하는데 도움을 주는 질문을 제안합니다.\n\n\n이때 질문을 위해서는 코드와는 다르게 주석에 q:와 a:형식을 맞춰야만 합니다.\n\n# q: QUESTION\n# a: \n\n개인적으로 이러한 방법의 활용을 위해서는 코드를 위주로 학습한 Copilot보다는, chatGPT를 바로 쓰거나 gptStudio, chattr 패키지를 사용해 LLM 모델을 사용하는 것도 좋다고 생각합니다.\n\n\n\n\n\n\n gptStudio 설명글 \n chattr 패키지 웹페이지\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 주석을 작성하는데에도 쓰일 수 있습니다.\n예를 들면, 아래의 표준 편차를 계산하는 함수에 대해 주석을 작성하게 할 수도 있습니다.\n\ncalc_se &lt;- function(x, na.rm = TRUE) {\n  if (!is.numeric(x)) {\n    stop(\"x must be numeric\")\n  }\n  if (na.rm) {\n    x &lt;- x[!is.na(x)]\n  }\n  sqrt(var(x) / length(x))\n}\n\ncalc_se(1:10)\n\n[1] 0.9574271\n\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 코드의 품질을 올리기 위한 목적의 테스트 코드를 작성하기 위해서도(!) 쓰일 수 있습니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#정리",
    "href": "posts/2023-11-21-copilot/index.html#정리",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "당연한 이야기지만, Copilot은 유용한 코드를 생성하는 경우가 많지만 항상 유효하거나 의도한 문제를 정확하게 해결하지 않을 수도 있습니다.\n또한 Github의 다양한 수준의 코드를 학습한 만큼 안전하지 않은 코딩 패턴이나, 버그, 비효율적인 관행등을 포함한 코드를 만들 수 도 있기 때문에 완전히 신뢰할 수는 없습니다.\n그러나 대부분의 R 사용자에게는 크게 체감될만한 문제가 없을 것으로 보이며, 특히 데이터 매니지먼트의 목적으로는 매우 유용하게 사용할 수 있을 것으로 보입니다.\n꼭 Rstudio가 아니더라도 다른 IDE에서 SQL, SASS 등의 다른 언어를 목적으로도 사용할 수 있기에 Copilot은 대체로 코드 작업에 아주 아주 효과적인 방법입니다.\n그러나 개인 기준 월 10달러의 비용이 들기 때문에, 코딩 작업이 많이 필요하지 않은 사람에게는 다소 부담스러울 수도 있으니 무료 기간동안 활용해보고 결정하는 것도 좋을 것 같습니다.\n비교를 위한 넷플릭스의 요금제\n\n\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "",
    "text": "OTP는 One Time Password의 약자로, 일회용 비밀번호를 뜻합니다. 고정된 비밀번호와 달리, 필요 할 때마다 발급되어 한 번만 사용할 수 있습니다. 대부분의 경우, 6자리 숫자가 30초마다 갱신되는 형태로, 휴대폰 어플리케이션이나 실물 OTP 생성기 등으로 발급합니다.\n차라투에서는 여러 개의 RStudio Server를 구동하고 있습니다. 이 중 인턴십을 위해 사용하는 서버에 시범적으로 도입 해 보고자 하였고, 후기를 남깁니다.\n본 게시글은 R-bloggers 게시글을 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#개요",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#개요",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "",
    "text": "OTP는 One Time Password의 약자로, 일회용 비밀번호를 뜻합니다. 고정된 비밀번호와 달리, 필요 할 때마다 발급되어 한 번만 사용할 수 있습니다. 대부분의 경우, 6자리 숫자가 30초마다 갱신되는 형태로, 휴대폰 어플리케이션이나 실물 OTP 생성기 등으로 발급합니다.\n차라투에서는 여러 개의 RStudio Server를 구동하고 있습니다. 이 중 인턴십을 위해 사용하는 서버에 시범적으로 도입 해 보고자 하였고, 후기를 남깁니다.\n본 게시글은 R-bloggers 게시글을 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-1",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-1",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 서버 1",
    "text": "과정 - 서버 1\nOTP 도입을 위해서는 서버와 사용자의 설정이 필요합니다. 이 문단에서는 우선 서버측의 작업에 대해 다루겠습니다.\napt-get update\n명령어를 통해 패키지를 최신화합니다. 그 후,\napt-get install -y libpam-google-authenticator\n로 libpam-google-authenticator 패키지를 설치합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---사용자",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---사용자",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 사용자",
    "text": "과정 - 사용자\n다음으로, 사용자 측에서 수행할 작업입니다.\n\n첫째로, 우선 RStudio Server에 로그인합니다.\n\n\n\n둘째로, Terminal에서,\n\ngoogle-authenticator\n를 실행합니다.\n\n\n셋째로, y 를 입력합니다.\n\n\n\n넷째로, 아래와 같은 QR코드와 secret key가 나타납니다.\n\n\n휴대폰의 “Google Authenticator”나, 또는 이와 유사한 OTP 지원 어플리케이션으로, 화면에 제시된 QR코드를 입력(촬영)하거나, 아래의 secret key를 OTP 어플리케이션에 등록합니다.\n \n\n중요한 작업은 거의 끝났습니다. 간단한 설정을 수행합니다.\n\n\n관련 사항을 저장하는 옵션입니다. y로 설정합니다.\n\n\n\n하나의 코드로 한 번만 로그인을 허용할지 결정하는 옵션입니다. y로 설정합니다.\n\n\n\n기본적으로 현재의 코드, 이전 코드, 이후 코드의 3개의 코드로만 로그인이 허용됩니다. 이 옵션을 허용하면 현재의 코드, 앞 8개 코드, 뒤 8개 코드의 총 17개로 로그인이 가능하게 허용됩니다. 사용자와 서버의 시간 문제가 발생하면 y로 설정하면 되나, 현재는 필요 없어 n으로 설정했습니다.\n\n\n\n30초에 3번만 로그인을 시도할 수 있도록 하는 옵션입니다. y로 설정합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-2",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-2",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 서버 2",
    "text": "과정 - 서버 2\n위 과정을 다 수행한 후, RStudio의 인증 프로파일을 수정하기 위하여 아래와 같이 서버 설정을 진행합니다.\nvi /etc/pam.d/rstudio 파일에,\nauth required pam_google_authenticator.so\n@include common-account\n@include common-session\n위 내용을 추가합니다.\nvi /etc/rstudio/rserver.conf 파일에,\n# Server Configuration File\nauth-pam-require-password-prompt=0\n위 내용을 추가합니다.\n이렇게 설정한 후, RStudio Server 서비스를 재시작합니다.\n우리 회사는 Docker Container 내에서 RStudio Server를 실행하고 있으므로, 아래와 같이 Docker Container를 재시작 하였습니다. 만약 다른 방법으로 사용하고 계신 경우, 적절한 방법으로 서비스를 재시작하시면 됩니다.\n# 컨테이너명: internship\ndocker restart internship\n이제, 휴대폰 OTP에 표시된 6자리 숫자로 로그인이 가능합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#마치며",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#마치며",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "마치며",
    "text": "마치며\n기존에는 정적인 비밀번호를 사용하고 있었지만, OTP를 통해 비밀번호를 계속 변경하는 효과를 누릴 수 있습니다. 우선 하나의 서버에만 적용하였지만, 사용성이 우수하다고 판명 될 경우 다른 서버에도 확대 적용 계획입니다.\nRStudio Server를 사용하시는 분들께 도움이 되었으면 좋겠습니다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html",
    "href": "posts/2024-03-04-shinylive/index.html",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "href": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive",
    "text": "shinylive\nshinylive는 python 버전과 r 버전이 있으며, 이 글에서는 r 버전을 기준으로 소개한다.\nshinylive는 웹 페이지 생성에 필요한 HTML, Javscript, CSS 등의 요소와 shiny 를 사용하기 위한 wasm 관련 파일들을 생성하는 일을 한다.\nshinylive로 만든 예시는 이 링크에서 확인할 수 있다.\n\nshinylive 설치\nshinylive는 CRAN에 올라가 있기도 하지만 최근 릴리즈 된 버전이 0.1.1인만큼 수시로 업데이트 될 수 있어 github의 최신 버전을 사용하는 것을 권장한다. 추가로 pak는 최근 posit에서 R 패키지를 설치하기 위해 권장하는 R 패키지로, 기존의 install.packages(), remotes::install_github() 등의 함수를 대체할 수 있다.\n\n# install.packages(\"pak\")\npak::pak(\"posit-dev/r-shinylive\")\n\n\n\n\n\n\n\nVersion\n\n\n\n통상적으로 1.0 이전의 버전은 아직 개발 중인 버전이라고 생각해도 좋다.\n\n\nshinylive 사용방법\nshinylive는 기존에 만든 shiny application에 wasm을 추가하는 것으로 생각할 수 있다. 즉, 먼저 shiny application을 만들어야 한다.\n예시 실습을 위해 shiny에서 기본으로 제공하는 코드를 사용한다.(이는 Rstudio 콘솔에서 shiny::runExample(\"01_hello\")를 입력해서 확인할 수도 있다.)\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n      breaks = bins, col = \"#75AADB\", border = \"white\",\n      xlab = \"Waiting time to next eruption (in mins)\",\n      main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n이 코드는 아래 그림과 같이 사용자의 입력에 반응하여 갯수만큼 histogram을 만드는 간단한 shiny application을 만들어 낸다.\n\n이 코드를 shinylive를 사용해 정적인 페이지를 만드는 방법은 2가지가 있는데 하나는 별도의 웹페이지로 만들어내는 것이고, 다른 하나는 이 기술 블로그 같은 quarto 블로그 페이지에 내부 콘텐츠로 심는 것이다.\n먼저 별도의 웹페이지를 만드는 방법은 다음과 같다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive via Web page",
    "text": "shinylive via Web page\n별도의 정적 웹페이지에서 shiny를 제공하려면, 이전에 설치했던 shinylive 패키지를 사용하여 app.R을 웹페이지로 변환하는 과정이 필요하다.\n내 문서(Documents)의 shinylive라는 폴더를 만들고 이 안에 app.R을 저장했을 때를 기준으로, export 함수의 사용 예시는 다음과 같다.\n\n# library(shinylive)\nshinylive::export('~/Documents/shinylive', '~/Documents/shinylive_out')\n\n이 코드를 실행하면 shinylive와 동일한 위치, 즉 내 문서(Documents)에 shinylive_out이라는 폴더를 새롭게 만들고 그 안에 shinylive 패키지를 사용해 변환된 wasm 버전의 shiny 코드를 생성한다.\n이 shinylive_out 폴더의 내용물을 확인해보면 다음과 같으며 이전 글에서 언급했던 webr, serviceworker 등이 포함되어 있는 것을 확인할 수 있다.\n\n조금 더 구체적으로 export 함수는 현재 R studio를 실행하고 있는 로컬 PC에서 shinylive 패키지의 파일들, 즉 shiny와 관련된 라이브러리 파일들을 out 디렉토리에 추가하는 역할을 한다.\n\n이제 이 폴더의 내용물을 기준으로 github page등을 만들면 shiny 를 제공하는 정적인 웹페이지를 제공할 수 있으며 그 결과는 아래의 명령어를 통해 미리 확인해 볼 수 있다.\n\n\n\n\n\n\ngithub page\n\n\n\n깃허브 페이지 배포를 위해서는 이전에 작성했던  pkgdown의 글 을 참고하길 권장하며, 이를 위해 shinylive_out 대신 docs 폴더로 결과를 내보내길 권장한다.\n\n\n\nhttpuv::runStaticServer(\"~/Documents/shinylive_out\")"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive in Quarto",
    "text": "shinylive in Quarto\nquarto 블로그에 shiny application을 추가하기 위해서는 별도의 extension을 사용해야한다. quarto extension은 quarto의 기능을 확장하는 별도의 패키지로, 기본 R에 R 패키지를 사용해 기능을 추가하는 것과 유사하다.\n먼저 Rstudio의 터미널에서 다음 코드를 실행하여 quarto extenstion을 추가해야 한다.\nquarto add quarto-ext/shinylive\nquarto 블로그에 shiny 를 심기 위해서 별도의 파일을 만들 필요는 없으며, {shinylive-r}이라는 코드 블록을 사용한다. 추가로 index.qmd의 yaml에 shinylive 를 설정해야만 한다.\nfilters: \n  - shinylive\n이후 shinylive-r 블록에 앞서 만든 app.R 의 내용을 작성한다.\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n아래는 실제 코드 블록이 어플리케이션으로 실행되는 결과이며 slider를 움직일때 반응하여 histogram을 그리는 것을 확인할 수 있다.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#정리",
    "href": "posts/2024-03-04-shinylive/index.html#정리",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "정리",
    "text": "정리\nshinylive는 wasm을 활용해 깃허브 페이지 또는 quarto 블로그 같은 정적 페이지에서 shiny를 실행할 수 있게 하는 기능으로 각각 R 패키지와 quarto extension을 통해 사용할 수 있다.\n물론 아직 나온지 1년이 되지 않은 기능인만큼 모든 기능이 제공되는 것은 아니며 정적 페이지를 사용하는 만큼 별도의 shiny server를 활용하는 것에 비하면 단점이 있기도 하다.\n그러나 shiny 사용법이나 간단한 통계 분석을 소개하고, 이를 웹사이트에서 별도의 R 설치 없이도 바로 실습할 수 있다는 점에서 많이 사용되고 있으며 앞으로도 더 많은 기능이 추가될 것으로 기대된다.\n이 블로그에 사용한 코드는 링크에서 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html",
    "href": "posts/2024-03-30-input-task-button/index.html",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslib이란 bootstrap의 css를 R에서 사용할 수 있도록 만든 패키지입니다. 정확한 원문 설명은 Tools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. 으로, Shiny와 Rmarkdown (당연히 Quarto를 포함)에서 여러 테마를 활용할 수 있게 합니다.\n\n이 글에서는 bslib의 활용 방법들중 Shiny에 집중하여 설명합니다.\n사실 Shiny는 기본적으로 디자인을 위해 bootstrap을 사용합니다. 그런데 Shiny는 패키지를 이루고 있는 구성 요소들과, 관계가 너무 복잡하게 얽혀 있는 상당히 무거운 패키지가 되어버렸고 이로 인해 업데이트에 영향을 받는 부분이 많아, 기능 위주의 업데이트를 하는 것으로 알려져 있습니다.\n즉, UI를 주로 다루는 bootstrap 부분은 5년 전의 버전인 3.4.1 버전을 사용하고 있고, 별도의 테마 설정을 하지 않는다면 특유의 파랑 / 회색 테마를 기본적으로 사용하게 됩니다.(최근 버전은 5.3.3)\n\n그래서 shiny에서는 정체된 UI를 업데이트 하기 위해 UI를 다루는 별도의 R 패키지를 만들어 덮어 씌우듯 최근 bootstrap의 기능들을 제공하게 됩니다.\n\n\n\n\n\n\n이 글에서는 bslib의 주요 사용법은 다루지 않습니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#bslib",
    "href": "posts/2024-03-30-input-task-button/index.html#bslib",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslib이란 bootstrap의 css를 R에서 사용할 수 있도록 만든 패키지입니다. 정확한 원문 설명은 Tools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. 으로, Shiny와 Rmarkdown (당연히 Quarto를 포함)에서 여러 테마를 활용할 수 있게 합니다.\n\n이 글에서는 bslib의 활용 방법들중 Shiny에 집중하여 설명합니다.\n사실 Shiny는 기본적으로 디자인을 위해 bootstrap을 사용합니다. 그런데 Shiny는 패키지를 이루고 있는 구성 요소들과, 관계가 너무 복잡하게 얽혀 있는 상당히 무거운 패키지가 되어버렸고 이로 인해 업데이트에 영향을 받는 부분이 많아, 기능 위주의 업데이트를 하는 것으로 알려져 있습니다.\n즉, UI를 주로 다루는 bootstrap 부분은 5년 전의 버전인 3.4.1 버전을 사용하고 있고, 별도의 테마 설정을 하지 않는다면 특유의 파랑 / 회색 테마를 기본적으로 사용하게 됩니다.(최근 버전은 5.3.3)\n\n그래서 shiny에서는 정체된 UI를 업데이트 하기 위해 UI를 다루는 별도의 R 패키지를 만들어 덮어 씌우듯 최근 bootstrap의 기능들을 제공하게 됩니다.\n\n\n\n\n\n\n이 글에서는 bslib의 주요 사용법은 다루지 않습니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "actionButton",
    "text": "actionButton\nShiny에서 제공하는 기능들은 정말 다양하지만, 핵심 기능을 꼽으라면 actionButton을 꼽을 수 있습니다.\nactionButton이란 사용자가 버튼을 누르면 server에서 미리 선언한 특정 동작을 수행하도록 하는 기능으로 보통은 사용자가 데이터를 업로드 하고 나면, 이 데이터를 활용해 계산 결과를 만들어내게 하는 것에 쓰입니다.\nactionButton의 사용 예시로는 아래의 코드와 같이 (?shiny::actionButton으로 확인할 수 있습니다) 사용자의 선택한 관측수에 맞는 히스토그램을 그릴 수 있게 합니다.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "long actionButton",
    "text": "long actionButton\n그런데 이 actionButton의 문제점 중 하나는 만약 연산에 시간이 오래걸린다면 사용자는 버튼을 누르고 결과를 기다리는 동안 아무것도 할 수 없다는 것입니다.\n심지어 단순히 아무것도 할 수 없는 것을 넘어, 버튼이 클릭되지 않은 것으로 오해하고 버튼을 여러번 클릭하기도 합니다.\n만약 대용량 유전체 데이터를 활용한 계산을 위한 Shiny라면 연산 한번에 분 단위 시간이 필요할 수도 있는데 이는 여러가지 문제점을 초래할 수 있습니다.\n특히 여러번 클릭을 했다면 오랜 시간을 거쳐 연산을 마친 직후 다시 동일한 연산을 또 하고, 또 기다리고, … 의 악순환에 빠지기도 합니다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "href": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "title": "bslib: input_task_button 소개",
    "section": "shiny with loading",
    "text": "shiny with loading\nactionButton의 이 문제를 해결하기 위해 여러 방법들이 존재했습니다.\n\n\nprogress indicator를 사용\n\n이 방법은 shiny에서 기본적으로 제공하는 Progress Indicator UI를 활용하는 방법으로 연산의 과정 / 단계가 진행됨에 따라 진행 정도를 사용자에게 보여줄 수 있습니다.\n\nserver &lt;- function(input, output) {\n  output$plot &lt;- renderPlot({\n    input$goPlot \n\n    dat &lt;- data.frame(x = numeric(0), y = numeric(0))\n\n    withProgress(message = 'Making plot', value = 0, {\n      n &lt;- 10\n\n      for (i in 1:n) {\n        dat &lt;- rbind(dat, data.frame(x = rnorm(1), y = rnorm(1)))\n        incProgress(1/n, detail = paste(\"Doing part\", i))\n        Sys.sleep(0.1)\n      }\n    })\n\n    plot(dat$x, dat$y)\n  })\n}\n\nui &lt;- shinyUI(basicPage(\n  plotOutput('plot', width = \"300px\", height = \"300px\"),\n  actionButton('goPlot', 'Go plot')\n))\n\nshinyApp(ui = ui, server = server)\n\n\n그러나 이는 withProgress, incProgress 또는 Progress등의 함수와 오브젝트를 시간이 오래걸리는 연산에 추가로 코드를 작성해야한다는 단점이 있습니다.\n\n별도의 R 패키지 사용\n\nR의 생태계에는 해결하려는 여러 문제가 있고, 그 문제마다의 R 패키지가 있다고 생각해도 과언이 아닌데요. actionButton과 연산 결과 사이의 긴 공백을 UI에 표기하기 위한 기능 또한 마찬가지입니다.\n이전의 progress indicator와 유사하게 추가 코드를 작성하여 해결해야하며 조금 더 디자인이나 세부 설정을 할 수 있는 커스텀 기능이 있다고 생각하면 좋습니다.\n아래는 몇가지 예시 패키지와 사례입니다 (알파벳순).\n\nshinybusy\n\n\n\nshinycssloaders\n\n\n\nshinycustomloader\n\n\n\nwaiter"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "href": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "title": "bslib: input_task_button 소개",
    "section": "input_task_button",
    "text": "input_task_button\ninput_task_button은 위의 방법들과는 다르게 actionButton을 확장한 기능으로, actionButton을 누르면 연산이 진행중임을 알리며 버튼이 비활성화 되며, 연산이 끝나면 다시 버튼이 활성화되는 기능을 제공합니다.\n무엇보다도 가장 큰 차이점은 actionButton을 대체할 수 있기에 추가 코드를 사용할 필요가 없다는 것입니다.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  # actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  input_task_button(\"goButton\", \"Go!\", type = \"success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    Sys.sleep(5)\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)\n\n\ninput_task_button의 사용을 위해 5번째 줄의 actionButton을 6번째 줄의 input_task_button으로 대체하였고, 추가로 의도적으로 오래 걸리는 연산을 만들기 위해 13번째 줄의 Sys.sleep() 코드를 활용하여 5초를 지연시켰습니다. input_task_button 사용 방법은 다음과 같습니다.\n\n\n\n\n\n\ninput_task_button은 actionButton을 무리없이 대체할 수 있지만 약간의 parameter 수정이 필요합니다.\n\n\n\n\n\nactionButton\ninput_task_button\n역할\n\n\n\ninputId\nid\n버튼 id\n\n\nlabel\nlabel\n버튼 라벨\n\n\nicon\nicon\n버튼 아이콘\n\n\n\nlabel_busy\n버튼 비활성화시 라벨\n\n\n\nicon_busy\n버튼 비활성화시 아이콘\n\n\nclass\ntype\n버튼 테마 / 색상\n\n\n\n\n릴리스 노트 원본\ninput_task_button 매뉴얼"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#정리",
    "href": "posts/2024-03-30-input-task-button/index.html#정리",
    "title": "bslib: input_task_button 소개",
    "section": "정리",
    "text": "정리\n이번 글에서는 bslib의 최신 기능인 input_task_button과 간단한 사용 사례를 소개했습니다.\n이는 잠재적으로 사용자 경험을 향상시키는 기능이며, 기존의 shiny에서는 추가 코드를 작성해야하는 번거로움을 해결해주는 기능이라고 생각합니다.\n특히 다른 웹 어플리케이션과 다르게 Shiny에서는 대용량 데이터 연산으로 인해 시간이 오래 걸리는 경우가 많지만, 상대적으로 빈약한 UI/UX를 가지고 있어 기존의 actionButton을 input_task_button으로 대체할 경우 이를 보완하는데 큰 도움이 될 것입니다.\n이번 글이 도움이 되었길 바라며, 다음 글에서 또 만나요!"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html",
    "href": "posts/2024-05-13-rhub/index.html",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECK란 R 패키지를 개발한 이후 “정상적으로 개발 되었는지” 검증하는 약 50개 이상의 체크리스트를 실행하는 과정으로, 함수의 사용법이 제대로 작성되었는지, 함수의 parameter가 제대로 작성되었는지 등을 포함한다.\n물론 R CMD CHECK를 엄격하게 수행하지 않아도 github 를 통해 패키지를 배포하고 실행하는 것에 문제는 없지만, 이러한 과정을 통해 패키지의 오류를 최소화하고 사용자에게 안정적인 패키지를 제공할 수 있다는 것이 검증 되어야만 CRAN과 같은 공식 리포지토리를 통해 패키지를 공유할 수 있다.\n이 포스팅에서는 구체적인 내용을 다루진 않지만, 관심이 있다면 Hadley Wickham의 R Packages를 참고하는 것도 좋다.\n아무튼 R CMD CHECK 는 devtools패키지를 사용하여 R 패키지를 만들었다면, devtools::check()함수 또는 Rstudio에서 Check 버튼으로 할 수 있고, Warnings, Errors, Notes 등을 통해 수정되길 권고하는 문제들을 확인할 수 있다.\n\n그러나 이 R CMD CHECK의 특징 중 한가지는 패키지를 개발하는 PC의 환경을 기준으로 체크를 진행한다는 것이다. 즉, 아래의 이미지 같은 경우 macOS (Apple clang) 환경에서는 패키지가 테스트 되었고 실행이 보장되지만 만약 사용자의 OS가 mac이 아닌 window, linux 같은 경우에는 패키지가 정상적으로 작동하지 않을 수도 있다.\nCRAN은 기본 OS를 정하지 않고 있지만 Windows, macOS, linux 중 최소 2개 이상의 OS에서의 테스트에서 문제가 없길 요구하는 만큼 R 패키지를 개발하는 것은 다양한 OS에서의 테스트를 포함하기도 한다.\n이를 위해 다양한 OS 하드웨어, 즉 windows PC, mac, linux 서버가 있다면 베스트겠지만 이러한 경우는 많지 않고 대부분 Github action, AppVeyor, Travis CI 등의 CI/CD 서비스를 활용해 다양한 OS에서의 테스트를 수행하게 된다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "href": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECK란 R 패키지를 개발한 이후 “정상적으로 개발 되었는지” 검증하는 약 50개 이상의 체크리스트를 실행하는 과정으로, 함수의 사용법이 제대로 작성되었는지, 함수의 parameter가 제대로 작성되었는지 등을 포함한다.\n물론 R CMD CHECK를 엄격하게 수행하지 않아도 github 를 통해 패키지를 배포하고 실행하는 것에 문제는 없지만, 이러한 과정을 통해 패키지의 오류를 최소화하고 사용자에게 안정적인 패키지를 제공할 수 있다는 것이 검증 되어야만 CRAN과 같은 공식 리포지토리를 통해 패키지를 공유할 수 있다.\n이 포스팅에서는 구체적인 내용을 다루진 않지만, 관심이 있다면 Hadley Wickham의 R Packages를 참고하는 것도 좋다.\n아무튼 R CMD CHECK 는 devtools패키지를 사용하여 R 패키지를 만들었다면, devtools::check()함수 또는 Rstudio에서 Check 버튼으로 할 수 있고, Warnings, Errors, Notes 등을 통해 수정되길 권고하는 문제들을 확인할 수 있다.\n\n그러나 이 R CMD CHECK의 특징 중 한가지는 패키지를 개발하는 PC의 환경을 기준으로 체크를 진행한다는 것이다. 즉, 아래의 이미지 같은 경우 macOS (Apple clang) 환경에서는 패키지가 테스트 되었고 실행이 보장되지만 만약 사용자의 OS가 mac이 아닌 window, linux 같은 경우에는 패키지가 정상적으로 작동하지 않을 수도 있다.\nCRAN은 기본 OS를 정하지 않고 있지만 Windows, macOS, linux 중 최소 2개 이상의 OS에서의 테스트에서 문제가 없길 요구하는 만큼 R 패키지를 개발하는 것은 다양한 OS에서의 테스트를 포함하기도 한다.\n이를 위해 다양한 OS 하드웨어, 즉 windows PC, mac, linux 서버가 있다면 베스트겠지만 이러한 경우는 많지 않고 대부분 Github action, AppVeyor, Travis CI 등의 CI/CD 서비스를 활용해 다양한 OS에서의 테스트를 수행하게 된다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#github-action",
    "href": "posts/2024-05-13-rhub/index.html#github-action",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Github action",
    "text": "Github action\n이 포스팅에서는 Github action을 기준으로 소개하지만 다른 서비스도 과정과 프로세스는 대동소이하다.\nGithub action은 Github에서 제공하는 CI/CD 서비스로, Github에서 제공하는 다양한 Action을 사용하여 (github 이 제공하는 서버에서의 명령어 실행을 통해) 자동화된 테스트, 빌드, 배포 등을 수행할 수 있다.\n\n여기서 Action이란 yml 파일로 구성된 명령어들의 모음 정도로 생각해도 충분하며, 예시로는 서버에 R을 설치하기, R 패키지를 설치하기, R CMD CHECK를 실행하기 등이 있다. R-hub action 모음, R-lib action 모음 참고.\n즉, 이 R CMD CHECK를 포함한 Action들을 github action을 통해 server에서 실행하고 그 결과를 확인하여 CRAN에 올리는 과정을 거치게 되는데 github action의 문제점 중 하나는 아래의 예시와 같이 action을 사용하기 위한 yml 문법이 상당히 이질적이라는 것이다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-hub2",
    "href": "posts/2024-05-13-rhub/index.html#r-hub2",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "r-hub2",
    "text": "r-hub2\nr-hub 프로젝트는 R 컨소시엄의 프로젝트 중 하나로, R 개발자들이 R 패키지를 더 잘 개발할 수 있게 하는 목적을 가지고 있다. 다양한 OS에서의 테스트를 수행하는 것을 돕는 것 또한 그 중 하나로 위에서 언급한 Github Action을 개발한다거나 인프라를 제공하거나, 커뮤니티를 통해 문제 해결을 돕는 등의 역할이 있다.\n그런데 이 R-hub 프로젝트에서 최근 이 GHA 설정을 돕기 위한 R 패키지, rhub를 개발하여 공개했다.\n공식 블로그에 설명 되어 있는 것처럼, 사용 방법은 기존의 Github Action을 사용하지 않았더라도 다양한 OS에서 R CMD CHECK를 손쉽게 수행할 수 있는 Github Action을 설정할 수 있다.\n제일 먼저 해야하는 것은 당연하게도 rhub 패키지를 설치하는 것이다. 여기서 pak은 기존의 install.packages와 remotes::install_github 같이 다양한 소스에서의 R 패키지 설치를 통합하여 지원하는 함수로 기존의 패키지 설치 방법들을 대체하기를 권장하고 있다.\npak::pkg_install(\"rhub\")\n주의할 점으로 rhub 패키지는 공식적인 버전은 2이지만 rhub2가 아닌 rhub로 설치, 실행해야 하며 포스팅에서는 rhub로 표기하겠다.\n이 rhub 패키지를 실행하기 위해서는 아래 3가지가 필요한데, R 패키지를 개발하여 github에 공유한 경험이 있다면 별도로 새롭게 준비해야 할 것은 없다.\n\ngithub 계정\nR package를 올린 repository. 이때 CRAN을 목적으로 한다면 당연히 Public이어야 한다.\nGithub PAT (Personal Access Token), github 에서 발급 받을 수 있고, gitcreds라는 또 다른 R 패키지를 사용해도 좋다.\n\n\nSetup\nrhub 패키지를 마친 이후 제일 먼저 해야하는 것은 R 패키지 디렉토리에서 rhub_setup 함수를 실행하는 것이다. 이 함수의 역할은 디렉토리의 git repository를 인식하고, Github Action을 위한 yml 파일을 생성한다.\n이전에 만들었던 gemini.R 패키지를 아래 이미지 예시로 사용했다.\n\n큰 문제가 없다면, rhub 패키지에서는 이후 진행해야 할 단계도 안내해준다. 즉, 추가된 yml 파일이 추가된 내용을 github에 커밋하여 업데이트를 반영하고 난 뒤 rhub_doctor 함수를 실행한다.\n\n\nDoctor\nrhub_doctor 함수에서는 Github PAT가 제대로 설정되어 있는지를 확인한다. 이후 소개할 rhub의 rhub_check함수는 Rstudio의 콘솔에서 언급한 Github PAT를 사용하여 수동으로 Github Action을 실행하는 역할을 하기 때문에 PAT의 설정 확인이 필요하다.\nGithub PAT는 “https://github.com/settings/tokens” 링크에서 생성하되 repo와 workflow 권한을 반드시 부여하여 생성해야하만 한다.\n\nRstudio에 Github PAT를 설정 하는 방법은 credential 패키지의 set_github_pat 함수를 이용한다. 링크 참고\nrhub_doctor 함수가 정상적으로 작동했다면 이제 남은 것은 rhub_check 함수를 실행하는 것이다.\n\n\n\nCheck\n이전 단계는 이 함수를 위한 준비 작업이었다 라고 해도 과언이 아니다.\n\nrhub_check 함수는 github repository와 PAT를 인식한 다음 어떤 OS에서 R CMD CHECK를 수행할 것인지 입력값으로 받는다.\n이 때 단순히 Windows, macOS, Linux 외에도 이미지처럼 (rhub 프로젝트에서 제공하는) 다양한 OS를 숫자와 쉼표를 통해 구분하여 선택할 수 있다.\n함수를 실행한 후에는 GHA 페이지로 연결할 수 있는 링크를 제공하는데 이를 통해 진행 상황을 확인할 수 있다.\n\n최종적으로 rhub와 GHA를 사용한 테스트 패스를 repository에 뱃지로 추가하면 아래와 같이 나타난다.\n\n이 때 readme에 뱃지 아이콘을 추가하기 위해서는 다음과 같이 작성해야한다.\n![example workflow](https://github.com/&lt;OWNER&gt;/&lt;REPOSITORY&gt;/actions/workflows/&lt;WORKFLOW_FILE&gt;/badge.svg)\n예시로 사용한 gemini.R은  대신 jhk0530,  대신 gemini.R,  대신 rhub.yaml로 대체한다.\n주의할 점으로 이 Github Action을 통한 R CMD CHECK에는 다소 시간이 소요되기 때문에 우선 개발중인 PC에서 R CMD CHECK를 완료한 이후에 실행하는 것을 권장한다.\n물론 rhub에는 github나 public repository가 아닌 경우를 위한 안내도 있지만, 이는 대부분의 R 패키지, 특히 CRAN과는 크게 연관이 없기 때문에 별도로 서술하지 않는다."
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#summary",
    "href": "posts/2024-05-13-rhub/index.html#summary",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Summary",
    "text": "Summary\n이번 포스팅에서는 R 패키지 개발자를 위한 Github Action을 활용하기 위한 rhub 패키지를 소개했다. 이를 통해 R 패키지 개발자는 로컬 환경에서 뿐 아니라 다양한 OS에서의 R CMD CHECK를 통해 패키지의 오류를 최소화하고 더 좋은 패키지를 만들 수 있을 것이다.\n원문을 포함한 더 자세한 정보는 rhub의 블로그 에서도 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html",
    "href": "posts/2024-05-24-app-design/index.html",
    "title": "Shiny app 꾸미기",
    "section": "",
    "text": "기본 shiny app은 스타일이 아쉽습니다. 디자인은 신경 쓰지 않고 기능에만 집중하시는 분들에게는 큰 문제가 되지 않지만 디자인도 서비스에서 매우 중요한 역할을 한다는 점은 부정할 수 없습니다. 이번 포스트에서는 간단하게 shiny app을 꾸밀 수 있는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#page_navbar-nav_panel",
    "href": "posts/2024-05-24-app-design/index.html#page_navbar-nav_panel",
    "title": "Shiny app 꾸미기",
    "section": "page_navbar, nav_panel",
    "text": "page_navbar, nav_panel\n전반적인 테마를 바꾸기 위해 bslib을 불러옵니다. bslib 패키지가 설치되어있지 않다면 install.packages(\"bslib\")을 입력하여 패키지를 설치합니다. 그리고 navbarPage를 page_navbar로, tabPanel을 nav_panel로 바꿔줍니다. 두 함수 모두 shiny 패키지의 함수와 기능은 동일하지만 스타일을 커스텀하기에 더 적합합니다.\n\nlibrary(shiny); library(bslib)\n\nui &lt;- page_navbar(title = \"Obesity classification\",\n                 nav_panel(title = \"Data\"),\n                 ...\n                 )\n\nserver &lt;- function(input, ouput) {\n  ...\n}\n\nshinyApp(ui = ui, server = server)\n\n\n전반적으로 버튼의 디자인이 바뀌었고 선택된 탭을 표시하는 방법도 바뀌었습니다.\npage_navbar함수에는 theme = bs_theme(version = 5)가 기본값으로 적용됩니다. 필요에 따라 해당 인자를 조정하여 부트스트랩 버전을 변경할 수 있습니다.\n차라투 홈페이지와 shiny app의 내비게이션 바의 배경색을 동일하게 하여 통일감을 주고 글자크기도 변경해보겠습니다.\n웹에서 마우스 오른쪽 버튼을 눌러 ’페이지 소스 보기’를 클릭하면 차라투 홈페이지 내비게이션 바 색상의 hex color code는 “#051F20”이고 shiny app의 내비게이션 바 타이틀의 클래스는 navbar-brand, 각 탭의 클래스는 nav-link인 것을 확인할 수 있습니다. 여기서 알아낸 것들을 아래와 같이 적용해보겠습니다.\n\nlibrary(shiny); library(bslib)\n\nui &lt;- page_navbar(title = \"Obesity classification\",\n                  bg = \"#051F20\",\n                  header = tags$head(\n                    tags$style(HTML(\"\n                        .navbar-brand {\n                            font-size: 30px;\n                        }\n                        .nav-link {\n                            font-size: 18px;\n                        }\")\n                        )\n                    ),\n                  nav_panel(title = \"Data\"),\n                  ...\n                  )\n\nserver &lt;- function(input, ouput) {\n  ...\n}\n\nshinyApp(ui = ui, server = server)\n\n\n설정했던 대로 내비게이션 바의 배경색과 글자크기가 바뀐 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#nav_spacer-nav_menu",
    "href": "posts/2024-05-24-app-design/index.html#nav_spacer-nav_menu",
    "title": "Shiny app 꾸미기",
    "section": "nav_spacer, nav_menu",
    "text": "nav_spacer, nav_menu\n여기를 참고해서 내비게이션 바의 우측에 드롭다운 메뉴로 회사 홈페이지와 커뮤니티 페이지 바로가기를 만들겠습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nlink_zarathu &lt;- tags$a(\n  tags$div(\n    style = \"display: inline-block; background-color: #white;\",\n    tags$img(src = \"https://raw.githubusercontent.com/zarathucorp/blog/master/img/logo_favicon.png\",\n             height = \"35px\", width = \"35px\")\n  ),\n  \"차라투 홈페이지\",\n  href = \"https://www.zarathu.com/\",\n  target = \"_blank\"\n)\n\nlink_community &lt;- tags$a(\n  tags$div(\n    style = \"display: inline-block; background-color: #white;\",\n    tags$img(src = \"https://raw.githubusercontent.com/zarathucorp/zarathu/master/public/img/about/logo.jpg\",\n             height = \"40px\", width = \"40px\")\n  ),\n  \"연구지원 신청\",\n  href = \"https://community.zarathu.com/\",\n  target = \"_blank\"\n)\n\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Logistic regression\",\n                            ...),\n                  nav_spacer(),\n                  nav_menu(\n                    title = \"Links\",\n                    align = \"right\",\n                    nav_item(link_zarathu),\n                    nav_item(link_community)\n                    )\n                  )\n\n\ntags$div()의 인자들을 가지고 로고, 글자크기, 배경색 등을 필요에 맞게 조절할 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#radiogroupbuttons",
    "href": "posts/2024-05-24-app-design/index.html#radiogroupbuttons",
    "title": "Shiny app 꾸미기",
    "section": "radioGroupButtons",
    "text": "radioGroupButtons\nshiny의 radioButtons를 shinyWidgets의 radioGroupButtons로 다음과 같이 박스로 바꾸어 선택된 박스에 색이 채워지도록 할 수 있습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Plot\",\n                            sidebarLayout(\n                              sidebarPanel(\n                                # radioButtons(inputId = \"plot_type\", label = \"Plot type\",\n                                #              choices = c(\"violin\", \"box\"), inline = TRUE),\n                                radioGroupButtons(inputId = \"plot_type\", label = \"Plot type\",\n                                                  choices = c(\"violin\", \"box\"), direction = \"horizontal\",\n                                                  individual = TRUE),\n                                ...\n                              ),\n                              mainPanel(...)\n                              )\n                            ),\n                  ...\n                  )\n\n\n\n\nshiny::radioButtons\n\n\n\nshinyWidgets::radioGroupButtons\n\n\n\n당연히 세 개 이상의 버튼도 만들 수 있습니다."
  },
  {
    "objectID": "posts/2024-05-24-app-design/index.html#virtualselectinput",
    "href": "posts/2024-05-24-app-design/index.html#virtualselectinput",
    "title": "Shiny app 꾸미기",
    "section": "virtualSelectInput",
    "text": "virtualSelectInput\nshiny의 selectInput을 shinyWidgets의 virtualSelectInput으로 바꿔보겠습니다.\n\nlibrary(shiny); library(bslib); library(shinyWidgets)\n\nvarlist &lt;- list(\n  base = c(\"Gender\", \"Age\", \"Height\", \"Weight\", \"family_history_with_overweight\"),\n  condition = c(\"FAVC\", \"FCVC\", \"NCP\", \"CAEC\", \"SMOKE\", \"CH2O\", \"SCC\", \"FAF\", \"TUE\", \"CALC\", \"MTRANS\"),\n  outcome = \"NObeyesdad\"\n)\n\nui &lt;- page_navbar(...,\n                  nav_panel(title = \"Linear regression\",\n                            sidebarLayout(\n                              sidebarPanel(\n                                ...,\n                                # selectInput(inputId = \"ind_var\", label = \"Independent variable\",\n                                #             choices = list(\"base\" = varlist$base, \"condition\" = varlist$condition),\n                                #             multiple = TRUE),\n                                virtualSelectInput(inputId = \"ind_var\", label = \"Independent variable\",\n                                                   choices = list(\"base\" = varlist$base, \"condition\" = varlist$condition),\n                                                   multiple = TRUE, search = TRUE, showValueAsTags = TRUE)\n                              ),\n                              mainPanel(...)\n                              )\n                            ),\n                  ...\n                  )\n\n\n\n\nshiny::selectInput\n\n\n\nshinyWidgets::virtualSelectInput\n\n\n\n기존의 seletInput과는 다르게 전체선택/해제가 가능하고, choices인자에 리스트를 넣었을 때 카테고리별로 선택/해제가 가능하다는 점이 큰 장점으로 다가왔습니다."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html",
    "href": "posts/2024-08-22-competingrisk/index.html",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "",
    "text": "관심이 있는 어떤 event가 outcome에 미치는 영향에 대한 생존분석을 진행할 때 다른 사건에 의해 follow up이 중단되는 상황이 발생할 수 있습니다. 이렇게 관측을 끝내게 만드는 의도치 않은 event를 ’competing risk’라 부릅니다. competing risk로 인한 데이터의 손실과, 결과의 왜곡을 줄이기 위한 2가지 방법으로 fine and gray method와 multi state model을 소개합니다."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#데이터-전처리",
    "href": "posts/2024-08-22-competingrisk/index.html#데이터-전처리",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "1.1 데이터 전처리",
    "text": "1.1 데이터 전처리\n실습에서는 mgus2 데이터를 사용하겠습니다. mgus 데이터는 다발성 골수종 환자에 대한 데이터로, pstat이 1인 경우 다발성 골수종에 걸린 것으로 표시됩니다.\n\ndata &lt;- mgus2\nhead(data)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1\n2  2  78   F 1968 11.5   1.2    2.0    25     0     25     1\n3  3  94   M 1980 10.5   1.5    2.6    46     0     46     1\n4  4  68   M 1977 15.2   1.2    1.2    92     0     92     1\n5  5  90   F 1973 10.7   0.8    1.0     8     0      8     1\n6  6  90   M 1990 12.9   1.0    0.5     4     0      4     1\n\n\n이후 data의 event를 다발성 골수종에 걸렸을 경우(pcm) 1, 다발성 골수종에 걸리지 않았지만 사망한 경우(death) 2, 다발성 골수종에 걸리지도 않았고 생존한 경우(censor) 0으로 코딩합니다. competing risk가 고려하고자 하는 데이터는 다벌성 골수종이 아닌 다른 이유로 사망한 2번 케이스 일것입니다.\n\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))"
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#competing-risk-제거",
    "href": "posts/2024-08-22-competingrisk/index.html#competing-risk-제거",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "1.2 competing risk 제거",
    "text": "1.2 competing risk 제거\nfinegray 함수를 사용하면 해당 데이터가 death로 끝났을 경우, 환자가 사망하지 않았다고 가정한 새로운 데이터들을 여러 시간대 별로 제작합니다. 각 시간대별로 환자가 생존했을 확률을 Kaplan-meier estimate로 계산함으로서 이를 weight로 사용합니다.\n\npdata &lt;- finegray(Surv(etime, event) ~ ., data=data)\nhead(pdata)\n\n  id age sex dxyr  hgb creat mspike ptime pstat futime death fgstart fgstop\n1  1  88   F 1981 13.1   1.3    0.5    30     0     30     1       0     35\n2  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      35     44\n3  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      44     47\n4  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      48     52\n5  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      53     56\n6  1  88   F 1981 13.1   1.3    0.5    30     0     30     1      56     57\n  fgstatus      fgwt\n1        0 1.0000000\n2        0 0.9990449\n3        0 0.9980368\n4        0 0.9959629\n5        0 0.9905896\n6        0 0.9873022\n\n\npdata에서 볼 수 있다 싶이 id 1번 환자는 실제로 30일 follow up 이후 사망했지만, pdata에서는 환자가 30일 이후 생존했을 경우를 가정하고, fgwt로 인위적인 데이터의 weight를 보여주고 있습니다. 실제로 시간이 길어질 수록 환자가 생존할 가능성은 떨어지기에 weight 역시 감소하는 것을 볼 수 있습니다. pdata의 event로 사용되는 fgstatus는 다발성 골수종의 발생에만 집중하여 다발성 골수종 발생 경우 1,(data의 pcm과 동일) 미발생시 0(censor와 동일)으로 표시하고 있습니다.\n이제 competing risk로 인한 데이터를 제거했기 때문에 기존의 cox 분석으로 결과를 얻을 수 있습니다.\n\nfgfit &lt;- coxph(Surv(fgstart, fgstop, fgstatus) ~ age+sex,\n               weight=fgwt, data=pdata, model = T)\nsummary(fgfit)\n\nCall:\ncoxph(formula = Surv(fgstart, fgstop, fgstatus) ~ age + sex, \n    data = pdata, weights = fgwt, model = T)\n\n  n= 41775, number of events= 115 \n\n          coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)   \nage  -0.017302  0.982847  0.007022  0.005528 -3.13  0.00175 **\nsexM -0.259757  0.771239  0.187049  0.181707 -1.43  0.15285   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     0.9828      1.017    0.9723    0.9936\nsexM    0.7712      1.297    0.5402    1.1012\n\nConcordance= 0.548  (se = 0.026 )\nLikelihood ratio test= 7.28  on 2 df,   p=0.03\nWald test            = 11.19  on 2 df,   p=0.004\nScore (logrank) test = 7.58  on 2 df,   p=0.02,   Robust = 9.28  p=0.01\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#데이터-전처리-1",
    "href": "posts/2024-08-22-competingrisk/index.html#데이터-전처리-1",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "2.1 데이터 전처리",
    "text": "2.1 데이터 전처리\n이번 실습에서도 mgus2 데이터를 사용합니다. 다만 해당 MSM 모델에서는 data의 event 역시 censor, pcm, death를 위 fine and gray method와 동일하게 설정했습니다. 이 경우 역시, pcm을 거치지 않고 바로 death로 가는 상황이 competing risk라 할 수 있습니다.\n\n\ndata &lt;- mgus2\ndata$etime &lt;- with(data, ifelse(pstat==0, futime, ptime))\ndata$event &lt;- with(data, ifelse(pstat==0, 2*death, 1))\ndata$event &lt;- factor(data$event, 0:2, labels=c(\"censor\", \"pcm\", \"death\"))"
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#msm-모델-제작",
    "href": "posts/2024-08-22-competingrisk/index.html#msm-모델-제작",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "2.2 MSM 모델 제작",
    "text": "2.2 MSM 모델 제작\n이후 data를 기존에 0과 1로 outcome이 이진 분류된 데이터로 cox 모델을 돌리듯 모델을 제작해주면 각 상태(pcm, death by pcm)로의 진행에 대한 회귀분석이 진행됩니다. MSM를 통해 ’다발성 골수종으로의 진행’에 대한 생존 분석과, ’그 이외 영향으로 인한 사망’에 대한 생존 분석을 한 번에 진행할 수 있습니다.\n\nMSMfit &lt;- coxph(Surv(etime, event) ~ sex+ age, data = data, id = id, model = T)\nsummary(MSMfit)\n\nCall:\ncoxph(formula = Surv(etime, event) ~ sex + age, data = data, \n    model = T, id = id)\n\n  n= 1384, number of events= 975 \n\n              coef exp(coef)  se(coef) robust se      z Pr(&gt;|z|)    \nsexM_1:2 -0.025138  0.975176  0.188456  0.189391 -0.133   0.8944    \nage_1:2   0.013039  1.013124  0.008259  0.006678  1.953   0.0509 .  \nsexM_1:3  0.393226  1.481753  0.069698  0.066156  5.944 2.78e-09 ***\nage_1:3   0.064824  1.066971  0.003620  0.003691 17.562  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nsexM_1:2    0.9752     1.0255    0.6728     1.413\nage_1:2     1.0131     0.9870    1.0000     1.026\nsexM_1:3    1.4818     0.6749    1.3016     1.687\nage_1:3     1.0670     0.9372    1.0593     1.075\n\nConcordance= 0.66  (se = 0.01 )\nLikelihood ratio test= 389.5  on 4 df,   p=&lt;2e-16\nWald test            = 326.7  on 4 df,   p=&lt;2e-16\nScore (logrank) test = 335.4  on 4 df,   p=&lt;2e-16,   Robust = 285.9  p=&lt;2e-16\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#fine-and-gray의-장단점-적용",
    "href": "posts/2024-08-22-competingrisk/index.html#fine-and-gray의-장단점-적용",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "3.1 fine and gray의 장단점, 적용",
    "text": "3.1 fine and gray의 장단점, 적용\nFine and Gray의 가장 큰 장점은 단순성입니다. competing risks를 다룰 때 문제를 단순화하여, multi state한 상황을 binomial한 문제로 축소시켜 데이터 해석이 쉬워집니다. 이런 장점은 lifetime risk를 추정하는 것을 용이하게 해주며, 특정 사건이 발생할 종합적인 위험을 평가하는데 강점을 가집니다(ex, 여성의 경우 PCM에 대한 평생 리스크가 나이와 혈청 m-스파이크를 조정한 후에도 1.2배 더 높다).\n다만 모델이 새로운 데이터를 인위적으로 만들고, 비례위험 가정을 통해 weight를 부여하기에 그만큼 잘못된 결과가 나올 위험성이 있습니다. 더불어 간단한 competing risk 상황에만 적용 가능하며, risk들간의 관계(ex. pcm으로 인한 death가 일어날 수 있는 케이스)를 반영할 수 없다는 한계가 있습니다."
  },
  {
    "objectID": "posts/2024-08-22-competingrisk/index.html#msm의-장단점-적용",
    "href": "posts/2024-08-22-competingrisk/index.html#msm의-장단점-적용",
    "title": "competing risk 생존분석, fine-and-gray method와 multi-state model",
    "section": "3.2 MSM의 장단점, 적용",
    "text": "3.2 MSM의 장단점, 적용\nMSM은 다양한 competing risk 상황을 구현할 수 있다는 장점이 있습니다. pstate, death 등의 변수로 event를 더 세부화할시 Fine and Gray에서는 구현하지 못하는 pcm으로 인한 death 등의 관계를 적용시킬 수 있습니다. 위에서 살펴본 것 같이 competing risk를 제거하는 것이 아닌, competing risk에 의한 효과도 확인할 수 있다는 점에서 더욱 더 세심한 분석이 가능합니다.\n다만 더 자세한 모델을 구현하기 위해 state간 전이시간 등 model의 제작을 위해 더 많은 정보를 필요로 한다는 것이 단점입니다. 따라서 MSM은 환자에 질병 진행 과정에 대한 세세한 데이터가 존재할 때 각 상태의 전이의 확인이 필요할 때 적용할 수 있습니다.\n따라서 ’관찰하려는 event의 발생에만 관심이 있고, 그 이외 영향은 모두 제거하고 싶다’는 목적으로 competing risk를 처리하고자 할 때는 fine and gray 방법을,’관측 중 발생한 모든 event에 대한 세세한 정보를 얻고 싶다’는 목적일 때는 MSM을 쓰면 되겠습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html",
    "href": "posts/2024-09-13 jstable/index.html",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "",
    "text": "이 문서에서는 GLM, GEE, GLMM, Cox 모델에 대한 개요와 이를 jstable 패키지와 더불어 활용하는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm의-개요",
    "href": "posts/2024-09-13 jstable/index.html#glm의-개요",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM의 개요",
    "text": "GLM의 개요\nGLM은 Generalized Linear Model의 약자로 일반화 선형 모델을 의미합니다. 기존의 선형 회귀 모델에서는 반응 변수가 정규 분포를 따라야한다는 가정이 필요하지만, 현실에서는 그렇지 않은 경우가 많습니다. GLM은 이러한 경우에도 사용하기 위해 만들어진 모델입니다. 정규 분포가 아닌 흔한 예시로는 이항분포, 포아송 분포 등이 있습니다. 나이에 따른 생존 상태(생존은 1, 사망은 0)인 이항분포 반응변수를 간단한 예시로 들어보겠습니다. 일반적 선형회귀 모델과 GLM을 통해 로지스틱 회귀 분석을 한 결과를 비교해보았습니다.\n\n\n\n\n\n\n\n\n일반적인 선형 회귀 함수를 적용했을 때 나이가 어린 경우에는 사망 확률이 음수가 나오며, 고령인 경우 사망 확률이 1이 넘는 등의 문제가 발생합니다. 반면, GLM(로지스틱 회귀모형)을 적용하면 모든 나이에서 사망 확률이 0에서 1사이가 됨을 알 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm을-적용하기-위한-데이터의-조건",
    "href": "posts/2024-09-13 jstable/index.html#glm을-적용하기-위한-데이터의-조건",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM을 적용하기 위한 데이터의 조건",
    "text": "GLM을 적용하기 위한 데이터의 조건\n이항 분포에 대해 GLM을 적용하였지만, 모든 데이터에 GLM을 적용할 수 있는 것은 아닙니다. GLM을 적용하기 위한 조건에 대해 알아보겠습니다.\n반응변수의 분포\n반응 변수의 분포 형태가 Exponential family(지수 분포군)를 따를 때 GLM을 적용할 수 있습니다. Exponential family는 아래와 같은 수식으로 나타낼 수 있습니다. \\[\nf(y | \\theta, \\phi) = \\exp\\left( \\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\right)\n\\] 여기서\\(y\\)는 반응 변수 \\(\\theta\\)는 정규화 매게 변수, \\(\\phi\\)는 분산을 나타냅니다. 이항분포를 예시로 생각해보겠습니다. 이항분포의 경우 성공 확률 \\(p\\)로 정의되며 아래 함수의 경우 다음과 같이 지수 분포군 함수로 변환이 가능합니다.\\[\nP(Y = y) = \\binom{n}{y} p^y (1 - p)^{n - y}\n\\] \\[\nP(Y = y) = \\binom{n}{y} \\exp\\left( y \\log\\left( \\frac{p}{1 - p} \\right) + n \\log(1 - p) \\right)\n\\] 위와 같이 Exponential family로 변형이 가능한 다른 흔한 분포로는 포아송 분포, 감마 분포 등이 있습니다. Exponential Family로 변형이 가능한 분포의 경우, 지수함수의 특성으로 최대우도 추정(Maximum Likelihood Estimation)을 효율적 계산을 통해 할 수 있으며, 링크함수를 통해 선형 예측을 쉽게 할 수 있다는 장점이 있습니다.\n링크 함수\nExponential family 함수의 경우 지수형태로 존재하기 때문에 곧바로 선형 회귀 분석을 할 수는 없습니다. 선형 회귀 분석과 Exponential family를 연결하기 위한 함수가 필요한데, 이를 링크함수라 부릅니다. 링크 함수를 통해 반응 변수의 평균과 예측 변수의 선형 관계를 나타낼 수 있습니다. 반응변수의 분포가 지수 분포군에 있을 때 링크함수를 사용할 수 있으며 이항 분포의 경우 예시는 이와 같습니다. 아래 함수를 적용하면 선형 관계의 그래프를 얻을 수 있습니다. 함수를 적용하니 평소에 알던 선형 회귀의 형태로 식이 나타나는 것을 알 수 있습니다. \\[\n\\log\\left( \\frac{1 - \\mu}{\\mu} \\right) = X\\beta\n\\]\n관측치 독립\nGLM을 적용하기 위해서는 관측치들이 독립되어 합니다. 예를 들어, 한 사람의 혈압을 반복측정하는 경우 관측치들이 독립되어 있지 않기 때문에 다른 모델을 사용하여야 합니다. 이 경우에 대해서는 문서의 아래 내용에서 다루도록 하겠습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm-table-해석하기",
    "href": "posts/2024-09-13 jstable/index.html#glm-table-해석하기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM table 해석하기",
    "text": "GLM table 해석하기\njstable 패키지를 사용하여 이항 분포와 가우스 분포의 GLM 모델의 결과를 직관적인 표로 만들 수 있습니다. 아래 코드는 R에서 기본으로 제공되는 mtcars 데이터를 사용하여과 이항 회귀 모델을 만들고 결과를 표로 나타낸 것입니다. 이항분포를 가지는 am(자동미션 =0, 수동미션 =1)을 wt(차량무게), hp(마력)을 통해 예측하는 GLM에 대해 알아보겠습니다.\n\nbinomial_model &lt;- glm(am ~ wt + hp, family = binomial, data = mtcars)\nglmshow.display(binomial_model, decimal = 2)\n\n$first.line\n[1] \"Logistic regression predicting am\\n\"\n\n$table\n   crude OR.(95%CI) crude P value adj. OR.(95%CI) adj. P value\nwt \"0.02 (0,0.3)\"   \"0.005\"       \"0 (0,0.13)\"    \"0.008\"     \nhp \"0.99 (0.98,1)\"  \"0.181\"       \"1.04 (1,1.07)\" \"0.041\"     \n\n$last.lines\n[1] \"No. of observations = 32\\nAIC value = 16.0591\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n이항 분포의 glm에 대하여 첫줄에서는 로지스틱 회귀 분석을 사용했음을 알 수 있습니다. 테이블 출력 결과를 통해 wt 변수는 p-value가 0.0001로 유의미하게 미션의 종류에 영향을 미치는 것을 알 수 있습니다. Crude OR 변수를 통해 차량 무게 변수가 1이 증가할때마다 수동미션을 가질 odds 비율이 98프로 씩 감소한다는 것을 알 수 있습니다. Adjusted OR 변수가 0이라는 것을 통해 마력 변수를 보정했을 경우에 Odds 비율이 더 강하게 연관되어 있다는 것을 알 수 있으며, adjusted p value또한 유의미하다는 것을 확인 할 수 있습니다. hp의 경우 OR과 p value를 통해 관련이 적음을 유추할 수 있습니다. 마지막 줄에서는 총 몇 개의 관측치가 있었는지, 그리고 AIC 수치(낮을 수록 더 좋은 모델)를 알 수 있습니다.\n\ngaussian_model &lt;- glm(mpg~cyl + disp, data = mtcars)\nglmshow.display(gaussian_model, decimal = 2)\n\n$first.line\n[1] \"Linear regression predicting mpg\\n\"\n\n$table\n     crude coeff.(95%CI)   crude P value adj. coeff.(95%CI)    adj. P value\ncyl  \"-2.88 (-3.51,-2.24)\" \"&lt; 0.001\"     \"-1.59 (-2.98,-0.19)\" \"0.034\"     \ndisp \"-0.04 (-0.05,-0.03)\" \"&lt; 0.001\"     \"-0.02 (-0.04,0)\"     \"0.054\"     \n\n$last.lines\n[1] \"No. of observations = 32\\nR-squared = 0.7596\\nAIC value = 167.1456\\n\\n\"\n\nattr(,\"class\")\n[1] \"display\" \"list\"   \n\n\n정규분포를 나타내는 데이터에 대해서도 적용해보겠습니다.첫줄에서는 정규분포에 대하여 선형 회귀 분석을 사용했음을 알 수 있고, 해당하는 p value 들 그리고 선형 회귀의 경우 OR 대신 Coeffecient를 제공함을 알 수 있습니다. 해당 테이블을 통해 cyl(실린더의 갯수)변수는 mpg(연비)에 유의미한 영향을 미치는 것을 알 수 있습니다. disp(배기량)변수는 보정 전에는 영향이 있어 보였으나 보정 이후 0.05보다 크기 때문에 통계적으로 유의미한 영향을 미치지 않는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#glm과-gee의-공분산-차이",
    "href": "posts/2024-09-13 jstable/index.html#glm과-gee의-공분산-차이",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GLM과 GEE의 공분산 차이",
    "text": "GLM과 GEE의 공분산 차이\n데이터간의 상관관계가 반영된다는 점에서 결과적으로 GEE는 복잡한 공분산을 반영하고 있다는 것을 알 수 있습니다.\nGLM의 경우 관측치들이 독립이라는 가정이 있기 때문에 공분산 행렬이 대각행렬입니다. 반면 GEE의 경우 관측치들이 상관관계가 있다는 가정을 따르기 때문에 공분산 행렬이 대각행렬이 아닌 다른 형태를 가질 수 있습니다. 따라서 GLM에서의 공분산 행렬은 \\(V_i = \\phi \\cdot W_i\\)와 같은 구조에서 \\(W_i\\)는 대각행렬이 되며, GEE에서의 공분산 행렬은 \\(V_i = \\phi \\cdot A_i^{1/2} R(\\alpha) A_i^{1/2}\\)와 같은 구조에서 \\(A_i\\)는 대각 가중치 행렬의 형태를 따르나, 상관관계를 반영하는 \\(R(\\alpha)\\)라는 상관행렬이 있게 됩니다. 공분산과 앞선 회귀식을 모두 반영하면 GLM 과 GEE는 모두 아래의 수식과 같은 형태를 띄게 되며, \\(V_i\\)에서 그 차이가 나타납니다. \\[\nY_i = g^{-1}(X_i^T \\beta) + \\epsilon_i \\quad \\text{with} \\quad \\epsilon_i \\sim N(0, V_i)\n\\]"
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#gee-table-해석하기",
    "href": "posts/2024-09-13 jstable/index.html#gee-table-해석하기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "GEE table 해석하기",
    "text": "GEE table 해석하기\njstable 패키지를 사용하여 군집 데이터의 GEE적용 후 결과 해석에 대해 알아보겠습니다. 아래 코드는 R에서 제공되는 geepack의 dietox 데이터를 이용하여 돼지의 체중을 예상하는 모델을 만드는 예시입니다. 시간과 구리 보충제 상태가 돼지의 체중에 미치는 영향을 분석해보겠습니다. 군집 데이터의 특성을 반영하여, 돼지별 데이터를 분석하였음을 코드에서도 알 수 있습니다.\n\nlibrary(geepack)  ## for dietox data\ndata(dietox)\ndietox$Cu &lt;- as.factor(dietox$Cu)\ndietox$ddn &lt;- as.numeric(rnorm(nrow(dietox)) &gt; 0)\ngee01 &lt;- geeglm (Weight ~ Time + Cu , id = Pig, data = dietox, family = gaussian, corstr = \"ex\")\ngeeglm.display(gee01)\n\n$caption\n[1] \"GEE(gaussian) predicting Weight by Time, Cu - Group Pig\"\n\n$table\n               crude coeff(95%CI)   crude P value adj. coeff(95%CI)  \nTime           \"6.94 (6.79,7.1)\"    \"&lt; 0.001\"     \"6.94 (6.79,7.1)\"  \nCu: ref.=Cu000 NA                   NA            NA                 \n      035      \"-0.59 (-3.73,2.54)\" \"0.711\"       \"-0.84 (-3.9,2.23)\"\n      175      \"1.9 (-1.87,5.66)\"   \"0.324\"       \"1.77 (-1.9,5.45)\" \n               adj. P value\nTime           \"&lt; 0.001\"   \nCu: ref.=Cu000 NA          \n      035      \"0.593\"     \n      175      \"0.345\"     \n\n$metric\n                                 crude coeff(95%CI) crude P value\n                                 NA                 NA           \nEstimated correlation parameters \"0.775\"            NA           \nNo. of clusters                  \"72\"               NA           \nNo. of observations              \"861\"              NA           \n                                 adj. coeff(95%CI) adj. P value\n                                 NA                NA          \nEstimated correlation parameters NA                NA          \nNo. of clusters                  NA                NA          \nNo. of observations              NA                NA          \n\n\n해석을 위해 jstable 패키지를 사용하여 결과를 표로 나타내었습니다. 시간의 p-value를 확인하였을 때, 시간은 돼지 체중 증가에 유의미한 영향을 미치고 있다고 할 수 있으며, 계수를 보았을 때 단위 시간당 체중이 6.94만큼 증가한다고 추정할 수 있습니다. 구리 보충제의 경우 두가지 종류의 보충제 모두 통계적으로 유의미한 영향을 미치지 않는다고 해석할 수 있습니다. Metric 부분에서는 모델의 상관구조와 관련된 정보를 알 수 있습니다. Estimated correlation parameters의 경우 같은 군집의 시간에 따른 관측값들 사이의 상관관계를 나타내며, 0.775는 높은 상관관계를 의미합니다. 아래의 cluster 와 observation에서는 총 72마리의 돼지에 대하여 861개의 관측치로 분석을 했음을 알 수 있습니다.\n\ngee02 &lt;- geeglm (ddn ~ Time + Cu , id = Pig, data = dietox, family = binomial, corstr = \"ex\")\ngeeglm.display(gee02)\n\n$caption\n[1] \"GEE(binomial) predicting ddn by Time, Cu - Group Pig\"\n\n$table\n               crude OR(95%CI)    crude P value adj. OR(95%CI)     adj. P value\nTime           \"1.01 (0.97,1.05)\" \"0.609\"       \"1.01 (0.97,1.05)\" \"0.615\"     \nCu: ref.=Cu000 NA                 NA            NA                 NA          \n      035      \"1.2 (0.91,1.58)\"  \"0.198\"       \"1.2 (0.91,1.58)\"  \"0.199\"     \n      175      \"1.21 (0.91,1.61)\" \"0.189\"       \"1.21 (0.91,1.61)\" \"0.19\"      \n\n$metric\n                                 crude OR(95%CI) crude P value adj. OR(95%CI)\n                                 NA              NA            NA            \nEstimated correlation parameters \"-0.022\"        NA            NA            \nNo. of clusters                  \"72\"            NA            NA            \nNo. of observations              \"861\"           NA            NA            \n                                 adj. P value\n                                 NA          \nEstimated correlation parameters NA          \nNo. of clusters                  NA          \nNo. of observations              NA          \n\n\n이번에는 이항분포를 사용하여 돼지의 체중 증가에 대한 모델을 만들어보았습니다. 이전 코드와는 다르게 family에 binomial을 입력하였으며, ddn은 임의로 0,1이 있는 이항분포 변수이며 이를 시간과 구리 보충제 상태로 예측하는 모델입니다. 이항분포이기 때문에 회귀계수 대신 OR이 있는 것을 알 수 있습니다. 변수들 모두 p-value가 통계적으로 의미 있는 수준은 아니라는 것을 알 수 있습니다. ddn이 임의 변수이기 때문에 시간이나 구리 보충제 여부에 따른 오즈비가 1에 가까울 것일 거라고 추측할 수 있고, 실제로 그러한 결과를 보여줍니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#jstable-패키지를-사용한-lmm-table만들기",
    "href": "posts/2024-09-13 jstable/index.html#jstable-패키지를-사용한-lmm-table만들기",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "jstable 패키지를 사용한 LMM table만들기",
    "text": "jstable 패키지를 사용한 LMM table만들기\n아래 코드는 R에서 제공되는 geepack의 dietox 데이터를 이용하여 돼지의 체중을 예상하는 모델을 만드는 예시입니다. GEE에서와 다르게 각 돼지별 체중의 차이가 다르다는 점을 반영한 모델을 만들어볼 것입니다.\n\nlibrary(lme4)\nl1 &lt;- lmer(Weight ~ Time + Cu + (1|Pig), data = dietox) \nlmer.display(l1, ci.ranef = T)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\nComputing profile confidence intervals ...\n\n\n$table\n                     crude coeff(95%CI) crude P value adj. coeff(95%CI)\nTime                   6.94 (6.88,7.01)     0.0000000  6.94 (6.88,7.01)\nCu: ref.=Cu000                     &lt;NA&gt;            NA              &lt;NA&gt;\n      035            -0.58 (-4.67,3.51)     0.7811327 -0.84 (-4.47,2.8)\n      175              1.9 (-2.23,6.04)     0.3670740  1.77 (-1.9,5.45)\nRandom effects                     &lt;NA&gt;            NA              &lt;NA&gt;\nPig                 40.34 (28.08,54.95)            NA              &lt;NA&gt;\nResidual             11.37 (10.3,12.55)            NA              &lt;NA&gt;\nMetrics                            &lt;NA&gt;            NA              &lt;NA&gt;\nNo. of groups (Pig)                  72            NA              &lt;NA&gt;\nNo. of observations                 861            NA              &lt;NA&gt;\nLog-likelihood                  -2400.8            NA              &lt;NA&gt;\nAIC value                        4801.6            NA              &lt;NA&gt;\n                    adj. P value\nTime                   0.0000000\nCu: ref.=Cu000                NA\n      035              0.6527264\n      175              0.3442309\nRandom effects                NA\nPig                           NA\nResidual                      NA\nMetrics                       NA\nNo. of groups (Pig)           NA\nNo. of observations           NA\nLog-likelihood                NA\nAIC value                     NA\n\n$caption\n[1] \"Linear mixed model fit by REML : Weight ~ Time + Cu + (1 | Pig)\"\n\n\n고정효과인 Time과 Cu 에 대해서는 이번에도 시간만 통계적으로 유의미한 변수로 나왔으며, 체중의 증가에 통계적으로 유의미한 영향을 미치는 것을 알 수 있습니다. 이번에는 임의 효과에 대해서 조금 더 자세히 알아보겠습니다. 돼지 데이터에서 임의 효과의 분산은 40.34로 나왔습니다. 이는 돼지마다 체중의 차이가 크다는 것을 알 수 있으며, 11.37이라는 잔차는 돼지별 임의 효과를 반영한 후에도 남는 체중의 변동이 있음을 설명합니다.\n\nl2 &lt;- glmer(ddn ~ Weight + Time + (1|Pig), data= dietox, family= \"binomial\")\n\nboundary (singular) fit: see help('isSingular')\n\nlmer.display(l2)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n$table\n                     crude OR(95%CI) crude P value  adj. OR(95%CI) adj. P value\nWeight                    1 (1,1.01)     0.7020746   1 (0.98,1.02)    0.8041261\nTime                1.01 (0.97,1.05)     0.6358656 1.03 (0.9,1.17)    0.7088793\nRandom effects                  &lt;NA&gt;            NA            &lt;NA&gt;           NA\nPig                                0            NA            &lt;NA&gt;           NA\nMetrics                         &lt;NA&gt;            NA            &lt;NA&gt;           NA\nNo. of groups (Pig)               72            NA            &lt;NA&gt;           NA\nNo. of observations              861            NA            &lt;NA&gt;           NA\nLog-likelihood               -596.49            NA            &lt;NA&gt;           NA\nAIC value                    1200.98            NA            &lt;NA&gt;           NA\n\n$caption\n[1] \"Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) : ddn ~ Weight + Time + (1 | Pig)\"\n\n\n이항 분포를 가지는 변수에는 GLM과 비슷한 방식으로 일반화 된 방식인 GLMM이 적용이 가능합니다. ddn의 경우 임의로 형성된 변수이기 때문에 통계적으로 유의미하지 않은 고정 변수들을 확인할 수 있으며 OR모두 1에 가깝다는 것을 확인할 수 있습니다. 임의효과인 돼지의 분산의 경우 앞선 LMM보다 낮게 나왔습니다. 이는 0과 1만 가지는 이항데이터를 다루기 때문에, 돼지 간의 기초 수준 차이가 연속형 데이터처럼 크게 반영되지 않았기 떄문일 것입니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#cluster-옵션을-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#cluster-옵션을-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "cluster 옵션을 사용한 Cox model",
    "text": "cluster 옵션을 사용한 Cox model\nlung의 데이터는 228명의 환자에 대한 생존 데이터를 포함하고 있습니다. ECOG와 나이에 따른 폐암 환자의 생존을 예측하는 모델을 만들고 싶은데, 같은 병원에서 치료를 받게 되면 병원의 비슷한 프로토콜 등에 영향을 받아 비슷한 치료 결과를 보일 수 있습니다. 따라서, 같은 병원 내의 환자들의 상관관계를 고려하여 만들고 싶을 때 cluster 옵션을 사용할 수 있습니다.\n\nfit1 &lt;- coxph(Surv(time, status) ~ ph.ecog + age, cluster = inst, lung, model = T)\ncox2.display(fit1)\n\n$table\n        crude HR(95%CI)       crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"38.34 (38.32,38.36)\" \"&lt; 0.001\"     \"33.03 (33,33.06)\" \"&lt; 0.001\"   \nage     \"14.67 (14.46,14.88)\" \"0.007\"       \"5.59 (4.73,6.61)\" \"0.085\"     \n\n$ranef\n        [,1] [,2] [,3] [,4]\ncluster   NA   NA   NA   NA\ninst      NA   NA   NA   NA\n\n$metric\n                        [,1] [,2] [,3] [,4]\n&lt;NA&gt;                      NA   NA   NA   NA\nNo. of observations  226.000   NA   NA   NA\nNo. of events        163.000   NA   NA   NA\nAIC                 1463.797   NA   NA   NA\n\n$caption\n[1] \"Marginal Cox model on time ('time') to event ('status') - Group inst\"\n\n\ntable의 결과를 보았을 때 ecog는 환자의 생존에 강한 영향을 미치며 통계적으로도 유의미하다는 것을 알 수 있습니다. age 변수의 경우 조정되지 않은 모델에서는 유의미한 위험 요인으로 나타났지만, ecog를 조정한 이후에는 통계적으로 유의미하지 않았다는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#frailty-옵션을-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#frailty-옵션을-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "frailty 옵션을 사용한 Cox model",
    "text": "frailty 옵션을 사용한 Cox model\n같은 병원 내의 환자들간의 상관관계 외에 병원간 차이를 고려하고 싶을 수도 있습니다. 일부 병원은 다른 병원보다 더 좋은 생존 결과를 가지고 있을 수 있기 때문에, 이에 따른 변수를 frailty 옵션을 사용해서 고려합니다.\n\nfit2 &lt;- coxph(Surv(time, status) ~ ph.ecog + age + frailty(inst), lung, model = T)\ncox2.display(fit1)\n\n$table\n        crude HR(95%CI)       crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"38.34 (38.32,38.36)\" \"&lt; 0.001\"     \"33.03 (33,33.06)\" \"&lt; 0.001\"   \nage     \"14.67 (14.46,14.88)\" \"0.007\"       \"5.59 (4.73,6.61)\" \"0.085\"     \n\n$ranef\n        [,1] [,2] [,3] [,4]\ncluster   NA   NA   NA   NA\ninst      NA   NA   NA   NA\n\n$metric\n                        [,1] [,2] [,3] [,4]\n&lt;NA&gt;                      NA   NA   NA   NA\nNo. of observations  226.000   NA   NA   NA\nNo. of events        163.000   NA   NA   NA\nAIC                 1463.797   NA   NA   NA\n\n$caption\n[1] \"Marginal Cox model on time ('time') to event ('status') - Group inst\"\n\n\ntable의 결과를 보았을 때 ecog와 age가 생존에 미치는 영향은 앞선 cluster 옵션을 이용했을 때와 비슷한 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2024-09-13 jstable/index.html#mixed-effect를-사용한-cox-model",
    "href": "posts/2024-09-13 jstable/index.html#mixed-effect를-사용한-cox-model",
    "title": "jstable을 이용한 분석 모형 table 만들기",
    "section": "mixed effect를 사용한 Cox model",
    "text": "mixed effect를 사용한 Cox model\n다층 구조나 복잡한 데이터 구조를 처리하고 싶은 경우 frailty 옵션만으로 부족할 수 있습니다. 예를 들어, 병원 간의 기초 수준 차이와 성별에 따른 기초 위험이 차이가 나는 것을 반영한 모델을 만들고 싶을 수 있습니다. 해당 경우 coxme함수를 이용할 수 있습니다.\n\nlibrary(coxme)\nfit &lt;- coxme(Surv(time, status) ~ ph.ecog + age + (1|inst)+(1|sex), lung)\ncoxme.display(fit) \n\nWarning in formula.character(object, env = baseenv()): Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n  Consider formula(paste(x, collapse = \" \")) instead.\nWarning in formula.character(object, env = baseenv()): Using formula(x) is deprecated when x is a character vector of length &gt; 1.\n  Consider formula(paste(x, collapse = \" \")) instead.\n\n\n$table\n        crude HR(95%CI)    crude P value adj. HR(95%CI)     adj. P value\nph.ecog \"1.66 (1.32,2.09)\" \"&lt; 0.001\"     \"1.63 (1.3,2.05)\"  \"&lt; 0.001\"   \nage     \"1.02 (1,1.04)\"    \"0.043\"       \"1.01 (0.99,1.03)\" \"0.234\"     \n\n$ranef\n                [,1] [,2] [,3] [,4]\nRandom effect     NA   NA   NA   NA\ninst(Intercept) 0.02   NA   NA   NA\nsex(Intercept)  0.14   NA   NA   NA\n\n$metric\n                    [,1] [,2] [,3] [,4]\n&lt;NA&gt;                  NA   NA   NA   NA\nNo. of groups(inst)   18   NA   NA   NA\nNo. of groups(sex)     2   NA   NA   NA\nNo. of observations  226   NA   NA   NA\nNo. of events        163   NA   NA   NA\n\n$caption\n[1] \"Mixed effects Cox model on time ('time') to event ('status') - Group inst, sex\""
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html",
    "href": "posts/2024-10-14-reportGeneration/index.html",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "",
    "text": "Quarto는 오픈소스 기반의 과학 및 기술 출판 시스템입니다.\n코드를 기반으로 문서를 생성하기 때문에 동일한 형식의 데이터으로부터 일정한 결과물을 얻을 수 있습니다.\n이는 문서의 일관성과 재사용성을 증대시키고 사람에 의한 실수를 줄이는 데 큰 도움이 됩니다.\n그러나 문서의 모든 요소를 코드의 형태로 만들 수 있는것은 아닙니다.\n이를테면 문서에 포함된 표 또는 그래프에 대한 설명은 작성자가 직접 작성해야합니다.\n이 과정을 효율적으로 해결하기 위해서 차라투에서는 LLM을 활용했습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#libraries",
    "href": "posts/2024-10-14-reportGeneration/index.html#libraries",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Libraries",
    "text": "Libraries\nhttr2\nOpenAI endpoint에 GET 요청을 실행하기 위한 라이브러리\nggplot2\n그래프를 그리기 위한 라이브러리\nbase64enc\n이미지를 base64로 인코딩 하기 위한 라이브러리\nflextable\n복잡한 table을 만들기 위한 라이브러리\nofficer\nflextable을 html format으로 변환할 때 사용되는 라이브러리\nmagrittr\n파이프 연산자를 사용하기 위한 라이브러리\nDBI (Optional)\nDB와 통신하기 위한 라이브러리\nRSQLite (Optional)\nSqlite3 인터페이스 라이브러리\ndigest (Optional)\n해쉬 알고리즘을 위한 라이브러리"
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#openai-api-key",
    "href": "posts/2024-10-14-reportGeneration/index.html#openai-api-key",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "OpenAI api key",
    "text": "OpenAI api key\n본 과정에서는 OpenAI 사의 GPT4o-mini 모델을 활용했습니다.\n본인의 OpenAI api 키를 발급받을 경우 동일한 방식으로 실습을 진행하실 수 있습니다.\nhttps://openai.com/index/openai-api/\n\nopenai_key &lt;- \"your openAI key\"\nendpoint &lt;- \"https://api.openai.com/v1/\""
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#instruction",
    "href": "posts/2024-10-14-reportGeneration/index.html#instruction",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Instruction",
    "text": "Instruction\nOpenAI endpoint로 요청 시 Text parameter가 포함 됩니다.\n이 Text에는 작업 지시 사항인 Instruction과 분석할 데이터가 포함됩니다.\nInstruction의 예시는 다음과 같습니다.\n\ninstruction &lt;- \"지침:\n- 한글로 4문장 이상 답변하세요.\n- 표에 있는 범주별로 내용을 설명하세요.\n- 각 범주에 포함된 모든 항목에 대해 설명하세요.\""
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#request",
    "href": "posts/2024-10-14-reportGeneration/index.html#request",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Request",
    "text": "Request\nEndpoint(https://api.openai.com/v1/chat/completions)로 요청 시 다음의 parameter가 사용되었습니다.\n\nHeader\n\nContent-Type: application/json\nAuthorization: Bearer $OPENAI_API_KEY\n\n\nBody\n\nmodel(*): gpt-4o-mini [openAI의 다른 모델도 사용할 수 있습니다]\ntemperature: 0.2 [0과 2 사이의 값, 높을수록 무작위적이며 창의적인 대답을 내놓음]\nmessages(*):\n\nrole: user\ncontent:\n\ntype: text\ntext: Introduction과 Table data\n\n\n\n\n\n\n\nTable\nLLM이 table을 인식하기 위해서는 table이 적절한 형식으로 변환되어야 합니다. LLM이 인식 할 수 있는 형식은 크게 두가지입니다.\nMarkdown\n마크다운은 일반 텍스트 기반의 경량 마크업 언어입니다. 누구나 쉽게 작성할 수 있다는 장점이 있으며 표 또한 텍스트를 통해 생성 할 수 있습니다. 다만 복잡한 형태의 표 (예: 계층 구조)의 경우 표현하기 어렵다는 단점이 있습니다.\n\nconvert_to_markdown &lt;- function(table) {\n  df_from_ft &lt;- as.data.frame(table)\n  markdown_table &lt;- capture.output(cat(knitr::kable(table, format = \"markdown\"), sep = \"\\n\"))\n  return(paste(markdown_table, collapse = \"\\n\"))\n}\n\n\nhead(mtcars[1:5])\n\n                   mpg cyl disp  hp drat\nMazda RX4         21.0   6  160 110 3.90\nMazda RX4 Wag     21.0   6  160 110 3.90\nDatsun 710        22.8   4  108  93 3.85\nHornet 4 Drive    21.4   6  258 110 3.08\nHornet Sportabout 18.7   8  360 175 3.15\nValiant           18.1   6  225 105 2.76\n\ncat(convert_to_markdown(head(mtcars[1:5])))\n\n|                  |  mpg| cyl| disp|  hp| drat|\n|:-----------------|----:|---:|----:|---:|----:|\n|Mazda RX4         | 21.0|   6|  160| 110| 3.90|\n|Mazda RX4 Wag     | 21.0|   6|  160| 110| 3.90|\n|Datsun 710        | 22.8|   4|  108|  93| 3.85|\n|Hornet 4 Drive    | 21.4|   6|  258| 110| 3.08|\n|Hornet Sportabout | 18.7|   8|  360| 175| 3.15|\n|Valiant           | 18.1|   6|  225| 105| 2.76|\n\n\nHTML\n일반적인 data.frame 객체는 마크다운으로 변환할 수 있습니다.\n그러나 flextable처럼 Cell이 합쳐진 형태나 계층적 구조일 경우 markdown으로의 변환이 어렵습니다.\n이 경우 html table 형태로 변환할 수 있습니다.\n\nproc_freq(mtcars, \"gear\", \"vs\")\n\n\n\n\n\n\ngear\n\nvs\n\n\n0\n1\nTotal\n\n\n\n\n3\nCount\n12 (37.5%)\n3 (9.4%)\n15 (46.9%)\n\n\nMar. pct (1)\n66.7% ; 80.0%\n21.4% ; 20.0%\n\n\n\n4\nCount\n2 (6.2%)\n10 (31.2%)\n12 (37.5%)\n\n\nMar. pct\n11.1% ; 16.7%\n71.4% ; 83.3%\n\n\n\n5\nCount\n4 (12.5%)\n1 (3.1%)\n5 (15.6%)\n\n\nMar. pct\n22.2% ; 80.0%\n7.1% ; 20.0%\n\n\n\nTotal\nCount\n18 (56.2%)\n14 (43.8%)\n32 (100.0%)\n\n\n (1) Columns and rows percentages\n\n\n\n\n\ncat(proc_freq(mtcars, \"gear\", \"vs\") %&gt;% to_html)\n\n&lt;div class=\"tabwid\"&gt;&lt;style&gt;.cl-d42a3a38{}.cl-d4230326{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d4230330{font-family:'DejaVu Sans';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-d425c64c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c660{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c661{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c662{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425c66a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d425e550{width:0.625in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e551{width:0.952in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e55a{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e55b{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e564{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e565{width:0.625in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e566{width:0.952in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e567{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e56e{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e56f{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e570{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e571{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e572{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e578{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e579{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e57a{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e57b{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e582{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e583{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e584{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e585{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58c{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58d{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58e{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e58f{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e596{width:0.625in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e597{width:0.952in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e598{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e599{width:1.279in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e59a{width:1.117in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e59b{width:0.625in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a0{width:0.952in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a1{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5a2{width:1.279in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d425e5aa{width:1.117in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.tabwid {\n  font-size: initial;\n  padding-bottom: 1em;\n}\n\n.tabwid table{\n  border-spacing:0px !important;\n  border-collapse:collapse;\n  line-height:1;\n  margin-left:auto;\n  margin-right:auto;\n  border-width: 0;\n  border-color: transparent;\n  caption-side: top;\n}\n.tabwid-caption-bottom table{\n  caption-side: bottom;\n}\n.tabwid_left table{\n  margin-left:0;\n}\n.tabwid_right table{\n  margin-right:0;\n}\n.tabwid td, .tabwid th {\n    padding: 0;\n}\n.tabwid a {\n  text-decoration: none;\n}\n.tabwid thead {\n    background-color: transparent;\n}\n.tabwid tfoot {\n    background-color: transparent;\n}\n.tabwid table tr {\nbackground-color: transparent;\n}\n.katex-display {\n    margin: 0 0 !important;\n}&lt;/style&gt;&lt;table data-quarto-disable-processing='true' class='cl-d42a3a38'&gt;&lt;thead&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;th  rowspan=\"2\"class=\"cl-d425e550\"&gt;&lt;p class=\"cl-d425c64c\"&gt;&lt;span class=\"cl-d4230326\"&gt;gear&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th  rowspan=\"2\"class=\"cl-d425e551\"&gt;&lt;p class=\"cl-d425c64c\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th  colspan=\"3\"class=\"cl-d425e55a\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;vs&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;th class=\"cl-d425e567\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;0&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class=\"cl-d425e56e\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;1&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th class=\"cl-d425e56f\"&gt;&lt;p class=\"cl-d425c660\"&gt;&lt;span class=\"cl-d4230326\"&gt;Total&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;3&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;12 (37.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;3 (9.4%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;15 (46.9%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e57b\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;span class=\"cl-d4230330\"&gt; (1)&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e582\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;66.7% ; 80.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e583\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;21.4% ; 20.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e584\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;4&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;2 (6.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;10 (31.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;12 (37.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e57b\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e582\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;11.1% ; 16.7%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e583\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;71.4% ; 83.3%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e584\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  rowspan=\"2\"class=\"cl-d425e570\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;5&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e571\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e572\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;4 (12.5%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e578\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;1 (3.1%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e579\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;5 (15.6%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e58c\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Mar. pct&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58d\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;22.2% ; 80.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58e\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;7.1% ; 20.0%&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e58f\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td class=\"cl-d425e596\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Total&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e597\"&gt;&lt;p class=\"cl-d425c661\"&gt;&lt;span class=\"cl-d4230326\"&gt;Count&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e598\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;18 (56.2%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e599\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;14 (43.8%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;td class=\"cl-d425e59a\"&gt;&lt;p class=\"cl-d425c662\"&gt;&lt;span class=\"cl-d4230326\"&gt;32 (100.0%)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt;&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;tfoot&gt;&lt;tr style=\"overflow-wrap:break-word;\"&gt;&lt;td  colspan=\"5\"class=\"cl-d425e59b\"&gt;&lt;p class=\"cl-d425c66a\"&gt;&lt;span class=\"cl-d4230330\"&gt; (1)&lt;/span&gt;&lt;span class=\"cl-d4230326\"&gt; Columns and rows percentages&lt;/span&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tfoot&gt;&lt;/table&gt;&lt;/div&gt;\n\n\nFigure\n만약 이미지에 대한 설명을 원한다면 이미지에 대한 파라미터를 추가해야합니다.\n\nHeader\n\nContent-Type: application/json\nAuthorization: Bearer $OPENAI_API_KEY\n\n\nBody\n\nmodel(*): gpt-4o-mini [openAI의 다른 모델도 사용할 수 있습니다]\ntemperature: 0.2 [0과 2 사이의 값, 높을수록 무작위적이며 창의적인 대답을 내놓음]\nmessages(*):\n\nrole: user\ncontent:\n\ntype: text\ntext: Introduction과 Table data\ntype: image_url\nimage_url:\n\nurl: base64로 인코딩된 이미지\n\n\n\n\n\n\n\n\n\n이미지는 base64로 인코딩되어 전달됩니다.\n\nLLM_description &lt;- function(inst = \"다음에 대해 설명하시오\", ## 지침\n                         add_inst = NULL, ## 추가 지침\n                         table = \"\", ## 데이터가 포함된 표\n                         image_path = NULL, ## 이미지 경로 (Local 또는 웹)\n                         model = \"gpt-4o-mini\", ## 사용 할 LLM 모델\n                         temperature = 0.2, ## 무작위성 (낮을수록 일관된 답변)\n                         endpoint = \"https://api.openai.com/v1/\" ## 엔드포인트\n                         ){\n  \n  ## LLM에게 전달 할 내용\n  ### 주요 지침\n  ### 추가 지침\n  ### 표\n  text &lt;- paste(\n    \"주요 지침: \",\n    inst,\n    ifelse(is.null(add_inst), ## 추가 지침이 존재 할 경우 추가\n           \" \",\n           \"추가 지침: \"),\n    add_inst,\n    \"도표: \",\n    table, ## Markdown 또는 html 형태의 table\n    sep = \"\\n\\n\"\n\n  )\n  \n  \n  url &lt;- NULL\n  if(!is.null(image_path)){ ## image path가 전달\n    if(file.exists(image_path)){\n      ## 로컬파일인 경우\n      url &lt;- paste0(\"data:image/png;base64,\",base64encode(image_path))\n    }else{\n      ## 로컬파일이 아닌 경우(Web 이미지 가정)\n      \n      ## 이미지 다운로드\n      resp &lt;- request(image_path) %&gt;% \n        req_perform()\n      \n      \n      if(resp$status_code == 200){\n        ## 정상적인 응답 시\n        image_raw &lt;- resp_body_raw(resp)\n        encoded_image &lt;- base64encode(image_raw)\n        url &lt;- paste0(\"data:image/png;base64,\",encoded_image )\n        \n      } else{\n        ## 비정상적인 응답 시\n        warning(\"Failed to download the image. Status code:\", response$status_code)\n      }\n      \n      \n    }\n  }\n  \n  ## 이미지 컴포넌트\n  image_cmp &lt;- NULL\n  \n  \n  if(!is.null(url)){\n    ## 이미지가 존재할 경우\n    \n    ## 이미지\n    image_cmp &lt;- list(\n            type = \"image_url\",\n            image_url = list(\n              url = url\n            )\n          )\n    \n    ## 이미지가 포함된 content\n    content &lt;- \n      list(\n          list(\n            type = \"text\",\n            text =  text\n          ),\n          image_cmp\n\n        )\n  }else{\n    ## 이미지가 존재하지 않을 경우\n    ## text만 content에 포함\n    content &lt;- list(\n          list(\n            type = \"text\",\n            text =  text\n          )\n\n        )\n  }\n  \n  \n\nreq &lt;- request(endpoint) %&gt;% ## 엔드포인트\n  req_url_path_append(\"chat\") %&gt;% ## 주소 1\n  req_url_path_append(\"completions\") %&gt;% ##주소 2\n  req_headers(\n    `Content-Type` = \"application/json\",\n    Authorization = paste(\"Bearer\", openai_key) ## API키 전달\n  ) %&gt;% \n  req_body_json(\n    list(\n      model = model, ## LLM 모델\n      temperature = temperature, \n      messages = list(list(\n        role = \"user\",\n        content = content\n      )\n    )),\n    auto_unbox = T\n  )\n\n\n\nresp &lt;- req %&gt;% \n  req_perform() ## 요청 실행\ntmp &lt;- resp %&gt;% \n  resp_body_json() ## 응답을 json 형태로 전환\n\n\n\nreturn(tmp$choices[[1]]$message$content) ## 응답 부분만 반환\n}"
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#database-optional",
    "href": "posts/2024-10-14-reportGeneration/index.html#database-optional",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "Database (Optional)",
    "text": "Database (Optional)\nQuarto 문서를 랜더링 할 때마다 OpenAI endpoint에 결과를 요청할 경우 상당한 비용을 초래할 수 있습니다. 따라서 동일한 parameter로 request 할 경우 DB에 저장된 결과를 대신 반환하도록 sqlite3를 이용하여 캐시 DB를 구성했습니다. 본 포스팅에서는 생략합니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#data.frame-형태의-표에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#data.frame-형태의-표에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "data.frame 형태의 표에 대한 설명",
    "text": "data.frame 형태의 표에 대한 설명\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\ncat(LLM_description(\n  inst = \"다음 표에 대해 설명하시오\",\n  add_inst = \"이 표는 자동차에 관한 표이다.\",\n  table = convert_to_markdown(mtcars)\n))\n\n이 표는 다양한 자동차 모델에 대한 여러 가지 성능 및 사양 데이터를 나열하고 있습니다. 각 열은 자동차의 특정 특성을 나타내며, 각 행은 특정 모델에 대한 정보를 제공합니다. 아래는 각 열의 설명입니다:\n\n1. **mpg (Miles Per Gallon)**: 연비를 나타내며, 차량이 1갤런의 연료로 주행할 수 있는 마일 수를 의미합니다. 이 값이 높을수록 연료 효율성이 좋습니다.\n\n2. **cyl (Cylinders)**: 엔진의 실린더 수를 나타냅니다. 일반적으로 실린더 수가 많을수록 엔진의 출력이 높아지지만, 연비는 낮아질 수 있습니다.\n\n3. **disp (Displacement)**: 엔진의 배기량을 나타내며, 보통 리터 또는 세제곱 인치로 측정됩니다. 배기량이 클수록 엔진의 힘이 강해지는 경향이 있습니다.\n\n4. **hp (Horsepower)**: 엔진의 출력, 즉 마력을 나타냅니다. 마력이 높을수록 차량의 성능이 좋습니다.\n\n5. **drat (Rear Axle Ratio)**: 후륜 구동차의 후축 비율을 나타내며, 엔진의 회전 속도와 바퀴의 회전 속도 간의 비율을 의미합니다. 이 값이 높을수록 가속력이 좋지만 연비는 떨어질 수 있습니다.\n\n6. **wt (Weight)**: 차량의 무게를 나타내며, 일반적으로 파운드로 측정됩니다. 무게가 가벼울수록 연비가 좋고, 주행 성능이 향상될 수 있습니다.\n\n7. **qsec (Quarter Mile Time)**: 1/4 마일(약 400미터) 주행에 소요되는 시간을 나타냅니다. 이 값이 낮을수록 차량의 가속력이 좋습니다.\n\n8. **vs (V/S)**: 엔진의 배치 방식으로, 0은 V형 엔진, 1은 직렬 엔진을 의미합니다.\n\n9. **am (Transmission Type)**: 변속기의 종류를 나타내며, 0은 자동 변속기, 1은 수동 변속기를 의미합니다.\n\n10. **gear (Number of Gears)**: 변속기의 기어 수를 나타냅니다. 기어 수가 많을수록 다양한 주행 조건에 적합할 수 있습니다.\n\n11. **carb (Carburetors)**: 카뷰레터의 수를 나타내며, 연료 혼합 비율을 조절하는 장치입니다. 카뷰레터 수가 많을수록 연료 공급이 원활해질 수 있습니다.\n\n이 표를 통해 다양한 자동차 모델의 성능과 사양을 비교할 수 있으며, 소비자들이 자신의 필요에 맞는 차량을 선택하는 데 유용한 정보를 제공합니다. 예를 들어, 연비가 중요한 소비자는 mpg가 높은 차량을 선호할 것이고, 성능이 중요한 소비자는 hp와 qsec 값이 높은 차량을 선택할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#flextable-형태의-표에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#flextable-형태의-표에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "flextable 형태의 표에 대한 설명",
    "text": "flextable 형태의 표에 대한 설명\n\nproc_freq(mtcars, \"gear\", \"vs\")\n\n\n\n\n\n\ngear\n\nvs\n\n\n0\n1\nTotal\n\n\n\n\n3\nCount\n12 (37.5%)\n3 (9.4%)\n15 (46.9%)\n\n\nMar. pct (1)\n66.7% ; 80.0%\n21.4% ; 20.0%\n\n\n\n4\nCount\n2 (6.2%)\n10 (31.2%)\n12 (37.5%)\n\n\nMar. pct\n11.1% ; 16.7%\n71.4% ; 83.3%\n\n\n\n5\nCount\n4 (12.5%)\n1 (3.1%)\n5 (15.6%)\n\n\nMar. pct\n22.2% ; 80.0%\n7.1% ; 20.0%\n\n\n\nTotal\nCount\n18 (56.2%)\n14 (43.8%)\n32 (100.0%)\n\n\n (1) Columns and rows percentages\n\n\n\n\n\ncat(LLM_description(\n  inst = \"다음 표에 대해 설명하시오.\",\n  add_inst = \"그래프는 gear 와 vs를 변수로 갖는 frequency table 이다.\",\n  table = proc_freq(mtcars, \"gear\", \"vs\") %&gt;% to_html\n  ))\n\n제공된 표는 \"gear\"와 \"vs\"라는 두 변수 간의 빈도 수를 나타내는 빈도표입니다. 이 표는 각 gear 값에 대해 vs 값이 0 또는 1인 경우의 수와 비율을 보여줍니다. \n\n### 표의 구성 요소 설명:\n\n1. **열 제목**:\n   - 첫 번째 열은 \"gear\" 값을 나타내며, 3, 4, 5의 세 가지 값이 있습니다.\n   - 두 번째 열은 비어 있으며, 주로 \"Count\"와 \"Mar. pct\"를 구분하는 역할을 합니다.\n   - 세 번째 열부터 다섯 번째 열은 \"vs\" 값에 대한 정보를 제공합니다. 세 번째 열은 vs가 0인 경우의 수, 네 번째 열은 vs가 1인 경우의 수, 다섯 번째 열은 두 경우의 합계입니다.\n\n2. **행 제목**:\n   - 각 gear 값에 대해 두 개의 행이 있습니다. 첫 번째 행은 각 경우의 수와 비율을 나타내고, 두 번째 행은 각 경우의 비율을 나타냅니다.\n\n3. **데이터**:\n   - 예를 들어, gear가 3일 때, vs가 0인 경우는 12건(37.5%), vs가 1인 경우는 3건(9.4%)이며, 총 15건(46.9%)입니다.\n   - gear가 4일 때, vs가 0인 경우는 2건(6.2%), vs가 1인 경우는 10건(31.2%)이며, 총 12건(37.5%)입니다.\n   - gear가 5일 때, vs가 0인 경우는 4건(12.5%), vs가 1인 경우는 1건(3.1%)이며, 총 5건(15.6%)입니다.\n\n4. **총계**:\n   - 마지막 행은 각 gear 값에 대한 총계를 보여줍니다. 예를 들어, vs가 0인 경우 총 18건(56.2%), vs가 1인 경우 총 14건(43.8%)이며, 전체 총계는 32건(100%)입니다.\n\n5. **비율**:\n   - \"Mar. pct\"는 각 gear 값에 대한 vs의 비율을 나타내며, 각 gear에 대해 두 개의 비율이 제공됩니다.\n\n### 요약:\n이 빈도표는 gear와 vs 간의 관계를 명확하게 보여주며, 각 gear 값에 따른 vs의 발생 빈도와 비율을 분석하는 데 유용합니다. 이를 통해 특정 gear 값이 vs에 미치는 영향을 이해할 수 있습니다."
  },
  {
    "objectID": "posts/2024-10-14-reportGeneration/index.html#그래프-등의-이미지에-대한-설명",
    "href": "posts/2024-10-14-reportGeneration/index.html#그래프-등의-이미지에-대한-설명",
    "title": "LLM을 이용한 분석 report 생성 (1)",
    "section": "그래프 등의 이미지에 대한 설명",
    "text": "그래프 등의 이미지에 대한 설명\n\nplot_am_bar &lt;- ggplot(mtcars) +\n  geom_bar(aes(factor(am)))\n\nplot_am_bar\n\n\n\n\n\n\nggsave(\"img/am.png\")\n\nSaving 7 x 5 in image\n\n\n\nplot_am_bar_build &lt;- ggplot_build(plot_am_bar)\ncat(LLM_description(\n  inst = \"다음 그래프에 대해 설명하시오.\",\n  add_inst = \"그래프는 자동차의 변속기 유형과 관련되어있다.\",\n  table = convert_to_markdown( plot_am_bar_build$data),\n  image_path = \"img/am.png\"\n  ))\n\n그래프는 자동차의 변속기 유형에 따른 분포를 나타내고 있습니다. x축은 변속기 유형을 나타내며, 0은 자동 변속기(automatic), 1은 수동 변속기(manual)를 의미합니다. y축은 각 변속기 유형에 해당하는 자동차의 수(count)를 나타냅니다.\n\n그래프를 보면, 자동 변속기를 가진 자동차의 수가 약 19대인 반면, 수동 변속기를 가진 자동차는 약 13대입니다. 이는 자동 변속기를 가진 자동차가 수동 변속기보다 더 많다는 것을 보여줍니다. \n\n전체적으로, 이 그래프는 자동차의 변속기 유형에 대한 분포를 시각적으로 표현하며, 자동 변속기가 더 일반적이라는 점을 강조하고 있습니다."
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#setup",
    "href": "posts/2024-10-28-Collapse/index.html#setup",
    "title": "collapse 패키지 소개 v2",
    "section": "Setup",
    "text": "Setup\n\n##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr);library(dplyr);library(data.table) \n\nlibrary(collapse);library(microbenchmark);library(lmtest)"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#basic",
    "href": "posts/2024-10-28-Collapse/index.html#basic",
    "title": "collapse 패키지 소개 v2",
    "section": "Basic",
    "text": "Basic\ndata.table처럼 fread & fwrite를 이용하여 csv파일을 처리한다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\nfwrite(dt, \"aa.csv\")\n\nColumns: ‘fselect’로 원하는 열을 불러올 수 있다.\n\nfselect(dt, 1:3, 13:16) |&gt; head()\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  334536     200911   162    51    63  19.4\n3:         2009  911867     200903   163    65    82  24.5\n4:         2009  183321     200908   152    51    70  22.1\n5:         2009  942671     200909   159    50    73  19.8\n6:         2009  979358     200912   157    55    73  22.3\n\nfselect(dt, EXMD_BZ_YYYY,RN_INDI,HME_YYYYMM )|&gt; head() # fselect(dt, \"EXMD_BZ_YYYY\",\"RN_INDI\",\"HME_YYYYMM\" )\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt;\n1:         2009  562083     200909\n2:         2009  334536     200911\n3:         2009  911867     200903\n4:         2009  183321     200908\n5:         2009  942671     200909\n6:         2009  979358     200912\n\n\nRows: ‘fsubset()’로 원하는 행/열을 불러올 수 있다.\n\nfsubset(dt, 1:3)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt;        &lt;int&gt;         &lt;int&gt;        &lt;int&gt;\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N  HGHT\n         &lt;int&gt;        &lt;int&gt;        &lt;int&gt;    &lt;int&gt;    &lt;int&gt;          &lt;int&gt; &lt;int&gt;\n1:           0            0           NA        3        1              0   144\n2:           0            0           NA        2        1              0   162\n3:           0            0           NA        3        1              0   163\n    WGHT  WSTC   BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT   HGB   FBS TOT_CHOL\n   &lt;int&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt; &lt;num&gt; &lt;int&gt;    &lt;int&gt;\n1:    61    90  29.4   0.7   0.8    120     80        1  12.6   117      264\n2:    51    63  19.4   0.8   1.0    120     80        1  13.8    96      169\n3:    65    82  24.5   0.7   0.6    130     80        1  15.0   118      216\n      TG   HDL   LDL  CRTN  SGOT  SGPT   GGT   GFR\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:   128    60   179   0.9    25    20    25    59\n2:    92    70    80   0.9    18    15    28    74\n3:   132    55   134   0.8    26    30    30    79\n\n#fsubset(dt, c(1:3, 13:16)) #rows\nfsubset(dt, 1:3, 13:16)  #(dt, row, col)\n\n    HGHT  WGHT  WSTC   BMI\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:   144    61    90  29.4\n2:   162    51    63  19.4\n3:   163    65    82  24.5\n\nfsubset(dt, c(1:nrow(dt)),c(1:3, 13:16)) |&gt; head() #cols\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  334536     200911   162    51    63  19.4\n3:         2009  911867     200903   163    65    82  24.5\n4:         2009  183321     200908   152    51    70  22.1\n5:         2009  942671     200909   159    50    73  19.8\n6:         2009  979358     200912   157    55    73  22.3\n\n# fsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) %&gt;%  fsubset(c(1:3),c(1:3,13:16))\nfsubset(dt, c(1:nrow(dt)),c(1:3, 13:16)) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) |&gt; head() # same\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  318669     200904   155    66    78  27.5\n3:         2009  668438     200904   160    71    94  27.7\n4:         2009  560878     200903   144    58    93  28.0\n5:         2009  375694     200906   151    70    94  30.7\n6:         2009  446652     200909   158    64    80  25.6\n\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25) %&gt;%\n  fsubset(c(1:nrow(dt)),c(1:3,13:16)) |&gt; head()\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM  HGHT  WGHT  WSTC   BMI\n          &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:         2009  562083     200909   144    61    90  29.4\n2:         2009  560878     200903   144    58    93  28.0\n3:         2011  562083     201111   144    59    88  28.5\n4:         2011  519824     201109   145    58    79  27.6\n5:         2011  914987     201103   145    70    95  33.3\n6:         2012  560878     201208   145    59    85  28.1"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#fast-statistical-function",
    "href": "posts/2024-10-28-Collapse/index.html#fast-statistical-function",
    "title": "collapse 패키지 소개 v2",
    "section": "Fast Statistical Function",
    "text": "Fast Statistical Function\n\n.FAST_STAT_FUN\n # [1]  \"fmean\"      \"fmedian\"    \"fmode\"      \"fsum\"       \"fprod\"      \n # [6]  \"fsd\"        \"fvar\"       \"fmin\"       \"fmax\"       \"fnth\"       \n # [11] \"ffirst\"     \"flast\"      \"fnobs\"      \"fndistinct\"\n\n# 데이터 구조에 구애받지않음.\nv1 &lt;- c(1,2,3,4)\nm1 &lt;- matrix(1:50, nrow = 10, ncol = 5)\n \nfmean(v1); fmean(m1); fmean(dt)\nfmode(v1); fmode(m1); fmode(dt)\n# fmean(m1): by columns\n\n\n# collapse; baseR과 비교했을 때 보다 빠른 속도를 보인다.\nx &lt;- rnorm(1e7)\nmicrobenchmark(mean(x), fmean(x), fmean(x, nthreads = 4)) \n\nUnit: milliseconds\n                   expr       min        lq      mean    median        uq\n                mean(x) 23.761096 23.786943 23.802908 23.799427 23.815928\n               fmean(x) 15.332085 15.367978 15.388554 15.387914 15.404170\n fmean(x, nthreads = 4)  4.217606  6.684896  7.634676  7.741456  8.499509\n      max neval cld\n 23.94914   100 a  \n 15.57999   100  b \n 11.20740   100   c\n\nmicrobenchmark(colMeans(dt), sapply(dt, mean), fmean(dt))\n\nUnit: microseconds\n             expr      min       lq       mean   median        uq      max\n     colMeans(dt) 3154.750 3302.700 3300.31781 3307.968 3312.7130 3641.641\n sapply(dt, mean)  190.076  199.417  208.52219  206.010  215.9145  318.417\n        fmean(dt)   52.889   53.803   56.23603   55.644   56.8805   90.947\n neval cld\n   100 a  \n   100  b \n   100   c\n\n\n\nData size가 더 클 경우, 보다 유용하다. (GGDC10S: 5000rows, 11cols, ~10% missing values)\n\n\n\nmicrobenchmark(base = sapply(GGDC10S[6:16], mean, na.rm = TRUE), fmean(GGDC10S[6:16]))\n\nUnit: microseconds\n                 expr     min      lq     mean   median       uq      max neval\n                 base 412.369 429.161 773.8810 807.4445 818.8705 7949.178   100\n fmean(GGDC10S[6:16])  94.481  95.856 102.7777 103.9790 108.0860  142.060   100\n cld\n  a \n   b\n\n\n\n이처럼, Collapse는 data 형식에 구애받지 않고, 보다 빠른 속도를 특징으로 하는 package이다.\n\n이들의 문법을 알아보자.\n-   Fast Statistical Functions\n\n  Syntax:\n\nFUN(x, g = NULL, \\[w = NULL,\\] TRA = NULL, \\[na.rm = TRUE\\], use.g.names = TRUE, \\[drop = TRUE,\\] \\[nthreads = 1,\\] ...)\n\n       \nArgument            Description\n      g             grouping vectors / lists of vectors or ’GRP’ object\n      w             a vector of (frequency) weights\n    TRA             a quoted operation to transform x using the statistics\n  na.rm             efficiently skips missing values in x\n  use.g.names       generate names/row-names from g\n  drop              drop dimensions if g = TRA = NULL\n  nthreads          number of threads for OpenMP multithreading\n사용예시 : fmean\n\n# Weighted Mean\nw &lt;- abs(rnorm(nrow(iris)))\nall.equal(fmean(num_vars(iris), w = w), sapply(num_vars(iris), weighted.mean, w = w))\n\n[1] TRUE\n\nwNA &lt;- na_insert(w, prop = 0.05)\nsapply(num_vars(iris), weighted.mean, w = wNA) # weighted.mean(): 결측치를 처리하지 못한다.\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n          NA           NA           NA           NA \n\nfmean(num_vars(iris), w = wNA) #결측치를 자동으로 무시한다.\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.797389     3.048473     3.683776     1.151507 \n\n# Grouped Mean\nfmean(iris$Sepal.Length, g = iris$Species)\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\nfmean(num_vars(iris), iris$Species)  \n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\n# Weighted Group Mean\nfmean(num_vars(iris), iris$Species, w)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa         5.015518    3.460443     1.479887   0.2495797\nversicolor     5.918636    2.698947     4.259102   1.2888099\nvirginica      6.568402    2.959146     5.577613   2.0433786\n\n# 속도 상의 이점. \nmicrobenchmark(fmean = fmean(iris$Sepal.Length, iris$Species),\n               tapply = tapply(iris$Sepal.Length, iris$Species, mean))\n\nUnit: microseconds\n   expr    min      lq     mean  median      uq     max neval cld\n  fmean  7.488  7.8230  8.49842  8.2905  8.5785  32.276   100  a \n tapply 46.609 47.8695 49.83706 48.5615 48.9820 152.046   100   b"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#consideration-w-missing-data-결측치-처리",
    "href": "posts/2024-10-28-Collapse/index.html#consideration-w-missing-data-결측치-처리",
    "title": "collapse 패키지 소개 v2",
    "section": "Consideration w/ missing data: 결측치 처리",
    "text": "Consideration w/ missing data: 결측치 처리\n\n#wlddev$GINI, g: country, function: mean, median, min, max, sum, prod\ncollap(wlddev, GINI ~ country, list(mean, median, min, max, sum, prod),\n       na.rm = TRUE, give.names = FALSE) |&gt; head()\n\n         country     mean median  min  max   sum         prod\n1    Afghanistan      NaN     NA  Inf -Inf   0.0 1.000000e+00\n2        Albania 31.41111   31.7 27.0 34.6 282.7 2.902042e+13\n3        Algeria 34.36667   35.3 27.6 40.2 103.1 3.916606e+04\n4 American Samoa      NaN     NA  Inf -Inf   0.0 1.000000e+00\n5        Andorra      NaN     NA  Inf -Inf   0.0 1.000000e+00\n6         Angola 48.66667   51.3 42.7 52.0 146.0 1.139065e+05\n\n# na.rm=T가 기본값이며, NA를 연산한 값은 모두 NA를 결과값으로 반영함. \ncollap(wlddev, GINI ~ country, list(fmean, fmedian, fmin, fmax, fsum, fprod),\n       give.names = FALSE) |&gt; head()\n\n         country    fmean fmedian fmin fmax  fsum        fprod\n1    Afghanistan       NA      NA   NA   NA    NA           NA\n2        Albania 31.41111    31.7 27.0 34.6 282.7 2.902042e+13\n3        Algeria 34.36667    35.3 27.6 40.2 103.1 3.916606e+04\n4 American Samoa       NA      NA   NA   NA    NA           NA\n5        Andorra       NA      NA   NA   NA    NA           NA\n6         Angola 48.66667    51.3 42.7 52.0 146.0 1.139065e+05\n\nmicrobenchmark(a = collap(wlddev, GINI ~ country, list(mean, median, min, max, sum, prod),\n                          na.rm = TRUE, give.names = FALSE) |&gt; head(),\n               b=collap(wlddev, GINI ~ country, list(fmean, fmedian, fmin, fmax, fsum, fprod),\n                        give.names = FALSE) |&gt; head())\n\nUnit: microseconds\n expr      min       lq       mean   median         uq       max neval cld\n    a 9783.454 9857.552 10483.5492 9942.606 10258.8435 15291.497   100  a \n    b  534.478  577.196   604.1118  615.318   626.4175   855.808   100   b\n\n# 속도 상 이점을 다시 한 번 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#tra-function",
    "href": "posts/2024-10-28-Collapse/index.html#tra-function",
    "title": "collapse 패키지 소개 v2",
    "section": "TRA function",
    "text": "TRA function\n\nTRA function을 이용, 여러 행/열의 연산을 간편하게 처리할 수 있다.\n\nSyntax:\n  TRA(x, STATS, FUN = \"-\", g = NULL, set = FALSE, ...)\n\n\n  setTRA(x, STATS, FUN = \"-\", g = NULL, ...)\n\n  STATS = vector/matrix/list of statistics\n\n0        \"replace_NA\"     replace missing values in x\n1        \"replace_fill\"   replace data and missing values in x\n2        \"replace\"        replace data but preserve missing values in x\n3        \"-\"              subtract (i.e. center)\n4        \"-+\"             center on overall average statistic\n5        \"/\"              divide (i.e. scale)\n6        \"%\"              compute percentages (i.e. divide and multiply by 100)   \n7        \"+\"              add\n8        \"*\"              multiply\n9        \"%%\"             modulus (i.e. remainder from division by STATS)\n10       \"-%%\"            subtract modulus (i.e. make data divisible by STATS)\n\ndt2 &lt;- as.data.table(iris)\n\nattach(iris)    #data.table에서처럼 변수명을 직접 호출하기 위해 attach를 사용할 수 있다.\n\n# 평균값과의 차: g= Species\nall_obj_equal(Sepal.Length - ave(Sepal.Length, g = Species),\n              fmean(Sepal.Length, g = Species, TRA= \"-\"),\n              TRA(Sepal.Length, fmean(Sepal.Length, g = Species), \"-\", g = Species))\n\n[1] TRUE\n\nmicrobenchmark(baseR= Sepal.Length - ave(Sepal.Length, g = Species),\n               fmean = mean(Sepal.Length, g = Species, TRA= \"-\"),\n               TRA_fmean = TRA(Sepal.Length, fmean(Sepal.Length, g = Species), \"-\", g = Species));detach(iris)\n\nUnit: microseconds\n      expr    min      lq     mean  median      uq     max neval cld\n     baseR 57.640 58.9120 61.35788 59.9555 61.2070 156.905   100 a  \n     fmean  3.754  3.9635  4.29474  4.1200  4.2485  19.150   100  b \n TRA_fmean 11.907 12.4290 13.49149 13.0765 13.4220  55.347   100   c\n\n\n\n\nTRA()를 사용하기보다 Fast Statistical Function에서 TRA 기능을 호출하자!\n\n\n#예시\nnum_vars(dt2) %&lt;&gt;%  na_insert(prop = 0.05)\n\n# NA 값을 median값으로 대체.\nnum_vars(dt2) |&gt; fmedian(iris$Species, TRA = \"replace_NA\", set = TRUE)\n# num_vars(dt2) |&gt; fmean(iris$Species, TRA = \"replace_NA\", set = TRUE) --&gt; mean으로 대체.\n\n\n# 다양한 연산 및 작업을 한 번에 다룰 수 있다.\nmtcars |&gt; ftransform(A = fsum(mpg, TRA = \"%\"),\n                     B = mpg &gt; fmedian(mpg, cyl, TRA = \"replace_fill\"),\n                     C = fmedian(mpg, list(vs, am), wt, \"-\"),\n                     D = fmean(mpg, vs,, 1L) &gt; fmean(mpg, am,, 1L)) |&gt; head(3)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb        A     B\nMazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 3.266449  TRUE\nMazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 3.266449  TRUE\nDatsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 3.546430 FALSE\n                 C     D\nMazda RX4      1.3 FALSE\nMazda RX4 Wag  1.3 FALSE\nDatsun 710    -7.6  TRUE"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#grouping-object",
    "href": "posts/2024-10-28-Collapse/index.html#grouping-object",
    "title": "collapse 패키지 소개 v2",
    "section": "Grouping Object",
    "text": "Grouping Object\n\n\nGRP function을 이용, group을 쉽게 연산할 수 있다.\nSyntax:\n\n    GRP(X, by = NULL, sort == TRUE, decreasing = FALSE, na.last = TRUE, \n    return.groups = TRUE, return.order = sort, method = \"auto\", ...)\n\n\n\ng &lt;- GRP(iris, by = ~ Species)\nprint(g)\n\ncollapse grouping object of length 150 with 3 ordered groups\n\nCall: GRP.default(X = iris, by = ~Species), X is sorted\n\nDistribution of group sizes: \n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     50      50      50      50      50      50 \n\nGroups with sizes: \n    setosa versicolor  virginica \n        50         50         50 \n\nstr(g)\n\nClass 'GRP'  hidden list of 9\n $ N.groups    : int 3\n $ group.id    : int [1:150] 1 1 1 1 1 1 1 1 1 1 ...\n $ group.sizes : int [1:3] 50 50 50\n $ groups      :'data.frame':   3 obs. of  1 variable:\n  ..$ Species: Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 2 3\n $ group.vars  : chr \"Species\"\n $ ordered     : Named logi [1:2] TRUE TRUE\n  ..- attr(*, \"names\")= chr [1:2] \"ordered\" \"sorted\"\n $ order       : int [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n  ..- attr(*, \"starts\")= int [1:3] 1 51 101\n  ..- attr(*, \"maxgrpn\")= int 50\n  ..- attr(*, \"sorted\")= logi TRUE\n $ group.starts: int [1:3] 1 51 101\n $ call        : language GRP.default(X = iris, by = ~Species)\n\n# GRP 기능- 호출하여 사용하자!\nfmean(num_vars(iris), g)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nfmean(num_vars(iris), iris$Species)\n\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#factors-in-operation",
    "href": "posts/2024-10-28-Collapse/index.html#factors-in-operation",
    "title": "collapse 패키지 소개 v2",
    "section": "Factors in operation",
    "text": "Factors in operation\nCollaspe는 형식에 구애받지 않는다; factor를 바로 연산할 수 있으며, qF로 빠르게 factor를 생성할 수 있다.\n\nx &lt;- na_insert(rnorm(1e7), prop = 0.01) \ng &lt;- sample.int(1e6, 1e7, TRUE)         \n# grp와 비교\nsystem.time(gg &lt;- GRP(g))\n\n   user  system elapsed \n  0.649   0.027   0.677 \n\nsystem.time(f &lt;- qF(g, na.exclude = FALSE))\n\n   user  system elapsed \n  0.254   0.044   0.298 \n\nclass(f)\n\n[1] \"factor\"      \"na.included\"\n\n\n\nmicrobenchmark(fmean(x, g), \n               fmean(x, gg), \n               fmean(x, gg, na.rm = FALSE), \n               fmean(x, f))\n ## Unit: milliseconds\n ##       expr                    min         lq          mean        median\n ## fmean(x, g)                   146.060983  150.493309  155.02585   152.197822\n ## fmean(x, gg)                  25.354564   27.709625   29.48497    29.022157\n ## fmean(x, gg, na.rm = FALSE)   13.184534   13.783585   15.61769    14.128067\n ## fmean(x, f)                   24.847271   27.503661   29.47271    29.248580\n\n# qF를 통해 grp와 유사한 성능 향상을 기대할 수 있다."
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#summary-fast-grouping-and-ordering",
    "href": "posts/2024-10-28-Collapse/index.html#summary-fast-grouping-and-ordering",
    "title": "collapse 패키지 소개 v2",
    "section": "Summary: FAST grouping and Ordering",
    "text": "Summary: FAST grouping and Ordering\n다양한 기능이 있다. \nGRP()           Fast sorted or unsorted grouping of multivariate data, returns detailed object of class ’GRP’ \nqF()/qG()       Fast generation of factors and quick-group (’qG’) objects from atomic vectors \nfinteraction()  Fast interactions: returns factor or ’qG’ objects \nfdroplevels()   Efficiently remove unused factor levels\n\nradixorder()    Fast ordering and ordered grouping \ngroup()         Fast first-appearance-order grouping: returns ’qG’ object \ngsplit()        Split vector based on ’GRP’ object \ngreorder()      Reorder the results\n\n- that also return ’qG’ objects \ngroupid()       Generalized run-length-type grouping seqid() Grouping of integer sequences \ntimeid()        Grouping of time sequences (based on GCD)\n\ndapply()        Apply a function to rows or columns of data.frame or matrix based objects. \nBY()            Apply a function to vectors or matrix/data frame columns by groups.\n\n-   Specialized Data Transformation Functions \nfbetween()      Fast averaging and (quasi-)centering. \nfwithin()\nfhdbetween()    Higher-Dimensional averaging/centering and linear prediction/partialling out \nfhdwithin()     (powered by fixest’s algorithm for multiple factors).\nfscale()        (advanced) scaling and centering.\n\n-   Time / Panel Series Functions \nfcumsum()       Cumulative sums \nflag()          Lags and leads \nfdiff()         (Quasi-, Log-, Iterated-) differences \nfgrowth()       (Compounded-) growth rates\n\n-    Data manipulation functions\nfselect(),      fsubset(),      fgroup_by(),    [f/set]transform[v](),          \nfmutate(),      fsummarise(),   across(),       roworder[v](),            \ncolorder[v](),  [f/set]rename(),                [set]relabel()"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#collapse는-빠르다",
    "href": "posts/2024-10-28-Collapse/index.html#collapse는-빠르다",
    "title": "collapse 패키지 소개 v2",
    "section": "Collapse는 빠르다!",
    "text": "Collapse는 빠르다!\n\nfdim(wlddev)    ##faster dim for dt. col/row: 13176 13\n\n# 1990년 이후를 기준으로, ODA/POP의 값 (g: region, income, OECD)\nmicrobenchmark( \n  \ndplyr = qDT(wlddev) |&gt;\n        filter(year &gt;= 1990) |&gt;\n        mutate(ODA_POP = ODA / POP) |&gt;\n        group_by(region, income, OECD) |&gt;\n        summarise(across(PCGDP:POP, sum, na.rm = TRUE), .groups = \"drop\") |&gt;\n        arrange(income, desc(PCGDP)),\n\ndata.table = qDT(wlddev)[, ODA_POP := ODA / POP][\n             year &gt;= 1990, lapply(.SD, sum, na.rm = TRUE),\n             by = .(region, income, OECD), .SDcols = PCGDP:ODA_POP][\n             order(income, -PCGDP)],\n\ncollapse_base = qDT(wlddev) |&gt;\n                fsubset(year &gt;= 1990) |&gt;\n                fmutate(ODA_POP = ODA / POP) |&gt;\n                fgroup_by(region, income, OECD) |&gt;\n                fsummarise(across(PCGDP:ODA_POP, sum, na.rm = TRUE)) |&gt;\n                roworder(income, -PCGDP),\n\ncollapse_optimized = qDT(wlddev) |&gt;\n                    fsubset(year &gt;= 1990, region, income, OECD, PCGDP:POP) |&gt;\n                    fmutate(ODA_POP = ODA / POP) |&gt;\n                    fgroup_by(1:3, sort = FALSE) |&gt; fsum() |&gt;\n                    roworder(income, -PCGDP)\n)\n\n\n## Unit: microseconds\n##        expr            min         lq            mean            median          uq            max         neval\n## dplyr                  71955.523   72291.9715    80009.2208      72453.1165      76902.671   393947.262  100 \n## data.table             5960.503    6310.7045     7116.6673       6721.3450       7046.837    18615.736     100   \n## collapse_base          859.505     948.2200      1041.1137       990.1375        1061.864     3148.804       100 \n## collapse_optimized     442.040     482.9705      542.6927        523.6950        574.921     1036.817      100"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#collapse-w-fast-statistical-function-다양한-활용",
    "href": "posts/2024-10-28-Collapse/index.html#collapse-w-fast-statistical-function-다양한-활용",
    "title": "collapse 패키지 소개 v2",
    "section": "Collapse w/ Fast Statistical Function: 다양한 활용",
    "text": "Collapse w/ Fast Statistical Function: 다양한 활용\n\n# 아래 셋은 동일한 결과를 보인다.\n# cyl별 mpg sum\n mtcars %&gt;% ftransform(mpg_sum = fsum(mpg, g = cyl, TRA = \"replace_fill\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(mpg_sum = fsum(mpg)) %&gt;% head(10)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4   138.2\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4   138.2\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   293.3\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1   138.2\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2   211.4\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1   138.2\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4   211.4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2   293.3\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2   293.3\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4   138.2\n\n\n\nad-hoc grouping, often fastest!\n\n\n\nmicrobenchmark(a=mtcars %&gt;% ftransform(mpg_sum = fsum(mpg, g = cyl, TRA = \"replace_fill\")),\n               b=mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")),\n               c=mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(mpg_sum = fsum(mpg)))\n\nUnit: microseconds\n expr    min      lq     mean  median      uq     max neval cld\n    a 27.362 28.9525 30.22495 29.9020 31.5925  39.924   100 a  \n    b 64.001 66.0010 68.26108 67.2155 68.9120 114.379   100  b \n    c 78.678 80.4105 84.21481 81.2445 82.4050 264.876   100   c\n\n\n\n\nftransform()은 앞의 fgroupby를 무시한다. 아래 둘은 값이 다르다. (fmutate, fsummarise만 이전 group을 반영한다.)\n\n\nmtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, GRP(.), TRA = \"replace_fill\")) %&gt;% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   138.2\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   138.2\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   293.3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   138.2\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   211.4\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   138.2\n\nmtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(mpg_sum = fsum(mpg, TRA = \"replace_fill\")) %&gt;% head()\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_sum\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4   642.9\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4   642.9\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   642.9\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1   642.9\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2   642.9\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1   642.9\n\n\n\n위 언급과 같이 baseR 의 “/”보다 collapse의 TRA function을 이용하는 것이 더 빠르다.\n\n\nmicrobenchmark(\n\"/\"=      mtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop = mpg / fsum(mpg))      |&gt; head(),     \n\"TRA=/\" = mtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop = fsum(mpg, TRA = \"/\")) |&gt; head()\n)\n\nUnit: microseconds\n  expr     min       lq     mean   median       uq     max neval cld\n     / 207.643 210.3915 214.5014 212.7320 215.4790 281.881   100  a \n TRA=/ 196.455 200.0970 205.9386 202.4655 205.0695 471.228   100   b\n\n\n\n\nfsum은 grp 별로 연산을 처리하나, sum은 전체를 반영한다.\n\n\nmtcars |&gt; fgroup_by(cyl) |&gt; fmutate(mpg_prop2 = fsum(mpg) / sum(mpg))|&gt; head() #\"!=1\" \n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg_prop2\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 0.2149634\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 0.2149634\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 0.4562140\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 0.2149634\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 0.3288225\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 0.2149634\n\n\n\n자유로운 %&gt;% 의 사용\n\n\n# 아래 둘은 동일하다.\n mtcars %&gt;% fgroup_by(cyl) %&gt;% ftransform(fselect(., hp:qsec) %&gt;% fsum(TRA = \"/\")) %&gt;% invisible()\n mtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(across(hp:qsec, fsum, TRA = \"/\")) %&gt;% head()\n\n                   mpg cyl disp         hp       drat         wt       qsec vs\nMazda RX4         21.0   6  160 0.12850467 0.15537849 0.12007333 0.13080102  0\nMazda RX4 Wag     21.0   6  160 0.12850467 0.15537849 0.13175985 0.13525111  0\nDatsun 710        22.8   4  108 0.10231023 0.08597588 0.09227220 0.08840435  1\nHornet 4 Drive    21.4   6  258 0.12850467 0.12270916 0.14734189 0.15448188  1\nHornet Sportabout 18.7   8  360 0.05974735 0.06967485 0.06144064 0.07248414  0\nValiant           18.1   6  225 0.12266355 0.10996016 0.15857012 0.16068023  1\n                  am gear carb\nMazda RX4          1    4    4\nMazda RX4 Wag      1    4    4\nDatsun 710         1    4    1\nHornet 4 Drive     0    3    1\nHornet Sportabout  0    3    2\nValiant            0    3    1\n\n\n\n\nset = TRUE를 통해 원본 데이터에 반영할 수 있다.\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# mtcars의 열 hp:qsec의 값과 해당하는 g:cyl별 합의 비율.\nmtcars %&gt;% fgroup_by(cyl) %&gt;% fmutate(across(hp:qsec, fsum, TRA = \"/\", set = TRUE)) %&gt;% invisible()\nhead(mtcars)\n\n                   mpg cyl disp         hp       drat         wt       qsec vs\nMazda RX4         21.0   6  160 0.12850467 0.15537849 0.12007333 0.13080102  0\nMazda RX4 Wag     21.0   6  160 0.12850467 0.15537849 0.13175985 0.13525111  0\nDatsun 710        22.8   4  108 0.10231023 0.08597588 0.09227220 0.08840435  1\nHornet 4 Drive    21.4   6  258 0.12850467 0.12270916 0.14734189 0.15448188  1\nHornet Sportabout 18.7   8  360 0.05974735 0.06967485 0.06144064 0.07248414  0\nValiant           18.1   6  225 0.12266355 0.10996016 0.15857012 0.16068023  1\n                  am gear carb\nMazda RX4          1    4    4\nMazda RX4 Wag      1    4    4\nDatsun 710         1    4    1\nHornet 4 Drive     0    3    1\nHornet Sportabout  0    3    2\nValiant            0    3    1\n\n\n\n\n.apply = FALSE를 통해 subset group에만 적용할 수 있다.\n\n\n# 각 g:cyl의 hp:qsec까지의 변수에 대한 부분 상관관계\nmtcars %&gt;% fgroup_by(cyl) %&gt;% fsummarise(across(hp:qsec, \\(x) qDF(pwcor(x), \"var\"), .apply = FALSE)) %&gt;% head()\n\n  cyl  var         hp       drat         wt       qsec\n1   4   hp  1.0000000 -0.4702200  0.1598761 -0.1783611\n2   4 drat -0.4702200  1.0000000 -0.4788681 -0.2833656\n3   4   wt  0.1598761 -0.4788681  1.0000000  0.6380214\n4   4 qsec -0.1783611 -0.2833656  0.6380214  1.0000000\n5   6   hp  1.0000000  0.2171636 -0.3062284 -0.6280148\n6   6 drat  0.2171636  1.0000000 -0.3546583 -0.6231083"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#이름순번vectors정규표현식으로-행열을-지칭할-수-있다.",
    "href": "posts/2024-10-28-Collapse/index.html#이름순번vectors정규표현식으로-행열을-지칭할-수-있다.",
    "title": "collapse 패키지 소개 v2",
    "section": "이름/순번/vectors/정규표현식으로 행/열을 지칭할 수 있다.",
    "text": "이름/순번/vectors/정규표현식으로 행/열을 지칭할 수 있다.\nget_vars(x, vars, return = \"names\", regex = FALSE, ...) \nget_vars(x, vars, regex = FALSE, ...) &lt;- value \n\n- 위치도 선택가능하다.\nadd_vars(x, ..., pos = \"end\") \nadd_vars(x, pos = \"end\") &lt;- value \n\n- data type을 지정할 수 있다. \nnum_vars(x, return = \"data\");   cat_vars(x, return = \"data\");   char_vars(x, return = \"data\"); \nfact_vars(x, return = \"data\");  logi_vars(x, return = \"data\");  date_vars(x, return = \"data\") \n\n- replace 또한 가능하다.\nnum_vars(x) &lt;- value;   cat_vars(x) &lt;- value;   char_vars(x) &lt;- value; \nfact_vars(x) &lt;- value;  logi_vars(x) &lt;- value;  date_vars(x) &lt;- value"
  },
  {
    "objectID": "posts/2024-10-28-Collapse/index.html#efficient-programming",
    "href": "posts/2024-10-28-Collapse/index.html#efficient-programming",
    "title": "collapse 패키지 소개 v2",
    "section": "Efficient programming",
    "text": "Efficient programming\n    &gt; quick data conversion\n-   qDF(),  qDT(),  qTBL(),   qM(),   mrtl(),   mctl()\n    \n-   anyv(x, value) / allv(x, value)     # Faster than any/all(x == value)\n-   allNA(x)                            # Faster than all(is.na(x))\n-   whichv(x, value, invert = F)        # Faster than which(x (!/=)= value)\n-   whichNA(x, invert = FALSE)          # Faster than which((!)is.na(x))\n-   x %(!/=)=% value                    # Infix for whichv(v, value, TRUE/FALSE)\n-   setv(X, v, R, ...)                  # x\\[x(!/=)=v\\]\\&lt;-r / x\\[v\\]\\&lt;-r\\[v\\] (by reference)\n-   setop(X, op, V, rowwise = F)        # Faster than X \\&lt;- X +/-/\\*// V (by reference)\n-   X %(+,-,\\*,/)=% V                   # Infix for setop,()\n-   na_rm(x)                            # Fast: if(anyNA(x)) x\\[!is.na(x)\\] else x,\n-   na_omit(X, cols = NULL, ...)        # Faster na.omit for matrices and data frames\n-   vlengths(X, use.names=TRUE)         # Faster version of lengths()\n-   frange(x, na.rm = TRUE)             # Much faster base::range\n-   fdim(X)                             # Faster dim for data frames"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html",
    "href": "posts/2024-12-06-tidyplots/index.html",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots는 ggplot2과 비슷하게 R에서 데이터를 이용하여 그래프를 그릴 수 있게 해 주는 라이브러리이다. tidyplots는 Rstudio에서 다음과 같이 설치 후 실행할 수 있다.\n\ninstall.packages('tidyplots')\nlibrary(tidyplots)\n\n\nlibrary(tidyplots)\nlibrary(DT)\nlibrary(magrittr)\nlibrary(ggplot2)\n\n\ntidyplots와 ggplot2는 그래프를 그릴 수 있다는 점은 동일하고 실제로 tidyplots object는 ggplot object와 동일하게 다른 작업이 모두 가능하다. 하지만 코드를 작성함에 있어서 차이점이 있다. study dataset의 treatment, score 컬럼으로 그래프를 그리는 코드를 통해 비교해 보자.\n\n데이터 확인\n\n\ndt &lt;- study\nhead(dt)\n\n# A tibble: 6 × 7\n  treatment group   dose  participant   age sex    score\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 A         placebo high  p01            23 female     2\n2 A         placebo high  p02            45 male       4\n3 A         placebo high  p03            32 female     5\n4 A         placebo high  p04            37 male       4\n5 A         placebo high  p05            24 female     6\n6 B         placebo low   p06            23 female     9\n\n\n\nggplot2로 작성한 코드\n\n\nggplot(dt, aes(x = treatment, y = score)) +\n  geom_boxplot() +   \n  geom_jitter(width = 0.2, color = \"blue\", alpha = 0.5) +  \n  labs(title = \"Treatment vs Score\",\n       x = \"Treatment\",\n       y = \"Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntidyplots로 작성한 코드\n\n\ndt %&gt;% \n  tidyplot(x = treatment, y = score) %&gt;% \n  add_data_points_jitter() %&gt;% \n  adjust_title(\"Treatment vs Score\") %&gt;% \n  adjust_x_axis_title(\"Treatment\") %&gt;% \n  adjust_y_axis_title(\"Score\") %&gt;%\n  add_boxplot(alpha = 0) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n코드로 확인할 수 있듯이 “+” 기호로 코드를 잇는 ggplot2와는 다르게, tidyplots는 공통적으로 ’tidyplot’이라는 코드로 그래프를 선언한 다음, 그 뒤에 파이프 연산자 %&gt;%를 이어서 그래프를 구체화시킬 수 있다. 각각의 함수의 이름에 기능이 직관적으로 잘 드러나 있기에 쉽게 그래프 코드를 작성할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#tidyplots",
    "href": "posts/2024-12-06-tidyplots/index.html#tidyplots",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots는 ggplot2과 비슷하게 R에서 데이터를 이용하여 그래프를 그릴 수 있게 해 주는 라이브러리이다. tidyplots는 Rstudio에서 다음과 같이 설치 후 실행할 수 있다.\n\ninstall.packages('tidyplots')\nlibrary(tidyplots)\n\n\nlibrary(tidyplots)\nlibrary(DT)\nlibrary(magrittr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#ggplot2와의-차이점",
    "href": "posts/2024-12-06-tidyplots/index.html#ggplot2와의-차이점",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "",
    "text": "tidyplots와 ggplot2는 그래프를 그릴 수 있다는 점은 동일하고 실제로 tidyplots object는 ggplot object와 동일하게 다른 작업이 모두 가능하다. 하지만 코드를 작성함에 있어서 차이점이 있다. study dataset의 treatment, score 컬럼으로 그래프를 그리는 코드를 통해 비교해 보자.\n\n데이터 확인\n\n\ndt &lt;- study\nhead(dt)\n\n# A tibble: 6 × 7\n  treatment group   dose  participant   age sex    score\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 A         placebo high  p01            23 female     2\n2 A         placebo high  p02            45 male       4\n3 A         placebo high  p03            32 female     5\n4 A         placebo high  p04            37 male       4\n5 A         placebo high  p05            24 female     6\n6 B         placebo low   p06            23 female     9\n\n\n\nggplot2로 작성한 코드\n\n\nggplot(dt, aes(x = treatment, y = score)) +\n  geom_boxplot() +   \n  geom_jitter(width = 0.2, color = \"blue\", alpha = 0.5) +  \n  labs(title = \"Treatment vs Score\",\n       x = \"Treatment\",\n       y = \"Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ntidyplots로 작성한 코드\n\n\ndt %&gt;% \n  tidyplot(x = treatment, y = score) %&gt;% \n  add_data_points_jitter() %&gt;% \n  adjust_title(\"Treatment vs Score\") %&gt;% \n  adjust_x_axis_title(\"Treatment\") %&gt;% \n  adjust_y_axis_title(\"Score\") %&gt;%\n  add_boxplot(alpha = 0) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n코드로 확인할 수 있듯이 “+” 기호로 코드를 잇는 ggplot2와는 다르게, tidyplots는 공통적으로 ’tidyplot’이라는 코드로 그래프를 선언한 다음, 그 뒤에 파이프 연산자 %&gt;%를 이어서 그래프를 구체화시킬 수 있다. 각각의 함수의 이름에 기능이 직관적으로 잘 드러나 있기에 쉽게 그래프 코드를 작성할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#plotting",
    "href": "posts/2024-12-06-tidyplots/index.html#plotting",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "1. Plotting",
    "text": "1. Plotting\ntidyplots는 기본적으로 항상 데이터와 함께 tidyplot()으로 그래프를 선언한 후 코드를 작성해야 한다. 아래의 예시는 모두 같은 결과를 수행한다. 예시에는 tidyplots 패키지에서 기본으로 제공하는 study dataset을 이용하였다.\n\n# example 1\nplot1 &lt;- tidyplot(study, x = treatment, y = score)\n\n# example 2\nplot2 &lt;- study %&gt;% \n  tidyplot(x = treatment, y = score)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-add",
    "href": "posts/2024-12-06-tidyplots/index.html#method-add",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "2. Method : Add",
    "text": "2. Method : Add\ntidyplots에서 그래프를 구체화하는 작업은 이후 %&gt;% 연산자를 이용하여 연결한다. tidyplots의 대부분의 함수는 add, adjust, remove로 구분할 수 있으며, 이 섹션에서는 그래프의 요소를 더하는 add 에 해당하는 대표적인 기능에 대해 알아본다.\nadd_data_points\n각각의 데이터들을 그래프 상에 표시하고 싶다면 add_data_points()를 사용하면 된다. 목적에 따라 add_data_points_jitter(), add_data_points_beeswarm() 옵션도 지원한다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points_jitter() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points_beeswarm() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n이외에도 함수 내부에서 다양한 argument를 조정하여 커스터마이징할 수 있다.\nadd_count, add_sum, add_mean, add_median\ntidyplots에서는 그래프를 그리고 난 이후 다양한 지표요소를 그래프에 추가할 수 있는 기능을 제공한다. 종류는 mean, median, sum, count가 있고, value를 통해 직접 값을 표시할 수도 있고 bar, dash, line, area 등 다양한 형식으로 표현할 수 있다. 가령 데이터의 평균값을 bar의 형태로 표시하고 싶다면 add_mean_bar() 함수를 쓰면 된다. 예시를 통해 알아보자.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_dash() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_dot() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_value() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_area() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score) %&gt;%\n  add_mean_bar(alpha = 0.4) %&gt;%\n  add_mean_dash() %&gt;%\n  add_mean_dot() %&gt;%\n  add_mean_value() %&gt;%\n  add_mean_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n기타\n이외에도 add_test_pvalue()를 통해 \"wilcox_test\", \"t_test\", \"sign_test\", \"dunn_test\", \"emmeans_test\", \"tukey_hsd\", \"games_howell_test\"등의 통계 검정 결과를 추가하거나 add_data_labels(), add_title() 등으로 차트 요소를 추가할 수 있다."
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-adjust",
    "href": "posts/2024-12-06-tidyplots/index.html#method-adjust",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "3. Method : Adjust",
    "text": "3. Method : Adjust\n이 섹션에서는 그래프의 요소를 수정하는 adjust 에 해당하는 대표적인 기능에 대해 알아본다.\n1. adjust_title, adjust_axis_title, adjust_caption\n그래프에서 제목이나, 축 제목을 수정해야 할 때 위의 함수들을 이용할 수 있다. 아래와 같은 방식으로 그래프의 요소를 추가한 후, 바로 파이프를 이어서 수정하고자 하는 제목, 축에 대해 값을 입력하면 된다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_data_points() %&gt;%\n  add_mean_bar(alpha = 0.4) %&gt;%\n  add_sem_errorbar() %&gt;%\n  adjust_title(\"This is my fantastic plot title\") %&gt;%\n  adjust_x_axis_title(\"Treatment group\") %&gt;%\n  adjust_y_axis_title(\"Disease score\") %&gt;%\n  adjust_legend_title(\"Legend title\") %&gt;%\n  adjust_caption(\"Here goes the caption\") %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n2. adjust_padding, adjust_axis\n그래프에서 여백을 조정해야 할 때, adjust_padding()함수를 사용하면 도움이 된다. 또한 adjust_axis()를 사용하면 각각의 축에 대하여 세세한 수정이 가능하며, parameter와 사용 예시는 아래에 첨부한다.\n\nadjust_x_axis(\n  plot,\n  title = ggplot2::waiver(),\n  breaks = ggplot2::waiver(),\n  labels = ggplot2::waiver(),\n  limits = NULL,\n  padding = c(NA, NA),\n  rotate_labels = FALSE,\n  transform = \"identity\",\n  cut_short_scale = FALSE,\n  force_continuous = FALSE,\n  ...\n)\n# y axis도 동일한 argument를 가짐\n\nbreaks, labels, limits는 ggplot2와 동일한 방식으로 수정할 수 있다. transform의 경우 필요하다면 log, date, exp 등 여러 가지 옵션으로 수정할 수 있다.\n\n# Axes limits\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(limits = c(-1000, 4000)) %&gt;%\n  adjust_y_axis(limits = c(-200, 600)) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Rotate labels\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(rotate_labels = 90) %&gt;%\n  adjust_y_axis(rotate_labels = 90) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Increase plot area padding\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(padding = c(0.2, 0.2)) %&gt;%\n  adjust_y_axis(padding = c(0.2, 0.2)) %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Scale transformation\nanimals %&gt;%\n  tidyplot(x = weight, y = size, color = family) %&gt;%\n  add_data_points() %&gt;%\n  adjust_x_axis(transform = \"log10\") %&gt;%\n  adjust_y_axis(transform = \"log2\") %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#method-remove",
    "href": "posts/2024-12-06-tidyplots/index.html#method-remove",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "4. Method : Remove",
    "text": "4. Method : Remove\n이 섹션에서는 그래프의 요소를 수정하는 remove에 해당하는 대표적인 기능에 대해 알아본다.\n1. remove_padding\n그래프를 그리다 보면 x축, y축 여백이 필요하지 않은 경우가 있는데, 이 경우 remove_padding()을 넣으면 바로 해결할 수 있다.\n\n# Before removing\nanimals %&gt;%\n  tidyplot(x = weight, y = speed, color = family) %&gt;%\n  add_data_points() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# After removing\nanimals %&gt;%\n  tidyplot(x = weight, y = speed, color = family) %&gt;%\n  add_data_points() %&gt;%\n  remove_padding() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n2. remove_title, remove_caption, remove_axis_\n제목, 캡션, 범례 뿐만 아니라 축과 관련된 요소 또한 선택적으로 제거할 수 있다.\n\n# Before removing\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# After removing\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_line() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_ticks() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_labels() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis_title() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add_mean_bar() %&gt;%\n  remove_x_axis() %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#ggplot-to-tidyplot-add",
    "href": "posts/2024-12-06-tidyplots/index.html#ggplot-to-tidyplot-add",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "1. ggplot to tidyplot, add",
    "text": "1. ggplot to tidyplot, add\nggplot2 라이브러리에서는 그래프의 세부 요소를 매우 다양하게 조정할 수 있는데, as_tidyplot()을 통해 미리 만들어 놓은 ggplot object를 tidyplots object로 변환하여 tidyplots 문법으로 나머지 작업을 시행할 수 있다.\n\ngg &lt;- ggplot(study, aes(x = treatment, y = score, color = treatment)) +\n  geom_point()\n\ngg\n\n\n\n\n\n\ngg %&gt;% as_tidyplot() %&gt;% \n  remove_legend() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n\n또한 tidyplot 코드 내부에서 ggplot2의 기능을 사용하고 싶을때는 add::ggplot2()함수를 사용하여 파이프로 연결하여 적용할 수 있다.\n\nstudy %&gt;%\n  tidyplot(x = treatment, y = score, color = treatment) %&gt;%\n  add(ggplot2::geom_point()) %&gt;% \n  adjust_size(width = NA, height = NA)"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#split",
    "href": "posts/2024-12-06-tidyplots/index.html#split",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "2. split",
    "text": "2. split\ntidyplots에서 제공하는 기능 중 하나는 그래프를 그린 후. 특정 컬럼을 기준으로 그룹핑 후 그래프를 분리할 수 있는 기능이다. 이는 split_plot()으로 시행 가능하다. argument는 다음과 같다.\n\nsplit_plot(\n  plot,\n  by,\n  ncol = NULL,\n  nrow = NULL,\n  byrow = NULL,\n  widths = 30,\n  heights = 25,\n  guides = \"collect\",\n  tag_level = NULL,\n  design = NULL,\n  unit = \"mm\"\n)\n\n특정 컬럼을 지정하면 그 컬럼을 기준으로 그래프가 분리되며, nrow, ncol을 지정하면 페이지가 분리가 된다. 주의할 점은 split_plot()은 코드의 맨 뒤에 위치해야 한다는 점이다.\n\n# Before splitting\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;% \n  adjust_size(width = NA, height = NA)\n\n\n\n\n\n\n# Split by year\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year)\n\n✔ split_plot: split into 4 plots across 1 page\n\n\n\n\n\n\n\n# Spread plots across multiple pages\nenergy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year, ncol = 2, nrow = 1) \n\n✔ split_plot: split into 4 plots across 2 pages\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n[[2]]"
  },
  {
    "objectID": "posts/2024-12-06-tidyplots/index.html#vector-graphics",
    "href": "posts/2024-12-06-tidyplots/index.html#vector-graphics",
    "title": "tidyplots: 간편한 그래프 작성 라이브러리",
    "section": "3. vector graphics",
    "text": "3. vector graphics\ntidyplots object도 ggplot2 object와. rvg 패키지로 동일하게 그래프를 벡터 그래픽으로 변환 후 officer 패키지와 연계하여 수정 가능한 pptx 파일로 저장할 수 있다. 그 예시를 소개한다.\n\nlibrary(officer)\nlibrary(rvg)\n\nplot3 &lt;- energy %&gt;%\n  dplyr::filter(year %in% c(2005, 2010, 2015, 2020)) %&gt;%\n  tidyplot(y = power, color = energy_source) %&gt;%\n  add_donut() %&gt;%\n  adjust_size(width = NA, height = NA) %&gt;% \n  split_plot(by = year) \n\nppt &lt;- read_pptx() %&gt;% \n  add_slide(layout = \"Title and Content\", master = \"Office Theme\") %&gt;% \n  ph_with(dml(ggobj = plot3), location = ph_location_type(type = \"body\")) %&gt;% \n  print(target = \"plot.pptx\")"
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "두 그룹의 분포가 같은 지 다른 지 검정하기 위한 방법 중 Mann-Whitney U test라는 비모수검정법이 있다. 비모수적검정법이기에 population의 분포에 대한 가정을 하지 않는다. 첫 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(X_1, X_2, …, X_n\\)이고, 두 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(Y_1, Y_2, …, Y_m\\)이라고 하자. Mann-Whitney U test에서 Null hypothesis, Alternative hypothesis는 다음과 같다.\nNull hypothesis\n\\(H_0\\): 두 그룹의 분포가 동일하다.\nAlternative hypothesis 대립가설은 다음 중 한 가지이다.\n\\(H_1\\): 두 그룹의 분포가 다르다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 오른쪽에 있다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 왼쪽에 있다.\nMann-Whitney statistic은 다음과 같다.\n\\[\nU = \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} S(X_i, Y_j) =  R_1 - \\frac{n_1 (n_1 + 1)}{2}\n\\]\n\\[\n\\text{단, } S(X, Y) =\n\\begin{cases}\n1 & \\text{if } X &gt; Y \\\\\n\\frac{1}{2} & \\text{if } X = Y \\\\\n0 & \\text{if } X &lt; Y\n\\end{cases}\n\\]\n\\(R_1\\)은 첫 번째 그룹의 순위합을 나타낸다.\n이 때 U를 n1, n2로 나누어 정규화를 하면 그 값은 X &gt; Y일 확률을 추정하는 통계량이 되며 이는 AUC의 값과 동일하다.\n\\[AUC = \\frac{U}{n_1 n_2} = P(X &gt; Y) + \\frac{1}{2} P(X = Y)\n\\]\n시각적으로 왜 AUC와 정규화한 U statistic이 같은 지를 시각적으로 보여주는 그림이다. positive case와 negative case를 rank순으로 배치한 뒤, positive case만 모아 다시 그림을 그린다.\n\n왼쪽 직사각형에 속하는 부분의 넓이가 U statistic이 되며 이를 n1과 n2로 나누어주면 AUC가 됨을 확인할 수 있다.\n\n\n새로운 영상 의학의 판독법의 효과를 종래의 판독법과 성능을 비교해야 하는 상황을 생각해보자. 영상 의학 검사는 reader마다 양성임을 판단하는 threshold가 다르기 때문에, sensitivity와 specificity의 variability가 다를 수 밖에 없다. 그렇기에 그런 효과를 보정하기 위하여 Multi-Reader Multi-Case analysis가 도입되었다. Each case는 multiple readers에 review되며, each reader는 multiple cases를 review한다. 통계적으로는 모형에서 case와 reader의 효과를 random factor로 처리하고, 그에 따라 각각의 variability를 설명할 수 있다.\ndifferent modality의 성능평가 뿐만 아니라, machine learning algorithm result도 비교할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#auc와-mann-whitney-statistic-관계",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#auc와-mann-whitney-statistic-관계",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "두 그룹의 분포가 같은 지 다른 지 검정하기 위한 방법 중 Mann-Whitney U test라는 비모수검정법이 있다. 비모수적검정법이기에 population의 분포에 대한 가정을 하지 않는다. 첫 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(X_1, X_2, …, X_n\\)이고, 두 번째 그룹에서 i.i.d로 뽑은 표본 데이터가 \\(Y_1, Y_2, …, Y_m\\)이라고 하자. Mann-Whitney U test에서 Null hypothesis, Alternative hypothesis는 다음과 같다.\nNull hypothesis\n\\(H_0\\): 두 그룹의 분포가 동일하다.\nAlternative hypothesis 대립가설은 다음 중 한 가지이다.\n\\(H_1\\): 두 그룹의 분포가 다르다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 오른쪽에 있다.\n\\(H_1\\): 첫 번째 그룹의 분포가 두 번째 그룹의 분포보도 왼쪽에 있다.\nMann-Whitney statistic은 다음과 같다.\n\\[\nU = \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} S(X_i, Y_j) =  R_1 - \\frac{n_1 (n_1 + 1)}{2}\n\\]\n\\[\n\\text{단, } S(X, Y) =\n\\begin{cases}\n1 & \\text{if } X &gt; Y \\\\\n\\frac{1}{2} & \\text{if } X = Y \\\\\n0 & \\text{if } X &lt; Y\n\\end{cases}\n\\]\n\\(R_1\\)은 첫 번째 그룹의 순위합을 나타낸다.\n이 때 U를 n1, n2로 나누어 정규화를 하면 그 값은 X &gt; Y일 확률을 추정하는 통계량이 되며 이는 AUC의 값과 동일하다.\n\\[AUC = \\frac{U}{n_1 n_2} = P(X &gt; Y) + \\frac{1}{2} P(X = Y)\n\\]\n시각적으로 왜 AUC와 정규화한 U statistic이 같은 지를 시각적으로 보여주는 그림이다. positive case와 negative case를 rank순으로 배치한 뒤, positive case만 모아 다시 그림을 그린다.\n\n왼쪽 직사각형에 속하는 부분의 넓이가 U statistic이 되며 이를 n1과 n2로 나누어주면 AUC가 됨을 확인할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#motivation",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#motivation",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "",
    "text": "새로운 영상 의학의 판독법의 효과를 종래의 판독법과 성능을 비교해야 하는 상황을 생각해보자. 영상 의학 검사는 reader마다 양성임을 판단하는 threshold가 다르기 때문에, sensitivity와 specificity의 variability가 다를 수 밖에 없다. 그렇기에 그런 효과를 보정하기 위하여 Multi-Reader Multi-Case analysis가 도입되었다. Each case는 multiple readers에 review되며, each reader는 multiple cases를 review한다. 통계적으로는 모형에서 case와 reader의 효과를 random factor로 처리하고, 그에 따라 각각의 variability를 설명할 수 있다.\ndifferent modality의 성능평가 뿐만 아니라, machine learning algorithm result도 비교할 수 있다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#roe-and-metz-model",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#roe-and-metz-model",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "Roe and Metz model",
    "text": "Roe and Metz model\n\\[X_{ijk}^{R\\&M} = \\mu_t + \\tau_{it} + R_{jt} + C_{kt} + [RC]_{jkt} + [\\tau R]_{ijt} + [\\tau C]_{ikt} + [\\tau RC]_{ijkt} + E_{ijkt}\\]\n이때, \\(X_{ijk}\\)에서 i는 modality, j는 reader, case는 k, t는 truth state를 의미한다. modality와 truth state는 fixed factor이고, reader와 case는 random factor이다. 이외 나머지 term들은 고유한 variance를 가진independent zero-mean Gaussian random variables이다."
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#imrmc-package",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#imrmc-package",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "iMRMC package",
    "text": "iMRMC package\niMRMC는 MRMC analysis/simulation을 하기 위한 package로 다음의 주요한 2개의 function을 포함하고 있다.\n\ndoIMRMC: ROC data를 받아서 MRMC analysis를 수행하는 function\nsim.gRoeMetz: MRMC data를 simulation하여 two modalities를 비교할 수 있는 function\n\ncardiac CT study 예시 자료를 통해 MRMC를 수행해보자. colorScaleStudyData dataset은 다음 링크에서 다운로드 받을 수 있다.\n이 study는 Grayscale, Rainbow and Hotiron등 세 가지의 modalities를 포함한다. 경험이 적은 판독자를 Low, 경험이 많은 판독자를 High로 표시하였으며, Low는 1~8까지 High는 1~4까지 총 12명의 판독자가 포함되어 있다. 총 210 cases를 포함하며, 그중 양성은 105개이다. 실습에 필요한 data는 cardioCTReaderData(판독자의 판독 결과)와 cardioCTGroundTruth data(정답)이다. 그리고 이 study는fully crossed가 아닌 split-plot study이다.\n\nlibrary(data.table); library(magrittr)\nlibrary(ggplot2); library(knitr)\n\nload(\"data/cardioCTGroundTruth.rda\")\nload(\"data/cardioCTReaderData.rda\")\n\n\nsetDT(cardioCTGroundTruth)\nsetDT(cardioCTReaderData)\n\ncardioCTGroundTruth와 cardioCTReaderData는 다음과 같다.\n\nnames(cardioCTReaderData)[4] &lt;- \"score\"\nkable(cardioCTGroundTruth[1:30,], caption = \"cardioCTGroundTruth\", align = 'c' )\n\n\ncardioCTGroundTruth\n\ncaseID\ntruth\n\n\n\n1\npositive\n\n\n2\npositive\n\n\n3\npositive\n\n\n4\npositive\n\n\n5\npositive\n\n\n6\npositive\n\n\n7\npositive\n\n\n8\npositive\n\n\n9\npositive\n\n\n10\npositive\n\n\n11\npositive\n\n\n12\npositive\n\n\n13\npositive\n\n\n14\npositive\n\n\n15\npositive\n\n\n16\npositive\n\n\n17\npositive\n\n\n18\npositive\n\n\n19\npositive\n\n\n20\npositive\n\n\n21\npositive\n\n\n22\npositive\n\n\n23\npositive\n\n\n24\npositive\n\n\n25\npositive\n\n\n26\npositive\n\n\n27\npositive\n\n\n28\npositive\n\n\n29\npositive\n\n\n30\npositive\n\n\n\n\nkable(cardioCTReaderData[1:30,], caption = \"cardioCTReaderData\", align = 'c' )\n\n\ncardioCTReaderData\n\nreaderID\ncaseID\nmodalityID\nscore\n\n\n\nLow1\n141\nGrayscale\n31\n\n\nLow1\n142\nGrayscale\n54\n\n\nLow1\n143\nGrayscale\n88\n\n\nLow1\n144\nGrayscale\n88\n\n\nLow1\n145\nGrayscale\n54\n\n\nLow1\n146\nGrayscale\n32\n\n\nLow1\n147\nGrayscale\n55\n\n\nLow1\n148\nGrayscale\n33\n\n\nLow1\n149\nGrayscale\n66\n\n\nLow1\n150\nGrayscale\n99\n\n\nLow1\n151\nGrayscale\n75\n\n\nLow1\n152\nGrayscale\n54\n\n\nLow1\n153\nGrayscale\n21\n\n\nLow1\n154\nGrayscale\n66\n\n\nLow1\n155\nGrayscale\n88\n\n\nLow1\n156\nGrayscale\n22\n\n\nLow1\n157\nGrayscale\n42\n\n\nLow1\n158\nGrayscale\n76\n\n\nLow1\n159\nGrayscale\n32\n\n\nLow1\n160\nGrayscale\n75\n\n\nLow1\n161\nGrayscale\n65\n\n\nLow1\n162\nGrayscale\n65\n\n\nLow1\n163\nGrayscale\n55\n\n\nLow1\n164\nGrayscale\n32\n\n\nLow1\n165\nGrayscale\n52\n\n\nLow1\n166\nGrayscale\n77\n\n\nLow1\n167\nGrayscale\n20\n\n\nLow1\n168\nGrayscale\n87\n\n\nLow1\n169\nGrayscale\n65\n\n\nLow1\n170\nGrayscale\n42\n\n\n\n\n\ncardioCTGroundTruth에 readerID와 modalityID columns를 추가하고 truth를 positive = 1, negative = 0이 되게끔 변형해준다.\n\ncardioCTGroundTruth$readerID &lt;- factor(\"truth\")\ncardioCTGroundTruth$modalityID &lt;- factor(\"truth\")\ncardioCTGroundTruth$truth &lt;- as.numeric(cardioCTGroundTruth$truth) -1 #truth는 factor형 pos: 2 neg: 1\nnames(cardioCTGroundTruth)[names(cardioCTGroundTruth) == \"truth\"] &lt;- \"score\"\n\nkable(cardioCTGroundTruth[1:30,], caption = \"cardioCTGroundTruth\", align = 'c' )\n\n\ncardioCTGroundTruth\n\ncaseID\nscore\nreaderID\nmodalityID\n\n\n\n1\n1\ntruth\ntruth\n\n\n2\n1\ntruth\ntruth\n\n\n3\n1\ntruth\ntruth\n\n\n4\n1\ntruth\ntruth\n\n\n5\n1\ntruth\ntruth\n\n\n6\n1\ntruth\ntruth\n\n\n7\n1\ntruth\ntruth\n\n\n8\n1\ntruth\ntruth\n\n\n9\n1\ntruth\ntruth\n\n\n10\n1\ntruth\ntruth\n\n\n11\n1\ntruth\ntruth\n\n\n12\n1\ntruth\ntruth\n\n\n13\n1\ntruth\ntruth\n\n\n14\n1\ntruth\ntruth\n\n\n15\n1\ntruth\ntruth\n\n\n16\n1\ntruth\ntruth\n\n\n17\n1\ntruth\ntruth\n\n\n18\n1\ntruth\ntruth\n\n\n19\n1\ntruth\ntruth\n\n\n20\n1\ntruth\ntruth\n\n\n21\n1\ntruth\ntruth\n\n\n22\n1\ntruth\ntruth\n\n\n23\n1\ntruth\ntruth\n\n\n24\n1\ntruth\ntruth\n\n\n25\n1\ntruth\ntruth\n\n\n26\n1\ntruth\ntruth\n\n\n27\n1\ntruth\ntruth\n\n\n28\n1\ntruth\ntruth\n\n\n29\n1\ntruth\ntruth\n\n\n30\n1\ntruth\ntruth\n\n\n\n\n\n이제 cardioCTReaderData와 cardioCTGroundTruth dataset 두 개를 합쳐준다.\n\ncomb_data &lt;- merge.data.frame(cardioCTGroundTruth, cardioCTReaderData, by = names(cardioCTReaderData), all = TRUE, sort = FALSE)\nkable(comb_data[1:30,], caption = \"combined dataset\", align = 'c' )\n\n\ncombined dataset\n\nreaderID\ncaseID\nmodalityID\nscore\n\n\n\ntruth\n1\ntruth\n1\n\n\ntruth\n2\ntruth\n1\n\n\ntruth\n3\ntruth\n1\n\n\ntruth\n4\ntruth\n1\n\n\ntruth\n5\ntruth\n1\n\n\ntruth\n6\ntruth\n1\n\n\ntruth\n7\ntruth\n1\n\n\ntruth\n8\ntruth\n1\n\n\ntruth\n9\ntruth\n1\n\n\ntruth\n10\ntruth\n1\n\n\ntruth\n11\ntruth\n1\n\n\ntruth\n12\ntruth\n1\n\n\ntruth\n13\ntruth\n1\n\n\ntruth\n14\ntruth\n1\n\n\ntruth\n15\ntruth\n1\n\n\ntruth\n16\ntruth\n1\n\n\ntruth\n17\ntruth\n1\n\n\ntruth\n18\ntruth\n1\n\n\ntruth\n19\ntruth\n1\n\n\ntruth\n20\ntruth\n1\n\n\ntruth\n21\ntruth\n1\n\n\ntruth\n22\ntruth\n1\n\n\ntruth\n23\ntruth\n1\n\n\ntruth\n24\ntruth\n1\n\n\ntruth\n25\ntruth\n1\n\n\ntruth\n26\ntruth\n1\n\n\ntruth\n27\ntruth\n1\n\n\ntruth\n28\ntruth\n1\n\n\ntruth\n29\ntruth\n1\n\n\ntruth\n30\ntruth\n1\n\n\n\n\n\n이제 iMRMC 패키지를 불러오고 doIMRMC함수를 이용하여 MRMC analysis를 수행한다. iMRMC에선 Ustat은 non-parametric estimation에 의한 result를 나타내며, MLEstat은 각각 parameter의 maximum likelihood estimation을 나타낸다.\n\nlibrary(iMRMC)\n\nWarning: package 'iMRMC' was built under R version 4.4.2\n\nresult &lt;- doIMRMC(data = comb_data)\n\nAUCDf &lt;- data.frame(rbind(result$MLEstat$AUCA[1:3],result$MLEstat$varAUCA[1:3], sqrt(result$MLEstat$varAUCA[1:3])),\nrow.names = c(\"AUC\", \"variance of AUC\", \"SE of AUC\"))\nnames(AUCDf) &lt;- result$MLEstat$modalityA[1:3]\nkable(AUCDf, caption = \"AUC for different modalities : MLEstat\", align = 'c' )\n\n\nAUC for different modalities : MLEstat\n\n\nGrayscale\nHot\nRainbow\n\n\n\nAUC\n0.5902954\n0.5671724\n0.5176793\n\n\nvariance of AUC\n0.0015535\n0.0008297\n0.0009073\n\n\nSE of AUC\n0.0394148\n0.0288037\n0.0301211\n\n\n\n\n\n\nAUCDf &lt;- data.frame(rbind(result$Ustat$AUCA[1:3],result$Ustat$varAUCA[1:3], sqrt(result$Ustat$varAUCA[1:3])),\nrow.names = c(\"AUC\", \"variance of AUC\", \"SE of AUC\"))\n\nWarning in sqrt(result$Ustat$varAUCA[1:3]): NaNs produced\n\nnames(AUCDf) &lt;- result$Ustat$modalityA[1:3] \nkable(AUCDf, caption = \"AUC for different modalities : Ustat\", align = 'c' )\n\n\nAUC for different modalities : Ustat\n\n\nGrayscale\nHot\nRainbow\n\n\n\nAUC\n0.5902954\n0.5671724\n0.5176793\n\n\nvariance of AUC\n0.0010402\n-0.0000249\n0.0000338\n\n\nSE of AUC\n0.0322528\nNaN\n0.0058162\n\n\n\n\n\ndifferent modalities에 대해 AUC의 difference도 Ustat, MLEstat 두 가지 방법으로 estimation할 수 있다.\n\nAUCDf &lt;- data.frame(rbind(result$MLEstat$AUCAminusAUCB[4:6],result$MLEstat$varAUCAminusAUCB[4:6], sqrt(result$MLEstat$varAUCAminusAUCB[4:6]),\nresult$MLEstat$AUCAminusAUCB[4:6] - 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[4:6]), result$MLEstat$AUCAminusAUCB[4:6] + 1.96 * sqrt(result$MLEstat$varAUCAminusAUCB[4:6])), row.names = c(\"difference of AUC\", \"variance of difference of AUC\", \"SE of different of AUC\", \"95% CI lower bound\", \"95% CI upper bound\"))\n\nnames(AUCDf) &lt;- paste(result$MLEstat$modalityA[4:6], result$MLEstat$modalityB[4:6], sep = \" vs. \") \nkable(AUCDf, caption = \"Difference of AUC among different modalities : MLEstat\", align = 'c' )\n\n\nDifference of AUC among different modalities : MLEstat\n\n\n\n\n\n\n\n\nGrayscale vs. Hot\nGrayscale vs. Rainbow\nHot vs. Rainbow\n\n\n\ndifference of AUC\n0.0231230\n0.0726161\n0.0494932\n\n\nvariance of difference of AUC\n0.0021459\n0.0024036\n0.0019731\n\n\nSE of different of AUC\n0.0463239\n0.0490261\n0.0444191\n\n\n95% CI lower bound\n-0.0676719\n-0.0234750\n-0.0375683\n\n\n95% CI upper bound\n0.1139178\n0.1687072\n0.1365547\n\n\n\n\n\n\nAUCDf &lt;- data.frame(rbind(result$Ustat$AUCAminusAUCB[4:6],result$Ustat$varAUCAminusAUCB[4:6], sqrt(result$Ustat$varAUCAminusAUCB[4:6]),\n                          result$Ustat$AUCAminusAUCB[4:6] - 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[4:6]), \n                          result$Ustat$AUCAminusAUCB[4:6] + 1.96 * sqrt(result$Ustat$varAUCAminusAUCB[4:6])), \n                    row.names = c(\"difference of AUC\", \"variance of difference of AUC\", \"SE of different of AUC\", \"95% CI lower bound\", \"95% CI upper bound\"))\n\nnames(AUCDf) &lt;- paste(result$Ustat$modalityA[4:6], result$Ustat$modalityB[4:6], sep = \" vs. \") \nkable(AUCDf, caption = \"Difference of AUC among different modalities : Ustat\", align = 'c' )\n\n\nDifference of AUC among different modalities : Ustat\n\n\n\n\n\n\n\n\nGrayscale vs. Hot\nGrayscale vs. Rainbow\nHot vs. Rainbow\n\n\n\ndifference of AUC\n0.0231230\n0.0726161\n0.0494932\n\n\nvariance of difference of AUC\n0.0012101\n0.0014347\n0.0010837\n\n\nSE of different of AUC\n0.0347870\n0.0378770\n0.0329203\n\n\n95% CI lower bound\n-0.0450596\n-0.0016227\n-0.0150307\n\n\n95% CI upper bound\n0.0913055\n0.1468550\n0.1140170\n\n\n\n\n\n이제 Roe and Metz model을 사용하여, MRMC analysis simulation을 위한 dataset을 만들어보자. 이때 sim.gRoeMetz.config, sim.gRoemetz 두 함수를 이용하여, simulated dataset을 만들 수 있다.\nsim.gRoeMetz.config 함수의 arguments는 다음과 같으며, 이를 통해 Roe and Metz model의 fixed/random effect를 조절할 수 있다.\nsim.gRoeMetz.config의 argument를 통해 Roe and Metz model을 조절할 수 있다. sim.gRoemetz(config)함수는 sim.gRoeMetz.config를 통해 생성된 configuration object를 이용하여, simulated data를 생성한다.\nconfig에 저장된 simulation parameter는 에 대한 자세한 설명은 iMRMC package확인할 수 있다.\n이제, 이 두 함수를 이용하여 MRMC simulation을 수행한다.\n\nlibrary(ggplot2)\n\nconfig &lt;- sim.gRoeMetz.config() # Create a sample configuration file\ndf.MRMC &lt;- sim.gRoeMetz(config) # Simulate an MRMC data set\n\n이 두 함수를 통해 simulated dataset을 visualization 시켜보았다.\n\nggplot(subset(df.MRMC, modalityID %in% c(\"testA\", \"testB\")),\n       aes(x = score,\n           color = factor(unlist(lapply(as.character(caseID),\n                                        function(x) {strsplit(x, 'Case')[[1]][1]}))))) +\n  geom_density(position = \"identity\", alpha = 0.2) +\n  facet_grid(rows = \"modalityID\") +\n  labs(x = \"MRMC Reading Score\", y = \"Density\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\nMRMC analysis를 수행해본 결과는 다음과 같다.\n\nresult &lt;- doIMRMC(df.MRMC)\n\nAUCDf &lt;- rbind(result$MLEstat$AUCA[1:2],\n      result$MLEstat$varAUCA[1:2],\n      sqrt(result$MLEstat$varAUCA[1:2])) %&gt;% as.data.frame()\nrownames(AUCDf) &lt;- c(\"AUC\", \"variance of AUC\", \"SE of AUC\")\nnames(AUCDf) &lt;- paste(\"test\", c(\"A\",\"B\"))\nAUCDf$`test A vs test B` &lt;- c(result$MLEstat$AUCAminusAUCB[3], result$MLEstat$varAUCAminusAUCB[3], sqrt(result$MLEstat$varAUCAminusAUCB[3]))\n\nkable(AUCDf, caption = \"MRMC analysis result with simulated data : MLEstat\", align = 'c')\n\n\nMRMC analysis result with simulated data : MLEstat\n\n\ntest A\ntest B\ntest A vs test B\n\n\n\nAUC\n0.8411250\n0.8797500\n-0.0386250\n\n\nvariance of AUC\n0.0016160\n0.0011408\n0.0024655\n\n\nSE of AUC\n0.0401991\n0.0337760\n0.0496538\n\n\n\n\n\nReference\n\n박서영, & 김화영. (2023). 바이오통계학. 한국방송통신대학교출판문화원.\nBischl et al. Chapter 04.13: AUC & Mann-Whitney-U Test Introduction to Machine Learning\n\nWen et al. R Data Packages of Multi-Reader Multi-Case Studies and Simulation Tools to Support the Development of Reader Performance Evaluation Methods Wen\n\nClarkson et al. (2006). A Probabilistic Model for the MRMC Method. Part 1. Theoretical Development. Acad Radiol.\ncolorScaleStudyData"
  },
  {
    "objectID": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#footnotes",
    "href": "posts/2024-12-27-Multi-Reader Multi-Case Analysis/index.html#footnotes",
    "title": "Multi-Reader Multi-Case Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nClarkson et al. (2006). A Probabilistic Model for the MRMC Method. Part 1. Theoretical Development. Acad Radiol.↩︎"
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Randomized controlled trial(RCT)를 시행할 때, control/treatment group의 baseline characteristics이 비슷하게 맞춰주기 위해 subject를 random하게 배정한다. Observational study에서도 RCT와 유사한 causality를 확보하기 위한 방법 중 propensity score analysis가 있다. propensity score가 비슷한 subject끼리는 baseline characteristics도 비슷하다는 특성이 있다. 이 때, 두 군 baseline covariates의 유사성을 어떻게 평가할 수 있을까? 이 때 Standardized Mean Difference(SMD)를 사용하면 유사성을 평가할 수 있다.\nSMD는 두 군의 mean차이를 measure하는 Effect size의 일종이다. 보통 의학 논문 연구에서는시 귀무/대립 가설 및 p-value를 표시하여 결과를 보고한다. 통계적 유의성을 검정하는 방식으로 진행되는데, 통계적으로 유의하다 할 지라도 effect가 얼마나 큰 지에 대해서는 말할 수가 없다는 한계가 있다. 이를 해결하기 위해 도입된 개념이 Effect size이다.\nEffect size는 세 가지 특징을 살펴보자. 첫째, Effect size는 연구 결과의 해석을 이분법이 아닌 연속 선상에서 할 수 있게 해준다. 기존에 사용되던 p-value는 귀무가설 기각 여부밖에 알려주지 못하지만, effect size는 실제로 얼마나 큰 차이가 있는 지를 구체적으로 보여준다. 둘째, effect size는 p-value와는 달리 표본 수에 의한 영향을 받지 않는다. p-value를 이용한 검정의 경우 표본 수가 커질 때, power가 증가한다. 그 말은 통계적으로 유의하지 않음에도 불구하고, 단지 표본 수가 많다는 이유 만으로 유의하다는 결과를 얻을 수 있다는 뜻이다. 이에 비해 effect size는 표본 수의 영향을 받지 않는다. 셋째, 효과크기는 다양한 형태의 결과들을 비교 가능한 공통의 단위로 변화 시켜줌으로써 다른 통계적 방법에 의해 시행된 연구 결과들을 비교할 수 있게 해준다. 이 특성은 메타분석 시 자주 이용된다.\n이제 SMD의 수식에 대해 살펴보자. 참고로 통상적으로 SMD는 0.2 - 0.5일 경우 small, 0.5 - 0.8일 경우 medium, 0.8을 넘을 때 large라고 한다.\n\nSMD는 크게 SMD for continuous or categorical baseline variables 두 가지가 있고, formula는 다음과 같다.\n\n\nContinuous baseline variable\n\\[\nd = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}\n\\]\n\n\n\\(\\overline{X}_1\\), \\(\\overline{X}_2\\)은 각 group의 baseline variable의 sample mean, \\(s_1, s_2\\)은 각 group의 sample variance를 의미한다. variable이 치우쳐 있다면(skewed), d를 rank statistics을 이용하여 modify하여 사용하면 된다.\n\n\nCategorical baseline variable\n1) Binary categorical variable\n\\[\nd = \\frac{\\hat{P}_1 - \\hat{P}_2}{\\sqrt{\\frac{\\hat{P}_1(1 - \\hat{P}_1) + \\hat{P}_2(1 - \\hat{P}_2)}{2}}}\n\\]\n\\(\\hat{P_1}, \\hat{P_2}\\)는 control/treatment group의 binary baseline variable의 proportion(mean)을 의미한다.\n2) Categorical variable with K levels\n\nTable 1처럼 baseline variable이 2개 초과의 level을 가지는 categorical variable의 SMD에 대해 살펴보자.\n\n\n\\[ T = (\\hat{P}_{12}, \\hat{P}_{13}, ..., \\hat{P}_{1K})'\\] \\[ C = (\\hat{P}_{22}, \\hat{P}_{23}, ..., \\hat{P}_{2K})'\\]\n이 때, \\(\\hat{P}_{JK} =\\) Pr(category k|treatment group j), $ j $ and \\(k \\in \\{2, 3, ..., K\\}\\)\nStandardized difference는 다음과 같이 정의된다.\n\\[\nd = \\sqrt{(T - C)'S^{-1}(T-C)}\n\\]\nS는 (k - 1)X(k - 1) covariance이며, 다음과 같다.\n\\[\nS = [S_{kl}] = \\begin{cases}\\dfrac{[\\hat{P}_{1k}(1-\\hat{P}_{1k})+\\hat{P}_{2k}(1-\\hat{P}_{2k})]}{2}, & k = l \\\\\\dfrac{[\\hat{P}_{1k}\\hat{P}_{1l}+\\hat{P}_{2k}\\hat{P}_{2l}]}{2}, & k \\neq l\\end{cases}\n\\]\n지금까지 SMD를 구하는 과정을 살펴보았다. RCT시 subject의 group 배정 후 혹은 propensity score analysis시 baseline covariates의 balance를 확인할 때, SMD를 이용한다. SMD가 0.1미만이면 작은 차이로 간주하고, 대부분의 변수에서 SMD 0.1미만이면서, 모두 0.2 미만이면 balance가 잘 맞는 것으로 평가한다.\n\nHedges and Olkin (1985)에서 SMD에 대한 confidence interval 공식을 내놓았는데 식은 다음과 같다.\n\\[\nd\\;\\pm 1.96\\times\\sigma[d]\n\\]\n\\[\n\\sigma[d] = \\sqrt{\\frac{n_1 + n_2}{n_1 \\times n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\\]\n\n예제 데이터와 tableone package를 이용해 SMD를 구하는 과정을 살펴보자. tableone package에 대한 자세한 사용법이 궁금한다면 이 포스트를 참고하길 바란다.\n데이터를 불러오는 과정이다.\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 8.3.0 with Schannel\n\nlibrary(magrittr)\nlibrary(tableone)\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url,header=T)\n\nhead(dt)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n4:         2009  183321     200908           NA            NA           NA\n5:         2009  942671     200909           NA            NA           NA\n6:         2009  979358     200912           NA            NA           NA\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N HGHT\n1:           0            0           NA        3        1              0  144\n2:           0            0           NA        2        1              0  162\n3:           0            0           NA        3        1              0  163\n4:          NA           NA           NA        3        1              0  152\n5:          NA           NA           NA        3        1              0  159\n6:          NA           NA           NA        2        1              0  157\n   WGHT WSTC  BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT  HGB FBS TOT_CHOL  TG HDL\n1:   61   90 29.4   0.7   0.8    120     80        1 12.6 117      264 128  60\n2:   51   63 19.4   0.8   1.0    120     80        1 13.8  96      169  92  70\n3:   65   82 24.5   0.7   0.6    130     80        1 15.0 118      216 132  55\n4:   51   70 22.1   0.8   0.9    101     62        1 13.1  90      199 100  65\n5:   50   73 19.8   0.7   0.8    132     78        1 13.0  92      162  58  40\n6:   55   73 22.3   1.5   1.5    110     70        1 11.9 100      192 109  53\n   LDL CRTN SGOT SGPT GGT GFR\n1: 179  0.9   25   20  25  59\n2:  80  0.9   18   15  28  74\n3: 134  0.8   26   30  30  79\n4: 114  0.9   18   14  11  61\n5: 111  0.9   24   23  15  49\n6: 117  0.7   15   12  14  83\n\n\ntableone object를 생성한 뒤 print function을 이용해 결과물을 출력할 수 있다. 이때 smd = TRUE로 처리해주면 smd값이 같이 출력된다.\n\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n                       \n                        Overall        \n  n                       1644         \n  HGHT (mean (SD))      164.55 (9.19)  \n  WGHT (mean (SD))       65.10 (12.53) \n  BMI (mean (SD))        23.92 (3.38)  \n  HDL (mean (SD))        55.90 (19.47) \n  LDL (mean (SD))       118.69 (201.99)\n  TG (mean (SD))        134.90 (104.75)\n  SGPT (mean (SD))       25.98 (27.18) \n  Q_PHX_DX_STK = 1 (%)      12 ( 1.1)  \n  Q_PHX_DX_HTDZ = 1 (%)     26 ( 2.4)  \n  Q_HBV_AG (%)                         \n     1                      77 ( 4.7)  \n     2                    1102 (67.1)  \n     3                     463 (28.2)  \n  Q_SMK_YN (%)                         \n     1                     995 (60.6)  \n     2                     256 (15.6)  \n     3                     391 (23.8)  \n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nprint(t2, smd = T)\n\n                       Stratified by Q_SMK_YN\n                        1               2               3               p     \n  n                        995             256             391                \n  HGHT (mean (SD))      160.67 (8.34)   168.83 (6.45)   171.61 (7.09)   &lt;0.001\n  WGHT (mean (SD))       61.17 (11.08)   70.09 (10.72)   71.76 (13.07)  &lt;0.001\n  BMI (mean (SD))        23.63 (3.39)    24.52 (2.93)    24.27 (3.54)   &lt;0.001\n  HDL (mean (SD))        57.83 (14.08)   53.91 (36.73)   52.37 (13.54)  &lt;0.001\n  LDL (mean (SD))       112.26 (32.81)  147.52 (505.27) 116.34 (56.89)   0.046\n  TG (mean (SD))        114.05 (76.97)  162.89 (126.51) 169.24 (133.28) &lt;0.001\n  SGPT (mean (SD))       23.33 (28.42)   28.61 (20.62)   31.00 (26.96)  &lt;0.001\n  Q_PHX_DX_STK = 1 (%)      11 (  1.8)       1 (  0.5)       0 (  0.0)   0.051\n  Q_PHX_DX_HTDZ = 1 (%)     18 (  2.9)       5 (  2.6)       3 (  1.1)   0.287\n  Q_HBV_AG (%)                                                           0.193\n     1                      40 (  4.0)      19 (  7.5)      17 (  4.3)        \n     2                     679 ( 68.3)     164 ( 64.3)     259 ( 66.2)        \n     3                     275 ( 27.7)      72 ( 28.2)     115 ( 29.4)        \n  Q_SMK_YN (%)                                                          &lt;0.001\n     1                     995 (100.0)       0 (  0.0)       0 (  0.0)        \n     2                       0 (  0.0)     256 (100.0)       0 (  0.0)        \n     3                       0 (  0.0)       0 (  0.0)     391 (100.0)        \n                       Stratified by Q_SMK_YN\n                        test SMD   \n  n                                \n  HGHT (mean (SD))            0.972\n  WGHT (mean (SD))            0.611\n  BMI (mean (SD))             0.181\n  HDL (mean (SD))             0.197\n  LDL (mean (SD))             0.091\n  TG (mean (SD))              0.341\n  SGPT (mean (SD))            0.196\n  Q_PHX_DX_STK = 1 (%)        0.137\n  Q_PHX_DX_HTDZ = 1 (%)       0.084\n  Q_HBV_AG (%)                0.109\n     1                             \n     2                             \n     3                             \n  Q_SMK_YN (%)                  NaN\n     1                             \n     2                             \n     3                             \n\n\nQ_HBV_AG, Q_SMK_YN등의 3개의 level을 가지는 categorical variable도 하나의 SMD를 출력해줌을 알 수 있다. 참고로 tableone package에서는 strata가 3개 이상일 때도 하나의 SMD를 출력해준다. 이는 n개의 strata에 대해 pairwise로 \\(\\binom{n}{2}\\)개의의 SMD를 구한 뒤, 절댓값을 취하고, 평균을 낸 값이다.\n\n\nYang, D. and Dalton, JE. (2012). A unified approach to measuring the effect size between two groups using SAS. SAS Global Forum 2012, Paper 335-2012.\nHedges LV, Olkin I. (1985). Statistical Methods for Meta-Analysis. Academic Press: San Diego, CA .\n남상건.(2015). 효과크기의 이해. Hanyang Medical Reviews. Paper 40-43\n이유진(2022). tableone 패키지 소개. 차라투 블로그"
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#background",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#background",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Randomized controlled trial(RCT)를 시행할 때, control/treatment group의 baseline characteristics이 비슷하게 맞춰주기 위해 subject를 random하게 배정한다. Observational study에서도 RCT와 유사한 causality를 확보하기 위한 방법 중 propensity score analysis가 있다. propensity score가 비슷한 subject끼리는 baseline characteristics도 비슷하다는 특성이 있다. 이 때, 두 군 baseline covariates의 유사성을 어떻게 평가할 수 있을까? 이 때 Standardized Mean Difference(SMD)를 사용하면 유사성을 평가할 수 있다.\nSMD는 두 군의 mean차이를 measure하는 Effect size의 일종이다. 보통 의학 논문 연구에서는시 귀무/대립 가설 및 p-value를 표시하여 결과를 보고한다. 통계적 유의성을 검정하는 방식으로 진행되는데, 통계적으로 유의하다 할 지라도 effect가 얼마나 큰 지에 대해서는 말할 수가 없다는 한계가 있다. 이를 해결하기 위해 도입된 개념이 Effect size이다.\nEffect size는 세 가지 특징을 살펴보자. 첫째, Effect size는 연구 결과의 해석을 이분법이 아닌 연속 선상에서 할 수 있게 해준다. 기존에 사용되던 p-value는 귀무가설 기각 여부밖에 알려주지 못하지만, effect size는 실제로 얼마나 큰 차이가 있는 지를 구체적으로 보여준다. 둘째, effect size는 p-value와는 달리 표본 수에 의한 영향을 받지 않는다. p-value를 이용한 검정의 경우 표본 수가 커질 때, power가 증가한다. 그 말은 통계적으로 유의하지 않음에도 불구하고, 단지 표본 수가 많다는 이유 만으로 유의하다는 결과를 얻을 수 있다는 뜻이다. 이에 비해 effect size는 표본 수의 영향을 받지 않는다. 셋째, 효과크기는 다양한 형태의 결과들을 비교 가능한 공통의 단위로 변화 시켜줌으로써 다른 통계적 방법에 의해 시행된 연구 결과들을 비교할 수 있게 해준다. 이 특성은 메타분석 시 자주 이용된다.\n이제 SMD의 수식에 대해 살펴보자. 참고로 통상적으로 SMD는 0.2 - 0.5일 경우 small, 0.5 - 0.8일 경우 medium, 0.8을 넘을 때 large라고 한다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#standardized-mean-difference-1",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#standardized-mean-difference-1",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "SMD는 크게 SMD for continuous or categorical baseline variables 두 가지가 있고, formula는 다음과 같다.\n\n\nContinuous baseline variable\n\\[\nd = \\frac{\\overline{X}_1 - \\overline{X}_2}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}\n\\]\n\n\n\\(\\overline{X}_1\\), \\(\\overline{X}_2\\)은 각 group의 baseline variable의 sample mean, \\(s_1, s_2\\)은 각 group의 sample variance를 의미한다. variable이 치우쳐 있다면(skewed), d를 rank statistics을 이용하여 modify하여 사용하면 된다.\n\n\nCategorical baseline variable\n1) Binary categorical variable\n\\[\nd = \\frac{\\hat{P}_1 - \\hat{P}_2}{\\sqrt{\\frac{\\hat{P}_1(1 - \\hat{P}_1) + \\hat{P}_2(1 - \\hat{P}_2)}{2}}}\n\\]\n\\(\\hat{P_1}, \\hat{P_2}\\)는 control/treatment group의 binary baseline variable의 proportion(mean)을 의미한다.\n2) Categorical variable with K levels\n\nTable 1처럼 baseline variable이 2개 초과의 level을 가지는 categorical variable의 SMD에 대해 살펴보자.\n\n\n\\[ T = (\\hat{P}_{12}, \\hat{P}_{13}, ..., \\hat{P}_{1K})'\\] \\[ C = (\\hat{P}_{22}, \\hat{P}_{23}, ..., \\hat{P}_{2K})'\\]\n이 때, \\(\\hat{P}_{JK} =\\) Pr(category k|treatment group j), $ j $ and \\(k \\in \\{2, 3, ..., K\\}\\)\nStandardized difference는 다음과 같이 정의된다.\n\\[\nd = \\sqrt{(T - C)'S^{-1}(T-C)}\n\\]\nS는 (k - 1)X(k - 1) covariance이며, 다음과 같다.\n\\[\nS = [S_{kl}] = \\begin{cases}\\dfrac{[\\hat{P}_{1k}(1-\\hat{P}_{1k})+\\hat{P}_{2k}(1-\\hat{P}_{2k})]}{2}, & k = l \\\\\\dfrac{[\\hat{P}_{1k}\\hat{P}_{1l}+\\hat{P}_{2k}\\hat{P}_{2l}]}{2}, & k \\neq l\\end{cases}\n\\]\n지금까지 SMD를 구하는 과정을 살펴보았다. RCT시 subject의 group 배정 후 혹은 propensity score analysis시 baseline covariates의 balance를 확인할 때, SMD를 이용한다. SMD가 0.1미만이면 작은 차이로 간주하고, 대부분의 변수에서 SMD 0.1미만이면서, 모두 0.2 미만이면 balance가 잘 맞는 것으로 평가한다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#confidence-interval-for-standardized-difference",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#confidence-interval-for-standardized-difference",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Hedges and Olkin (1985)에서 SMD에 대한 confidence interval 공식을 내놓았는데 식은 다음과 같다.\n\\[\nd\\;\\pm 1.96\\times\\sigma[d]\n\\]\n\\[\n\\sigma[d] = \\sqrt{\\frac{n_1 + n_2}{n_1 \\times n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\\]"
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#example",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#example",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "예제 데이터와 tableone package를 이용해 SMD를 구하는 과정을 살펴보자. tableone package에 대한 자세한 사용법이 궁금한다면 이 포스트를 참고하길 바란다.\n데이터를 불러오는 과정이다.\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 8.3.0 with Schannel\n\nlibrary(magrittr)\nlibrary(tableone)\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url,header=T)\n\nhead(dt)\n\n   EXMD_BZ_YYYY RN_INDI HME_YYYYMM Q_PHX_DX_STK Q_PHX_DX_HTDZ Q_PHX_DX_HTN\n1:         2009  562083     200909            0             0            1\n2:         2009  334536     200911            0             0            0\n3:         2009  911867     200903            0             0            0\n4:         2009  183321     200908           NA            NA           NA\n5:         2009  942671     200909           NA            NA           NA\n6:         2009  979358     200912           NA            NA           NA\n   Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG Q_SMK_YN Q_DRK_FRQ_V09N HGHT\n1:           0            0           NA        3        1              0  144\n2:           0            0           NA        2        1              0  162\n3:           0            0           NA        3        1              0  163\n4:          NA           NA           NA        3        1              0  152\n5:          NA           NA           NA        3        1              0  159\n6:          NA           NA           NA        2        1              0  157\n   WGHT WSTC  BMI VA_LT VA_RT BP_SYS BP_DIA URN_PROT  HGB FBS TOT_CHOL  TG HDL\n1:   61   90 29.4   0.7   0.8    120     80        1 12.6 117      264 128  60\n2:   51   63 19.4   0.8   1.0    120     80        1 13.8  96      169  92  70\n3:   65   82 24.5   0.7   0.6    130     80        1 15.0 118      216 132  55\n4:   51   70 22.1   0.8   0.9    101     62        1 13.1  90      199 100  65\n5:   50   73 19.8   0.7   0.8    132     78        1 13.0  92      162  58  40\n6:   55   73 22.3   1.5   1.5    110     70        1 11.9 100      192 109  53\n   LDL CRTN SGOT SGPT GGT GFR\n1: 179  0.9   25   20  25  59\n2:  80  0.9   18   15  28  74\n3: 134  0.8   26   30  30  79\n4: 114  0.9   18   14  11  61\n5: 111  0.9   24   23  15  49\n6: 117  0.7   15   12  14  83\n\n\ntableone object를 생성한 뒤 print function을 이용해 결과물을 출력할 수 있다. 이때 smd = TRUE로 처리해주면 smd값이 같이 출력된다.\n\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n                       \n                        Overall        \n  n                       1644         \n  HGHT (mean (SD))      164.55 (9.19)  \n  WGHT (mean (SD))       65.10 (12.53) \n  BMI (mean (SD))        23.92 (3.38)  \n  HDL (mean (SD))        55.90 (19.47) \n  LDL (mean (SD))       118.69 (201.99)\n  TG (mean (SD))        134.90 (104.75)\n  SGPT (mean (SD))       25.98 (27.18) \n  Q_PHX_DX_STK = 1 (%)      12 ( 1.1)  \n  Q_PHX_DX_HTDZ = 1 (%)     26 ( 2.4)  \n  Q_HBV_AG (%)                         \n     1                      77 ( 4.7)  \n     2                    1102 (67.1)  \n     3                     463 (28.2)  \n  Q_SMK_YN (%)                         \n     1                     995 (60.6)  \n     2                     256 (15.6)  \n     3                     391 (23.8)  \n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nprint(t2, smd = T)\n\n                       Stratified by Q_SMK_YN\n                        1               2               3               p     \n  n                        995             256             391                \n  HGHT (mean (SD))      160.67 (8.34)   168.83 (6.45)   171.61 (7.09)   &lt;0.001\n  WGHT (mean (SD))       61.17 (11.08)   70.09 (10.72)   71.76 (13.07)  &lt;0.001\n  BMI (mean (SD))        23.63 (3.39)    24.52 (2.93)    24.27 (3.54)   &lt;0.001\n  HDL (mean (SD))        57.83 (14.08)   53.91 (36.73)   52.37 (13.54)  &lt;0.001\n  LDL (mean (SD))       112.26 (32.81)  147.52 (505.27) 116.34 (56.89)   0.046\n  TG (mean (SD))        114.05 (76.97)  162.89 (126.51) 169.24 (133.28) &lt;0.001\n  SGPT (mean (SD))       23.33 (28.42)   28.61 (20.62)   31.00 (26.96)  &lt;0.001\n  Q_PHX_DX_STK = 1 (%)      11 (  1.8)       1 (  0.5)       0 (  0.0)   0.051\n  Q_PHX_DX_HTDZ = 1 (%)     18 (  2.9)       5 (  2.6)       3 (  1.1)   0.287\n  Q_HBV_AG (%)                                                           0.193\n     1                      40 (  4.0)      19 (  7.5)      17 (  4.3)        \n     2                     679 ( 68.3)     164 ( 64.3)     259 ( 66.2)        \n     3                     275 ( 27.7)      72 ( 28.2)     115 ( 29.4)        \n  Q_SMK_YN (%)                                                          &lt;0.001\n     1                     995 (100.0)       0 (  0.0)       0 (  0.0)        \n     2                       0 (  0.0)     256 (100.0)       0 (  0.0)        \n     3                       0 (  0.0)       0 (  0.0)     391 (100.0)        \n                       Stratified by Q_SMK_YN\n                        test SMD   \n  n                                \n  HGHT (mean (SD))            0.972\n  WGHT (mean (SD))            0.611\n  BMI (mean (SD))             0.181\n  HDL (mean (SD))             0.197\n  LDL (mean (SD))             0.091\n  TG (mean (SD))              0.341\n  SGPT (mean (SD))            0.196\n  Q_PHX_DX_STK = 1 (%)        0.137\n  Q_PHX_DX_HTDZ = 1 (%)       0.084\n  Q_HBV_AG (%)                0.109\n     1                             \n     2                             \n     3                             \n  Q_SMK_YN (%)                  NaN\n     1                             \n     2                             \n     3                             \n\n\nQ_HBV_AG, Q_SMK_YN등의 3개의 level을 가지는 categorical variable도 하나의 SMD를 출력해줌을 알 수 있다. 참고로 tableone package에서는 strata가 3개 이상일 때도 하나의 SMD를 출력해준다. 이는 n개의 strata에 대해 pairwise로 \\(\\binom{n}{2}\\)개의의 SMD를 구한 뒤, 절댓값을 취하고, 평균을 낸 값이다."
  },
  {
    "objectID": "posts/2025-01-23-Standardized Mean Difference/index.html#reference",
    "href": "posts/2025-01-23-Standardized Mean Difference/index.html#reference",
    "title": "Standardized Mean Difference",
    "section": "",
    "text": "Yang, D. and Dalton, JE. (2012). A unified approach to measuring the effect size between two groups using SAS. SAS Global Forum 2012, Paper 335-2012.\nHedges LV, Olkin I. (1985). Statistical Methods for Meta-Analysis. Academic Press: San Diego, CA .\n남상건.(2015). 효과크기의 이해. Hanyang Medical Reviews. Paper 40-43\n이유진(2022). tableone 패키지 소개. 차라투 블로그"
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html",
    "href": "posts/2025-02-28-reg1/index.html",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "",
    "text": "차라투 블로그 “Exploring Regression Models for Regression Analysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한 여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는 Regression Analysis의 개념, (simple, multiple or general) linear regression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의 특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로 추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는 방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent standard errors (heteroskedasticity-robust standard errors)와 Wild Bootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의 clustered data 버전인 Cluster-robust standard error를 다룹니다. 3장에서는 GLM에서 여전히 남아있는 observations의 independent 조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를 고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust (sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다. {4장은 계획 중에 있습니다.}\n결국 “Exploring Regression Models for Regression Analysis”는 Regression Analysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며, 이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들 중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게 수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를 공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로 제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련 분석이 필요하시다면 해당 연구를 차라투로 문의해주시길 바랍니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#들어가며",
    "href": "posts/2025-02-28-reg1/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "",
    "text": "차라투 블로그 “Exploring Regression Models for Regression Analysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한 여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는 Regression Analysis의 개념, (simple, multiple or general) linear regression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의 특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로 추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는 방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent standard errors (heteroskedasticity-robust standard errors)와 Wild Bootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear regression에서 link function을 도입하여 regression의 개념을 outcome of single yes/no, outcome of single K-way, count 등 non-normal한 종속변수로 확장한 Generalized linear model의 개념을 Exponential Family, Link Function와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의 clustered data 버전인 Cluster-robust standard error를 다룹니다. 3장에서는 GLM에서 여전히 남아있는 observations의 independent 조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를 고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust (sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다. {4장은 계획 중에 있습니다.}\n결국 “Exploring Regression Models for Regression Analysis”는 Regression Analysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며, 이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들 중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게 수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를 공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로 제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련 분석이 필요하시다면 해당 연구를 차라투로 문의해주시길 바랍니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#regression-analysis",
    "href": "posts/2025-02-28-reg1/index.html#regression-analysis",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "1. Regression Analysis",
    "text": "1. Regression Analysis\nRegression Analysis는 통계학에서 가장 널리 사용되는 방법론 중 하나로, 어떤 종속변수(dependent variable)와 하나 이상의 독립변수(independent variables) 간의 관계를 추정하고 해석하는 분석 기법입니다. (종속변수 또한 벡터, 즉 여러 개일 수 있습니다.)\n의학 분야에서는 예후를 예측하는 모델을 만들거나(예: 특정 약물 투여 후 혈압, outcome 등의 변화를 예측), 특정 위험인자(risk factor)가 결과에 유의미한 영향을 미치는지(예: 어떤 치료법이 환자의 생존율에 유의미한 차이를 주는지) 등을 살펴보기 위해 Regression Analysis가 필수적으로 활용됩니다. 이러한 Regression Analysis을 완벽하게 수행하기 위해서는 본인의 data가 어떤 특성을 갖고 있는지 파악하고, 이에 대해 어떠한 Regression Model을 고려해야 하는지 알고, 이 모델이 어떻게 data에 적합(fit)하는지에 대한 대략적인 수학 또는 알고리즘의 원리와, 적합된 모델이 통계적으로 유의미한지 검정(test)하고 예측 성능이나 해석력을 평가하는 방법까지 전 과정을 이해해야 합니다. 본 1장에서는 가장 간단한, 대신 많은 가정이 필요한 Regression Model인 (General) Linear Regression에 대해서 위 과정과 함께 살펴볼 것이며, 이후의 장들에서는 data의 가정이 조금씩 깨질 때 (data의 특성이 변할 때) 어떤 Regression Model을 고려해야 하는지와 이 Model들의 parameters는 어떻게 수학적으로 추정하는지 살펴볼 것입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#linear-regression-lm",
    "href": "posts/2025-02-28-reg1/index.html#linear-regression-lm",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "2. Linear Regression (LM)",
    "text": "2. Linear Regression (LM)\n2.1. (Simple, Multiple) Linear Regression 정의\n\n(General) Linear Regression은 의학뿐만 아니라 다양한 분야에서 가장 basic한 Regression Model입니다. Linear Regression은 독립변수 \\(X\\) 와 종속변수 \\(Y\\) 간의 선형(linear) 관계를 가정하고, 이를 통해 종속변수를 설명하거나 예측합니다. 독립변수가 1개일 때는 Simple Linear Regression, 2개 이상일 때는 Multiple Linear Regression라고 부릅니다. 의학 연구에서 예를 들면, 혈압(종속변수)을 나이, 체중, 성별(독립변수) 등으로 설명하거나 예측하는 과정을 생각할 수 있습니다. 이러한 Linear regression은 아래 네 가지 가정을 기반으로 추정하며, 이 가정들에 대해서 잘 이해하는 것은 매우 중요하고, 이들 중 특정 가정들이 위배될 경우 이후에 다룰 Regression Model들을 고려해야 함을 명심해주시면 좋을 것 같습니다.\n\n\n선형성(Linearity) 가정\n독립변수와 종속변수가 선형 관계에 있다고 가정합니다. 산점도(scatter plot)를 통해 대략적인 선형 관계 여부를 확인할 수 있습니다.\n\n\n오차항의 정규성(Normality) 가정\n주어진 독립변수의 값에서 종속변수의 확률분포는 정규분포를 따른다고 가정합니다. 이는 오차항 ε이 정규분포를 따른다고도 표현할 수 있습니다.(종속변수가 정규분포를 따른다는 뜻은 Linear Model이 종속변수의 mean을 예측하므로, 이 둘의 차인 오차항은 평균이 0이고 분산은 종속변수와 같은 정규분포를 따르게 되기 때문입니다.) 이는 정규 P-P plot 혹은 Q-Q plot 등을 통해 가정 위배 여부를 대략적으로 확인할 수 있습니다.\n\n\n오차항의 독립성(Independence) 가정\n각 관측치(Observations, Data(set)) 또는 잔차(residual)가 서로 독립이라고 가정합니다. 즉, data간의 상관관계가 없다고 가정하는 것이고, 잔차산점도(residual plot), Durbin-Watson 통계량 등을 통해 자기상관(autocorrelation)이 있는지 살펴볼 수 있습니다.\n\n\n오차항의 등분산성(Homoscedasticity) 가정\n모든 독립변수의 값에서 종속변수의 분산이 동일하다고 가정합니다. 잔차산점도 등을 통해 잔차가 일정한 분산을 가지는지 대략적으로 확인할 수 있습니다.\n\n\n이후 Linear Regression Model은 다음과 같은 과정으로 분석을 수행하게 됩니다; (1) 최소제곱법(Least Squares)를 통해 model parameters를 추정(estimate), (2) 결정계수(\\(R^2\\))를 통해 모델이 종속변수를 얼마나 잘 설명하는지 확인, (3) F-검정(F-test)으로 전체 회귀식의 유의성을 검정, (4) 검정(t-test)으로 각 회귀계수(regression coefficient)가 유의미한지 확인. 특히, 독립변수 각각의 Model coefficient(parameter)를 검정 함으로써 종속변수와의 상관성을 분석하는 (4) 과정은 유의성을 판단하는 가장 중요한 검정 과정으로, 고전적으로 Wald Test, Likelihood Ratio Test, Score Test가 자주 사용됩니다.\n2.2. Linear Regression 수학적 표현 및 추정\n\nLinear Regression은 다음과 같은 형태를 가집니다:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_k X_{ik} + \\varepsilon_i\n\\]\n혹은 Matrix 형태로 간단히 쓰면,\n\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\n\\[\nwhere, \\quad \\mathbf{y} \\in \\mathbb{R}^n, \\mathbf{X}\n\\in \\mathbb{R}^{n \\times p}, \\boldsymbol{\\beta} \\in \\mathbb{R}^p,\n\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\n\\]\n\n\\(\\mathbf{y} \\in \\mathbb{R}^n\\): 관측된 종속변수들의 벡터\n\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\): 설계 행렬, 각 행은 하나의 관측치(Observation)이며 (\\(n\\)), 각 열은 하나의 독립변수에 (\\(p\\)) 해당\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\): 추정하고자 하는 회귀 계수(or 모수 or parameters) 벡터 (여기선 intercept \\(\\beta_0\\) 제외)\n\\(\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\\): 랜덤 오차 항 벡터, 위 네 가정에 따라 랜덤 오차항은 평균이 \\(E(\\boldsymbol{\\varepsilon}) = 0\\)이고, 분산이 모든 관측치에서 같으며 독립인 정규분포\n\n입니다. Multiple Linear Regression의 모델 \\(\\boldsymbol{\\beta}\\)의 분산은 scalar variance가 아니라 (co)variance matrix 형태가 될 것입니다. (simple도 intercept를 고려하면 matrix) 앞으로 세 장에 걸쳐 이를 분산 행렬이라고 부르겠지만, 정확히는 분산-공분산 행렬이라고도 부릅니다. 또한, 위 matrix form 수식처럼 intercept를 제외하는 경우도 많은데, 항상 있다고 가정하며 가독성을 위해 제외한 것이니 혼동할 필요 없이 자연스럽게 읽으시면 됩니다. Covariance matrix에서는 회귀계수 서로 간의 공분산이 들어있으므로, 각 회귀계수(독립변수의 유의성)에 대한 검정은 Covariance Matrix에 있는 각 diagonal 원소들을 사용하여 수행하게 됩니다. (회귀계수 각각에서 스스로의 대한 공분산 = 회귀계수의 분산) 예를 들어 \\(\\beta_2\\)의 분산은 \\(p\\)가 5 (독립변수가 4개, intercept 포함)일 때 Covariance Matrix의 3행 3열 값이 될 것입니다. (intercept가 없다면 2행 2열) 여기서 확인할 수 있듯 Regression Model의 parameter 추정 역시 중요하지만, parameter의 Covariance Matrix를 추정하는 것 또한 유의성 판단에서 매우 중요하며, 이후의 장들 또한 자연스럽게 Model의 소개, paramater 추정법, parameter의 (Co)variance Matrix를 추정법으로 이루어지게 될 것입니다.\n또한, Linear Model 식은 선형대수학에서 Hyper Plane으로 정의하는 형태가 되는데, 쉽게 설명하자면 독립변수가 하나일 경우 종속변수와의 2차원 평면에서 직선(2차원의 Hyper Plane)을 띄고, 독립변수가 두 개일 경우 종속변수와의 3차원 공간에서 평면을 띄는(3차원의 Hyper Plane), 선형적으로 공간을 두 부분으로 가르는 공간이라고 생각하시면 될 것 같습니다. (마찬가지로 쉽게 생각하면, 비선형적일 경우 직선이 아니라 곡선, 평면이 아니라 곡면일 것입니다.)\n이제 어떻게 Linear Model을 regression, 즉 fit(parameter를 추정)할 것인지 설명하겠습니다. 가장 기본적으로 알려진 Model의 통계학적 parameter 추정법으로는 Ordinary Least Squares(OLS), Maximum Likelihood Estimation(MLE), Method of Moment(MOM)가 있습니다. 이후의 모델들은 대부분 MLE로 모델을 추정하지만, Linear Model은 특별하게도 MLE와 OLS의 추정 결과가 같고, OLS의 추정 결과는 BLUE(Best Linear Unbiased Estimator) 입니다. 최소제곱법(OLS, Ordinary Least Squares) 추정량(estimator)은 오차 제곱합(SSR, Sum of Squared Residuals), 즉 모든 데이터에서 실제 종속변수 값과 모델의 예측 값의 차이를 제곱한 수들의 합을 최소화하는 \\(\\boldsymbol{\\beta}\\)를 찾는 것이며, 다음의 closed-form solution을 갖습니다: \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]\n그리고, 고전적 가정(특히 등분산성, 독립성, 정상성)이 모두 충족된다고 할 때, 이 \\(\\hat{\\boldsymbol{\\beta}}\\)의 추정오차의 공분산행렬(covariance matrix)은 \\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}\n\\] 이며, 실제 계산할 때는 데이터에서 추정한 \\(\\hat{\\sigma}^2\\)를 통해 \\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}, \\quad where \\; \\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}, \\; e_i = y_i - x_i \\hat{\\boldsymbol{\\beta}}\n\\]\n로 추정합니다. (분모에서 K를 뺀 이유는 degree of freedom에 의한 것이며, 에러 term이 정확히는 차원에 맞춰 \\(x_i^\\top \\hat{\\boldsymbol{\\beta}}\\) 지만, 앞으로 이정도 표기는 가독성을 위해 넘어가겠습니다.) 이렇게 구한 모델 추정량과 추정오차의 공분산행렬을 통해 각 회귀계수에 대한 검정을 수행하거나 신뢰구간(CI)을 계산할 수 있게 됩니다.\n다음으로 넘어가기 전에, Linear Regression에서 (1) \\(\\hat{\\beta}\\)이 closed-form solution \\((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\)을 갖게 되는 과정과, (2) 위에서 언급하였듯 model의 parameter (\\(\\hat{\\beta}\\)) OLS 추정량이 MLE(Maximum likelihood estimation) 추정량과 같음을 증명하겠습니다.\n(1) Prove \\(\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\\).\n우선, OLS 추정량은 오차 제곱합(SSR)을 최소화하는 식이므로 오차 SSR을 표현하는 식 \\(J(\\beta)\\)를 적으면 다음과 같습니다.\n\\[\nJ(\\beta) = \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) = \\frac{1}{2} \\sum_{i=1}^{n} \\big( x_i \\beta - y^{(i)} \\big)^2\n\\]\n이제 위 \\(J(\\beta)\\)를 \\(\\beta\\)에 대해 미분하였을 때 0이 나오는 \\(\\hat{\\beta}\\)이 SSR이 최소가 되는 OLS 추정량입니다.(이에 대한 증명은 안하겠지만, 간단하게 볼록한 2차 함수에서는 미분값이 0인 점이 최소점인 것과 같은 원리라고 생각하시면 되겠습니다.) 식을 풀어보면,\n\\[ \\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big) \\] 입니다. \\(\\nabla\\)는 미분하는 변수가 scalar가 아니라 vector이기 때문에 사용되는 기호이며(gradient), 그냥 \\(\\beta\\)로 미분한다고만 생각하시면 됩니다. 이후 미분하는 과정 또한 크게 다르지 않습니다. 식을 계속 진행해보면, \\(y^\\top y\\)는 식에 \\(\\beta\\)가 없고 \\((X\\beta)^\\top y\\)와 \\(y^\\top (X\\beta)\\) 둘다 단순히 두 벡터의 내적인 똑같은 식이므로 \\[ = \\frac{1}{2} \\nabla_\\beta \\big( \\beta^\\top (X^\\top X) \\beta - 2 (X^\\top y)^\\top \\beta \\big) \\]이고, 이제 \\(\\beta\\)로 미분하면, 우항은 그냥 미분, 좌항은 두 번 나오므로 곱미분을 수행하여\\[ = \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big) \\]\\[ = X^\\top X \\beta - X^\\top y = 0 \\]\n입니다. 결국 OLS 추정량 \\(\\hat{\\beta}\\)는\n\\[ X^\\top X \\hat{\\beta} = X^\\top y \\] 이고, 양변 앞에 역행렬을 곱해주면\\[ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y \\]가 됩니다.\n(2) Prove OLS estimator is same as MLE estimator in Linear Model (Regression).\n\n위 Linear Regression Model 식을 다음과 같이 작성할 수 있습니다. \\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n식이 어색할 수 있지만 정확히 이전의 수학적 표현과 동일하며, \\(\\boldsymbol{\\varepsilon}\\) term이 복잡해 보이는 이유는, 이전의 네 가지 조건들을 수식으로 반영하여 모델을 확률 기반으로 해석하였기 때문입니다. 식을 설명드리자면, 선형성(Linearity) 가정에 의해 error의 mean이 (기댓값이) 0이 되고, 오차항의 정규성 가정에 의해 정규 분포를 따르며, 오차항의 독립성 및 등분산성 가정에 의해 분산(Covariance Matrix)이 값이 모두 \\(\\sigma^2\\)인 대각행렬, 즉 다른 observations간의 correlation은 0이고(없고), 각 observations의 분산은 \\(\\sigma^2\\)로 일정함을 표현한 것입니다.\n최대가능도방법 또는 최대우도법(Maximum Likelihood Method)은 어떤 모수와 표집한 값들이 주어졌을 때, 표집한 값들이 나올 가능도(확률)을 최대로 만드는 모수를 선택하는 아주 general한 모델의 점 추정 방법이며, 처음 들어보셨다면 대표님께서 더욱 자세하게 설명하신 블로그 글을 읽으시면 좋을 것 같습니다. 철학적으로는 종속변수의 확률 분포를 데이터에 맞게 가정한 후, 모든 observations(data) 각각이 나올 확률(분포)을 모두 곱한 식이 최대가 되도록 하는 모수값이 바로 MLE를 통해 추정한 parameter라고 생각하시면 됩니다. 간단한 예시를 들어보자면, 동전을 던져 앞면이 7번, 뒷면이 3번 나온 데이터에서 동전이 앞면이 나올 확률을 모수로 두고 종속변수의 분포를 베르누이 분포로 가정하면, 각각의 동전을 던져 얻은 관측치들 10개의 data의 Likelihood(확률)를 모두 곱하면 모수가 하나였으니 변수가 하나인 하나의 식(함수)가 나오고, 이 식(함수)을 최대화 하는 모수를 찾는 method가 MLE라고 간단하게 정리할 수 있을 것 같습니다. (이때 추정된 결과는 0.7일 것이고, 모수로 식을 미분한 함수(score function)이 0인 값을 찾음으로써 모수를 추정하게 되며, MLE 이외에도 MAP, 베이지안 등 다른 추정 기법들도 있지만 여기에서는 MLE 정도를 이해하면 충분할 것 같습니다.)\n이제 돌아와서 LM에서는 OLS estimator와 MLE estimator가 상응함을 보이겠습니다. 이후의 식 전개는 dim이 1일 때로 증명하겠습니다. Multi-dimension에서는 분포식과 term이 더 복잡하지만 meaning은 1차원과 정확히 동일하기 때문입니다. 오차 항 \\(\\boldsymbol{\\varepsilon}\\)는 정규분포를 따르므로, n개의 관측치에 대한 Likelihood는 \\[\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n\\]\n이며, 로그를 씌운 log Likelihood는 \\[\n\\ell(\\boldsymbol{\\beta}) = \\log \\mathcal{L}(\\boldsymbol{\\beta})\n\\]\n입니다. 식이 복잡해 보이지만 데이터 n개에 대하여 정규(가우시안)분포를 따르는 종속변수가 모델의 예측값을 가질 확률을 단순히 모두 곱한 식입니다. 결국 얻어진 표본 data 자체가 나올 확률에 대한 식을 세운 후, 이를 최대화 하는 parameter를 찾는 과정이 MLE estimation이라고 직관적으로 해석할 수 있습니다.\n돌아와서, log를 씌운 이유를 생각해보겠습니다. log의 그래프를 생각해보시면 log함수는 직관적으로도 monotonically increase하기 때문에 likelihood를 최대화 하는 parameter와 log likelihood를 최대화 하는 parameter는 같으며, log는 곱셈을 덧셈으로, 지수 term을 아래로 만들어주어 likelihood 식이 매우 간단해지기 때문에 (아래에서 실제로 그런지 확인해보세요.) MLE에서는 항상 log likelihood를 최대화하는 parameter를 찾는 방향으로 parameter를 추정합니다. 이어서 전개하면,\\[\n= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n\\]\\[\n= \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2}{2 \\sigma^2}\n\\]\\[\n= n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2\n\\]\n입니다. Log likelihood를 최대화 하는 것은 negative Log likelihood를 최소화 하는 것과 같으므로 양변에 -를 곱하면\\[\n- \\ell(\\boldsymbol{\\beta}) = -n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n이고, 상수 \\(-n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\)는 \\(\\boldsymbol{\\beta}\\)에 영향을 주지 않으므로 제거하면, \\[\n- \\log \\mathcal{L}(\\boldsymbol{\\beta}) \\approx \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n가 됩니다. 아까 보았듯 OLS estimator는 SSR을 최소화하는 아래 문제로 정의되므로, \\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n\\]\n결국 위 두 식, OLS estimator와 MLE estimator의 목적식이 같음을 알 수 있고(종속변수와 모델 예측의 차이의 제곱을 모든 data point에서 더한 값을 최소화 하는 beta), 따라서 추정량 또한 같습니다. \\[\n\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}\n\\]\n위 증명은 MLE로 LM의 parameter를 추정한 것이 아니라 식 전개를 통해 OLS와 목적식이 같음을 보인 것이고, 실제 MLE로 parameter를 추정하는 과정은 2장부터 자세히 보게 될 것입니다. 한 가지 첨언하자면, Linear model에서는 이 두 추정량이 같지만, 정규성 가정이 깨진 경우 두 추정량은 같지 않을 수 있습니다. 이유를 생각해보면, 정규분포는 확률이 가장 큰, 즉 mode가 mean이며 mean을 기준으로 양 옆이 symmetric한 분포이기 때문에 두 추정량이 같으며, 정규분포가 아닐 경우 mean을 추정하려고 하는 MLE는 절대적인 거리를 좁히려는 OLS와 다른 값을 추정하게 된다고 직관적으로 생각해볼 수 있습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#linear-regression-in-homo-vs.-heteroskedasticity",
    "href": "posts/2025-02-28-reg1/index.html#linear-regression-in-homo-vs.-heteroskedasticity",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "3. Linear Regression in Homo vs. Heteroskedasticity",
    "text": "3. Linear Regression in Homo vs. Heteroskedasticity\n앞서 언급한 Linear Regression 모형은 “오차항의 등분산성(homoscedasticity)”을 가정합니다. 그러나 실제 연구 상황에서는 이 가정이 위배되는 경우가 흔합니다. Heteroskedasticity는 이러한 오차항은 독립 변수에 따라 같지 않을 수 있다는 뜻으로, 등분산성 가정이 깨진 경우를 의미합니다. 즉,\n\n\nHomoscedasticity: 모든 관측값에 대해 오차항(또는 종속변수)의 분산이 동일\n\nHeteroskedasticity: 오차항(또는 종속변수)의 분산이 관측값에 따라 다름\n\n입니다. 즉 이전 수식 중 오차 항을 \\(\\boldsymbol{\\varepsilon}\\)로 두었다면, observations의 분산은 더이상 \\(\\sigma^2\\)로 일정하지 않고 observations마다 다른 값을 갖을 수 있다는 뜻입니다. 예를 들어, 임상 연구에서 나이(age)에 따라 혈압(blood pressure)의 분산이 달라질 수 있습니다. 이런 상황에선 등분산성을 가정하는 것이 부적절해집니다. 등분산성 가정이 깨진 상황에서 Linear regression을 수행하면, \\(\\hat{\\boldsymbol{\\beta}}\\) 자체가 편향되진 않더라도(즉, 일관성(consistency)은 여전히 유지), \\(\\hat{\\boldsymbol{\\beta}}\\) 의 분산 추정치 \\(\\hat{\\sigma}^2 ( \\mathbf{X}^\\top \\mathbf{X} )^{-1}\\) 가 bias를 갖게 되어 잘못된 표준오차, 잘못된 유의성 검정 결과로 이어질 수 있습니다. 따라서, Heteroskedasticity가 의심되는 상황에서는 모델의 분산을 안정적으로 추정해야 하며, 이때 1번으로 고려되는 방법 중 하나가 Heteroskedasticity-consistent(heteroskedasticity-robust) standard errors, 줄여서 HC standard errors를 통한 (Co)variance Matrix estimation입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#heteroskedasticity-consistent-standard-errors-hc-standard-errors",
    "href": "posts/2025-02-28-reg1/index.html#heteroskedasticity-consistent-standard-errors-hc-standard-errors",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)",
    "text": "4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)\n4.1. HC Standard Errors 정의\n\nHC(Heteroskedasticity-Consistent) standard errors는, 고전적 선형회귀 가정 중 등분산성만 깨졌을 때(나머지 선형성, 정규성, 독립성 가정 in LM 은 그대로 유지) 회귀계수 추정치의 분산을 “강건(robust)”하게 추정하기 위한 method입니다. 이는 heteroskedasticity-robust standard errors 또는 HCCME(Heteroskedasticity-Consistent Covariance Matrix Estimation) 라고도 부르며, 이전에로 구한 \\(\\hat{\\beta}\\)를 그대로 사용하되, 그 공분산행렬 추정만 새롭게(robust하게) 구하는 방식입니다. 다만, 독립성(오차항들이 서로 독립), 선형성, 정규성 등의 다른 가정이 또 깨져 있다면 HC SE만으로는 대응할 수 없음을 명심하시길 바랍니다. 예를 들어, 오차항이 서로 상관되어 있는 clustered data에서는 더 이상 추정량이 consist하지 않기 때문에, 이보다 한 단계 더 나아간 cluster-robust standard errors, 혹은 GEE, GLMM 등의 모델을 고려해야 합니다. 즉, “등분산성 가정만 깨졌을 때” 쓰는 표준오차(or covariance matrix) 추정량이라고 생각하면 됩니다. 또한, 모델이 실제로 크게 잘못 설정되어 있다면(선형성조차 안 맞는 경우, 모델의 estimator가 크게 편향, HC se가 모델에서 구한 se와 크게 차이가 나는 경우 등), 그때는 variance(standard error)만 “robust standard errors”로 바꾼다고 해서 문제가 해결되지 않고, 모델을 수정하거나 data를 다시 확인하고 분석을 다른 방식으로 수행해야 합니다.\n즉 Greene의 말처럼,\n\n“Simply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.”\n\n이라는 점을 늘 유의해야 합니다. 정리하자면, 모델을 잘 구성하고, 모델의 분산과 큰 차이가 없을 경우에는 모델의 parameter의 분산 효율성이 최대가 아니거나 MLE 추정을 통해 얻은 모델의 분산이 더 이상 불편향이 아닐 때도 heteroskedasticity-robust standard errors를 통해서 모델의 분산을 robust하게 estimate할 수 있으며, 분석 결과 편향이 너무 크지 않다면 이후의 검정에서 이 HC se를 사용함으로써 이후의 통계 분석에서 설득력을 높일 수 있습니다. 추가로 스포하자면, 아래에서 다룰 Heteroskedasticity-robust Standard Errors 식은 Linear Model에서 robust하게 분산을 추정하는 Robust(or Sandwich) Estimation이며, 이후의 모델에서도 해당 모델에 대한 Sandwich Estimator를 고려함으로써 안정적인 분산 추정 방법에 대해 다룰 것입니다.\n4.2. HC(CME)0 수학적 표현\n\n이제 위에서 설명한 Heteroskedasticity한 data에서 robust하게 standard errors, 혹은 Covariance Matrix를 estimate 하는 heteroskedasticity-consistent standard errors의 수식을 유도하겠습니다.\n이전에 선형 회귀 모델을 다음과 같이 정의한 적이 있습니다.\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\] 이때, 위에서는 등분산성을 가정하였기 때문에 \\(\\boldsymbol{\\varepsilon}\\)를 원소값이 모두 \\(\\sigma^2\\) 인 대각행렬로 가정하여 각 observations의 분산은 \\(\\sigma^2\\)로 일정함을 가정했다면, 이번에 정의한 \\(\\boldsymbol{\\varepsilon}\\)은 \\(E(\\boldsymbol{\\varepsilon}) = 0\\), \\(E(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^\\top) = \\Phi\\) 입니다. 이때 \\(\\Phi\\)는 \\(\\operatorname{diag}(\\varepsilon_i^2)\\)로 정의되는 대각행렬이며(대각 성분을 제외하면 모두 0인 행렬), 식을 통해서 여전히 error term의 mean이 0이고 observations들은 independent하지만(대각행렬이므로), 더 이상 observations의 분산이 서로 일치하지 않음을 표현한 것입니다. 즉, 이제 Homoscedasticity에서 Heteroskedasticity을 고려하기 시작했다는 것을 수식을 해석함으로써 알 수 있습니다.\nVariance estimatation\n위 식의 OLS 추정량은 여전히 \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\]\n입니다. (still consistent) 이제 모델의 variance 식을 전개해보면, \\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\operatorname{Var} \\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\right )\\]\n이며, \\((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top\\)는 determinant한 값으로, variance는 식 내의 determinant한 salar는 Var 밖으로 제곱과 함께 나오는 것처럼 (\\(\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X)\\)), 행렬 또는 벡터에 대해도 비슷하게 \\[\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} ) \\left ((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\right ) ^ \\top \\\\\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} )  \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\] 가 됩니다. (벡터와 행렬을 다룰 때에는 위처럼 제곱한다고 생각하시면 될 것 같습니다.) 참고로, \\[\\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\right )^ \\top = \\left ( (\\mathbf{X}^\\top \\mathbf{X})^\\top \\right )^ {-1} =  (\\mathbf{X}^\\top \\mathbf{X})^ {-1}\\] 입니다.\n따라서, 처음 설정한대로 \\(\\operatorname{Var} ( \\mathbf{y} ) = \\Phi\\)를 넣으면, 최종적으로 추정된 consistent model \\(\\hat{\\boldsymbol{\\beta}}\\)의 (co)variance matrix \\(\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}})\\)는 다음과 같습니다. \\[\n(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n하나 확인할 수 있는 사실은, 등분산성 가정(\\(\\Phi = \\sigma^2 \\mathbf{I}\\)) 하에선 위 식이 아래처럼 단순화되어 이전 (co)variance matrix가 나왔던, 가정하의 특수한 결과라는 것입니다. \\[ \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X} ^ \\top \\mathbf{X})^ {-1} \\mathbf{X} ^ \\top \\Phi \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\sigma ^ 2 \\mathbf{I} \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\] 또한, 이 때의 (OLS 추정량)의 분산 \\(\\sigma^2\\)의 식도 그냥 보여드리고 넘어갔었는데, 이는 대각 성분이 모두 같기 때문에 단순히 잔차 제곱합 \\(\\sum e_i^2\\)을 자유도(\\(N-K\\))로 나눈 값(\\(s^2\\))으로 추정하여 \\(\\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}\\) 로 한번에 표현한 식이었다는 것도 확인할 수 있습니다.\nHC0’s Robustness\n이분산성이 존재할 때 \\(\\Phi\\)는 대각행렬이지만 대각원소가 서로 다릅니다. White는 1980년에 이 heteroskedasticity에 robust하게 분산을 추정하기 위해 위 식에서 \\(\\Phi = \\sigma^2 \\mathbf{I}\\)로 term이 소거되게 하는 대신, \\(\\Phi_{ii}\\)를 잔차 제곱 \\(e_i^2\\)로 추정하는 HC0을 제안했습니다: \\[\n\\hat{\\Phi}_{ii} = e_i^2 \\quad \\Rightarrow \\quad \\hat{\\Phi} = \\operatorname{diag}(e_i^2)\n\\]\n이를 위 (co)variance 식에 대입하면 HC0 분산 추정량이 얻어집니다: \\[\n\\operatorname{HC0} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n이는 variance 식으로부터 시작해서 얻은 식이고, 데이터로부터 얻은 matrix \\(\\hat{\\Phi} = \\operatorname{diag}(e_i^2)\\)를 사용하여 이분산성의 구체적 형태를 가정하지 않았기 때문에 안정적으로 heteroskedasticity를 고려합니다. 이처럼 앞 뒤가 같은 모양 사이에 데이터로부터 얻은 분산 추정 식이 들어간 형태가 샌드위치 같다고 해서 Sandwich Estimator라고도 부르며, 여전히 가운데 matrix 항이 diagonal 함을 통해 오차(또는 관측치)간의 상관관계는 없다는 것도 기억해야 합니다.\n4.3. HC(CME) 1~3 수학적 표현\n\n이 HC Standard Error가 나온 이후, 위 식으로부터 구한 분산은 소표본에서는 모델의 overfitting(과적합, 딥러닝과 머신러닝에서도 자주 대두되는 문제로 데이터가 적어 학습한 데이터에서는 오차가 지만 보지 못한 데이터에 대해서는 오차가 크게 나오는 상황, 통계 분석은 test data를 따로 두지 않기 때문에 overfitting 대처가 더욱 중요.)에 의해 residual이 작게 나오고, 이에 따라 모델의 covariance가 과소평가될 가능성이 크다는 문제가 제시되었습니다. 이에, 소표본에서도 안정적으로 추정하기 위해 White(1980)의 HC Standard Errors를 HC0로 두고, 여러 버전의 HC Standard Errors가 최근까지 다양한 학자들에 의해 연구되고 있습니다. 보통 HC0~HC5까지를 HC Standard Errors로 부르며, HC 오른쪽 숫자가 버전을 뜻하여 이 수가 커질수록 최근에 나온 것이고, 각각은 simulation 실험을 통해 소표본에서 더욱 강건함을 보이는 식으로 논문이 나옵니다. 중요한 점은, 첫 번째로 이 HC Standard Errors들은 소표본에서는 값이 다르지만 표본의 크기가 무수히 커질수록 asymptotically하게 같으며 소표본에서의 고려를 위해 값을 더 크게 만드는 추가 term을 넣어준다는 점(물론 수식적으로 필요성을 증명하지만), 둘 째로 R의 sandwich 패키지에서는 4까지 지원하고, 3을 넘는 HC 시리즈가 쓰이는 경우는 거의 볼 수 없기 때문에 3까지의 식 정도만 다루어도 충분하다는 점, 마지막으로 위 직관적인 이유로나 수식적으로나 식을 해석해 보면 거의 모든 표본 cases에 대해 버전이 클수록 분산을 더욱 크게 추정한다는 점입니다. 따라서 유의성을 보이기 위해서는 버전이 작은게 유리할 것입니다.^^ 아래에서는 HC1부터 3까지의 수식을 살펴보겠습니다. 각각의 철학에 대한 자세한 증명 과정 또한 다룰까 했지만 중요하지 않기 때문에 간단히 소개한 후 로컬에서 돌려 보실 R 예시 코드를 보여드릴 것입니다.\nHC1: 소표본 편향 보정\nHC1은 소표본에서 자유도 조정 인자 \\(\\frac{n}{n-k}\\)를 도입하여 편향을 줄인다는 철학에서 출발하여, 잔차 \\(e_i^2\\)의 기대값이 \\(\\sigma_i^2(1-h_{ii})\\)이므로, \\(E(e_i^2) \\approx \\sigma_i^2\\)이 되도록 HC0에 \\(\\frac{N}{N-K}\\)을 나누어서(1보다 크며, n이 커짐에 따라 1로 수렴하는 term 추가) 스케일링합니다. 즉, 식은 다음과 같습니다: \\[\n\\operatorname{HC1} = \\frac{N}{N-K} \\cdot \\operatorname{HC0} \\\\\n= \\frac{N}{N-K} \\cdot (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nHC2: Leverage 보정\n위 HC1에서 \\(E(e_i^2) = \\sigma_i^2(1-h_{ii})\\) 라고 이야기 하였습니다. 일단 그렇다고 하면, 기존에는 \\(\\sigma_i^2\\)를 \\(e_i^2\\)으로 추정하였다면, 사실 \\(\\frac{e_i^2}{1-h_{ii}}\\)의 기대값이 \\(\\sigma_i^2\\)이기 때문에 HC1처럼 앞에 scalar term을 추가하는 대신, HC2는 이를 직접적으로 반영하여 \\(\\hat{\\Phi}\\)를 \\(\\operatorname{diag}(e_i^2)\\) 대신 \\(\\operatorname{diag}( \\frac{e_i^2}{1-h_{ii}})\\)로 추정합니다. 이 때 \\(h_{ii}\\)은 Leverage(래버리지), \\(H(or \\;h)\\)는 Hat Matrix라고 불리고, 식은 \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\)이며, 위 \\(h_{ii}\\)은 이 matrix의 대각원소를 뜻합니다. Leverage에 대한 이야기 또한 길지만 간단하게 설명하면, \\(\\mathbf{H}\\)가 Hat Matrix라고 부르는 이유는 이 \\(\\mathbf{H}\\) term에 \\(\\mathbf{y}\\)를 곱하면, \\[\n\\mathbf{Hy} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y} \\\\\n= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n=\\hat{\\mathbf{y}}\n\\] 이 되어 \\(\\mathbf{y}\\)에 hat을(추정치) 씌운다는 의미에서 비롯되었으며, 이는 기하학적으로 더욱 복잡하게 설명될 수 있지만 간단한 의미는 각 관측치가 얼마나 모델의 estimation에 영향을 주었는지를 보여주는 matrix입니다. 이 matrix의 diagonal값인 레버리지를 보며 관측치 하나하나의 영향력을 볼 수 있고, 이 값이 큰 관측치에 대해서는 아래 식에서처럼 분모를 작게 함으로써 분산을 크게 추정한다고 직관적으로 생각할 수 있습니다. \\[\n\\operatorname{HC2} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{1-h_{ii}} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\n넘어가기 전에, 증명 없이 사용해왔던 수식 \\(E(e_i^2) = \\sigma_i^2(1-h_{ii})\\) 를 간단하게만 증명해보겠습니다. 예측 오차는 실제값 \\(y_i\\)와 예측값 \\(\\hat{y}_i\\)의 차이입니다: \\[\ne_i = y_i - \\hat{y}_i.\n\\] 잔차 벡터 \\(\\mathbf{e}\\)는 다음과 같이 표현됩니다: \\[\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{y}.\n\\]\n에러 제곱의 기댓값은 잔차의 분산을 의미합니다. 잔차 벡터 \\(\\mathbf{e}\\)의 공분산 행렬은 다음과 같이 계산됩니다: \\[\n\\text{Cov}(\\mathbf{e}) = \\text{Cov}((\\mathbf{I} - \\mathbf{H}) \\mathbf{y}) = (\\mathbf{I} - \\mathbf{H}) \\text{Cov}(\\mathbf{y}) (\\mathbf{I} - \\mathbf{H})^T.\n\\] \\(\\text{Cov}(\\mathbf{y}) = \\sigma_i^2 \\mathbf{I}\\)이므로 (당연히 \\(\\sigma\\)는 관측치 \\(i\\)마다 다를 수 있습니다.): \\[\n\\text{Cov}(\\mathbf{e}) = (\\mathbf{I} - \\mathbf{H}) \\sigma_i^2 \\mathbf{I} (\\mathbf{I} - \\mathbf{H})^T = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n\\] 여기서 \\(\\mathbf{H}\\)는 대칭 행렬(\\(\\mathbf{H} = \\mathbf{H}^T\\))이고 멱등 행렬(\\(\\mathbf{H}^2 = \\mathbf{H}\\))이므로 (이 부분은 기하학적인 설명이 많이 필요해서 증명하지 않겠다만, 이들은 단지 특정 행렬들의 성질이며 이를 만족한다고 하고 넘기겠습니다.): \\[\n\\text{Cov}(\\mathbf{e}) = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n\\]\n\\(i\\)번째 잔차 \\(e_i\\)의 분산은 공분산 행렬의 \\(i\\)번째 대각 성분이고, \\[\n\\text{Var}(e_i) = [\\text{Cov}(\\mathbf{e})]_{ii} = \\sigma_i^2 (1 - h_{ii}).\n\\] 에러 제곱의 기댓값은 잔차의 분산과 동일하므로 (에러의 mean이 0이므로), \\[\nE[e_i^2] = \\text{Var}(e_i) = \\sigma_i^2 (1 - h_{ii}).\n\\] 가 증명됩니다.\nHC3: 강화된 Leverage 보정\nHC3은 Jackknife 접근법에서 영감을 받아 HC2에서 \\(\\Phi\\)의 분모 term을 더 강하게 \\((1-h_{ii})^2\\)로 둡니다. 직관적으로, 같은 \\(h_{ii}\\)에 대해 더 크게 분산을 추정 (\\((1-h_{ii})^2 &lt; 1-h_{ii}\\))하여, 고레버리지, 즉 outlier에 대해 더 안정적으로 소표본의 분산을 추정할 수 있습니다. \\[\n\\operatorname{HC3} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{(1-h_{ii})^2} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\]\nHC(CME)s 비교\n위 내용을 간단하게 표로 정리하면 다음과 같습니다.\n\n\n\n\n\n\n\n\n유형\n수식\n보정 요소\n사용 시나리오\n\n\n\nHC0\n\\(\\operatorname{diag}(e_i^2)\\)\n없음\n대표본\n\n\nHC1\n\\(\\frac{n}{n-k}\\operatorname{HC0}\\)\n자유도\n소표본 기본\n\n\nHC2\n\\(\\frac{e_i^2}{1-h_{ii}}\\)\n레버리지 1차\n고레버리지 존재 시\n\n\nHC3\n\\(\\frac{e_i^2}{(1-h_{ii})^2}\\)\n레버리지 2차\n소표본 + 고레버리지\n\n\n\n정리하자면, 동분산성 검정을 검정하여 이분산성이 확인되면 HC se 시리즈의 적용을 고려해야 하고, 표본 크기가 \\(n &lt; 50\\)인 경우는 HC3 또는 아래에서 다룰 bootstrap기반의 (Clustered) Bootstrap Covariance Matrix Estimation를, \\(n &gt; 500\\)인 경우는 HC0/HC1로 충분하다고 간단하게 생각해볼 수 있습니다. (물론 이는 정해진 규칙이 아니고, 시작은 항상 HC0부터 검정해보는 것이 맞습니다.)"
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#wild-bootstrap",
    "href": "posts/2025-02-28-reg1/index.html#wild-bootstrap",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "5. (Wild) Bootstrap",
    "text": "5. (Wild) Bootstrap\n마지막으로, sandwich estimator는 아니지만, 모든 모델에서 data로부터 안정적으로 분산을 추정하여 의학 연구에서 인정하는 기법 중 하나인 Bootstraping을 통한 분산 term 추정 기법을 간단하게 소개드리겠습니다. 이 method는 특정 가정으로부터 수학적인 추정식을 세워 분산을 추정하는 대신, Bootstraping 통계 기법을 사용하여 data 만으로 경험적인 분산을 추정합니다. 때문에 이번에 소개하는 (Wild) Bootstrap 은 어떤 가정이나 모델이든 사용할 수 있는, 심지어 2장에서 다룰 clustered(grouped) data 에서도 사용할 수 있는 방법입니다.(사용 대상인 모델이 정해지지 않은, 데이터 만으로 구하는 분산이기 때문에 가정이 전혀 들어가지 않아서입니다. 예를 들어 LM에서 HC se를 사용하는 경우 나머지 세 가정이 만족해야 했지만, Bootstrap은 그렇지 않습니다.) 이후에 다룰 샌드위치 추정량(Sandwich Estimator)과, 그의 OLS 버전인 HC(Heteroskedasticity-Consistent) 시리즈가 개발되었지만, 이들은 결국 대표본에서의 점근적 성질로부터 얻은 수식이고, 극단적인 쇼포본 같은 작은 클러스터 수에서는 이러한 데이터 만을 고려하여 추정하는 부트스트랩이 더 강력한 대안이 될 수도 있습니다. 이를 소개하기 전에 간단히 중요한 통계학적 resampling 기법인 잭나이프(Jackknife)와 부트스트랩(Bootstrap)을 얘기해보겠습니다. 이 둘은 모두 표본을 계속해서 얻기 힘든 상황 (현실에서는 대부분 그럴 것입니다.)에서 모수를 추정하기 위해, 이미 갖고 있는 표본의 data로부터 resampling을 하여 추정하는 통계학적 방법론입니다.\n잭나이프(Jackknife)\nJackknife method는 한 번에 하나의 클러스터를 제외하고 모델을 추정 하는 것을 모든 클러스터에 대해 반복하여 분산을 계산합니다:\n\\[\n\\mathrm{Var}(\\hat{\\beta}_c) = \\frac{n-1}{n} \\sum_{c=1}^C \\left(\\hat{\\beta}_c - \\bar{\\hat{\\beta}}\\right)^2,\n\\]\n여기서 \\(\\hat{\\beta}_c\\)는 c번째 클러스터를 제외한 추정치, \\(\\bar{\\hat{\\beta}}\\)는 추정치의 평균이고, 이는 resampling 기법이지만 위에서 설명드렸던 HC3 추정량이 이 식으로부터 도출된 결과입니다. 물론, 이는 이론적인 결과이므로 각각이 서로 다른 결과를 도출할 수도 있을 것입니다. 단, resampling 기법들은 그 특성상 클러스터나 그 data point의 수가 많을 경우 계산 부하가 큽니다.\n부트스트랩(Bootstrap)\nBootstrap method는 데이터를 무작위로 resampling하여 여러 버전의 데이터셋을 생성하고, 각각에서 모델을 추정한 후 추정값의 변동성을 분산으로 사용합니다. 대표적인 예시로 와일드 부트스트랩 (Wild Bootstrap)은 잔차(residual)에 랜덤 가중치를 곱해 인위적 으로 데이터의 분산을 크게 보정하고, 케이스 기반 (XY/Pairs 부트스트랩)은 기본적인 방법으로 클러스터 자체를 복원 추출하여 새 data를 생성한 뒤 구하는 과정을 반복하며, 잔차 기반 (Residual 부트스트랩)은 잔차 만을 리샘플링하고 재추정하는 방법입니다다.\nWild Bootstrap\n위에서 설명드렸듯, 잔차 \\(e_i\\)에 “랜덤 가중치” \\(w_i\\)를 곱해 새로운 반응 변수를 생성하는 방법으로 다음과 같을 것입니다:\n\\[\ny^* = \\hat{y} + w_i \\hat{e}.\n\\]\n이때 가중치 분포 로 사용되는 대표적인 예시는 Rademacher: \\(w_i \\in \\{-1, 1\\}\\), Mammen: \\(w_i = \\frac{\\sqrt{5} \\pm 1}{2}\\), Webb: 6점 분포(\\(w_i \\in \\left\\{ -\\sqrt{\\frac{3}{2}}, -\\sqrt{\\frac{2}{2}}, -\\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{2}{2}}, \\sqrt{\\frac{3}{2}} \\right\\}\\))로 클러스터별 정규분포 등 다른 적용도 가능합니다. 위에서 언급하였듯, 이외에도 케이스 기반 (XY) 부트스트랩, 잔차 부트스트랩, 프랙셔널 부트스트랩(Fractional Bootstrap)등의 방법론이 있지만, data를 크게 왜곡하지 않는 Wild가 이러한 분산 추정에서 디폴트하게 고려됩니다."
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#r-예시-hc03-및-부트스트랩",
    "href": "posts/2025-02-28-reg1/index.html#r-예시-hc03-및-부트스트랩",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "6. R 예시: HC0~3 및 부트스트랩",
    "text": "6. R 예시: HC0~3 및 부트스트랩\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. 위 내용 중 “시리즈가 클 수록 강건하게 추정한다”는 점을 기억하시고 해석하면 됩니다.\n\n## mtcars 데이터에서 mpg(연비)를 wt(차량 무게), hp(마력)으로 설명하는 회귀모형\n#data(mtcars)\n#model &lt;- lm(mpg ~ wt + hp, data = mtcars)\n#\n## 기본 OLS 표준오차 (등분산 가정)\n#summary(model)  \n#\n## HC 표준오차 계산\n#library(sandwich)\n#library(lmtest)\n#\n## HC 유형별 분산-공분산 행렬\n#cov_hc0 &lt;- vcovHC(model, type = \"HC0\")  # 기본 White 추정량\n#cov_hc1 &lt;- vcovHC(model, type = \"HC1\")  # 자유도 보정\n#cov_hc2 &lt;- vcovHC(model, type = \"HC2\")  # 잔차 스케일링\n#cov_hc3 &lt;- vcovHC(model, type = \"HC3\")  # 작은 표본 보정\n#\n## 계수 테스트 결과 비교\n#coeftest(model, vcov = cov_hc0)  \n#coeftest(model, vcov = cov_hc1)  \n#\n## 부트스트랩 분산-공분산 행렬 (기본 100회 복제)\n#cov_wild &lt;- vcovBS(fit, cluster = ~cluster_id, type = \"wild\", R = 1000)\n#cov_xy &lt;- vcovBS(model, R = 200, type = \"xy\")  # xy-쌍 부트스트랩\n#\n## 결과 비교\n#coeftest(model, vcov = cov_wild)  \n#coeftest(model, vcov = cov_xy)  \n#\n## 주의: 부트스트랩 결과는 실행 횟수 R, 실행 시마다 다를 수 있음"
  },
  {
    "objectID": "posts/2025-02-28-reg1/index.html#마무리하며",
    "href": "posts/2025-02-28-reg1/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors",
    "section": "마무리하며",
    "text": "마무리하며\n이번 1장에서는 Regression Analysis의 기본 개념과 (simple, multiple or General) Linear Regression의 이론적 배경, 그리고 등분산성 가정이 깨진(Heteroskedasticity) 상황에서 유용하게 쓰이는 Heteroskedasticity-Consistent Standard Errors(HC standard errors)에 대해 살펴보았습니다.\nHC standard errors를 사용하면, Linear Regression 모델에서 등분산성 가정이 위배되더라도 Standard Errors(or Covariance Matrix)를 좀 더 타당하게 추정할 수 있습니다. 하지만, “오차항의 독립성, 선형성, 정규성” 등 나머지 가정이 크게 어긋난다면, 단순히 HC SE로 분산을 보정하는 것만으로는 해결되지 않습니다. HC SE가 기본 OLS 분산추정치와 크게 다르다면, “정말 모델이 맞는지”를 다시 고민하고, 필요하다면 모델을 재설정하거나 다른 방법을 모색해야 합니다.\n어쨌든, Heteroskedasticity가 의심되는 상황에서 가장 먼저 고려할 만한 접근인 HC(Heteroskedasticity-Consistent) standard errors는 모델의 유의성 검정을 위해 안정적으로 분산을 추정할 때 널리 쓰이며, 의학 연구에서도 일상적으로 활용되고 있습니다. 다음 2장에서는 이런 Linear Regression을 더 확장한 Generalized Linear Model(GLM)의 개념을 본격적으로 다루고, HC standard errors의 clustered data 버전인 Cluster-robust standard error에 대해서도 다룰 예정입니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html",
    "href": "posts/2025-02-28-reg3/index.html",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "",
    "text": "3장에서는 2장에서 다룬 Generalized Linear Model (GLM)에서 더 나아가, 데이터 내에 군집(Clustered) 구조가 존재하거나, 반복측정(Repeated measures) 데이터로 인해 독립성 가정이 깨지는 경우를 다루는 방법론인 GEE (Generalized Estimating Equation)와 GLMM (Generalized Linear Mixed Model)을 다루며, 그 전에 중요한 추정 방법론 중 하나인 M-estimation과 Robust(Sandwich) estimation에 대해서 다루겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#들어가며",
    "href": "posts/2025-02-28-reg3/index.html#들어가며",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "",
    "text": "3장에서는 2장에서 다룬 Generalized Linear Model (GLM)에서 더 나아가, 데이터 내에 군집(Clustered) 구조가 존재하거나, 반복측정(Repeated measures) 데이터로 인해 독립성 가정이 깨지는 경우를 다루는 방법론인 GEE (Generalized Estimating Equation)와 GLMM (Generalized Linear Mixed Model)을 다루며, 그 전에 중요한 추정 방법론 중 하나인 M-estimation과 Robust(Sandwich) estimation에 대해서 다루겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#m-estimation",
    "href": "posts/2025-02-28-reg3/index.html#m-estimation",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "1. M-estimation",
    "text": "1. M-estimation\n1.1. M-estimation 정의\n\n통계 분석에서 통계 모델이 비모수(non-parametric)가 아니라 모수(parametric)인 경우, 우리는 model의 모수, 즉 parameter (\\(\\boldsymbol{\\theta}\\), \\(\\boldsymbol{\\beta}\\) 등)를 추정해야 합니다. 이는 생각보다 어려운 일이 될 수 있으며, 이전에 언급한 MLE, OLS, Method of Moments (MOM) 등 다양한 model의 estimation 방법이 제안되어 왔습니다. 그런데, 이러한 추정 방법들은 사실상 하나의 추정방정식(estimating equation, 통계 모델의 parameter 추정 방향을 제시하는 모든 식)을 세운 뒤, 그 방정식을 만족하는 \\(\\hat{\\boldsymbol{\\theta}}\\)를 찾는 과정으로 해석할 수 있습니다. 예를 들어, MLE에서는 log likelihood를 parameter로 미분한 함수(score function)가 0이 되는 parameter point를 추정하는 과정이었고, OLS에서는 cost function (SSR, 오차 제곱합)을 parameter로 미분한 함수가 0이 되는 parameter point을 찾는 과정이었습니다. M-estimation은 이러한 공통된 개념을 일반화하여 공통된 parameter의 성질을 제시해줍니다.\n즉, M-estimation은 다음 과 같은 형태를 지닌 추정 방정식을 세우고, 이를 만족하는 모수의 값을 해로 삼습니다:\n\\[\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0},\n\\]\n여기서 \\(\\psi_i(\\boldsymbol{\\theta})\\)는 (i)번째 관측치에 대해 정의된 estimating function (e.g. score function), \\(\\boldsymbol{\\theta}\\)는 추정하고자 하는 parameter(위 식에서는 우항이 scalar 0이 아닌 벡터 0이므로 \\(\\boldsymbol{\\theta}\\) 또한 벡터.)입니다. 위에서 언급한 것처럼 MLS와 OLS, OLS의 일반화 버전인 Non-linear Least Squares 모두 M-estimation입니다. 1, 2장에 걸쳐 이미 익숙하시겠지만 다시 확인해보면, MLE를 M-estimation 형태로 작성해보면 다음과 같고,\n\\[\n\\psi_i(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\psi_i(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) = \\mathbf{0}.\n\\]\nOLS는 다음과 같습니다. \\[\n\\psi_i(\\boldsymbol{\\beta}) = (Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\mathbf{x}_i, \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\mathbf{x}_i(Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) = \\mathbf{0}.\n\\]\n여기서 OLS의 estimation equation이 다음과 같은 이유는,\\[\n\\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big)\n\\]\n\\[\n= \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big)\n\\]\n\\[\n= X^\\top X \\beta - X^\\top y\n\\]\n\\[\n=\\sum_{i=1}^n \\mathbf{x}_i(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} - Y_i) = \\mathbf{0}\n\\] 에서 유도된 식입니다.\n1.2. M-estimation 특징\n\nM-estimation의 가장 중요한 특징은 일반성과 확장성입니다. 즉, parameter estimation 문제를\n\\[\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0}\n\\]\n의 해로서 바라보면, 여러 기존 추정법들을 하나의 큰 이론적 틀에서 이해할 수 있고, 이로부터 발생하는 성질들은 해당되는 방법론 모두에 적용됩니다. 이렇게 M-estimation을 강조하여 설명하는 이유는, M-estimation은 아래 두 가지 수렴 이론(Asymptotic theory)을 제공하기 때문입니다.\n(1) 적절한 정규성 조건(regularity conditions) 하에서(종속변수의 정규 분포 가정이 아니며, 언급드린 적이 없지만 아주 general한 조건이라고 생각해주시면 됩니다.), 위 M-estimation의 estimating equation의 추정해 \\(\\hat{\\boldsymbol{\\theta}}\\)가 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 일치성(Consistency)과 점근정규성(Asymptotic Normality)을 가집니다.\n\n(2) 또한, 정규성을 갖는 모수의 점근분포가 중심극한정리(CLT)의 연장선상에 있다고 볼 수 있으며, 그 결과 위 \\(\\hat{\\boldsymbol{\\theta}}\\)의 asymptotic Normality에서 parameter의 분산은 Asymptotically&robust하게 추정 가능하고, 이 Robust Estimator의 형태가 샌드위치(sandwich) 형태로 생겼기 때문에 Sandwich Estimation(or)이라고도 부릅니다.\n즉, M-estimation으로부터 얻는 의의를 살펴보자면, 우리가 Regression Model의 parameters를 추정하는 과정에서 estimating equation이 위 M-estimation의 형태를 만족한다면, 어떠한 methods를 사용하든 이를 통해 추정한 parameter \\(\\hat{\\boldsymbol{\\theta}}\\)는 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 consistent함과, robust한 parameter의 분산을 얻을 수 있다는 것입니다. 이제 (1)과 (2)에 대한 수학적 증명을 걸친 뒤, 이들의 의미를 살펴보겠습니다.\n1.3. M-estimation의 Asymptotic Normality 증명\n\nM-estimation 추정량 \\(\\hat{\\theta}\\)의 점근적 성질을 유도하기 위해 1차 Taylor 전개를 사용합니다. 아래와 같은 M-estimation 추정 방정식 (증명의 편리를 위해 양변에 \\(\\frac{1}{n}\\)을 나누었으며, 나누지 않아도 똑같이 증명 가능하고, 등식에서 상수 term을 곱하고 나누는 것은 당연히 문제되지 않습니다. ) \\[\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) = 0\n\\] 을 참 모수 \\(\\theta_0\\) Taylor 식으로 전개하면,\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0) + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} (\\hat{\\theta} - \\theta_0) = 0.\n\\]\n가 됩니다. 이때 좌항 \\(\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta})\\)은 우리가 위 estimating equation에서 보았듯이, 이 항이 0이 되도록 하는 parameter를 추정한 결과가 \\(\\hat{\\theta}\\)였기 때문에 당연히 0일 것이고, 따라서 중앙항 (\\(\\theta_0\\)에 대한 Taylor 1차 식 전개) 또한 0이 되는 것입니다. 이제 \\(\\theta\\)에 대한 식을 도출하기 위해 \\(\\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0)\\) term을 넘기고 양변에 \\(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}\\)을 inverse하여(Matrix 이므로) 곱해주고 \\(\\sqrt{n}\\)을 곱하면,\n\\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\]\n가 됩니다. 여기서 다음 두 Matrix들을 정의하겠습니다:\n\n\n\\[ \\mathbf{A} = \\mathbb{E}\n\\left[ -\\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n\\] (2차 도함수 또는 score function의 미분의 기댓값)\n\n\\[ \\mathbf{B} = \\mathbb{E}\n\\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] \\] (score function의 분산의 기댓값)\n\n이제 대수의 법칙(LLN)과 중심극한정리(CLT)를 각각 적용하면 다음 두 식을 얻을 수 있습니다.\n\\[\n-\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} \\mathbf{A}\n\\] \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n\\]\n각 정리를 간단하게 설명드리자면, 대수의 법칙(LLN, Law of Large Numbers)은 표본 크기 \\(n\\)이 충분히 크면, 표본 평균이 모평균에 점근적으로 수렴한다는 법칙으로, 확률 변수 \\(X_i\\)가 동일 분포이고 기대값 \\(\\mathbb{E}[X_i] = \\mu\\)를 가지면, 수학적으로 표현하면 아래 식과 같습니다.\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_i \\xrightarrow{p} \\mu.\n\\]\n즉, 위 식에서는 \\[\n- \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}\n\\approx - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n= \\mathbf{A}\n\\] 가 되는 것입니다. 중심극한정리(CLT, Central Limit Theorem)는 독립이고 동일한 분포를 따르는 확률변수들의 표본 평균이 정규 분포를 따른다는 정리로, 분산이 \\(\\sigma^2\\)인 확률변수 \\(X_i\\)들에 대해,\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} (X_i - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2).\n\\] 입니다. (양변에 \\(\\sqrt{n}\\)이 나눠진 식이 더 친숙하실 겁니다.) 즉, 위 식에서는 \\(\\psi_i(\\theta_0)=0\\)이고, 따라서 \\[\n\\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] = Var(\\psi_i(\\theta_0))\n\\] 이므로,\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n\\] 임을 확인할 수 있습니다. 정리하자면, 대수의 법칙이 평균값으로의 수렴을 보장한다면, 중심극한정리는 표본 평균이 정규성을 띤다는 것을 보장하고, 이를 통해 우리가 고려하던 아래 식 \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\] 의 우항에 대한 두 정보를 얻을 수 있었습니다. 최종적으로 점근정규성을 보이기 위해선 이 두 수렴하는 분포의 곱을 나타낼 수 있는 Slutsky 정리를 보고, 최종적으로 식을 도출하겠습니다. Slutsky 정리는 두 개의 점근적 확률 분포를 결합하는 방법으로, 만약 \\(X_n \\xrightarrow{d} X\\) (약한 수렴)과, \\(Y_n \\xrightarrow{p} c\\) (확률적 수렴)이면,\n\\[\nX_i Y_i \\xrightarrow{d} Xc.\n\\]\n입니다. 즉, 확률적으로 수렴하는 변수와 분포적으로 수렴하는 변수를 곱하면, 여전히 위 식과 같이 분포적으로 수렴한다는 것이 증명된 정리이고, 위 식에서는\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} A\n\\]\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, B)\n\\] 이므로, Slutsky 정리를 사용하면 최종적으로\n\\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n\\]\n\\[\n\\xrightarrow{d} A^{-1} \\mathcal{N}(0, B) = \\mathcal{N}(0, A^{-1} B A^{-1})\n\\]\n입니다.(deteminant한 값은 분산 term에서 제곱된다는 것은 몇 번 보았었습니다.) 결국 M-estimation의 추정을 통해 얻은 \\(\\hat{\\boldsymbol{\\theta}}\\)가 참 모수 \\(\\boldsymbol{\\theta}_0\\)에 대해 일치성(Consistency)을 갖고, \\(\\hat{\\boldsymbol{\\theta}}\\)는 점근정규성(Asymptotic Normality)을 갖으며 그 식은 \\(\\mathcal{N}(0, A^{-1} B A^{-1})\\)입니다. 또한, 이 샌드위치(sandwich) 형태(\\(A^{-1}\\) 빵 사이에 껴있는 고기 \\(B\\))처럼 생긴 점근적 분산 식이 바로 Sandwich estimator의 general version입니다. 이제 이 sandwich estimator에 대해 좀더 설명드리겠습니다.\n1.4. Sandwich(Robust) Estimator\n\nSandwich(Robust) Estimator의 식은 써보면 다음과 같습니다: \\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\theta}}) = A^{-1} B A^{-1}\n\\] \\[\nwhere, \\; \\mathbf{A} = - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right],\n\\; \\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right]\n\\]\n2장에서 GLM case에 대해 log likelihood의 1차 도함수를 score function, 이의 negative 2차 도함수를 Fisher Information matrix라고 언급한 적이 있습니다. 이의 general한 버전이 위와 같으며, 여기에서는 이 score function \\(\\psi_i(\\theta_0)\\)의 분산을 \\(\\mathbf{B}\\), 2차 도함수를 \\(\\mathbf{A}\\)로 표기하고 있습니다.\n\\(\\mathbf{A}\\)는 모형의 곡률(curvature)을, \\(\\mathbf{B}\\)은 모형의 분산을 반영합니다. 또한, Estimating equation이 log likelihood로부터 MLE 철학으로 나온 parameter라면, \\(\\mathbf{A} = \\mathbf{B}\\)입니다. 이 이유는, 2장에서 2차 도함수가 정의되는 임의의 distribution을 따르는 종속변수 \\(Y\\)와 그의 parameter \\(\\theta\\)에 대해서 \\[ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} \\] 가 참임을 보였고, \\(\\ell''\\)은 \\(\\mathbf{A}\\), \\(\\frac{d^2\\ell}{d\\theta^2}\\)는 \\(\\mathbf{B}\\)와 같기 때문입니다. (\\(\\ell' = \\psi\\) 이므로.)\n즉 철학적으로 해석해보면, Regression Model의 selection이 정확한 경우 Fisher Information 행렬 동일성에 의해 \\(\\mathbf{A} = \\mathbf{B}\\)가 성립하게 되고, 이에 따라 parameter의 분산은 \\(A^{-1}\\) 만으로 추정될 수 있습니다. 그러나 Regression Model이 정확하지 않은 경우, consistent한 parameter estimation을 한다고 하더라도 이 모델의 추정 분산 \\(A^{-1}\\)은 더이상 신뢰할 수 없으며, 이때 Sandwich estimtor는 경험적 분산(empirical variance) \\(\\mathbf{B}\\)를 통해 robust하게 이를 추정할 수 있습니다. 즉, Regression Model의 몇 가지 가정이 의심될 때, 심지어는 의심되지 않더라도 Sandwich estimtor는 robust하게 parameter의 분산을 추정할 수 있는 것입니다.\n또한 이전에 스포한대로, 이전 장들에서 다루어 왔던 robust한 parameter variance estimator인 Heteroskedasticity-Consistent SE, Cluster-robust SE는 모두 이 Sandwich estimator의 special한 case입니다.(생김새부터 짐작할 수 있으셨을 겁니다.) LM version에서만 이를 증명한 뒤(GLM 버전도 같습니다.), GLM을 복습하고 GEE, GLMM에 대해서 설명드리겠습니다.\n\nProve HC0 is Sandwich estimator. (LM version)\n\nOLS의 score function은은 위에서 보았듯 다음과 같습니다:\n\\[\n\\psi_i(\\beta) = x_i (Y_i - x_i^T \\beta).\n\\] 그렇다면, \\(A\\)는 베타로 미분 후 -를 씌워주면 다음과 같이 계산되며,\n\\[\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_i(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ x_i x_i^T \\right].\n\\]\n\\(A\\)의 추정치는 결국\n\\[\n\\hat{A} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T = \\frac{1}{n} X^T X.\n\\] 가 됩니다. 마찬가지로 \\(B\\)를 계산하면,\n\\[\nB = \\mathbb{E} \\left[ \\psi_i(\\beta) \\psi_i(\\beta)^T \\right] = \\mathbb{E} \\left[ x_i x_i^T (Y_i - x_i^T \\beta)^2 \\right].\n\\]\n이며, 추정치는\n\\[\n\\hat{B} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T e_i^2 = \\frac{1}{n} X^T \\text{diag}(e_i^2) X.\n\\] 입니다. 결국\n\\[\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} X^T \\text{diag}(e_i^2) X \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n\\]\n\\[\n= (X^T X)^{-1} X^T \\text{diag}(e_i^2) X (X^T X)^{-1}.\n\\]\n가 되고, 이 식은 1장에서 보았던 HC0의 식과 동일함을 확인할 수 있습니다.\n\nProve Clustered-Robust SE is Sandwich estimator. (LM version)\n\n이 또한 OLS와 같은 환경이므로(LM, cluster가 \\(g\\)개로 구성되어 있다고 할 때, score function은 다음과 같습니다:\n\\[\n\\psi_g(\\beta) = \\sum_{i \\in g} x_i (Y_i - x_i^T \\beta).\n\\]\n\\(A\\)의 식과 추정치 또한 비슷하게 구해지고,\n\\[\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_g(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ \\sum_{i \\in g} x_i x_i^T \\right].\n\\] \\[\n\\hat{A} = \\frac{1}{n} \\sum_{g=1}^{G} \\sum_{i \\in g} x_i x_i^T = \\frac{1}{n} X^T X.\n\\]\n\\(B\\)도 비슷하게 계산되며, cluster간의 independent는 여전히 가정됩니다.\n\\[\nB = \\mathbb{E} \\left[ \\psi_g(\\beta) \\psi_g(\\beta)^T \\right].\n\\]\n\\[\n\\hat{B} = \\frac{1}{n} \\sum_{g=1}^{G} \\left( \\sum_{i \\in g} x_i e_i \\right) \\left( \\sum_{i \\in g} x_i e_i \\right)^T = \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g.\n\\]\n이에 따라 분산의 Sandwich estimator를 구하면\n\\[\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n\\]\n\\[\n= (X^T X)^{-1} \\left( \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) (X^T X)^{-1}.\n\\]\n이고, 이는 Cluster-robust SE의 식과 동일합니다.\n1.5. GLM 복습\n\nGeneralized Linear Model (GLM)의 모델 식은 다음과 같이 표현됩니다: \\[\ng(\\mathbb{E}[Y_i | X_i]) = g(\\mu_i) =\\eta_i = X_i^T \\beta \\\\\nwhere, Y_i \\sim \\text{Exponential Family}(\\mu_i, \\phi).\n\\] 이때 \\(g(\\cdot)\\)은 링크 함수(link function)로 logit, log의 예시를 보았고, \\(\\mu_i = \\mathbb{E}[Y_i | X_i]\\)는 반응 변수의 기대값으로 모델의 mapping의 목적이 되는 값(예측하고자 하는 값), \\(\\phi\\) 분산과 관련된 parameter(dispersion parameter) 로 정규 분포의 경우 \\(\\sigma^2\\)였습니다.\n간략하게 복습하면 링크 함수(link function) \\(g(\\cdot)\\)를 통해 \\(E(Y_i) = \\mu_i\\)와 \\(\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)를 연결하고, 분산 함수(variance function) \\(V(\\mu_i)\\)를 이용해 \\(\\operatorname{Var}(Y_i)\\)를 표현하며, 추정방정식(estimating equation)을 세워\n\\[\n\\sum_{i=1}^n \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)} = \\mathbf{0}\n\\]\n을 푸는 방식으로 \\(\\hat{\\boldsymbol{\\beta}}\\)를 구합니다.\n이 해석 또한 M-estimation의 한 사례로 볼 수 있습니다. GLM에서 score 함수(추정방정식)는 \\(\\psi_i(\\boldsymbol{\\beta}) = \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)}\\) 꼴로 정의되며, 이를 0으로 만드는 \\(\\hat{\\boldsymbol{\\beta}}\\)가 우리가 구하고자 하는 파라미터 추정치가 됩니다. 2장에서는 GLM의 parameter \\(\\hat{\\boldsymbol{\\beta}}\\)를 추정하는 방법으로 IRLS(Iteratively Reweighted Least Squares)이나 Newton-Raphson/Fisher Scoring을 소개하였으며, 이는 결국 M-estimation에서 구체적으로 어떻게 “estimating equation을 수치적으로 풀어낼지” 알고리즘으로 구현한 예시 중에 하나였다고 이해할 수 있습니다. 또한, 2장에서 예고한대로, 왜 parameter \\(\\hat{\\boldsymbol{\\beta}}\\)의 분산이\n\\[\nVar(\\hat{\\boldsymbol{\\beta}}) = -fisher\n\\] 라고 했었는지 이제 살펴보면, GLM의 추정 또한 M-estimation에 해당하므로 GLM의 estimating equation을 만족하는 estimator에 대해서 위에서 확인한 점근정규성이 만족함을 알 수 있고, 때문에 consistent한 parameter estimator에 대해서 다음과 같은 robust한 Sandwich 분산을 갖습니다.\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1} \\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1}\n\\]\n그리고, 계속 보아왔던 것처럼 여기서 \\(\\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) = \\mathcal{I}(\\boldsymbol{\\beta}_0)\\)가 만족(스코어 함수의 분산의 기댓값과 Fisher information matrix가 같습니다.) 하기 때문에 이를 대입하면 위 식이 아래 처럼 소거되고,\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\mathcal{I}(\\boldsymbol{\\beta}_0)^{-1}\n\\]\n가 됩니다. 결국 GLM의 모형 기반 분산은 다음과 같습니다:\n\\[ \\mathbb{V}ar_{\\text{모형}}(\\hat{\\beta}) = \\mathbf{A}^{-1} = \\left( \\sum_{i=1}^{n} \\frac{\\partial^2 \\log f(Y_i; \\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}. \\]\n또한, 이때 경험적 분포를 고려하여 Sandwich로 추정한 분산은,\n\\[ \\mathbb{V}ar_{\\text{robust}}(\\hat{\\beta}) = \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{A}^{-1}, \\]\n\\[ where, \\mathbf{B} = \\sum_{i=1}^{n} \\psi_i(\\hat{\\beta}) \\psi_i(\\hat{\\beta})^T, \\; \\psi_i(\\beta) = (Y_i - \\mu_i) x_i / V(\\mu_i). \\]\n로, 이는 이전에 확인한 HC0의 형태와도 이어집니다.\n다시 돌아와서.. (for clustered data)\n일반적으로 선형 모델(Linear Model)과 일반화 선형 모델(Generalized Linear Model, GLM)은 독립 동일 분포(i.i.d.)를 가정합니다. 즉, 기존의 GLM은 관측치(observations, data points)들이 서로 독립이며(Independent)일 때 동일한 분산 구조에서 잘 작동합니다. 그러나 학교나 병원 등 군집(클러스터) 단위로 샘플이 묶여 있는, 비슷한 특성을 지닌 대상들을 클러스터(cluster)로 묶은 패널 데이터(panel data)나 동일한 실험 대상(피험자)에게서 반복 측정된 데이터(longitudinal data)의 경우, 같은 cluster(또는 group: 같은 피험자, 같은 단위 등)에 속한 data간에는 correlation이 존재합니다. 때문에 더이상 data들이 독립이 아니게 되며, GLM만으로는 이 상관구조를 모델 자체에서 고려할 수 없기에, GEE와 GLMM 와 같은, 더욱 general한 Regression Model이 개발되었습니다. 이제 아래에서 위 두 model에 대해서 살펴보겠습니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#generalized-estimating-equation-gee",
    "href": "posts/2025-02-28-reg3/index.html#generalized-estimating-equation-gee",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "2. Generalized Estimating Equation (GEE)",
    "text": "2. Generalized Estimating Equation (GEE)\n2.1. GEE 정의\n\nGEE (Generalized Estimating Equation)는 GLM이 독립성 가정을 전제로 하는 한계마저 뛰어넘어, 군집(Clustered) 자료나 반복측정 자료 등 상관구조가 존재하는 데이터에 적용될 수 있도록 확장한 방법론입니다. 가장 critical하게 다른 점을 보면, GLM은 \\(\\operatorname{Var}(Y_i) = \\phi V(\\mu_i)\\)로 종속변수의 분산을 표현할 때 diagonal matrix로 두어 data points 간에는 correlation이 없음을 표현하였다면, GEE는 \\(\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}\\)와 같은 working correlation 행렬 \\(\\mathbf{R}_i(\\alpha)\\)를 사용하여, 관측치들 간의 상관관계를(반복 측정, 클러스터 내 상관) 모델에 반영합니다. 또한, 이러한 가정을 위해 종속변수의 확률 모델(공동 확률 분포)을 완전히 명시하지 않아도, Quasi-Likelihood(준 우도) 접근법을 통해 점근적(score) 방정식을 확장하여 모델을 적합합니다. 간단히 말하면, GEE는 “평균 모형은 GLM처럼 유지하되, 상관 구조를 적절히 지정하여 군집성이나 반복측정을 고려하자”라는 접근입니다.\nLM이나 GLM은 서로 독립적인(i.i.d.) 표본을 가정하여 이를 기반으로 추정하는 반면, GEE에서는 상관 구조(correlation structure) \\(R(\\alpha)\\)를 추가하여 이러한 독립 가정을 완화하고, 평균 모형과 분산-공분산 구조에 대한 가정을 분리해서 Quasi 형태로 추정합니다. 이 때, Quasi-likelihood에 대한 적용을 짧게 설명하자면, GEE는 확률 모델을 직접 설정(완전한 공동 확률 밀도 함수 명시)하지 않고, GLM의 log likelihood function에 상관구조를 추가하는 형태로 접근합니다. 즉 GLM에서 종속 변수의 Exponential family 가정 -&gt; 독립 가정 후 모든 data point의 확률(likelihood)를 곱해서 얻은 likelihood finction -&gt; 로그 씌워서 log likelihood -&gt; model parameter로 미분한 결과인 score function 순으로 추정 과정을 설명했었다면, GEE는 처음부터 직접적인 종속변수의 가정으로 시작하는 대신 log likelihood에서 시작하고, 이 때 독립이 아님을 고려하기 위해 variance function에 상관 행렬 \\(\\mathbf{R}_i(\\alpha)\\)을 추가하여 \\(\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}\\)로 둔 후 추정하는 것입니다. 이러한 접근은, 실제로 종속변수의 완전한 확률적 기반(joint PDF)이 존재하지 않아도, 점근적 성질을 활용하여 일관된 추정량을 얻을 수 있고, 오류 항이 독립적이지 않은 경우에도 GLM과 유사한 방식으로 추정할 수 있는 장점이 있습니다. 마지막으로, GEE는 상관행렬과 Quasi의 개념을 통해 GLM과 같이 data points들을 marginal하게 고려하여 fit하기 때문에 Population-Average GEE(or 모델) 이고, 이는 무작위 효과(Random Effect)를 통해 각 실험 단위(피험자)에 특화된 효과를 추정하는 GLMM(Generalized Linear Mixed Model)과 철학이 다르며,이 GLMM은 Subject-Specific GEE(or 모델)이라고도 부릅니다.\n2.2. GEE 수학적 표현 및 추정\n\n위에서 언급하였듯, GLM과 동일하게 GEE는 아래와 같은 marginal 모델입니다:\\[\ng\\bigl(\\boldsymbol{\\mu}_i\\bigr) = \\mathbf{X}_i \\boldsymbol{\\beta},\n\\] 여기서 \\(\\mathbf{Y}_i\\)는 (i)번째 클러스터(또는 피험자)에서 나온 \\(n_i\\)개의 관측치 벡터, \\(\\boldsymbol{\\mu}_i = E(\\mathbf{Y}_i) \\; or \\; E(\\mathbf{Y}_i|X_i)\\)입니다. 또한, 언급한 대로 Working correlation을 아래와 같이 설정하며,\n\\[\n\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}.\n\\] 이때 \\(\\mathbf{A}_i\\)가 기존 \\(V(\\mu_{ij})\\)의 역할 이었다면 이에 루트를 씌워 A라고 두고 (행렬에서의 square root, 또는 1/2 승은 기존 \\(V\\)가 diagonal 이었으므로 이때는 단순히 각 대각 성분을 루트 씌운 값입니다.) 그 사이에 클러스터 당 상관관계를 \\(\\mathbf{R}_i(\\alpha)\\)로 표현합니다. 이때 상관행렬 \\(\\mathbf{R}(\\alpha)\\)의 종류로는 크게 아래와 같은 예시들이 있습니다.\n(1) Independent (기존 GLM)\n\\[\nR(\\alpha) = I, \\quad V_k = V_k'\n\\]\n(2) Exchangeable Correlation (동일 상관 구조)\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha \\\\\n\\alpha & \\ddots & \\alpha \\\\\n\\alpha & \\alpha & 1\n\\end{pmatrix}\n\\]\n(3) Autoregressive (AR-1)\n\\[\n\\text{Corr}(y_{ki}, y_{kj}) = \\alpha^{|i-j|}\n\\]\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha^2 & \\dots & \\alpha^{n_k} \\\\\n\\alpha & 1 & \\alpha & \\dots & \\alpha^{n_k-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha^{n_k} & \\alpha^{n_k-1} & \\alpha^{n_k-2} & \\dots & 1\n\\end{pmatrix}\n\\]\n(4) Unstructured Form\n\\[\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha_1 & \\alpha_2 & \\alpha_3 \\\\\n\\alpha_1 & 1 & \\alpha_4 & \\alpha_5 \\\\\n\\alpha_2 & \\alpha_4 & 1 & \\alpha_6 \\\\\n\\alpha_3 & \\alpha_5 & \\alpha_6 & 1\n\\end{pmatrix}\n\\]\n이러한 상관행렬 \\(\\mathbf{R}(\\alpha)\\)는 사전에 정의되어야 하므로 분석 대상인 data의 성질에 따라 선정해야 하며, 이러한 관계의 구조를 어떻게 선택하는 지에 따라 분산이 다르게 나오므로, 위에서 다룬 Sandwich를 통한 robust한 추정이 GEE에서 대게 사용됩니다.\nGEE’s Estimating Equation\n이전에 GLM에서는 다음과 같이 score functions로 부터 estimating equation을 세웠습니다:\n\\[\n\\Psi = \\sum \\psi_i = \\sum_{i=1}^{N} \\frac{y_i - \\mu_i}{a(\\phi)V(\\mu_i)} \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i = 0\n\\]\n이제 이를 GLM 때와 다르게 각 cluster \\(k\\)에 대해 \\(D_k = diag(\\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i)\\), \\(V_k = \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}\\)라고 하면,\n\\[\n\\sum_{k=1}^{K} \\frac{1}{a(\\phi)}  D_k V_k^{-1} (y_k - \\mu_k) = 0\n\\] 으로 식을 GLM의 score function 으로부터 변형해서 얻을 수 있고, 최종적으로 벡터와 행렬 연산으로 모든 클러스터 \\(k\\)에 대해 block diagonal로 한 번에 표현하면(\\(V\\)), \\[\n\\Psi = \\frac{1}{a(\\phi)} D V^{-1} (y - \\mu) = 0\n\\] 가 되며, \\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr) = \\mathbf{0},\n\\] 로 표현할 수 있습니다. 이는 GLM의 score 함수와 같은 시작이지만, GEE는 \\(\\mathbf{V}_i\\)가 군집/반복측정 상관을 반영하도록 조작합니다. 결국 이 추정방정식을 품으로써 GEE의 추정이 가능할 것입니다.\n가장 중요한 GEE에서 분산 term의 변형을 다시 한 번 강조하자면, \\(V_k\\) 는 cluster 별 (Co)variance 행렬로, data가 independent하다면 \\(V_k\\)와 이를 모두 합친 \\(V\\)가 diagonal matrix가 되지만, 그룹 내 상관을 고려할 경우 \\(V_k\\)가 더 이상 diagonal하지 않고, 이에 따라 \\(V\\)는 block diagonal matrix 형태를 갖습니다. (block diagonal한 이유는 cluster끼리 독립이고 cluster안은 상관관계가 있는 1차 clustered data에서 다룬 2장의 cluster-robust를 떠올리면 좋을 것 같습니다.)\n\\[\nV = \\begin{pmatrix}\nV_1 & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & V_K\n\\end{pmatrix}\n\\]\nGEE parameter 추정(IRLS)\nGEE의 parameter 추정 또한 GLM에서 비롯된 만큼, 이전에 다루었던 방식과 유사한 반복 알고리즘으로 \\(\\hat{\\boldsymbol{\\beta}}\\)를 추정할 수 있습니다. 하나의 스텝을 예시로 들어보면,\n\n현재 추정치 \\(\\hat{\\boldsymbol{\\beta}}^{(t)}\\)에서, 각 클러스터 \\(i\\)에 대해 \\(\\mathbf{D}_i\\) (편미분 행렬), \\(\\boldsymbol{\\mu}_i = \\mu(\\hat{\\boldsymbol{\\beta}}^{(t)})\\), \\(\\mathbf{V}_i\\) (working correlation \\(\\mathbf{R}_i(\\alpha)\\), dispersion parameter \\(\\phi\\) 포함)을 계산합니다. 이때, working correlation \\(\\mathbf{R}_i(\\alpha)\\)와 \\(\\phi\\)도 반복적으로 업데이트됩니다. 예컨대, gee나 geepack 패키지에서는 각 반복 단계에서 잔차(residual)를 기반으로 \\(\\alpha\\)와 \\(\\phi\\)를 재추정하여 새로운 \\(\\mathbf{V}_i\\)를 구합니다.\n\n아래 식을 만족하도록 \\(\\hat{\\boldsymbol{\\beta}}^{(t+1)}\\)를 업데이트합니다:\n\\[\n\\hat{\\boldsymbol{\\beta}}^{(t+1)}\n= \\hat{\\boldsymbol{\\beta}}^{(t)}\n- \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1}\n\\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr).\n\\]\n\n이전처럼 parameter의 변화량(distance between \\(\\hat{\\boldsymbol{\\beta}}^{(t+1)}\\)and \\(\\hat{\\boldsymbol{\\beta}}^{(t)}\\))가 특정 threshold 아래로 수렴할 때까지 이 과정을 반복합니다.\n\nR에서는 geepack이나 gee 라이브러리에서 내부적으로 이러한 절차를 수행합니다. 1에서 어떻게 잔차로부터 \\(\\alpha\\)를 추정할 수 있는지 간단하게 예시를 보면 아래와 같습니다. 이는 이전과 원리는 같으며, 상관행렬에서 추정해야 하는 parameter 개수에 따른 자유도를 고려하기 때문에 식이 좀더 복잡해진 것입니다.\n잔차는 아래와 같이 계산됩니다(Pearson): \\[\n\\hat{r}_{ki} = \\frac{y_{ki} - \\hat{\\mu}_{ki}}{\\sqrt{V(\\hat{\\mu}_{ki})}},\n\\] where \\(y_{ki}\\)는 observed response for cluster \\(k\\) and observation \\(i\\), \\(\\hat{\\mu}_{ki}\\)는 predicted mean for cluster \\(k\\) and observation \\(i\\), \\(V(\\hat{\\mu}_{ki})\\)는 variance function evaluated at \\(\\hat{\\mu}_{ki}\\). 이제 \\(n_k = n\\)라고 가정하면 다음과 같습니다. (이는 클러스터 당 data point 개수가 같다는 가정이며, 이를 만족하지 않아도 식이 복잡해질 뿐 똑같이 계산됩니다.)\n(2) Exchangeable Correlation: \\[\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i &gt; j} \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K \\cdot \\frac{1}{2}n(n-1) - p},\n\\]\n(3) Autoregressive (AR-1): \\[\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i=1}^{n_k - 1} \\frac{\\hat{r}_{ki} \\hat{r}_{k(i+1)}}{K(n-1) - p},\n\\]\n(4) Unstructured Form: \\[\n\\hat{a}_{ij} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K - p},\n\\]\n위 식들은 그저 잔차로부터 (co)variance를 추정하는 것일 뿐이고, 분산 term은 degree of freedom을 고려하기 때문에 그저 각각의 상관 행렬 속 미지수(parameter)의 개수에 따른 반영입니다.\n2.3. GEE parameter’s Variance\n\nGEE의 모수 추정치 (\\(\\hat{\\boldsymbol\\beta}\\))도 M-estimation의 범주에 속하므로, 점근 분산은 Sandwich 형태를 갖습니다.\n\\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol\\beta})_{\\text{robust}}\n= \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1}\n\\mathbf{D}_i \\right)^{-1} \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} (\\mathbf{Y}_i -\n\\boldsymbol\\mu_i) (\\mathbf{Y}_i - \\boldsymbol\\mu_i)^\\top\n\\mathbf{V}_i^{-1} \\mathbf{D}_i \\right) \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1}.\n\\]\n이를 robust 또는 empirical 표준오차라고 하며, 실질적으로 상관구조 (\\(\\mathbf{R}_i(\\alpha)\\))가 부정확하게 지정되었을지라도 일관성을 보장해 줍니다. 즉, 위 M-estimation으로 GLM을 해석할 때와 일치하게, Model-based SE 는 \\(\\mathbf{A}^{-1}\\)만을 사용해서 계산하는 것이고, 이는 설정한 working correlation(상관행렬) 가정이 정확하다고 믿을 때이기 때문에, 이를 신뢰할 수 없는 경우 거의 무조건 Robust SE\\(\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{A}^{-1}\\) 를 사용합니다. 이는 R에서 GEE를 계산할 때 기본 SE(model-based)와 robust SE(empirical) 두 가지가 함께 리포팅되는 이유이기도 합니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#generalized-linear-mixed-model-glmm",
    "href": "posts/2025-02-28-reg3/index.html#generalized-linear-mixed-model-glmm",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "3. Generalized Linear Mixed Model (GLMM)",
    "text": "3. Generalized Linear Mixed Model (GLMM)\n3.1. GLMM 정의\n\nGLMM(Generalized Linear Mixed Model)은, 우리가 이미 익숙하게 다뤄온 GLM(Generalized Linear Model)을 GEE와는 다른 방식으로 (Mixed model) “군집(cluster) 또는 계층적 구조를 가지는 자료”에까지 확장하기 위한 방법론입니다. 즉, GLMM은 이러한 내재된 상관(또는 군집성)을 모델화하기 위해서 고정 효과 + 무작위 효과의 결합으로 모형을 설정합니다. 즉, GLMM은\n\n\n고정 효과(fixed effects): 전체 모집단에 공통적으로 적용되는 회귀계수(예: 전체 평균 경향)에 해당,\n\n무작위 효과(random effects): 피험자(또는 군집, 클러스터)별로 달라지는 편차(“개인별 random intercept” 혹은 “개인별 random slope” 등)를 도입\n\n을 둘다 고려하는 모델이며, 즉 “Generalized Linear Model + Linear Mixed Model(Random Effects)”의 결합이라고 요약할 수 있습니다. GEE와 비교하여 이 GLMM은 각 cluster(또는 group)마다 직접적인 고려를 모델에 넣기 때문에(random effect) Subject-Specific 모델(또는 GEE)라고도 불리며, 이는 Population-Average GEE와 대비되는 특징입니다. 무작위 효과는 정규분포로 가정하는 것이 일반적이며, 경우에 따라서는 다른 분포(예: Gamma)로 가정하기도 하고, GLMM에서은 이러한 LMM을 GLM으로 확정한 것이기 때문에 종속변수의 분포를, Exponential Family로 확장합니다.\n3.2. LMM 수학적 표현 및 추정\n\nGLMM을 이해하기 위해서는 먼저 선형혼합모형(LMM; Linear Mixed Model)을 확실하게 이해할 필요가 있습니다. (이 LMM과 GLMM을 완벽하게 이전처럼 분석하려면 내용이 산만해지기 때문에 여기선 중요한 점을 위주로 짚고, 추가적인 공부가 필요하신 분들은 위키피디아에서 비롯되는 교재 및 논문 내용들을 집중적으로 살펴보시면 좋을 것 같습니다.) LMM은 종속변수 \\(Y\\)의 분포가 정규분포라는 전제하에서, 고정 효과(fixed effects)와 무작위 효과(random effects)가 동시에 존재한다고 보는 모형입니다.\nLMM 수학적 표현\n\n가장 단순한 형태의 LMM(임의절편 모형, random intercept model)을 생각해 보겠습니다. (이때 LMM에서 모형을 나누는 기준은 random effect, 즉 group을 어느 정도로 복잡하게 고려하는 지에 따른 설계의 차이입니다. random effect의 분포, 차원 등을 다양하게 고려할 수 있겠지요.) 예를 들어, \\(i\\)번째 클러스터(또는 피험자) 내에서 \\(j\\)번째 관측값을 나타내는 \\(Y_{ij}\\)를 다음과 같이 모델링합니다:\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + b_i + \\varepsilon_{ij},\n\\quad i=1,\\dots,K,\\quad j=1,\\dots,n_i.\n\\]\n이때 \\(\\beta_0, \\beta_1\\)은 고정 효과(fixed effects) parameter, 즉 모든 클러스터에 공통 적용되는 평균적인 효과)이고, \\(b_i\\)는 무작위 효과(random effect) parameter로, 클러스터 \\(i\\)마다 서로 다른 절편(intercept) 편차를 갖는 것을 모델링하고 있습니다. \\(\\varepsilon_{ij}\\)는 흔히 오차항(residual)으로 간주하고, 대게 \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\)로 가정합니다.\n추가적으로, 무작위 효과 \\(b_i\\)는 다음과 같은 분포로 가정합니다:\n\\[\nb_i \\sim N(0,\\;\\tau^2).\n\\]\n이는 “(피험자마다) 임의로 달라지는 절편(intercept)”이 정규분포를 따른다는 것을 의미합니다. 모든 \\(b_i\\)를 독립으로 가정하면,\n\\[\n\\operatorname{Var}(b_i) = \\tau^2,\\quad \\operatorname{Var}(\\varepsilon_{ij}) = \\sigma^2.\n\\]\n결국, 어떤 \\(Y_{ij}\\)에 대해서는\n\\[\nY_{ij} = \\beta_0 + b_i + \\beta_1 X_{ij} + \\varepsilon_{ij},\n\\]\n이고,\n\\[\n\\operatorname{Var}(Y_{ij}) = \\tau^2 + \\sigma^2.\n\\]\n이먀, 더 일반화 된 모델로 무작위 절편 + 무작위 기울기(random intercept + random slope)를 도입하여 독립변수 \\(X\\)에 대해서도 개인별로 기울기가 달라지도록 만들 수 있습니다. 이 경우,\n\\[\nY_{ij}\n= (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) X_{ij} + \\varepsilon_{ij},\n\\quad\nb_{0i} \\sim N(0,\\;\\tau_{00}),\\;\nb_{1i} \\sim N(0,\\;\\tau_{11}),\\;\n\\operatorname{Cov}(b_{0i}, b_{1i}) = \\tau_{01}.\n\\]\n가 될 것입니다. 이처럼 무작위 효과를 하나 혹은 여러 개 갖는다는 것은, “클러스터마다 고유하게 발생하는 변동”을 모델에 포함하는 방식으로, LMM은 이러한 방식로 상관구조를 모델링 해낸다고 생각할 수 있습니다. 이를 벡터와 행렬 형태로 표현해보면, 각 클러스터(또는 피험자) \\(i\\)에 대해\n\\[\n\\mathbf{Y}_i\n= \\mathbf{X}_i\\,\\boldsymbol{\\beta}\n\\;+\\; \\mathbf{Z}_i\\,\\mathbf{b}_i\n\\;+\\; \\boldsymbol{\\varepsilon}_i,\n\\]\n\n\n\\(\\mathbf{Y}_i\\): \\(i\\)번째 클러스터에서의 \\(n_i\\)차원 응답벡터\n\n\\(\\mathbf{X}_i\\): \\(n_i \\times p\\) 차원의 고정 효과 설계 행렬(fixed effect parameter \\(\\boldsymbol{\\beta}\\)와 매칭)\n\n\\(\\mathbf{Z}_i\\): \\(n_i \\times q\\) 차원의 무작위 효과 설계 행렬(random effect parameter \\(\\mathbf{b}_i\\)와 매칭)\n\n\\(\\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G})\\) 이며 \\(\\boldsymbol{G}\\)는 \\(q \\times q\\) 공분산 행렬\n\n\\(\\boldsymbol{\\varepsilon}_i \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I}_{n_i})\\)로 일반적으로 가정(독립 동일 분포)\n\n여기서 설계 행렬이란, 1장의 LM에서부터 사용하였지만, data point(observation)당 미리 input으로 지정되는 행렬로, 정확한 의미는 “일련의 개체에 대한 설명 변수 값을 나열한 행렬로 각 행은 개별 개체를 나타내며, 열은 해당 개체에 대한 변수 및 특정 값에 해당한다”입니다. X는 계속 봐왔지만 Z는 이번에 처음 나온 설계 행렬인데, 이는 각 data point마다 해당되는 cluster에는 1, 해당되지 않는 나머지 cluster는 0의 값을 갖는, cluster를 선택하는 스위치 느낌으로, input으로 정해지는 행렬이라고 생각하시면 됩니다.\n이 LMM의 (Co) variance matrix는 단순하게 분산 term을 씌우면 random한 (determinant하지 않은) 항만 남아 다음과 같이 계산 될 것입니다:\n\\[\n\\operatorname{Var}(\\mathbf{Y}_i)\n= \\mathbf{Z}_i\\,\\boldsymbol{G}\\,\\mathbf{Z}_i^\\mathsf{T}\n+ \\sigma^2\\,\\mathbf{I}_{n_i}.\n\\]\nLMM’s parameter 추정(Maximum Likelihood, REML)\n\n이 LMM에서 \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{G}\\) (또는 \\(\\tau^2\\) 등), \\(\\sigma^2\\) 모두 미지수입니다. 이를 추정하기 위해 주로 최대우도추정법(ML; Maximum Likelihood) 또는 제한최대우도추정법(REML; Restricted Maximum Likelihood)을 사용합니다. 각각이 어떻게 계산될지 설명드리면 다음과 같습니다.\n(1) With ML(MLE).\\(b_i\\)가 정규분포라는 가정 하에, \\(\\mathbf{Y}_i\\)의 joint distribution도 다변량 정규분포로 표현할 수 있습니다. 모든 \\(i\\)에 대해 \\(b_i\\) 또는 \\(\\mathbf{Y}_i\\)가 독립이라 가정하면(cluster간은 독립), 전체 자료의 joint density를 곱해서 likelihood 함수를 정의할 수 있고, 이를 최대화하는 \\(\\hat{\\boldsymbol{\\beta}}\\)와 \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\sigma}^2\\)를 찾으면 됩니다. 단점으로는, ML은 \\(\\boldsymbol{\\beta}\\) 추정에서 편향(bias)이 발생할 수 있어, 표본 크기가 작거나 모형 구조가 복잡해질 때 문제될 수 있습니다. 수식을 중요 부분만 전개해보면, LMM에서 모든 \\(b_i\\)를 각각 적분하여(즉 클러스터 마다 적분) 얻은 \\(\\mathbf{Y}_i\\)의 분포는\n\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, V_i) \\quad \\text{with} \\quad V_i = \\mathbf{Z}_i \\mathbf{G} \\mathbf{Z}_i^T + \\sigma^2 I_{n_i}.\n\\]\n입니다. 이제 \\(i\\)번째 클러스터 자료 \\(\\mathbf{Y}_i\\)에 대한 log-likelihood를 계산해보면, 다차원 정규분포이므로 다음과 같이 나옵니다:\n\\[\n\\ell_i(\\beta, \\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ n_i \\log(2\\pi) + \\log |V_i| + (\\mathbf{Y}_i - \\mathbf{X}_i \\beta)^T V_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\beta) \\right].\n\\]\n이를 전체 \\(K\\)개의 cluster에 대해 모두 합하면, 전체 자료에 대한 log-likelihood 함수 \\(\\ell(\\beta, \\mathbf{G}, \\sigma^2)\\)가 도출될 것입니다.\n\\[\n\\ell(\\beta, \\mathbf{G}, \\sigma^2) = \\sum_{i=1}^{K} \\ell_i(\\beta, \\mathbf{G}, \\sigma^2).\n\\]\nMLE(\\(\\hat{\\beta}_{ML}, \\hat{\\mathbf{G}}_{ML}, \\hat{\\sigma}^2_{ML}\\))를 구하기 위해서는 위의 log-likelihood를 \\(\\beta, \\mathbf{G}, \\sigma^2\\)에 대해 미분하여 0이 되게 하는 해를 찾으면 됩니다. 그러나 일반적으로 \\(\\mathbf{G}\\)와 \\(\\sigma^2\\)에 대한 미분은 해석적으로 단순화하기 어렵고, 또한 \\(\\mathbf{G}\\)가 양의 정부호(positive definite)가 되어야 하는 제약이 있으므로, 수치적 최적화(EM 알고리즘, Newton-Raphson, Fisher scoring 등)를 사용해야 합니다.\n(2) With REML.\n이는 ML를 직접 바로 계산하는 대신, 고정 효과 \\(\\boldsymbol{\\beta}\\)와 관련이 없는 term을 이용해 \\(\\boldsymbol{G}\\)와 \\(\\sigma^2\\)를 먼저 추정한 후 모델을 추정하는 방식입니다. 일반적으로 LMM 을 추정할 때는 REML이 고정 효과 추정에 대한 편의를 줄여주고, 분산 요소에 대한 추정이 좀 더 안정적이기 때문에 ML보다 선호됩니다. 이는 모델에서 무작위 효과를 적분(marginal likelihood)하는 접근을 통해 \\(\\boldsymbol{\\beta}, \\boldsymbol{G}, \\sigma^2\\)에 대한 우도 함수를 세우고(restricted likeli hood), 이 함수를 최대화하는 방식으로 진행됩니다. 실제 계산은 Iterative 알고리즘(EM 알고리즘, 또는 Fisher scoring, Newton-Raphson 등)을 사용합니다. 식을 보면,\n\\[\n\\ell_{REML} (\\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ \\sum_{i=1}^{K} \\log |V_i| + \\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}| + (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta})^T \\mathbf{V}^{-1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}) \\right] + \\text{const}.\n\\]\n이고, 이때 \\(\\mathbf{V} = \\text{blockdiag}(V_1, \\dots, V_K)\\), \\(\\hat{\\beta}\\)는 \\(\\mathbf{G}, \\sigma^2\\)가 주어졌을 때의 일반화최소제곱(GLS) 해입니다. 여기서 \\(\\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}|\\)가 REML에서 추가로 나타나는 항으로, 이것이 \\(\\beta\\)를 제거(또는 \\(\\beta\\)에 무관한 부분만 모아놓음)하여 우선적으로 식을 전개한 효과라고 이해할 수 있습니다. REML에서는 이 \\(\\ell_{REML} (\\mathbf{G}, \\sigma^2)\\)를 \\(\\mathbf{G}, \\sigma^2\\)에 대해 최대화한 뒤, 그 해 \\(\\hat{\\mathbf{G}}_{REML}, \\hat{\\sigma}^2_{REML}\\)를 이용해 최종적으로 \\(\\hat{\\beta}_{REML}\\) 을 구합니다. REML은 ML보다 fixed effect estimator의 편향 문제가 덜하며, 분산 성분 \\(\\mathbf{G}, \\sigma^2\\)에 대해 좀 더 안정적인 추정을 제공합니다. 특히 샘플이 작거나 무작위효과 구조가 복잡할 때 일반적으로 더욱 안정적인 REML을 권장하는 편입니다.\n이를 통해 얻은 \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\sigma}^2\\)는 LMM의 MLE(or REML) 추정치이며, R에서는 lme4 패키지 등에서 이 과정을 내부적으로 수행합니다.\n3.3. GLMM의 수학적 표현 및 추정\n\n이제 LMM에서 정규 오차항을 일반화하여, 종속변수가 이항, 포아송, 혹은 다른 지수분포족을 따를 수 있도록 확장하면, GLMM으로 이어집니다. GLMM은\n\\[\ng\\bigl(\\mu_{ij}\\bigr)\n= \\mathbf{x}_{ij}^\\mathsf{T}\\,\\boldsymbol{\\beta}\n+ \\mathbf{z}_{ij}^\\mathsf{T}\\,\\mathbf{b}_i\n\\]\n\\[\nwhere, \\; \\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G}),\n\\]\n\\[\nY_{ij} \\mid \\mathbf{b}_i \\sim \\text{Exponential Family}(\\mu_{ij}, \\phi),\n\\]\n의 구조입니다. 직관적으로도 GLMM은 LMM+GLM임을 볼 수 있고, 당연히 \\(g(\\cdot)\\)는 link function으로, GLM과 마찬가지로 \\(\\mu_{ij} = E(Y_{ij} \\mid \\mathbf{b}_i)\\)를 적절한 링크 함수 \\(g\\)로 mapping하는 함수이며, 예시로 binomial case에서 로짓 링크(logit)를 사용하면 \\(Y_{ij}\\)는 0 또는 1 값을 가지는 이항분포가 될 수 있고, 이는 \\(\\log\\bigl(\\mu_{ij}/(1-\\mu_{ij})\\bigr)\\)를 회귀식을 표현하는 것이었습니다.\n이때, 위 식의 likelihood는\n\\[\n\\mathbf{Y}_i \\mid \\mathbf{b}_i\n\\sim \\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr),\n\\]\n로 쓸 수 있으며, \\(\\mathbf{b}_i\\)를 적분(marginalizing over \\(\\mathbf{b}_i\\))하면,\n\\[\np(\\mathbf{Y}_i)\n= \\int\n\\prod_{j=1}^{n_i}\nf\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr)\\,\n\\varphi\\bigl(\\mathbf{b}_i\\bigr)\\,\nd\\mathbf{b}_i,\n\\]\n가 최종적으로 cluster \\(i\\)에 대한 (marginal) 분포를 만들어냅니다.\n\\[\np(\\mathbf{Y}_i) = \\int \\left[ \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\right] \\varphi (b_i) db_i, \\quad \\varphi (b_i) = \\frac{1}{\\sqrt{(2\\pi)^q |\\mathbf{G}|}} \\exp \\left(-\\frac{1}{2} b_i^T \\mathbf{G}^{-1} b_i \\right).\n\\]\n모든 \\(\\mathbf{Y}_i\\)가 (조건부) 독립이라면, 전체 자료에 대한 marginal likelihood는\n\\[\nL(\\beta, \\mathbf{G}, \\phi) = \\prod_{i=1}^{K} \\int \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\varphi (b_i) db_i.\n\\]\n입니다. 문제는 \\(\\mu_{ij} (b_i)\\)가 비선형이기 때문에 적분이 closed-form으로 풀리지 않는 경우가 대부분이라는 것이고, 따라서 실제로는 이 적분을 수치적(또는 근사적)으로 계산한 뒤, 그 결과(근사치)를 최대화하여 \\(\\hat{\\beta}, \\hat{\\mathbf{G}}, \\hat{\\phi}\\)를 구하게 됩니다.\nGLMM’s parameter 추정(Marginal Likelihood & Approximation)\n\n다시 한 번 언급하자면 문제는 \\(\\mathbf{b}_i\\)가 랜덤효과이므로 이를 적분해야 한다는 것인데, \\(\\mu_{ij}(\\mathbf{b}_i)\\)가 비선형이기 때문에 이 적분이 closed-form으로 표현되지 않는 것이고, 다음과 같은 근사화 기법을 사용하여 계산합니다.\n\nLaplace Approximation\\(\\mathbf{b}_i\\) 주변에서 2차 근사를 수행하여 적분을 근사화하는 방법입니다. 한 번(1차) 또는 고차(AGQ, Adaptive Gauss-Hermite Quadrature) 버전으로 더 정확하게 시도할 수 있습니다.\nGauss-Hermite Quadrature\n적분을 수치적(Numerical)으로 가까운 근사값으로 계산합니다. 무작위 효과 차원이 높아질수록 계산량이 기하급수적으로 늘어날 수 있으므로, 실무에서는 차원이 작은 랜덤 효과 구조(예: 랜덤 인터셉트만)에서 자주 사용합니다.\nPenalized Quasi-Likelihood (PQL)\n고전적으로 제안된 근사 기법으로, GLM의 IRLS 절차를 변형하여 무작위효과를 순차적으로 추정합니다. 데이터가 크거나, 근사 정밀도가 크게 중요하지 않은 상황에서 가볍게 쓰일 수 있습니다.\n\n최종적으로, (1) 적분으로 정의된 marginal likelihood를 (2) 수치적 근사화를 통해 (3) 최적화(예: Newton-Raphson, EM 등)하여, \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\boldsymbol{G}}\\), \\(\\hat{\\phi}\\) 등을 찾습니다.\n\\[\n\\hat{\\boldsymbol{\\beta}}, \\;\\hat{\\boldsymbol{G}}, \\;\\hat{\\phi}\n= \\underset{\\boldsymbol{\\beta}, \\boldsymbol{G}, \\phi}{\\mathrm{argmax}}\n\\;\\;\\Bigl\\{\n\\prod_{i=1}^{K}\n\\int\n\\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i), \\phi\\bigr)\n\\, \\varphi(\\mathbf{b}_i)\\, d\\mathbf{b}_i\n\\Bigr\\}.\n\\]\nGLMM vs. GEE\n\n이 data간 상관관계를 고려하기 위해 개발된 두 모델을 짧게 정리해보면, GEE는 “Population-Average” 접근으로 군집 내 상관을 working correlation 방식으로 모델링하며, 완전한 joint PDF를 명시하지 않고 Quasi-likelihood처럼 추정하는 기법이었고, GLMM은 “Subject-Specific” 접근으로 군집/클러스터 효과를 무작위 효과로 모델링하여 종속변수를 (조건부) Exponential Family distribution으로 가정하고, 이 likelihood를 marginal하게 적분함으로써 추정합니다.\n3.4. GLMM parameter’s Variance\n\n마지막으로, GLMM에서의 추정된 파라미터(고정 효과 \\(\\boldsymbol{\\beta}\\), 무작위 효과 분산-공분산 행렬 \\(\\boldsymbol{G}\\) )의 분산 추정(표준 오차, 신뢰구간 등) 방법을 보겠습니다. GLMM의 경우, 근사화하여 최대화한 marginal log-likelihood에서의 헤시안 행렬(Hessian)을 기반으로 고정효과 \\(\\hat{\\boldsymbol{\\beta}}\\) 의 분산을 추정할 수 있습니다. 구체적으로, 아래와 같은 일반적 형식을 취합니다:\n\\[\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}})\n=\n\\bigl[\n  -\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}\n   \\,\\ell(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{G}}, \\hat{\\phi})\n\\bigr]^{-1},\n\\]\n여기서 \\(\\ell\\)은 GLMM의 (근사) marginal log-likelihood, \\(\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}\\)는 고정효과 파라미터 \\(\\boldsymbol{\\beta}\\)에 대한 2차 미분(Hessian)으로, 이 헤시안을 (적절한 수치 방법으로) 근사화하여 얻고, 그 역행렬이 분산 추정의 결과에 해당합니다. 이는 여전히 log likelihood을 통한 추정이기 때문에 Fisher information matrix로 분산을 추정한다고 생각하면 될 것 같습니다. GEE(2장에서)와 마찬가지로, GLMM에서도 모델의 설계에서의 작은 misspecification이 있을 가능성을 고려하여 안정적으로 Sandwich estimator를 통해 추정할 수 있는지 고민할 수 있습니다. GLMM의 경우, 군집 간 독립 이나 군집 내 random effect의 정규성 가정과 같은 가정이 크게 벗어나지 않는다고 믿으면 위 모델 기반(model-based) 추정 분산을 사용하면 되고, 그렇지 않은 “무작위 효과 분포가 정규가 아닐 가능성” 혹은 “link/variance function 형태가 부정확할 가능성” 등을 고려하기 위해 적절한 샌드위치 추정(sandwich-type variance) 기법을 시도할 수도 있습니다. 다만, GEE와 달리 GLMM에서의 robust variance estimation은 쉽게 구현되지 않으며, 근사기법, 부트스트랩(bootstrap) 등을 통해 대안적으로 접근하는 사례도 많습니다.\nrandom effect의 분산-공분산 행렬 \\(\\boldsymbol{G}\\) 또한 우도(또는 제한 우도)에서 편미분이 0 조건을 이용하여 추정하지만, 그 표준 오차(불확실성)를 추정하는 과정 역시 (1) 2차 미분, (2) 프로파일(profile) likelihood, (3) 수치적 근사화 등을 거쳐야 합니다."
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#r-코드-예제-gee-glmm",
    "href": "posts/2025-02-28-reg3/index.html#r-코드-예제-gee-glmm",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "4. R 코드 예제: GEE, GLMM",
    "text": "4. R 코드 예제: GEE, GLMM\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요.\n\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary &lt;- ifelse(Orthodont$distance &gt; 25, 1, 0)  # 이항 변환\n#\n## GEE 모델 적합 (Exchangeable 상관 구조)\n#library(geepack)\n#gee_fit &lt;- geeglm(binary ~ age + Sex,\n#                 id = Subject,          # 클러스터 변수\n#                 data = Orthodont,\n#                 family = binomial,\n#                 corstr = \"exchangeable\")  # 동일 상관 가정\n#summary(gee_fit)  # 결과 출력\n#\n## GLMM 모델 적합 (랜덤 절편 모델)\n#library(lme4)\n#glmm_fit &lt;- glmer(binary ~ age + Sex + (1|Subject),  # 랜덤 절편\n#                 data = Orthodont,\n#                 family = binomial)\n#summary(glmm_fit)  # 결과 출력"
  },
  {
    "objectID": "posts/2025-02-28-reg3/index.html#마무리하며",
    "href": "posts/2025-02-28-reg3/index.html#마무리하며",
    "title": "Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation",
    "section": "마무리하며",
    "text": "마무리하며\n이번 장에서는 M-estimation 개념부터 시작하여, GLM이 어떻게 “estimating equation”의 한 사례로 해석되는지, GEE가 GLM을 확장하여 상관구조를 모델링하고, robust 분산을 제공함으로써 군집/반복측정 데이터를 다루는 과정을, GLMM이 임의효과를 통해 계층적 구조를 명시적으로 모델링하는 방식을 자세히 살펴보았습니다. 그리고 샌드위치 추정량(robust variance) 형태가 M-estimation의 일반 이론에서 비롯된다는 점도 수식과 함께 설명했습니다.\n정리하자면, M-estimation은 MLE, OLS, GEE, GLMM 모두를 포괄하는 추정 이론적 틀로서, 샌드위치 분산은 그 점근 정규성(Asymptotic Normality)의 결과물이며, GEE는 marginal mean에 주목하고 robust한 표준오차를 산출해주는 반면, GLMM은 임의효과를 통해 개체별(군집별) 차이를 직접 모델링합니다. 실제 데이터 분석에서는 연구 목적(개체별 효과 추정 vs 전체 평균 효과 추정), 데이터 특성(정확한 상관 구조 가정 vs 모형 가정의 유연성) 등을 종합하여 GEE와 GLMM 중 적절한 접근을 택하거나 비교하는 것이 중요합니다. 사실 Regression Model에는 이번 블로그 “Exploring Regression Models for Regression Analysis”에서 다룬 모델들을 제외하고도 아주 다양한 철학과 수식을 가진 모델들이 있습니다. 다만 여기서는 의학 분석에서 자주 사용되는 모델을 다루었으며, 이를 어느 정도 이해하셨다면 이외의 모델을 이해하는 데에 부족함이 없을 것이라고 생각합니다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html",
    "href": "posts/2025-03-18-Kappa/index.html",
    "title": "Kappa 분석 이해하기",
    "section": "",
    "text": "Kappa 통계는 두 명 이상의 평가자(rater)가 범주형 데이터를 얼마나 일관되게 평가하는지를 측정하는 방법이다. 단순한 일치율과 달리, Kappa 계수는 무작위로 일치할 가능성을 보정하여 보다 신뢰할 수 있는 평가 일치도를 제공한다.이는 평가자의 주관적인 판단이 개입되는 연구에서 필수적이라고 볼 수 있다.\nKappa 분석에는 여러 가지 변형이 있으며, 상황에 따라 적절한 방법을 선택해야 한다. 대표적인 Kappa 분석 방법은 다음과 같다:\n\nCohen’s Kappa (κ): 두 명의 평가자가 범주형 데이터를 평가할 때 사용\nCohen’s Weighted Kappa: 평가자 간의 불일치 정도를 가중치로 고려할 때 사용 (순위형 변수)\nFleiss’ Kappa: 두 명 이상의 평가자가 있을 때 사용 (범주형 변수)\nGeneralized Fleiss’ Kappa: Fleiss’ Kappa의 확장판으로, 순위형 데이터를 다룰 때 사용\nKrippendorff’s Alpha: 범주형, 순위형, 연속형 데이터 모두 적용 가능\n\nCohen’s와 Cohen’s Weighted는 평가자가 두 명일때 적용되고, Fleiss’와 Generalized Fleiss’는 두 명 이상일 때 사용된다.\n이 글에서는 R을 활용하여 다양한 Kappa 분석 방법을 구현하는 방법을 설명한다.\n\n\n단순한 일치율(Percent Agreement)은 평가자 간의 동일한 판단이 나온 비율을 계산하는 방식이다. 하지만, 이는 무작위로 일치한 경우도 포함하기 때문에 신뢰도가 낮을 수 있다.\n예를 들어, 두 평가자가 100개의 사례를 평가했을 때, 70개에서 동일한 판단을 내렸다면 단순 일치율은 70%이다. 하지만, 무작위로도 70%의 일치가 나올 가능성이 있다면, 실제 평가자의 일치 정도를 과대평가할 수 있다.\n이를 보완하기 위해 Kappa 계수(\\(κ\\))는 무작위 일치율(Expected Agreement)을 고려하여 조정된 값을 제공한다. 즉, Kappa 계수는 실제 일치율과 무작위 일치율 간의 차이를 기반으로 계산되며, 0~1 사이의 값으로 표현된다.\n\nKappa 계수(\\(κ\\)) 해석:\n\n\\(κ\\) = 1: 완벽한 일치\n\\(κ\\) = 0: 무작위 일치 수준\n\\(κ\\) &lt; 0: 평가자가 오히려 무작위보다 더 불일치\n0.6 ≤ \\(κ\\) ≤ 0.8: 상당한 일치\n0.4 ≤ \\(κ\\) &lt; 0.6: 중간 수준의 일치\n\n\n따라서 \\(κ\\)의 값은 크면 클수록 좋은 것이라고 본다.\n이제 Kappa 분석이 왜 중요한지를 이해했으므로, 다음 섹션에서는 각 Kappa 분석 방법을 살펴보고, R을 이용하여 실제 데이터를 분석하는 방법을 설명한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#단순-일치율-vs.-kappa-계수",
    "href": "posts/2025-03-18-Kappa/index.html#단순-일치율-vs.-kappa-계수",
    "title": "Kappa 분석 이해하기",
    "section": "",
    "text": "단순한 일치율(Percent Agreement)은 평가자 간의 동일한 판단이 나온 비율을 계산하는 방식이다. 하지만, 이는 무작위로 일치한 경우도 포함하기 때문에 신뢰도가 낮을 수 있다.\n예를 들어, 두 평가자가 100개의 사례를 평가했을 때, 70개에서 동일한 판단을 내렸다면 단순 일치율은 70%이다. 하지만, 무작위로도 70%의 일치가 나올 가능성이 있다면, 실제 평가자의 일치 정도를 과대평가할 수 있다.\n이를 보완하기 위해 Kappa 계수(\\(κ\\))는 무작위 일치율(Expected Agreement)을 고려하여 조정된 값을 제공한다. 즉, Kappa 계수는 실제 일치율과 무작위 일치율 간의 차이를 기반으로 계산되며, 0~1 사이의 값으로 표현된다.\n\nKappa 계수(\\(κ\\)) 해석:\n\n\\(κ\\) = 1: 완벽한 일치\n\\(κ\\) = 0: 무작위 일치 수준\n\\(κ\\) &lt; 0: 평가자가 오히려 무작위보다 더 불일치\n0.6 ≤ \\(κ\\) ≤ 0.8: 상당한 일치\n0.4 ≤ \\(κ\\) &lt; 0.6: 중간 수준의 일치\n\n\n따라서 \\(κ\\)의 값은 크면 클수록 좋은 것이라고 본다.\n이제 Kappa 분석이 왜 중요한지를 이해했으므로, 다음 섹션에서는 각 Kappa 분석 방법을 살펴보고, R을 이용하여 실제 데이터를 분석하는 방법을 설명한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#cohens-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#cohens-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "1.1 Cohen’s Kappa 공식",
    "text": "1.1 Cohen’s Kappa 공식\nCohen’s Kappa는 다음과 같은 공식으로 계산된다:\n\\[\nκ = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\): 평가자간 일치 확률 (Observed Accuracy)\n\\(P_e\\): 우연히 일치된 평가를 받을 비율 (Expected Accuracy)\n\n여기서 \\(P_e\\)는 ’우연히 일치할 확률’을 나타낸다. 예를 들어, 두 명의 상담사가 환자에게 우울증이 있는지 없는지에 대해 완전히 무작위로 판단했다고 가정한다. 마치 동전을 던지는 것처럼 말이다. 그럼에도 불구하고 두 상담사가 우연히 같은 결론을 내릴 가능성이 어느 정도 존재하게 되는데, 이 우연에 의한 일치 확률을 나타내는 값이 바로 \\(P_e\\)가 된다. 즉, \\(P_e\\)는 평가자들이 실제로 동의한 정도가 아니라, 순전히 우연으로 평가 결과가 같아질 가능성을 나타내는 가상의 확률이라고 이해하면 된다. \\(P_e\\)의 비율이 높을 수록 우연하게 일치한다는 것이고, 이 값이 최소에 가까워질수록 높은 \\(κ\\)의 값을 얻을 수 있게 된다.\n\nObserved Accuracy \\(P_o\\)\n\\[\nP_0 = \\frac{1}{n} \\sum_{i=1}^{g} f_{ii}\n\\]\n\n\\(n\\): 전체 평가 개수\n\\(g\\): 평가 범주의 개수 (예: 3개의 등급, 5개의 점수 등)\n\\(f_{ii}\\): 평가자 두 명이 동일한 범주를 선택한 횟수\n\n즉, \\(P_0\\)는 전체 평가 중에서 평가자들이 동일한 범주를 선택한 비율을 의미한다. 이는 실제 데이터에서 평가자들이 얼마나 일치했는지를 보여주는 값이다.\n\n\nExpected Accuracy \\(P_e\\)\n\\[\nP_e = \\frac{1}{n^2} \\sum_{i=1}^{g} f_{i+} f_{+i}\n\\]\n\n\\(f_{i+}\\): 특정 범주의 행 합 (첫 번째 평가자가 해당 범주를 선택한 횟수)\n\\(f_{+i}\\): 특정 범주의 열 합 (두 번째 평가자가 해당 범주를 선택한 횟수)\n\n\\(P_e\\)는 평가자들이 무작위로 평가했을 때 동일한 범주를 선택할 확률을 의미한다. 이는 두 평가자가 특정 범주를 선택할 확률을 각각 곱하여 계산되며, 모든 범주에 대해 합산하여 전체적인 기대 일치도를 구하는 방식이다.\n\n\nBinary Classifications\nCohen’s Kappa는 이진 분류에서도 모델의 예측 신뢰도를 평가하는 중요한 지표로 활용될 수 있다. 이는 다음과 같은 공식으로 표현된다.\n\\[\nκ = \\frac{2 \\times (TP \\times TN - FN \\times FP)}\n{(TP + FP) \\times (FP + TN) + (TP + FN) \\times (FN + TN)}\n\\]\n여기서 각 항목은 다음과 같은 의미를 가진다:\n\n\\(TP\\) (True Positives): 실제로 긍정(positive)인 경우를 정확하게 예측한 수\n\\(FP\\) (False Positives): 실제로는 부정(negative)이지만, 긍정으로 잘못 예측한 수\n\\(TN\\) (True Negatives): 실제로 부정인 경우를 정확하게 예측한 수\n\\(FN\\) (False Negatives): 실제로는 긍정이지만, 부정으로 잘못 예측한 수\n\n이진 분류에서 Cohen’s Kappa는 단순한 정확도보다는 무작위 예측과 비교하여 모델이 얼마나 신뢰할 만한지를 측정하는 데 유용하다. 예를 들어, 불균형한 데이터에서 단순한 정확도는 높은 값이 나올 수 있지만, Kappa 값이 낮게 나오는 경우가 있을 수 있다. 이는 모델이 특정 클래스를 과도하게 예측하고 있음을 나타낼 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-1",
    "href": "posts/2025-03-18-Kappa/index.html#example-1",
    "title": "Kappa 분석 이해하기",
    "section": "1.2 Example 1",
    "text": "1.2 Example 1\n첫 번째 예시에서는 두 평가자가 다섯 개 항목을 각각 평가한 뒤, Cohen’s Kappa 통계량을 이용해 두 평가자 간의 일치도를 분석했다. R에서 Cohen’s Kappa를 구하기 위해서는 irr 패키지를 사용한다 (Desc Tools, psych 등 다른 패키지도 존재).\nlibrary(irr)\n\n# 두 평가자가 5개의 항목을 평가\nratings &lt;- data.frame(\n  rater1 = c(\"A\", \"A\", \"B\", \"A\", \"C\"),\n  rater2 = c(\"A\", \"B\", \"B\", \"A\", \"C\")\n)\n\n# Cohen's Kappa 계산\nresult &lt;- kappa2(ratings, weight = \"unweighted\")\nprint(result)\n출력:\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 5 \n   Raters = 2 \n    Kappa = 0.688 \n\n        z = 2.28 \n  p-value = 0.0224\n이 예시의 결과는 Kappa 값이 0.688로 나타났고, z 통계량은 2.28, p값은 0.0224로 나타나 통계적으로 유의미한 일치도를 보였다. 이는 두 평가자가 단순히 우연히 일치하는 것을 넘어 실제로 상당히 일치된 평가를 내렸다는 의미로 해석할 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-2",
    "href": "posts/2025-03-18-Kappa/index.html#example-2",
    "title": "Kappa 분석 이해하기",
    "section": "1.3 Example 2",
    "text": "1.3 Example 2\n다음 예시는 실제 의료 환경에서 자주 발생하는 사례를 바탕으로 Cohen’s Kappa를 적용한 것이다. 두 명의 영상의학 전문의가 CT, MRI, PET, X-ray 총 4가지 영상진단 방식으로 환자를 각각 독립적으로 평가했을 때, 두 전문의 간 평가가 얼마나 일치하는지를 분석한다.\nlibrary(ggplot2); library(officer)\n\nimaging.modalities &lt;- c(\"CT\", \"MRI\", \"PET\", \"Xray\")\n\nkappa.results &lt;- sapply(imaging.modalities, function(modality){\n  var1 &lt;- paste0(modality, \"_Rater1\")\n  var2 &lt;- paste0(modality, \"_Rater2\")\n\n  kappa_calc &lt;- irr::kappa2(diagnosis_data[, .SD, .SDcols = c(var1, var2)], weight = \"unweighted\")\n  standard_error &lt;- kappa_calc$value / kappa_calc$statistic\n  conf_interval &lt;- c(kappa_calc$value - qnorm(0.975) * standard_error,\n                     kappa_calc$value + qnorm(0.975) * standard_error)\n\n  return(paste0(round(kappa_calc$value, 3), \" (95% CI: \", round(conf_interval[1], 3),\n                \"-\", round(conf_interval[2], 3), \")\"))\n})\n이 예시에서 계산된 Kappa 값은 각 영상진단 방식별로 제공되며, 각 값에 대한 표준오차와 95% 신뢰구간도 함께 계산된다. irr::kappa2 함수를 통해 두 평가자 간의 Cohen’s Kappa 값을 계산했고, weight = “unweighted” 옵션으로 평가 항목 간의 차이에 동일한 가중치를 부여한다.\n이와 같이 Cohen’s Kappa는 두 평가자가 같은 값을 측정했는지 여부를 고려하지만, 불일치의 정도는 고려하지 않는다. Example 1을 보면 Cohen’s Kappa는 평가자가 A와 C를 선택했을 때와 A와 B를 선택했을 때를 동일한 불일치로 간주하고 있는 것을 확인할 수 있다. 그렇다면 범주형 변수가 아닌 순위형 변수가 있다면 어떨까?"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#weighted-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#weighted-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "2.1 Weighted Kappa 공식",
    "text": "2.1 Weighted Kappa 공식\n\\[\n\\kappa_w = \\frac{P_o - P_e}{1 - P_e}\n\\]\n이 식은 일반적인 Cohen’s Kappa의 공식과 같지만, Weighted Kappa의 경우 Observed Accuracy \\(P_o\\)와 Expected Accuracy \\(P_e\\)을 계산할 때 가중치를 적용한 값을 사용한다는 점에서 차이가 있다. 각 항목은 다음과 같이 계산한다.\n\nObserved Accuracy (\\(P_o\\))\n\\(P_o\\)는 두 평가자의 실제 평가 결과를 바탕으로 각 범주 간에 가중치를 적용하여 계산한 값이다.\n\\[\nP_{o} = \\sum_{i}\\sum_{j} W_{ij}P_{ij}\n\\]\n\n\\(W_{ij}\\) : 각 범주(i,j) 간의 가중치\n\\(P_{ij}\\) : 두 평가자가 범주 (i,j)를 선택한 관측 비율\n\n\n\nExpected Accuracy (\\(P_e\\))\n\\(P_e\\)는 각 평가자의 범주별 평가 확률의 곱에 가중치를 곱하여 계산된다.\n\\[\nP_e = \\sum_{i}\\sum_{j} W_{ij}P_{i+}P_{+j}\n\\]\n\n\\(P_{i+}\\) : 평가자 1이 범주 i를 선택한 전체 비율 (행 방향)\n\\(P_{+j}\\) : 평가자 2가 범주 j를 선택한 전체 비율 (열 방향)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#가중치w의-종류",
    "href": "posts/2025-03-18-Kappa/index.html#가중치w의-종류",
    "title": "Kappa 분석 이해하기",
    "section": "2.2 가중치(\\(W\\))의 종류",
    "text": "2.2 가중치(\\(W\\))의 종류\nWeighted Kappa에서 주로 사용하는 대표적인 가중치 부여 방식은 두 가지다: 선형(linear)과 제곱(quadratic)\n\n선형 가중치 (Linear weights)\n선형 가중치는 Cicchetti-Allison weights라고도 하며, 평가 항목 간의 불일치 정도에 따라 일정한 간격으로 가중치를 부여한다. 즉, 두 평가자 간의 평가가 한 단계씩 멀어질 때마다 일정한 비율로 일치도가 감소한다. 특징은 각 평가 간의 차이에 비례하여(선형적으로) 가중치를 부여한다는 것이다.\n수식 표현:\n\\[\nW_{ij}^{linear} = 1 - \\frac{|i - j|}{k - 1}\n\\]\n\n\\(i\\), \\(j\\): 두 평가자가 선택한 범주(단계)\n\\(k\\): 범주의 전체 개수\n\n예시:\n범주가 1~4단계로 구성된 경우 (\\(k\\)=4):\n\n평가자가 각각 1단계와 2단계를 선택했다면:\n\n\\[\nW_{12}^{linear} = 1 - \\frac{|1 - 2|}{4 - 1} = \\frac{2}{3} \\approx 0.67\n\\]\n이는 평가자들이 약 67% 정도로 일치하고 있다고 볼 수 있으며, 33%는 불일치한다고 해석할 수 있다. 이 결과를 제곱 가중치를 적용했을 때와 비교해 본다.\n\n\n제곱 가중치 (Quadratic weights)\n제곱 가중치는 Fleiss-Cohen weights라고도 하며, 평가 항목 간의 불일치가 클수록 가중치를 제곱에 비례하여(비선형적으로) 부여한다. 특히 평가자 간의 작은 불일치는 가볍게, 큰 불일치는 더 무겁게 여긴다. 즉, 불일치의 정도가 증가할수록 가중치는 비선형적으로(제곱의 비율로) 감소하게 된다.\n수식 표현:\n\\[\nW_{ij}^{quadratic} = 1 - \\frac{(i - j)^2}{(k - 1)^2}\n\\]\n\n\\(i\\), \\(j\\): 두 평가자가 선택한 범주(단계) - (\\(k\\)): 범주의 전체 개수\n\n예시:\nLinear의 예시와 마찬가지로 범주가 1~4단계로 구성된 경우(\\(k\\)=4):\n\n평가자가 각각 1단계와 2단계를 선택했다면:\n\n\\[\nW_{12}^{quadratic} = 1 - \\frac{(1 - 2)^2}{(4 - 1)^2} = 1 - \\frac{1}{9} = \\frac{8}{9} \\approx 0.89\n\\]\n이는 평가자들이 약 89%로 상당히 높은 일치도를 나타내며, 한 단계만 차이가 나기에 작은 불일치 정도에 높은 가중치를 부여한 것이다. 그러나 두 단계 이상의 큰 차이가 발생하면 가중치가 급격히 감소하여, 큰 불일치로 인식을 한다.\n이와 같이 Quadratic 방식에서는 큰 불일치를 더욱 엄격하게 평가한다.\n\n\n가중치 선택 방법\n\n선형 가중치(Linear weights)는 모든 단계 간의 차이를 동일한 중요도로 평가할 때 사용한다.\n\n진단 결과가 1단계에서 2단계로 바뀌는 것과, 2단계에서 3단계로 바뀌는 것의 중요도가 같은 경우\n\n제곱 가중치(Quadratic weights)는 작은 차이보다 큰 차이에 더 큰 중요성을 부여할 때 적합하다.\n\n1단계와 2단계 간의 차이는 그리 크지 않지만, 2단계와 3단계 간의 차이는 매우 큰 의미를 가지는 경우\n\n\n결국, 분석하려는 데이터와 평가 기준의 특성에 따라 적절한 가중치를 선택하면 된다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-1-1",
    "href": "posts/2025-03-18-Kappa/index.html#example-1-1",
    "title": "Kappa 분석 이해하기",
    "section": "2.3 Example 1",
    "text": "2.3 Example 1\nWeighted Kappa는 R코드를 활용해 수월하게 구할 수 있다.\n아래는 두 명의 영상의학 전문의가 MRI 영상을 보고 병변의 심각도를 5단계 척도(1: 정상, 2: 경미, 3: 중등도, 4: 심함, 5: 매우 심함)로 평가한 경우를 예로 든 것이다. 두 평가자 간의 일치도를 Weighted Kappa를 사용하여 분석하며, 선형(linear) 가중치를 적용하여 평가 간의 차이를 부분적으로 반영한다.\nlibrary(irr)\n\n# 평가 데이터 예시 (랜덤하게 생성)\nset.seed(123)\nlesion_assessment &lt;- data.frame(\n  Rater1 = sample(1:5, 100, replace = TRUE),\n  Rater2 = sample(1:5, 100, replace = TRUE)\n)\n\n# 두 평가자 간의 Weighted Cohen's Kappa\nkap &lt;- irr::kappa2(lesion_assessment, weight = \"equal\")\nse &lt;- kap$value / kap$statistic\nci_lower &lt;- kap$value - qnorm(0.975) * se\nci_upper &lt;- kap$value + qnorm(0.975) * se\n\nweighted_kappa_result &lt;- paste0(round(kap$value, 3), \" (95% CI: \", round(ci_lower, 3), \"~\", round(ci_upper, 3), \")\")\nweighted_kappa_result\n이 예시를 통해 두 평가자 간의 의견 일치 수준을 파악할 수 있다.위 코드 실행 결과로 선형 가중치를 사용한 Weighted Kappa 값을 확인할 수 있으며, 범주 간의 불일치 정도를 반영한 평가자 간 일치도를 평가할 수 있다. 여기서 quadratic 가중치를 부여하기 위해서는 weight = “squared”로 수정하면 된다.\n출력:\n[1] \"0.048 (95% CI: -0.089~0.185)\"\n처음에 설명했던 것 처럼, Cohen’s Kappa 값이 0.048이라는 것은 두 평가자가 거의 무작위로 평가를 했거나, 일치도가 매우 낮다는 것을 의미한다. Example의 결과 값으로 0.048로 나타났기 때문에 이런 경우 평가자 간의 의견이 거의 일치하지 않는다고 결론을 내린다.\n신뢰구간을 살펴보면 -0.089에서 0.185 사이에 위치하고 있다. 95% 신뢰구간은 실제 모집단에서의 Kappa 값이 이 범위 안에 있을 확률이 95%라는 뜻이다. 하지만 이 신뢰구간에는 음수 값(-0.089)이 포함되어 있으므로 일반적으로 평가자 간의 일치도가 단순한 우연보다도 낮다는 뜻이다. 이는 평가자들이 무작위로 답변한 것보다도 더 일치도가 낮을 가능성이 있다는 것이며, 신뢰구간이 넓다는 것은 데이터가 불안정하거나, 평가자 간의 일치도가 일정하지 않다는 것을 나타낸다.\n예시에서 이러한 결과가 나오는 이유는 평가 데이터 자체가 랜덤하게 생성되었기 때문이다. 그러므로 당연히 통계적으로 신뢰하기 어려운 결과가 나온다. 실제 상황에서 이러한 Kappa 값이 나온다면, 평가 기준을 명확하게 정리하고, 데이터의 질을 개선하는 것이 필요할 것이다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#fleiss-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#fleiss-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "3.1 Fleiss’ Kappa 공식",
    "text": "3.1 Fleiss’ Kappa 공식\nFleiss’ Kappa(\\(\\kappa\\))를 정의하는 수식은 다음과 같다.\n\\[\n\\kappa = \\frac{\\bar{P_o} - \\bar{P_e}}{1 - \\bar{P_e}}\n\\]\n\n\\(\\bar{P_o}\\): 관찰된 평균 일치율 (Observed agreement)\n\\(\\bar{P_e}\\): 우연에 의한 평균 일치율 (Expected agreement)\n\n이와 같이 Fleiss’ Kappa는 관찰된 평균 일치율(\\(\\bar{P}_0\\))과 우연에 의한 평균 일치율(\\(\\bar{P}_e\\))을 사용하여 계산한다. 각각의 공식은 아래와 같다.\n\nObserved agreement \\(\\bar{P}_o\\):\n\\(\\bar{P}_o\\)는 평가자들이 실제로 얼마나 의견이 일치했는지를 측정하는 값인데, 이는 실제 평가에서 동일한 범주를 선택한 평가자들의 비율을 정량적으로 나타내는 값이며, 다음과 같은 수식으로 계산된다.\n\\[\n\\bar{P}_o = \\frac{1}{N n (n - 1)}\\left(\\sum_{i=1}^{N}\\sum_{j=1}^{k}n_{ij}^{2} - N n\\right)\n\\] - \\(N\\): 평가 항목의 총 개수 - \\(n\\): 각 대상당 평가자의 수 - \\(k\\): 평가 범주의 수 - \\(n_{ij}\\): \\(i\\)번째 대상에서 \\(j\\)번째 카테고리를 선택한 평가자의 수\n이 공식에서 첫 번째 항인 \\(\\sum_{i=1}^{N}\\sum_{j=1}^{k}n_{ij}^{2}\\)는 특정 범주를 선택한 평가자 수를 제곱하고 합산한다. 동일한 범주를 선택한 평가자가 많을수록 그 값이 더 커지며, 평가자들이 일관된 결정을 내릴수록 이 항의 값이 증가하게 된다. 이 식에서 \\(Nn\\)을 빼는 이유는, \\(\\sum n_{ij}^2\\) 항에는 평가자가 한 명만 특정 범주를 선택한 경우도 포함되기 때문이다. 평가자 수가 많을수록 이 값이 증가하므로, 이를 보정하기 위해 전체 평가 대상 개수(\\(N\\))와 평가자 수(\\(n\\))를 곱한 값을 빼준다. 이를 통해, 평가자가 많을수록 발생할 수 있는 단순한 일치 효과를 제거하고, 실제로 의미 있는 평가자 간의 일치도를 측정할 수 있도록 조정한다.\n마지막으로, 이 값을 \\(N n (n - 1)\\)로 나누어 정규화한다. 이는 전체 평가 수에 대해 평균을 구하는 과정이며, 결과적으로 Observed Agreement는 “평균적인 일치도”를 나타내는 값이 된다. 이 값이 클수록 평가자들이 동일한 평가를 내린 비율이 높다는 것을 의미하며, 평가자 간의 의견이 더욱 일관되게 나타난다는 것을 보여준다. 이 값은 Expected Agreement(\\(\\bar{P}_e\\))와 비교하여 Fleiss’ Kappa 값을 계산하는 핵심 요소다.\n\n\nExpected agreement by chance \\(\\bar{P_e}\\)\n\\(\\bar{P_e}\\)은 평가자들이 평가를 무작위로 진행했을 때 결과가 우연히 일치하게 될 확률을 나타낸다.\n\\[\n\\bar{P}_e = \\sum_{j=1}^{k} P_j^{2}\n\\]\n여기서 \\(P_j\\)는 모든 대상과 평가자를 통틀어 \\(j\\)번째 카테고리에 선택된 횟수의 합을 전체 평가 횟수(\\(N \\times n\\))로 나눈 값이다. 이를 식으로 표현할 수 있다.\n\\[\nP_j = \\frac{1}{Nn} \\sum_{i=1}^{N} n_{ij}\n\\] 이 공식에서 \\(\\sum_{i=1}^{N} n_{ij}\\)는 모든 평가 대상에서 특정 범주 \\(j\\)가 선택된 총 횟수를 의미하며, 이를 전체 평가 횟수(\\(N \\times n\\))로 나누어 특정 범주 \\(j\\)가 선택될 확률을 구한다.\n무작위로 평가를 했다고 가정하면, 한 평가자가 특정 범주 \\(j\\)를 선택할 확률은 \\(P_j\\)이고, 또 다른 평가자도 동일한 범주를 선택할 확률 역시 \\(P_j\\)이므로, 이 두 확률을 곱한 \\(P_j^2\\)이 해당 범주에서 평가가 우연히 일치할 확률이 된다. 따라서 모든 범주 \\(j=1\\)부터 \\(k\\)까지에 대해 이러한 우연 일치 확률을 더하면, 전체적으로 평가자들이 무작위로 평가했을 때 기대되는 평균적인 일치 확률 \\(\\bar{P}_e\\)을 계산할 수 있다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example",
    "href": "posts/2025-03-18-Kappa/index.html#example",
    "title": "Kappa 분석 이해하기",
    "section": "3.2 Example",
    "text": "3.2 Example\n다음 예시 데이터로 Fleiss’ Kappa의 개념을 살펴본다.\n\n\n\n항목\n범주1\n범주2\n범주3\n\n\n\n\n1\n0\n0\n5\n\n\n2\n0\n1\n4\n\n\n3\n1\n0\n4\n\n\n4\n0\n2\n3\n\n\n5\n0\n1\n4\n\n\n\n\n평가 항목 수: \\(N = 5\\)\n평가자 수: \\(n = 5\\) (각 항목마다 평가자가 5명)\n평가 범주 수: \\(k = 3\\)\n\nR 코드로 구하는 방식:\n# `irr` 패키지의 `kappam.fleiss()` 함수를 사용한다\ninstall.packages(\"irr\")\nlibrary(irr)\n\n# 데이터 행렬 생성\nratings &lt;- matrix(c(\n  0, 0, 5,\n  0, 1, 4,\n  1, 0, 4,\n  0, 2, 3,\n  0, 1, 4), \n  nrow = 5, byrow = TRUE)\n\n# Fleiss' Kappa 계산\nfleiss_kappa &lt;- kappam.fleiss(ratings)\nprint(fleiss_kappa)\n출력:\n Fleiss' Kappa for m Raters\n\n Subjects = 5 \n   Raters = 3 \n    Kappa = -0.25 \n\n        z = -1.83 \n  p-value = 0.067 \nCohen’s Kappa와 같이 Fleiss’ Kappa는 평가 값이 동일한 경우 1, 다르면 0으로 단순 비교한다. 순위형 변수로 Weight를 부여해야 할 경우, Generalized Fleiss’ Kappa를 사용하면 된다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#generalized-fleiss-kappa-공식",
    "href": "posts/2025-03-18-Kappa/index.html#generalized-fleiss-kappa-공식",
    "title": "Kappa 분석 이해하기",
    "section": "4.1 Generalized Fleiss’ Kappa 공식",
    "text": "4.1 Generalized Fleiss’ Kappa 공식\nGeneralized Fleiss’ Kappa (\\(\\kappa_G\\))의 수식은 다음과 같다.\n\\[\n\\kappa_{G} = \\frac{P_o - P_e}{1 - P_e}\n\\]\n\n\\(P_o\\): 관찰된 가중 평균 일치율 (Observed weighted agreement)\n\\(P_e\\): 우연히 기대되는 가중 평균 일치율 (Expected weighted agreement by chance)\n\n각 항목의 계산법은 아래와 같다.\n\nObserved weighted agreement \\(P_o\\)\n\\[\nP_o = \\frac{1}{\\sum_{i=1}^{N} n_i(n_i - 1)} \\sum_{i=1}^{N}\\sum_{j=1}^{k}\\sum_{l=1}^{k} W_{jl} \\cdot n_{ij}(n_{il}-\\delta_{jl})\n\\] - \\(N\\): 평가된 항목(대상)의 총 개수 - \\(k\\): 평가 범주의 수 - \\(n_i\\): \\(i\\)번째 항목을 평가한 평가자의 수 - \\(n_{ij}\\): \\(i\\)번째 항목을 \\(j\\)번째 범주로 평가한 평가자의 수 - \\(W_{jl}\\): 범주 간 가중치 - \\(\\delta_{jl}\\): 범주가 같으면 1, 다르면 0인 값\nGeneralized Fleiss’ Kappa에서 Observed Weighted Agreement \\(P_o\\)는 평가자들이 실제로 얼마나 일치했는지를 측정하는 값이다. 기존 Fleiss’ Kappa가 단순한 평가 일치율을 계산하는 방식이라면, Generalized Fleiss’ Kappa는 평가 값 사이의 차이를 반영하여 가중치를 적용하는 방식으로 평가자 간의 일치도를 보다 정교하게 측정한다.\n\\(P_o\\)는 평가자들이 동일한 평가를 내린 정도를 가중(weighted) 방식으로 계산하며, 전체 평가 대상에서 발생한 모든 평가 쌍에 대한 가중 평균을 구하는 방식으로 정의된다. 먼저, 전체 평가자들이 내린 평가 쌍의 총 개수는 \\(\\sum_{i=1}^{N} n_i(n_i - 1)\\)로 계산된다. 여기서 \\(N\\)은 평가 대상의 개수이며, \\(n_i\\)는 특정 평가 대상에 대해 평가를 수행한 평가자 수이다. 평가 대상마다 평가자 수가 다를 수 있으므로 이를 반영하여 전체적인 합을 계산한다.\n식에서 그 외의 부분은 평가자 간의 일치도를 계산한다. \\(W_{jl}\\)은 범주 \\(j\\)와 범주 \\(l\\) 사이의 가중치로, 두 평가 값이 얼마나 다른지를 수치적으로 반영하는 역할을 하는데, 앞서 설명한 linear 또는 quadratic 방식으로 설정된다. \\(n_{ij}\\)는 평가 대상 \\(i\\)에서 범주 \\(j\\)를 선택한 평가자의 수를 의미하며, \\(n_{il}\\)은 동일한 평가 대상에서 범주 \\(l\\)을 선택한 평가자의 수를 나타낸다. 또한 \\(\\delta_{jl}\\)은 Kronecker Delta로, \\(j\\)와 \\(l\\)이 동일한 경우 1, 다르면 0을 반환하는 지표이다.\n\\(P_o = 1\\)이면 평가자들이 완벽하게 동일한 평가를 내린 경우이며, \\(P_o = 0\\)이면 평가자 간 평가가 완전히 무작위로 이루어진 경우를 의미한다.\n\n\nExpected weighted agreement by chance \\(P_e\\)\n\\[\nP_e = \\frac{1}{\\left(\\sum_{i=1}^{N} n_i\\right)^2 - \\sum_{i=1}^{N} n_i} \\sum_{j=1}^{k}\\sum_{l=1}^{k} W_{jl}\\left(\\sum_{i=1}^{N}n_{ij}\\right)\\left(\\sum_{i=1}^{N}n_{il}-\\delta_{jl}\\right)\n\\] \\(P_e\\)**는 Fleiss’ Kappa에서 Expected agreement by chance (\\(P_e\\))를 확장한 개념으로, 평가 값들 사이의 거리를 고려하여 가중(weighted) 방식으로 측정한다.\n\\(\\left(\\sum_{i=1}^{N} n_i\\right)^2 - \\sum_{i=1}^{N} n_i\\)는 전체 평가 데이터에서 발생할 수 있는 모든 평가 쌍의 개수를 나타낸다. 여기서 \\(\\sum_{i=1}^{N} n_i\\)는 전체 평가자가 수행한 총 평가 개수이며, 이를 제곱한 값에서 자기 자신과의 비교를 제외하기 위해 \\(\\sum_{i=1}^{N} n_i\\)를 빼준다. 이는 평가자들이 임의로 범주를 선택했을 때 가능한 모든 평가 쌍의 개수를 정규화하는 역할을 한다.\n\\(\\sum_{i=1}^{N} n_{ij}\\)와 \\(\\sum_{i=1}^{N} n_{il}\\)은 특정 범주 \\(j\\)와 \\(l\\)이 전체 평가에서 각각 몇 번 선택되었는지를 나타낸다. 이 값에 가중치 행렬 \\(W_{jl}\\)을 곱하는데, \\(W_{jl}\\)은 범주 \\(j\\)와 범주 \\(l\\) 사이의 거리를 반영하는 값으로, linear 또는 quadratic 가중치로 결정된다.마지막으로, \\(\\delta_{jl}\\)는 두 범주가 동일할 경우 1, 다를 경우 0을 갖는 함수다.\n결과적으로 \\(P_e\\)를 \\(P_o\\)와 비교하여 Kappa 값이 계산되며, \\(P_o\\)가 \\(P_e\\)보다 클수록 평가자 간의 신뢰도가 높다는 것을 의미한다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-3",
    "href": "posts/2025-03-18-Kappa/index.html#example-3",
    "title": "Kappa 분석 이해하기",
    "section": "4.2 Example",
    "text": "4.2 Example\n다음과 같은 예시 데이터를 통해 계산 방식을 간단히 이해해 본다. 평가 범주는 1~3으로 순서형이며, 일부 평가자는 특정 항목을 평가하지 않았다.\n\n\n\n항목\n평가자1\n평가자2\n평가자3\n평가자4\n\n\n\n\n1\n1\n2\n2\n-\n\n\n2\n2\n2\n3\n2\n\n\n3\n3\n3\n-\n-\n\n\n4\n1\n1\n1\n2\n\n\n\nGeneralized Fleiss’ Kappa는 R의 irrCAC 패키지에 있는 fleiss.kappa.raw 함수를 사용하여 쉽게 계산할 수 있다.\nlibrary(irrCAC)\n\n# 여러 평가자의 병변 평가 데이터\nratings &lt;- data.frame(\n  rater1 = c(1, 2, 3, 1),\n  rater2 = c(2, 2, 3, 1),\n  rater3 = c(2, 3, NA, 1),\n  rater4 = c(NA, 2, NA, 2)\n)\n\n# Linear Weighted Fleiss Generalized Kappa\nkappa_linear &lt;- fleiss.kappa.raw(ratings, weights = \"linear\")\nprint(kappa_linear)\n\n# Quadratic Weighted Fleiss Generalized Kappa\nkappa_quadratic &lt;- fleiss.kappa.raw(ratings, weights = \"quadratic\")\nprint(kappa_quadratic)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#신뢰도-데이터",
    "href": "posts/2025-03-18-Kappa/index.html#신뢰도-데이터",
    "title": "Kappa 분석 이해하기",
    "section": "5.1 신뢰도 데이터",
    "text": "5.1 신뢰도 데이터\n먼저 Krippendorff’s Alpha를 계산하기 위해서는 평가자들이 동일한 단위(unit)에 대해 내린 평가 데이터를 분석해야 한다. 평가자들은 독립적으로 하나 이상의 값을 할당할 수 있으며, 이러한 데이터를 m × N 행렬로 표현할 수 있다. 여기서 m은 평가자의 수, N은 평가된 항목의 개수이다.\n평가자가 특정 단위 \\(u_j\\)에 할당한 값 \\(v_{ij}\\)를 포함하는 행렬을 다음과 같이 정의할 수 있다.\n\\[\n\\begin{array}{c|cccc}\n    & u_1 & u_2 & u_3 & \\cdots & u_N \\\\\\hline\nc_1 & v_{11} & v_{12} & v_{13} & \\cdots & v_{1N} \\\\\nc_2 & v_{21} & v_{22} & v_{23} & \\cdots & v_{2N} \\\\\nc_3 & v_{31} & v_{32} & v_{33} & \\cdots & v_{3N} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_m & v_{m1} & v_{m2} & v_{m3} & \\cdots & v_{mN}\n\\end{array}\n\\]\n\n\\(m\\): 평가자의 수\n\\(N\\): 평가된 항목(unit)의 개수\n\\(v_{ij}\\): 평가자 \\(c_i\\)가 특정 단위 \\(u_j\\)에 대해 부여한 값\n\\(m_j\\): 단위 \\(u_j\\)에 대해 평가된 개수\n\n여기서 \\(m_j\\)는 특정 단위 \\(u_j\\)에 대해 평가된 개수이다. 일부 평가자가 특정 단위에 대한 평가를 하지 않을 수도 있기 때문에, \\(m_j\\)는 평가자가 동일하지 않은 경우 \\(m\\)보다 작을 수도 있다.\nKrippendorff’s Alpha를 계산하려면 평가된 값들이 서로 비교 가능(pairable) 해야 하므로, \\(m_j \\geq 2\\)의 조건이 필요하다. 즉, 특정 단위에서 최소 두 명 이상의 평가자가 값을 할당해야 한다. 전체 데이터에서 가능한 쌍의 개수는 다음과 같다.\n\\[\n\\sum_{j=1}^{N} m_j = n \\leq mN\n\\]\n이러한 행렬 표현은 Krippendorff’s Alpha에서 관찰된 불일치 \\(D_o\\)와 기대되는 불일치 \\(D_e\\)를 계산하는 기본적인 데이터 구조를 나타낸다."
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#krippendorffs-alpha-공식",
    "href": "posts/2025-03-18-Kappa/index.html#krippendorffs-alpha-공식",
    "title": "Kappa 분석 이해하기",
    "section": "5.1 Krippendorff’s Alpha 공식",
    "text": "5.1 Krippendorff’s Alpha 공식\nKrippendorff’s Alpha는 평가자 간의 일치도를 분석하기 위해 관찰된 불일치(\\(D_o\\))와 우연히 예상된 불일치(\\(D_e\\))를 비교하여 계산된다. 평가자들이 응답할 수 있는 모든 가능한 값들의 집합을 \\(R\\)이라고 하고, 평가자들이 특정 예제에 대해 내린 응답을 하나의 단위(unit)라고 할 때, Krippendorff’s Alpha는 다음과 같이 정의된다.\n\\[\n\\alpha = 1 - \\frac{D_o}{D_e}\n\\]\n\n\\(D_o\\): 실제로 관찰된 불일치 (Observed Disagreement)\n\\(D_e\\): 우연히 예상된 불일치 (Expected Disagreement by chance)\n\n\nObserved Disagreement \\(D_o\\)\n\\(D_o\\)는 다음과 같이 정의된다.\n\\[\nD_o = \\frac{1}{n} \\sum_{c \\in R} \\sum_{k \\in R} \\delta(c, k) \\sum_{u \\in U} m_u \\frac{n_{cku}}{P(m_u, 2)}\n\\]\n\n\\(\\delta(c, k)\\): 두 개의 평가 값 \\(c\\)와 \\(k\\) 사이의 차이\n\\(n\\): 총 가능한 쌍(pair)의 개수 - \\(m_u\\): 특정 단위 \\(u\\)에 포함된 평가 수\n\\(n_{cku}\\): 단위 \\(u\\)에서 평가 값 \\((c, k)\\) 쌍이 나타난 횟수\n\\(P\\): 순열(permutation)\n\n이 식은 평가자들이 특정 단위에서 얼마나 불일치했는지를 개념적으로 가중 평균(weighted average)한 것으로 해석할 수 있다. 여기서 중요한 역할을 하는 \\(\\delta(c, k)\\)는 평가 값 \\(c\\)와 평가 값 \\(k\\) 사이의 차이를 의미하는데, 순위형 데이터의 경우 제곱을 취하여 거리의 크기를 강조하는 방식으로 계산한다. 즉, 두 평가 값이 동일하면 \\(\\delta(c, k) = 0\\)이 되고, 평가 값 간 차이가 크면 불일치도가 더 커지는 방식이다.\n\nKrippendorff’s Alpha는 데이터의 유형에 따라 다른 \\(\\delta(c, k)\\)의 정의를 사용한다. 이 값을 기반으로 평가자 간의 불일치를 측정하므로, 이 함수가 어떻게 정의가 되느냐에 따라 Alpha \\(α\\)의 값이 달라진다.\n\n또한, \\(P(m_u, 2)\\)는 특정 단위 \\(u\\)에서 평가된 값들 중에서 2개의 값을 선택하여 순서를 고려한 쌍의 개수를 의미한다. 이는 순열 함수로 표현되며, Krippendorff’s Alpha에서 관찰된 불일치도를 계산할 때 중요한 역할을 한다. 공식은 다음과 같다.\n\\[\nP(m_u, 2) = \\frac{m_u (m_u - 1)}{2}\n\\]\n이러한 계산이 필요한 이유는, Krippendorff’s Alpha에서 평가자 간의 불일치를 측정할 때 단순한 개별 평가 값을 비교하는 것이 아니라, 각 단위에서 이루어진 모든 평가 값들 간의 관계를 분석해야 하기 때문이다.\n이 식은 평가 값 자체의 범주를 기반으로 전체적인 불일치도를 직접적으로 계산하는 방식이라면, 단위별 불일치도를 먼저 계산한 후 전체 평균을 구하는 방식도 존재한다. \\(D_o\\)에 대한 단위 중심의 접근법은 다음과 같다.\n\\[\nD_o = \\frac{1}{n} \\sum_{j=1}^{N} m_j E(\\delta_j)\n\\]\n여기서 \\(E(\\delta_j)\\)는 모든 가능한 쌍에 대해 평균적인 거리이며, 아래와 같은 식으로 표현할 수 있다.\n\\[\nE(\\delta_j) = \\frac{ \\sum_{i&gt;i'} \\delta(v_{ij}, v_{i'j}) }{\\binom{m_j}{2}}\n\\]\n\n\\(v_{ij}\\): 평가자 \\(i\\)가 단위 \\(j\\)에 대해 부여한 평가 값\n\\(\\delta(v_{ij}, v_{i'j})\\): 두 평가 값 \\(v_{ij}\\)와 \\(v_{i'j}\\) 간의 거리\n\\(\\sum_{i&gt;i'}\\): 단위 \\(j\\)에서 모든 평가자 간의 가능한 쌍을 고려한 합 - \\(\\binom{m_j}{2}\\): 단위 \\(j\\)에서 가능한 모든 평가 쌍의 개수. \\(\\frac{m_j(m_j - 1)}{2}\\)로 계산된다.\n\n이 수식은 단위 \\(j\\)에서 평가자들이 부여한 모든 값들을 비교하여 평균적인 불일치도를 구하는 과정이다. 평가자들이 같은 값을 부여했다면 \\(\\delta(v_{ij}, v_{i'j}) = 0\\)이 되어 \\(E(\\delta_j)\\) 값이 작아지고, 평가 값이 크게 차이 날수록 \\(E(\\delta_j)\\) 값이 증가한다.\n또한, 만약 모든 단위에서 평가자 수가 일정하다면, \\(D_o\\)는 전체 평가 단위에서 가능한 모든 평가 쌍에 대한 평균적인 거리로 해석할 수 있다. 이는 일반적으로 평가 행렬에서 대각선에서의 평균적인 거리로 볼 수 있으며, 평가자들이 특정 경향을 가지고 평가했는지 또는 무작위로 평가했는지를 판단하는 중요한 지표가 된다.\n\n\nExpected Disagreement by chance \\(D_e\\)\n우연히 예상된 불일치 \\(D_e\\)는 평가자들이 무작위로 응답했다고 가정할 때 예상되는 불일치도를 의미하는데, 모든 가능한 평가 값 쌍(c, k)의 발생 확률을 이용해 불일치를 추정한다. 이는 Krippendorff’s Alpha에서 평가자 간의 일치도를 측정할 때, 실제 관찰된 불일치도(\\(D_o\\))와 비교하는 기준이 된다.\n\\[\nD_e = \\frac{1}{P(n,2)} \\sum_{c \\in R} \\sum_{k \\in R} \\delta(c,k) P_{ck}\n\\] - \\(P(n,2)\\): 전체 가능한 평가 쌍(pair)의 개수 - \\(\\delta(c,k)\\): 두 개의 평가 값 \\(c\\)와 \\(k\\) 사이의 차이 - \\(P_{ck}\\): 특정 평가 값 \\((c,k)\\) 쌍이 발생할 확률\n\\[\nP_{ck} = \\begin{cases}\n  n_c n_k & \\text{if } c \\neq k \\\\\n  n_c (n_c - 1) & \\text{if } c = k\n\\end{cases}\n\\]\n이 식에서 \\(n_c\\)와 \\(n_k\\)는 각각 평가 값 \\(c\\)와 \\(k\\)가 전체 데이터에서 나타난 횟수이다. 만약 \\(c \\neq k\\)이면, 서로 다른 두 개의 평가 값이 선택될 확률은 \\(n_c n_k\\)로 표현된다. 반면, \\(c = k\\)이면 동일한 평가 값이 두 번 선택될 확률은 \\(n_c (n_c - 1)\\)로 계산된다. 이러한 확률을 고려하여 평가 값들이 랜덤하게 분포되었을 때 기대되는 불일치도를 구할 수 있다.\n즉, \\(D_e\\)는 평가자들이 일관된 기준 없이 평가한 경우 예상되는 불일치도의 평균적인 크기를 나타낸다.\nKrippendorff’s Alpha는 개념적으로 직관적이지만, 계산적으로는 다소 복잡할 수 있다. 그러나 이 지표는 다양한 데이터 유형에 적용할 수 있으며, 평가자의 수가 일정하지 않거나 결측값이 포함된 경우에도 안정적인 신뢰도 분석을 수행할 수 있는 장점이 있다.\n\n\nKrippendorff’s Alpha 값 해석\n\\(α\\)에 대한 해석은 다음과 같다.\n\nα = 1: 완벽한 평가자 간 일치 (모든 평가자가 동일한 응답)\n0.8 ≤ α ≤ 1: 높은 신뢰도를 의미하며 연구 결과로 활용 가능\n0.67 ≤ α &lt; 0.8: 신뢰할 수 있는 수준이지만 엄격한 연구에서는 보완이 필요함\n0 ≤ α &lt; 0.67: 신뢰도가 낮아 추가적인 평가 기준 수정 또는 평가자 교육이 필요함\nα &lt; 0: 평가자 간 일치도가 우연보다도 낮음 (평가 기준이 모호하거나 데이터에 문제 가능성)"
  },
  {
    "objectID": "posts/2025-03-18-Kappa/index.html#example-4",
    "href": "posts/2025-03-18-Kappa/index.html#example-4",
    "title": "Kappa 분석 이해하기",
    "section": "5.2 Example",
    "text": "5.2 Example\n아래는 irrCAC 패키지를 사용하여 Krippendorff’s Alpha를 계산하는 R 코드 예제이다.\nlibrary(irrCAC)\n\n# 여러 평가자의 병변 평가 데이터\nratings &lt;- data.frame(\n  rater1 = c(1, 2, 3, 1, 2, NA, 4, 3, NA, 2),\n  rater2 = c(2, 2, 3, 1, 3, 2, 4, 3, 2, 1),\n  rater3 = c(2, 3, NA, 1, 4, 2, NA, 3, 2, NA),\n  rater4 = c(NA, 2, NA, 2, 3, 1, 4, NA, 3, 2)\n)\n\n# Krippendorff’s Alpha 계산 (순위형 데이터)\nalpha_result &lt;- krippen.alpha.raw(ratings, weights = \"ordinal\")\nprint(alpha_result)\n이 코드는 순위형 데이터를 기반으로 Krippendorff’s Alpha를 계산하는 방법을 보여준다. 평가자 간 일치도를 보다 유연하게 측정할 수 있으며, Fleiss Kappa나 Weighted Cohen’s Kappa보다 데이터 특성에 덜 제한을 받는다는 장점이 있다. 또한, 결측값이 포함된 데이터를 그대로 분석할 수 있다는 점에서 다른 신뢰도 측정법보다 강력한 활용성을 가진다.\nKrippendorff’s Alpha는 평가자가 많을수록, 그리고 평가 기준이 명확할수록 신뢰도가 높게 나타난다. 하지만 평가자 간 의견 차이가 크다면 신뢰도 값이 낮아질 수 있으며, 이런 경우 평가 기준을 조정하거나 추가적인 훈련이 필요할 수 있다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html",
    "href": "posts/2025-03-27-Sankey/index.html",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "",
    "text": "Sankey Plot은 데이터의 흐름이나 분포를 시각적으로 표현하는 그래프다.\n선(링크)의 굵기로 양 또는 비율을 보여주며, 한 집단에서 다른 집단으로 얼마나 이동했는지를 한눈에 확인할 수 있게 도와 전체적인 구조와 흐름을 직관적으로 이해할 수 있도록 한다.\n\nR에는 sankeyNetwork() 함수로 Sankey Plot을 생성할 수 있는 기능이 존재한다. 이 함수는 networkD3 패키지에서 제공되며, 비교적 간단한 코드로 노드와 링크 데이터를 연결해 Sankey Plot을 만들 수 있다는 장점이 있다 (특히 마우스 오버 시 강조되거나, 노드를 드래그할 수 있는 등의 동작은 시각적으로 유용하다).\n하지만 이 함수만으로는 각 링크 위에 텍스트를 추가하거나, 노드 옆에 퍼센트 또는 샘플 수(n=) 같은 세부 정보를 표시하는 것이 어렵다. 그 이유는 Sankey Plot 내부 요소들의 위치(x, y 좌표)가 JavaScript에서 실시간으로 계산되기 때문이다. 따라서 R에서는 그 위치 정보를 직접 참조하거나 조작할 수가 없어 sankeyNetwork()를 이용한 기본적인 시각화 요소 외에는 커스터마이징 옵션이 제한적이다.\n따라서 Sankey Plot의 시각적 완성도를 높이기 위해서는 결국 R 코드와 JavaScript를 함께 사용하는 접근이 요구된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#텍스트를-추가하려고-했더니",
    "href": "posts/2025-03-27-Sankey/index.html#텍스트를-추가하려고-했더니",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "",
    "text": "R에는 sankeyNetwork() 함수로 Sankey Plot을 생성할 수 있는 기능이 존재한다. 이 함수는 networkD3 패키지에서 제공되며, 비교적 간단한 코드로 노드와 링크 데이터를 연결해 Sankey Plot을 만들 수 있다는 장점이 있다 (특히 마우스 오버 시 강조되거나, 노드를 드래그할 수 있는 등의 동작은 시각적으로 유용하다).\n하지만 이 함수만으로는 각 링크 위에 텍스트를 추가하거나, 노드 옆에 퍼센트 또는 샘플 수(n=) 같은 세부 정보를 표시하는 것이 어렵다. 그 이유는 Sankey Plot 내부 요소들의 위치(x, y 좌표)가 JavaScript에서 실시간으로 계산되기 때문이다. 따라서 R에서는 그 위치 정보를 직접 참조하거나 조작할 수가 없어 sankeyNetwork()를 이용한 기본적인 시각화 요소 외에는 커스터마이징 옵션이 제한적이다.\n따라서 Sankey Plot의 시각적 완성도를 높이기 위해서는 결국 R 코드와 JavaScript를 함께 사용하는 접근이 요구된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#각-node-옆에-비율-및-개수-표시",
    "href": "posts/2025-03-27-Sankey/index.html#각-node-옆에-비율-및-개수-표시",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "각 node 옆에 비율 및 개수 표시",
    "text": "각 node 옆에 비율 및 개수 표시\n\np &lt;- htmlwidgets::onRender(p, '\n  function(el, x) {\n    var sankey = this.sankey;\n\n    d3.select(el).selectAll(\".node text\")\n      .text(function(d) {\n        var perc = (d.value / 120) * 100;  // 총 샘플 수: 120\n        return d.name + \" \" + perc.toFixed(1) + \"% (n=\" + d.value + \")\";\n      })\n      .attr(\"x\", function(d) {\n        return (d.x === 0) ? -10 : x.options.nodeWidth + 10;\n      })\n      .attr(\"text-anchor\", function(d) {\n        return (d.x === 0) ? \"end\" : \"start\";\n      });\n  }\n')\n\nhtmlwidgets::onRender(p, '...')는 p라는 Sankey 플롯 객체에 JavaScript 코드를 추가해주는 함수다. 이 코드는 Sankey plot이 브라우저에 렌더링된 이후 실행된다.\n위 코드는 먼저 var sankey = this.sankey로 Sankey Plot의 내부 구조를 가져온 뒤, .selectALL (\".node text\")로 Sankey의 모든 노드 텍스트를 선택한다.\n이어서 function(el, x) { ... }의 el은 Sankey 그래프가 들어있는 HTML의 요소이고, x는 sankeyNetwork 함수에서 설정된 옵션들(fontSize, nodeWidth 등)을 담고 있다. 텍스트를 추가하기 위해서 이 function 안에 있는 .text 함수를 가장 중심적으로 사용한다:\n\n.text(function(d) {\n        var perc = (d.value / 87) * 100;  // 총 샘플 수: 87\n        return d.name + \" \" + perc.toFixed(1) + \"% (n=\" + d.value + \")\";\n      }\n\n이 부분은 d.name(각 노드의 이름)에 전체 값 대비 해당 노드가 차지하는 비율(%)과 개수(n=)를 텍스트로 함께 붙이는 역할을 한다. 여기서 120이라는 숫자는 전체 합계(sum of N)를 의미하기 때문에, 사용하는 데이터의 전체 값에 따라 이 숫자는 다르게 삽입한다. 혹은, R에서 전체 합을 미리 계산해서 onRender()에 넘겨주는 방식을 사용해도 된다.\n그 밑에 있는 .attr(\"x\", function(d) {...})와 .attr(\"text-anchor\", function(d) {...})는 위치 설정에 관련된 코드다. 노드가 왼쪽에 있을 경우(d.x === 0)에는 텍스트를 왼쪽 바깥에 정렬하고, 그렇지 않으면 오른쪽 바깥에 정렬하도록 설정한다. 텍스트가 노드와 겹치거나 너무 떨어져 보일 경우, -10 또는 x.options.nodeWidth + 10 값을 조정해서 위치를 맞춰주면 된다."
  },
  {
    "objectID": "posts/2025-03-27-Sankey/index.html#각-flow의-확률-텍스트-추가",
    "href": "posts/2025-03-27-Sankey/index.html#각-flow의-확률-텍스트-추가",
    "title": "Sankey Diagram 텍스트 삽입",
    "section": "각 flow의 확률 텍스트 추가",
    "text": "각 flow의 확률 텍스트 추가\n아래 코드는 Sankey plot의 각 링크 위에 해당 링크가 출발 노드 전체에서 차지하는 비율(%)을 텍스트로 표시해주는 역할을 한다.\n\np &lt;- htmlwidgets::onRender(p, '\n  function(el) {\n    var sankey = this.sankey;\n    var nodeWidth = sankey.nodeWidth();\n    var links = sankey.links();\n\n    // 각 링크에 대해 비율 계산 및 텍스트 추가\n    links.forEach(function(d) {\n      var outflow = d3.sum(d.source.sourceLinks, function(l) {\n        return l.value;\n      });\n\n      var ratio = (d.value / outflow) * 100;\n\n      // 텍스트 위치 계산 (링크 중간)\n      var startX = d.source.x + nodeWidth;\n      var startY = d.source.y + d.sy + (d.dy / 2);\n\n      // 텍스트 추가\n      d3.select(el).select(\"svg g\")\n        .append(\"text\")\n        .attr(\"text-anchor\", \"middle\")\n        .attr(\"alignment-baseline\", \"middle\")\n        .attr(\"x\", startX + 25)\n        .attr(\"y\", startY)\n        .text(ratio.toFixed(1) + \"%\");\n    });\n  }\n')\n\nvar sankey = this.sankey는 sankey 객체를 가져오는 부분이며, nodeWidth는 노드의 너비, links는 그래프에 존재하는 모든 링크를 가져온다.\n위 코드의 핵심은 links.forEach(function(d) {...}) 반복문이다. 각 링크에 대해 다음과 같은 작업이 이루어진다:\n\nvar outflow = d3.sum(d.source.sourceLinks, function(l) {\n  return l.value;\n})\n\n이 부분은 d.source(해당 링크가 출발하는 노드)에서 outflow(나가는 전체 흐름의 합)을 계산한다.\n이어서 그 링크가 차지하는 비율을 아래와 같이 계산한다.\n\nvar ratio = (d.value / outflow) * 100;\n\n예를 들어 10명이 이 링크를 통해 이동했고, 전체 outflow가 40이면, 비율은 25%가 된다.\n텍스트를 표시할 위치도 직접 계산된다:\n\nvar startX = d.source.x + nodeWidth;\nvar startY = d.source.y + d.sy + (d.dy / 2);\n\nstartX은 출발 노드의 x좌표와 노드 너비의 합으로, 텍스트의 x 좌표 위치는 노드의 오른쪽 바깥이 된다. 비슷하게 startY은 출발 노드의 y좌표, 링크가 시작되는 위치, 그리고 링크 높이의 절반의 합이다. 따라서 텍스트의 위치는 링크 중간의 y좌표가 된다. 이 계산 과정으로 텍스트는 선의 중간쯤 되는 위치에 뜨게 된다.\n마지막으로 텍스트를 실제로 추가한다:\n\nd3.select(el).select(\"svg g\")\n  .append(\"text\")\n  .attr(\"text-anchor\", \"middle\")\n  .attr(\"alignment-baseline\", \"middle\")\n  .attr(\"x\", startX + 25)      \n  .attr(\"y\", startY)\n  .text(ratio.toFixed(1) + \"%\");\n\n이 부분에서 숫자 포맷, 텍스트 위치(x, y), 텍스트 내용을 자유롭게 커스터마이징할 수 있다. d3.select(el).select(\"svg g\").append(\"text\")로 텍스트 요소를 생성하고, .attr(\"text-anchor\", \"middle\")과 .attr(\"alignment-baseline\", \"middle\")을 통해 텍스트가 x와 y좌표의 가운데 정렬로 위치하도록 설정한다. x 좌표는 앞서 계산한 startX에 25를 더해 노드에서 약간 떨어진 위치에 텍스트가 뜨게 하고, y 좌표는 startY로 그대로 둔다.\n마지막으로 .text(ratio.toFixed(1) + \"%\")를 통해 소수점 첫째자리까지 계산된 비율을 문자열로 표시한다. 소수점 없이 정수로만 표시하고 싶다면 ratio.toFixed(0)으로 바꿀 수 있고, 텍스트에 단어를 붙이고 싶다면 “비율:” + ratio.toFixed(1) + “%”처럼 바꿀 수도 있다.\n이와 같이 추가적으로 코드를 작성한 것을 실행하면 처음 만들었던 Sankey plot에 원하는 텍스트가 추가된 것을 확인할 수 있다:"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html",
    "href": "posts/2025-04-08-ADaM/index.html",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "",
    "text": "CDISC: SDTM과 ADaM이 무엇인지 알아보자.\n직접 SDTM을 ADaM으로 변환해보자.\nADaM을 이용, TLG(table, list, graphics)를 빠르게 만들어보자.\nShiny module을 이용, ADaM dataset에서 복잡한 분석을 빠르게 수행해보자."
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#sdtm-vs-adam-변수-비교",
    "href": "posts/2025-04-08-ADaM/index.html#sdtm-vs-adam-변수-비교",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "SDTM vs ADaM 변수 비교",
    "text": "SDTM vs ADaM 변수 비교\n1. SDTM 주요 변수 구조\n\n\n\n\n\n\n\n도메인\n핵심 변수\n설명\n\n\n\nDM\n\nUSUBJID, AGE, SEX, RACE, ARM\n\n인구통계학 및 치료군 정보\n\n\nAE\n\nAETERM, AESTDTC, AEENDTC, AESEV\n\n이상반응 데이터\n\n\nVS\n\nVSTEST, VSORRES, VSSTRESN\n\n생체징후 데이터\n\n\nLB\n\nLBTEST, LBORRES, LBSTRESN\n\n검사실 데이터\n\n\nEX\n\nEXTRT, EXDOSE, EXSTDTC\n\n투여 정보\n\n\n\n2. ADaM으로 전환 시 주요 변경 사항\n\n\nSDTM 변수\nADaM 변환\n변경 이유\n\n\n\n\nAESTDTC(문자형)\n\nASTDT(숫자형 날짜)\n분석용 날짜 포맷\n\n\nAGE\n\nAGEGR1(연령그룹)\n카테고리 분석\n\n\nARM\n\nTRT01P(계획 치료군)\n프로토콜 위반 처리\n\n\n-\n\nTRTEMFL(치료기간 발생 여부)\n안전성 분석 필터링\n\n\n\nADaM 전용 파생 변수\n\n\n기본 변수:\n\n\nAVAL: 분석용 수치값 (예: LB도메인의 VSSTRESN → AVAL)\n\nCHG: 기준값 대비 변화량\n\nPCHG: 백분율 변화량\n\n\n\n플래그 변수:\n\n\nSAFFL: 안전성 평가 집단 플래그\n\nITTFL: Intent-to-Treat 플래그\n\nANL01FL: 주요 분석 플래그\n\n\n\n날짜 변수:\n\n\nADT: 분석용 숫자형 날짜\n\nADY: 기준일(DAY 1) 대비 일수\n\n\n\n\n\n\n\n\nflowchart TB\n    SDTM[\"SDTM Domain: LB\"] --&gt;|Transform| ADAM[\"ADaM ADLB Dataset\"]\n    \n    SDTM --&gt; LB1[(\"LBTEST(Test Name)\")]\n    SDTM --&gt; LB2[(\"LBORRES(Result)\")]\n    SDTM --&gt; LB3[(\"LBDTC(Date)\")]\n    \n    ADAM --&gt; AD1[(\"PARAMCD(Standard Code)\")]\n    ADAM --&gt; AD2[(\"AVAL(Numeric Value)\")]\n    ADAM --&gt; AD3[(\"ADT(Analysis Date)\")]\n    ADAM --&gt; AD4[(\"ANRIND(Reference Range)\")]\n    \n    ADAM --&gt; Analysis[[\"Statistical Analysis\"]]"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#adsl",
    "href": "posts/2025-04-08-ADaM/index.html#adsl",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "ADSL",
    "text": "ADSL\nADSL은 임상 시험에서 환자별 기초 정보를 담는 핵심 데이터셋으로, 주요 변수와 분석에 활용된다.\n목적:          임상 시험 대상자(Subject)의 인구통계학적 정보, 치료 그룹, 기초 값 등 개인 수준 데이터를 제공. 구조:          한 행에 한 환자의 정보가 포함되며, USUBJID (고유 환자 식별자)로 구분. 활용:          효능/안전성 분석의 기초 데이터로 사용. \n주요 변수 (Columns)\n\n\n\n\n\n\n\nCategory\nVariables\nDescription\n\n\n\nIdentifiers\n• USUBJID• SUBJID• SITEID\n\n• 고유 환자 ID• 대상자 ID• 연구기관 ID\n\n\nDemographics\n• AGE• SEX• RACE• COUNTRY\n\n• 연령• 성별• 인종• 국가\n\n\nTreatment\n• TRT01P• TRT01A• TRTGRPn\n\n• 계획 치료• 실제 치료• 치료그룹\n\n\nTrial\n• ARM• ACTARM• RFSTDTC• RFENDTC\n\n• 할당 치료군• 실제 치료군• 첫 투여일• 마지막 방문일\n\n\nFlags\n• SAFFL• ITTFL• COMPLFL\n\n• 안전성 평가 대상• ITT 대상• 연구 완료 여부\n\n\nCDISC\nAll variables\nCDISC ADaM 표준 준수\n\n\n\n\nADSL_example\n예제는 아래와 같은 순서로 진행한다.\n\ndata/Packages loading\nDerivation Building\nVariables Grouping\nDerive exposure variables\nDerive Treatment Variables\nDerive Disposition Variables\nDerive Cause of Death\nDerive Other Grouping Variables\nApplying metadate\n\n\n#load packages\n\nlibrary(metacore)\nlibrary(metatools)\nlibrary(pharmaversesdtm)\nlibrary(admiral)\nlibrary(xportr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(DT)\n\n# Read in input SDTM data\ndm &lt;- pharmaversesdtm::dm\nds &lt;- pharmaversesdtm::ds\nex &lt;- pharmaversesdtm::ex\nae &lt;- pharmaversesdtm::ae\nvs &lt;- pharmaversesdtm::vs\nsuppdm &lt;- pharmaversesdtm::suppdm\n\n#in case; importing SAS datasets using haven::read_sas()\n# NA가 아닌 \"\"로 읽히기 때문에, NA처리를 따로 해줘야한다.\ndm &lt;- convert_blanks_to_na(dm)\nds &lt;- convert_blanks_to_na(ds)\nex &lt;- convert_blanks_to_na(ex)\nae &lt;- convert_blanks_to_na(ae)\nvs &lt;- convert_blanks_to_na(vs)\nsuppdm &lt;- convert_blanks_to_na(suppdm)\n\n이후 편의를 위해 dm과 suppdm을 아래와 같이 합칠 수 있다.\n\nnames(dm)\n#  [1] \"STUDYID\"  \"DOMAIN\"   \"USUBJID\"  \"SUBJID\"   \"RFSTDTC\"  \"RFENDTC\"  \"RFXSTDTC\" \"RFXENDTC\" \"RFICDTC\"  \"RFPENDTC\" \"DTHDTC\"   \"DTHFL\"    \"SITEID\"  \n# [14] \"AGE\"      \"AGEU\"     \"SEX\"      \"RACE\"     \"ETHNIC\"   \"ARMCD\"    \"ARM\"      \"ACTARMCD\" \"ACTARM\"   \"COUNTRY\"  \"DMDTC\"    \"DMDY\"    \nnames(suppdm)\n# [1] \"STUDYID\"  \"RDOMAIN\"  \"USUBJID\"  \"IDVAR\"    \"IDVARVAL\" \"QNAM\"     \"QLABEL\"   \"QVAL\"     \"QORIG\"    \"QEVAL\"   \n\n# merge 대신 suppdm을 통해 간편하게 확인할 수 있다.\ndm_suppdm &lt;- combine_supp(dm, suppdm)\n\nnames(dm_suppdm)\n#  [1] \"STUDYID\"  \"DOMAIN\"   \"USUBJID\"  \"SUBJID\"   \"RFSTDTC\"  \"RFENDTC\"  \"RFXSTDTC\" \"RFXENDTC\" \"RFICDTC\"  \"RFPENDTC\" \"DTHDTC\"   \"DTHFL\"    \"SITEID\"  \n# [14] \"AGE\"      \"AGEU\"     \"SEX\"      \"RACE\"     \"ETHNIC\"   \"ARMCD\"    \"ARM\"      \"ACTARMCD\" \"ACTARM\"   \"COUNTRY\"  \"DMDTC\"    \"DMDY\"     \"IDVARVAL\"\n# [27] \"COMPLT16\" \"COMPLT24\" \"COMPLT8\"  \"EFFICACY\" \"ITT\"      \"SAFETY\"\n\n그 이후 specification file을 metacore object로 합친다. metacore::spec_to_metacore() \n\n# Read in metacore object\nmetacore &lt;- spec_to_metacore(\n  path = \"./safety_specs.xlsx\",\n  # All datasets are described in the same sheet\n  where_sep_sheet = FALSE\n) %&gt;%\n  select_dataset(\"ADSL\")\n\nDerivation Building\n이제 SDTM dataset(DM, SUPPDM)에서 필요한 변수들을 추출하여 ADaM(ADSL)을 생성하는 과정이 시작된다. metatools:build_from_derived()\n\nadsl_preds &lt;- build_from_derived(metacore,\n  ds_list = list(\"dm\" = dm_suppdm, \"suppdm\" = dm_suppdm),\n  predecessor_only = FALSE, keep = FALSE\n)\n\nNot all datasets provided. Only variables from DM, SUPPDM will be gathered.\n\nhead(adsl_preds, 5) %&gt;% datatable()\n\n\n\n\n\n\nVariable Grouping\n이제 subgroup을 만들어보자. admiral::derive_vars_cat()\n\nagegr1_lookup &lt;- exprs(\n  ~condition,            ~AGEGR1, ~AGEGR1N,\n  is.na(AGE),          \"Missing\",        4,\n  AGE &lt; 18,                \"&lt;18\",        1,\n  between(AGE, 18, 64),  \"18-64\",        2,\n  !is.na(AGE),             \"&gt;64\",        3\n)\n\nadsl_cat &lt;- derive_vars_cat(\n  dataset = adsl_preds,\n  definition = agegr1_lookup\n)\n\nhead(adsl_cat %&gt;% select(contains(c(\"ID\",\"AGE\")))) %&gt;% datatable()\n\n\n\n\n\n 만일 metacore data(.xlsx)가 terminology definition을 갖고있다면, get_control_term()을 이용해 정의를 불러올 수 있다.\n\nget_control_term(metacore, variable = AGEGR1)\n\n# A tibble: 3 × 2\n  code  decode\n  &lt;chr&gt; &lt;chr&gt; \n1 &lt;18   &lt;18   \n2 18-64 18-64 \n3 &gt;64   &gt;64   \n\nadsl_ct &lt;- adsl_preds %&gt;%\n  create_cat_var(metacore,\n    ref_var = AGE,\n    grp_var = AGEGR1, num_grp_var = AGEGR1N\n  )\n\nhead(adsl_ct %&gt;% select(contains(c(\"ID\",\"AGE\")))) %&gt;% datatable()\n\n\n\n\n\n 기존의 방식을 이용해서 진행해도 무방하다.\n\nformat_agegr1 &lt;- function(age) {\n  case_when(\n    age &lt; 18 ~ \"&lt;18\",\n    between(age, 18, 64) ~ \"18-64\",\n    age &gt; 64 ~ \"&gt;64\",\n    TRUE ~ \"Missing\"\n  )\n}\n\nformat_agegr1n &lt;- function(age) {\n  case_when(\n    age &lt; 18 ~ 1,\n    between(age, 18, 64) ~ 2,\n    age &gt; 64 ~ 3,\n    TRUE ~ 4\n  )\n}\n\nadsl_cat3 &lt;- adsl_preds %&gt;%\n  mutate(\n    AGEGR1 = format_agegr1(AGE),\n    AGEGR1N = format_agegr1n(AGE)\n  )\nhead(adsl_cat3 %&gt;% select(contains(c(\"ID\",\"AGE\")))) %&gt;% datatable()\n\n\n\n\n\n 인종, 성별 등 고정되어 있는 code는 codelist를 참조, 아래와 같이 변경할 수 있다. 다만, codelist에 정의되어 있어야한다.\nmetatools::create_var_from_codelist()\n\nadsl_ct &lt;- adsl_ct %&gt;%\n  create_var_from_codelist(\n    metacore = metacore,\n    input_var = RACE,\n    out_var = RACEN\n  )\n\nhead(adsl_ct %&gt;% select(contains(c(\"ID\",\"AGEGR\",\"RACE\")))) %&gt;% datatable()\n\n\n\n\n\n\ncreate_var_from_codelist 사용:  그룹이 이미 문자형으로 존재하고, 숫자 코드만 필요할 때  (예: RACE → RACEN 변환/ 인종, 성별 등 factor variable)\ncreate_cat_var 사용:  연속형 데이터를 처음부터 그룹핑해야 할 때  (예: AGE(숫자) → AGEGR1(문자) + AGEGR1N(숫자) 생성/ AGE, BMI 등 continuous variable)\n\nExposure Derivations\n기존 변수들을 바탕으로 새로운 변수들을 생성해보자. 관련 변수들의 예시는 admiral에서 더 자세한 정보를 확인 가능하다.\n\n# DTC &gt; DTM: `derive_vars_dtm`\nex_ext &lt;- ex %&gt;%\n  derive_vars_dtm(\n    dtc = EXSTDTC,   # 시작 일시\n    new_vars_prefix = \"EXST\"   # EXSTDTM\n  ) %&gt;%\n  derive_vars_dtm(\n    dtc = EXENDTC,   # 종료 일시 \n    new_vars_prefix = \"EXEN\",  # EXENDTM\n    time_imputation = \"last\"   #종료 시간 결측 시 \"23:59\"로 대체.\n  )\n    # 다음과 같은 변수 생성: EXEN, EXSTTMF, EXENDTM, EXENTMF\n    \n\n# ADSL에 병합\nadsl_raw &lt;- adsl_ct %&gt;%\n  # Treatment Start Datetime\n  derive_vars_merged(\n    dataset_add = ex_ext,\n    filter_add = (EXDOSE &gt; 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\"))) & !is.na(EXSTDTM),\n    new_vars = exprs(TRTSDTM = EXSTDTM, TRTSTMF = EXSTTMF),\n    order = exprs(EXSTDTM, EXSEQ),   # 가장 빠른 시작 일시 + 낮은 EXSEQ\n    mode = \"first\",                  # 첫번째 기록 선택\n    by_vars = exprs(STUDYID, USUBJID)\n  ) %&gt;%\n  \n  # Treatment End Datetime\n  derive_vars_merged(\n    dataset_add = ex_ext,\n    filter_add = (EXDOSE &gt; 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\"))) & !is.na(EXENDTM),\n    new_vars = exprs(TRTEDTM = EXENDTM, TRTETMF = EXENTMF),\n    order = exprs(EXENDTM, EXSEQ),   # 가장 늦은 종료 일시 + 높은 EXSEQ\n    mode = \"last\",                   # 마지막 기록 선택\n    by_vars = exprs(STUDYID, USUBJID)\n  ) %&gt;%\n  \n  # Treatment Start and End Date (TRTSDT, TRTEDT 생성.)\n  derive_vars_dtm_to_dt(source_vars = exprs(TRTSDTM, TRTEDTM)) %&gt;% # Convert Datetime variables to date\n      \n  # Treatment Start Time (TRTSM)\n  derive_vars_dtm_to_tm(source_vars = exprs(TRTSDTM)) %&gt;%\n  \n  # Treatment Duration ( TRTDURD = TRTEDT - TRTSDT + 1 )\n  derive_var_trtdurd() %&gt;%\n  \n  # Safety Population Flag (SAFFL: \"Y\" or `NA`)\n  derive_var_merged_exist_flag(\n    dataset_add = ex,\n    by_vars = exprs(STUDYID, USUBJID),\n    new_var = SAFFL,\n    condition = (EXDOSE &gt; 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\")))\n  )\n\n# adsl_raw = adsl_ct + 치료(exposure)관련 변수 및 SAFFL.\n\nhead(adsl_raw %&gt;% select((c(\"STUDYID\",\"TRTSDTM\",\"TRTSTM\",\"TRTSTMF\",\"TRTSDT\",\"TRTEDTM\",\"TRTETMF\",\"TRTEDT\",\"TRTDURD\",\"SAFFL\")))) %&gt;% datatable()\n\n\n\n\n\nDerive Treatment Variables\nADSL data에 치료 그룹 변수(TRT01P, TRT01A)와 숫자형 코드 변수(TRT01PN, TRT01AN)를 추가하자.\n\nadsl &lt;- adsl_raw %&gt;%\n  mutate(\n    TRT01P = if_else(ARM %in% c(\"Screen Failure\", \"Not Assigned\", \"Not Treated\"), \"No Treatment\", ARM),\n    TRT01A = if_else(ACTARM %in% c(\"Screen Failure\", \"Not Assigned\", \"Not Treated\"), \"No Treatment\", ACTARM)\n  ) %&gt;%\n  create_var_from_codelist(metacore, input_var = TRT01P, out_var = TRT01PN) %&gt;%\n  create_var_from_codelist(metacore, input_var = TRT01A, out_var = TRT01AN)\n\nTRT01P: 계획된 치료 그룹 (Planned Treatment) TRT01A: 실제 치료 그룹 (Actual Treatment)\ncode mapping (example)\n- \"Placebo\"     → 1\n- \"High Dose\"   → 2\n- \"Low Dose\"    → 3\n- \"No Treatment\"→ NA\n\nhead(adsl %&gt;% select((c(\"STUDYID\", \"USUBJID\",\"TRT01P\", \"TRT01PN\", \"TRT01A\", \"TRT01AN\" )))) %&gt;% datatable()\n\n\n\n\n\nDerive Disposition Variables\nADSL data에 연구 종료 변수(EOSDT, EOSSTT) 및 날짜 변수(RANDDT, SCRFDT 등)들을 추가하자.\n\n# 1. 연구 종료 일자(EOSDT)\n  # DS 데이터에서 문자형 날짜(DTC) → 숫자형 날짜(DT) 변환\n  ds_ext &lt;- derive_vars_dt(\n    ds,\n    dtc = DSSTDTC,           # 문자형 일자 (예: \"2014-07-02\")\n    new_vars_prefix = \"DSST\" # 생성 변수: DSSTDT, DSSTDTF\n  )\n  \n  #. ADSL에 연구 종료 일자 병합 (EOSDT, ex: 2014-07-02)\n  adsl &lt;- adsl %&gt;%\n    derive_vars_merged(\n      dataset_add = ds_ext,\n      by_vars = exprs(STUDYID, USUBJID),\n      new_vars = exprs(EOSDT = DSSTDT),\n      filter_add = DSCAT == \"DISPOSITION EVENT\" & DSDECOD != \"SCREEN FAILURE\"\n    )\n\n# 2. 연구 종료 상태(EOSSTT)\n  # 사용자 정의 매핑 함수\n  format_eosstt &lt;- function(x) {\n    case_when(\n      x == \"COMPLETED\" ~ \"COMPLETED\",\n      x == \"SCREEN FAILURE\" ~ NA_character_,\n      TRUE ~ \"DISCONTINUED\"\n    )\n  }\n  \n  # ADSL에 상태 변수 병합\n  adsl &lt;- adsl %&gt;%\n    derive_vars_merged(\n      dataset_add = ds,\n      by_vars = exprs(STUDYID, USUBJID),\n      filter_add = DSCAT == \"DISPOSITION EVENT\",\n      new_vars = exprs(EOSSTT = format_eosstt(DSDECOD)),\n      missing_values = exprs(EOSSTT = \"ONGOING\")  # 결측 시 \"ONGOING\" 할당\n    )\n  \n  head(adsl %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"EOSDT\", \"EOSSTT\"))))  %&gt;% datatable()\n\n\n\n\n\n    매핑 규칙:\n    DSDECOD             EOSSTT\n    \"COMPLETED\"         \"COMPLETED\"\n    \"SCREEN FAILURE\"    NA\n    기타                \"DISCONTINUED\"\n    \n    특이 사항: 처분 데이터가 없는 대상자는 \"ONGOING\"으로 표시\n\n# 3. 사망일자(DTHDT)\n  adsl &lt;- adsl %&gt;%\n    derive_vars_dt(\n      new_vars_prefix = \"DTH\",      # 생성 변수: DTHDT, DTHDTF\n      dtc = DTHDTC,                 # 사망 일자 (DTC)\n      highest_imputation = \"M\",     # 월까지만 있는 경우 \"01\"일로 대체\n      date_imputation = \"first\"     # 일 결측 시 월의 첫째 날 대체\n    )\n  # 대체 규칙 (예시):\n    # \"2014-07\" → 2014-07-01\n    # \"2014\"    → 2014-01-01\n  \n# 4. 추가 날짜 변수  \n  adsl &lt;- adsl %&gt;%\n  # 무작위 배정 일자(RANDDT)\n  derive_vars_merged(\n    dataset_add = ds_ext,\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(RANDDT = DSSTDT),\n    filter_add = DSDECOD == \"RANDOMIZED\"\n  ) %&gt;%\n  # 스크린 실패 일자(SCRFDT)\n  derive_vars_merged(\n    dataset_add = ds_ext,\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(SCRFDT = DSSTDT),\n    filter_add = DSDECOD == \"SCREEN FAILURE\"\n  ) %&gt;%\n  # 최종 방문 일자(FRVDT)\n  derive_vars_merged(\n    dataset_add = ds_ext,\n    by_vars = exprs(STUDYID, USUBJID),\n    new_vars = exprs(FRVDT = DSSTDT),\n    filter_add = DSDECOD == \"FINAL RETRIEVAL VISIT\"\n  )\n  \nhead(adsl %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"RANDDT\", \"SCRFDT\", \"FRVDT\")))) %&gt;% datatable()\n\n\n\n\n\n\n# 5. 사망 관련 기간 변수\nadsl &lt;- adsl %&gt;%\n  # 사망 상대일(DTHADY): 치료 시작일(TRTSDT) 기준\n  derive_vars_duration(\n    new_var = DTHADY,\n    start_date = TRTSDT,\n    end_date = DTHDT,\n    add_one = TRUE  # 시작일 포함 (+1일)\n  ) %&gt;%\n  \n  # 최종 투여-사망 간격(LDDTHELD): 치료 종료일(TRTEDT) 기준\n  derive_vars_duration(\n    new_var = LDDTHELD,\n    start_date = TRTEDT,\n    end_date = DTHDT,\n    add_one = FALSE  # 시작일 미포함\n  )\n\n# DTHADY    = DTHDT - TRTSDT + 1\n# LDDTHELD  = DTHDT - TRTEDT\n\n# 사용자 정의 함수\nassign_randfl &lt;- function(x) {\n  if_else(!is.na(x), \"Y\", NA_character_)\n}\n\n# 플래그 할당\nadsl &lt;- adsl %&gt;%\n  mutate(RANDFL = assign_randfl(RANDDT))\n\nhead(adsl %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"TRTSDT\", \"TRTEDT\", \"DTHADY\", \"LDDTHELD\",  \"RANDDT\", \"RANDFL\")))) %&gt;% datatable()\n\n\n\n\n\nDerive Cause of Death\nadmiral::derive_vars_extreme_event()를 이용,  dm/suppdm이 아닌 다른 위치(AE/DS 등)의 사망 원인 정보를 통합, ADSL에 추가할 수 있다.\n\n\nDTHDT : death date\n\nDTHCAUS : death cause\n\nDTHDOM : death domain (ex: “AE”/“DS”)\n\n\nadsl &lt;- adsl %&gt;%\n  derive_vars_extreme_event(\n    # 그룹화 변수\n    by_vars = exprs(STUDYID, USUBJID),\n\n    # 사망 원인 이벤트 정의\n    events = list(\n      # 1. AE 도메인에서 사망 원인 추출\n      event(\n        dataset_name = \"ae\",\n        condition = AEOUT == \"FATAL\",  # 치명적 부작용 필터\n        set_values_to = exprs(\n          DTHCAUS = AEDECOD,  # 부작용 용어 사용\n          DTHDOM = \"AE\"       # 출처 도메인 표시\n        )\n      ),\n      # 2. DS 도메인에서 사망 원인 추출\n      event(\n        dataset_name = \"ds\",\n        condition = DSDECOD == \"DEATH\" & grepl(\"DEATH DUE TO\", DSTERM),\n        set_values_to = exprs(\n          DTHCAUS = DSTERM,   # 처분 용어 사용\n          DTHDOM = \"DS\"       # 출처 도메인 표시\n        )\n      )\n    ),\n\n    # 소스 데이터셋 지정\n    source_datasets = list(ae = ae, ds = ds),\n\n    # 기술적 옵션\n    tmp_event_nr_var = event_nr,  # 임시 이벤트 번호 변수\n    order = exprs(event_nr),      # 정렬 기준 (첫 번째 이벤트 선택)\n    mode = \"first\",               # 최초 발생 이벤트 우선\n\n    # 출력 변수\n    new_vars = exprs(DTHCAUS, DTHDOM)\n  )\n\nadsl %&gt;% filter(!is.na(DTHDT) )  %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"DTHDT\", \"DTHCAUS\", \"DTHDOM\"))) %&gt;% datatable()\n\n\n\n\n\nDerive Other Grouping Variables\n\n지역, 인종, 사망 범주 등 다양한 grouping variable들을 추가할 수 있다.\n\nderive_vars_cat()을 이용, factor와 numeric variable 추가을 추가할 수 있다.\n\n주요 변수\n변수명    설명                   생성 규칙 예시\nREGION1   지역 그룹 (문자형)   북미 vs. 기타 국가\nREGION1N  지역 코드 (숫자형)   1=North America, 2=Rest of the World\nRACEGR1   인종 그룹 (문자형)   White vs. Non-white\nRACEGR1N  인종 코드 (숫자형)   1=White, 2=Non-white\nDTHCGR1   사망 원인 그룹       Adverse Event vs. Progressive Disease\nDTHCGR1N  사망 원인 코드       1=ADVERSE EVENT, 2=PROGRESSIVE DISEASE\n\n# REGION1\nregion1_lookup &lt;- exprs(\n  ~condition,                     ~REGION1,          ~REGION1N,\n  COUNTRY %in% c(\"CAN\", \"USA\"),   \"North America\",     1,\n  !is.na(COUNTRY),                \"Rest of the World\", 2,\n  is.na(COUNTRY),                 \"Missing\",           3\n)\n\n# RACEGR1\nracegr1_lookup &lt;- exprs(\n  ~condition,        ~RACEGR1,    ~RACEGR1N,\n  RACE == \"WHITE\",   \"White\",     1,\n  RACE != \"WHITE\",   \"Non-white\", 2,\n  is.na(RACE),       \"Missing\",   3\n)\n\n#DTHCG1\ndthcgr1_lookup &lt;- exprs(\n  ~condition,                                                   ~DTHCGR1,              ~DTHCGR1N,\n  DTHDOM == \"AE\",                                               \"ADVERSE EVENT\",       1,\n  str_detect(DTHCAUS, \"(PROGRESSIVE DISEASE|DISEASE RELAPSE)\"), \"PROGRESSIVE DISEASE\", 2,\n  !is.na(DTHCAUS),                                              \"OTHER\",               3,\n  is.na(DTHDOM),                                                NA_character_,         NA\n)\n\n# 변수 생성\nadsl &lt;- adsl %&gt;%\n  derive_vars_cat(definition = region1_lookup) %&gt;%\n  derive_vars_cat(definition = racegr1_lookup) %&gt;%\n  derive_vars_cat(definition = dthcgr1_lookup)\n\nadsl %&gt;% head %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"COUNTRY\", \"REGION1\", \"REGION1N\", \"RACE\", \"RACEGR1\", \"RACEGR1N\")))%&gt;% datatable()\n\n\n\n\nadsl %&gt;% filter(!is.na(DTHDT) )  %&gt;% select((c(\"STUDYID\", \"USUBJID\", \"DTHDOM\", \"DTHCAUS\", \"DTHCGR1\", \"DTHCGR1N\"))) %&gt;% datatable()\n\n\n\n\n\nData check, create eSub XPT\nADSL data에 metadata를 적용, 검토해보자. 검토와 동시에 최종 결과를 확인할 수 있다. \n\ndir &lt;- tempdir()\n\nadsl %&gt;%\n  # 1. 변수 존재 여부 검증\n  check_variables(metacore) %&gt;%  # 메타데이터에 정의된 변수 모두 존재하는지 확인\n  \n  # 2. 코드리스트(Controlled Terminology) 검증\n  check_ct_data(metacore, na_acceptable = TRUE) %&gt;%  # 허용된 값만 포함하는지 확인 (NA 허용)\n  \n  # 3. 컬럼 순서 정렬\n  order_cols(metacore) %&gt;%  # 메타데이터 스펙에 정의된 순서로 재배열\n  \n  # 4. 키 기준 행 정렬\n  sort_by_key(metacore) %&gt;%  # 메타데이터의 sort_key 변수(예: USUBJID) 기준 정렬\n  \n  # 5. eSub XPT로 저장\n  xportr_type(metacore, domain = \"ADSL\") %&gt;%  # 변수 타입 강제 변환 (예: 문자 → 숫자)\n  xportr_length(metacore) %&gt;%                 # SAS 길이 지정 (예: 문자열 200자)\n  xportr_label(metacore) %&gt;%                  # 변수 라벨 할당 (예: \"Age at Baseline\")\n  xportr_df_label(metacore) %&gt;%               # 데이터셋 라벨 할당 (예: \"Adverse Events Analysis Dataset\")\n  xportr_write(file.path(dir, \"adsl.xpt\"), metadata = metacore, domain = \"ADSL\") %&gt;% \n  datatable()"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#adae",
    "href": "posts/2025-04-08-ADaM/index.html#adae",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "ADAE",
    "text": "ADAE\n임상시험에서의 이상반응(Adverse Events, AE) 데이터를 분석하기 위한 ADAE 데이터셋 생성 과정 ADSL + AE = ADAE라 생각하자.\n1. 개요\n∙ 목적: 이상반응 데이터의 표준화된 분석을 위한 데이터셋 생성       안전성 평가(Safety Analysis)의 핵심 데이터셋으로, 다음 목적에 사용:\n      ∘ 이상반응 발생률 계산\n      ∘ 치료군 간 AE 비교\n      ∘ 중대한 이상반응(SAE) 추적\n      ∘ 규제 기관(FDA, PMDA 등) 제출용  ∙ 입력 데이터:\n      ∘ AE (기본 이상반응 데이터)\n      ∘ ADSL (환자 기본 정보)  ∙ 출력 데이터:\n      ∘ ADAE (Analysis Dataset for Adverse Events)\n2. 주요변수\n\n\n\n식별자 변수\n\n변수명\n설명\n예시\n필수여부\n\n\n\nSTUDYID\n연구 식별자\n\"CDISCPILOT01\"\nRequired\n\n\nUSUBJID\n환자 고유 ID\n\"01-701-1015\"\nRequired\n\n\nAESEQ\nAE 시퀀스 번호\n1, 2, 3...\nRequired\n\n\n\n\n\n\n시간 관련 변수\n\n변수명\n설명\n예시\n\n\n\nAESTDT\nAE 시작일\n2023-05-20\n\n\nAEENDT\nAE 종료일\n2023-05-25\n\n\nAEDUR\nAE 지속일수 (파생 변수)\n6\n\n\n\n\n\n\n이상반응 특성\n\n변수명\n설명\n예시.값\n\n\n\nAETERM\n보고된 AE 명칭\n\"Headache\"\n\n\nAEDECOD\n표준화된 AE 용어 (PT)\n\"HEADACHE\"\n\n\nAEBODSYS\n신체계 (SOC)\n\"NERVOUS SYSTEM\"\n\n\nAESEV\n중증도\n\"MILD\", \"SEVERE\"\n\n\n\n\n\n\n평가 변수\n\n변수명\n설명\n값.범위\n\n\n\nAEREL\n약물 관련성\n\"Y\"/\"N\"/NA\n\n\nAESER\n중대한 AE 여부\n\"Y\"/\"N\"\n\n\nAEOUT\n결과\n\"RECOVERED\"\n\n\n\n\n\n\n파생 플래그\n\n변수명\n설명\n생성.로직\n\n\n\nTRTEMFL\n치료 기간 중 발생 여부\nAESTDT ≤ TRTEDT\n\n\nAESERFL\n중대한 AE 플래그\nAESER == 'Y'\n\n\nAERELFL\n관련성 플래그\nAEREL == 'Y'\n\n\n\n\n\n\n치료 정보\n\n변수명\n설명\n출처\n\n\n\nTRT01A\n실제 치료군\nADSL\n\n\nTRTSDT\n치료 시작일\nADSL\n\n\n\n\n\nADAE_example\n예제는 아래와 같은 순서로 진행한다.\n\ndata/Packages loading\nmetacore specification loading\nADSL variable selection\nDerivation Building\nDerive Analysis Dates\nDerive Duration/Date of last dose\nDerive Analysis/Occurence Flags\nDerive Query Variables\nAdd ADSL variables\nApplying metadate\n\nData/Packages loading\n\nlibrary(metacore)\nlibrary(metatools)\nlibrary(pharmaversesdtm)\nlibrary(pharmaverseadam)\nlibrary(admiral)\nlibrary(xportr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(reactable)\n\n# Read in input data\nadsl &lt;- pharmaverseadam::adsl\nae &lt;- pharmaversesdtm::ae\nex &lt;- pharmaversesdtm::ex\n\n# When SAS datasets are imported into R using haven::read_sas(), missing\nae &lt;- convert_blanks_to_na(ae)\nex &lt;- convert_blanks_to_na(ex)\n\nMetacore specification loading\n\n# 메타데이터 Excel 파일에서 사양 추출\nmetacore &lt;- spec_to_metacore(\n  path = \"./safety_specs.xlsx\",  # 메타데이터 파일 경로\n  where_sep_sheet = FALSE  # 모든 데이터셋이 한 시트에 있는 경우\n) %&gt;%\n  select_dataset(\"ADAE\")  # ADAE 데이터셋 선택\n\nADSL variable selection\n\n# ADSL에서 필요한 변수 선택\nadsl_vars &lt;- exprs(TRTSDT, TRTEDT, DTHDT)  # 치료 시작/종료일, 사망일\n\n# AE 데이터에 ADSL 변수 병합\nadae &lt;- ae %&gt;%\n  derive_vars_merged(\n    dataset_add = adsl,          # ADSL 데이터\n    new_vars = adsl_vars,        # 추가할 변수\n    by_vars = exprs(STUDYID, USUBJID)  # 병합 키\n  )\n\nDerive Analysis Dates 1. Analysis Date/Relative Analysis Day admiral::derive_vars_dt() and admiral::derive_vars_dy() \n\n  # AE 종료일(AENDT) 파생 (부분일자 대체)\n  adae &lt;- adae %&gt;%\n    derive_vars_dt(\n      new_vars_prefix = \"AEN\",           # 생성 변수: AENDT, AENDTF\n      dtc = AEENDTC,                     # 원본 AE 종료일 문자형\n      date_imputation = \"last\",          # 결측일은 월의 마지막 날로 대체 (\"2023-02\" → 2023-02-28)\n      highest_imputation = \"M\",          # 월까지만 있는 경우 대체 수행\n      flag_imputation = \"auto\"           # 대체 여부 플래그 자동 생성 (AENDTF)\n    ) %&gt;%\n    \n    # AE 시작일(ASTDT) 파생 (추가 제약조건 적용)\n    derive_vars_dt(\n      new_vars_prefix = \"AST\",           # 생성 변수: ASTDT, ASTDTF\n      dtc = AESTDTC,\n      highest_imputation = \"M\",\n      flag_imputation = \"auto\",\n      min_dates = exprs(TRTSDT),         # 치료시작일보다 이전일 수 없음\n      max_dates = exprs(AENDT)           # AE 종료일보다 이후일 수 없음\n    ) %&gt;%\n    \n    # 치료 시작일 기준 상대일자 파생\n    derive_vars_dy(\n      reference_date = TRTSDT,           # 기준일자\n      source_vars = exprs(ASTDT, AENDT)  # 계산 대상\n    )                                    # 생성 변수: ASTDY, AENDY\n\nhead(adae) %&gt;% select(c(\"USUBJID\", \"AESTDTC\",\"ASTDT\",\"ASTDTF\",\"ASTDY\",\"AEENDTC\",\"AENDT\",\"AENDTF\",\"AENDY\")) %&gt;% datatable()\n\n\n\n\n\n2. AE duration admiral::derive_vars_duration() admiral::derive_vars_joined() : 다른 data(예: ex)로 부터 join할 수 있다. \n\nadae &lt;- adae %&gt;%\n  derive_vars_duration(\n    new_var = ADURN,        # 지속일수 변수명\n    new_var_unit = ADURU,   # 단위 변수명 (기본값 \"DAYS\")\n    start_date = ASTDT,     # 시작일\n    end_date = AENDT,       # 종료일\n    add_one = TRUE          # 시작일 포함 계산 (기본값 TRUE)\n  )\n\n# EX 데이터 전처리\n# &gt; last dose date을 위해 ex에서 exposure information을 불러오자.\nex &lt;- ex %&gt;%\n  derive_vars_dt(\n    dtc = EXENDTC,\n    new_vars_prefix = \"EXEN\"  # EXENDT 생성\n  )\n\n# ADAE에 최종 투여일 병합\nadae &lt;- adae %&gt;%\n  derive_vars_joined(\n    dataset_add = ex,\n    by_vars = exprs(STUDYID, USUBJID),  # 기본 조인 키\n    order = exprs(EXENDT),              # EXENDT 기준 정렬\n    new_vars = exprs(LDOSEDT = EXENDT), # 생성 변수\n    join_vars = exprs(EXENDT),          # 조인 조건에 사용할 변수\n    join_type = \"all\",\n    filter_add = (EXDOSE &gt; 0 | (EXDOSE == 0 & str_detect(EXTRT, \"PLACEBO\"))) & !is.na(EXENDT),\n    filter_join = EXENDT &lt;= ASTDT,      # AE 시작일 이전의 투여기록만\n    mode = \"last\"                       # 가장 가까운 기록 선택\n  )\n\n\nhead(ex) %&gt;% select(c(\"USUBJID\", \"EXTRT\",\"EXDOSE\",\"EXENDT\")) %&gt;% datatable\n\n\n\n\nhead(adae) %&gt;% select(c(\"USUBJID\", \"ASTDT\",\"AENDT\",\"ADURN\",\"ADURU\",\"LDOSEDT\")) %&gt;% datatable()\n\n\n\n\n\nDerive Flags\nadmiral::derive_var_trtemfl() : Treatment Emergent Flag (TRTEMFL)       # TRTSDT &lt; ASTDT,    TRTEMFL: Y admiral::derive_var_ontrtfl() : On-Treatment Flag (ONTRTFL)       # TRTSDT &lt; ASTDT &lt; TRTEDT +30day,    ONTRTFL: Y admiral::derive_var_extreme_flag() : AE occurance initial Flag (AOCCIFL) \n\n# Derive TRTEMFL and ONTRTFL\nadae &lt;- adae %&gt;%\n  derive_var_trtemfl(\n    start_date = ASTDT,       # AE 시작일\n    end_date = AENDT,         # AE 종료일\n    trt_start_date = TRTSDT,  # 치료 시작일\n    trt_end_date = TRTEDT     # 치료 종료일\n  ) %&gt;%\n  derive_var_ontrtfl(\n    start_date = ASTDT,        # AE 시작일\n    ref_start_date = TRTSDT,   # 치료 시작일\n    ref_end_date = TRTEDT,     # 치료 종료일\n    ref_end_window = 30        # 치료 종료 후 추가 기간 (일)\n  )\nhead(adae) %&gt;% select(c(\"USUBJID\", \"ASTDT\", \"AENDT\",\"TRTSDT\",\"TRTEDT\",\"TRTEMFL\",\"ONTRTFL\")) %&gt;% datatable()\n\n\n\n\n#Derive AOCCIFL\n\nadae &lt;- adae %&gt;%\n  # create temporary numeric ASEVN for sorting purpose\n  mutate(TEMP_AESEVN = as.integer(factor(AESEV, levels = c(\"SEVERE\", \"MODERATE\", \"MILD\")))) %&gt;%\n  derive_var_extreme_flag(\n    new_var = AOCCIFL,                  # 새 플래그 변수명\n    by_vars = exprs(STUDYID, USUBJID),  # 그룹화 기준 (환자별)\n    order = exprs(TEMP_AESEVN, ASTDT, AESEQ),  # 정렬 순서\n    mode = \"first\"                      # 첫 번째 레코드 선택\n)\nhead(adae) %&gt;% select(c(\"USUBJID\", \"ASTDT\", \"AESEQ\",\"AESEV\",\"AOCCIFL\")) %&gt;% datatable()\n\n\n\n\n\nDerive Query Variables\nQuery dataset: AE 관련 용어들을 정리해놓은 basket으로 mapping 규칙을 정의. ex) SMQs, CQs. {admiral}Queries Dataset Vignette 참고. admiral::derive_vars_query()을 이용한다. \n\nqueries &lt;- admiral::queries %&gt;%\n  filter(PREFIX %in% c(\"CQ01\", \"SMQ02\"))  # CQ01(커스텀)과 SMQ02(표준)만 필터링\n\nqueries %&gt;% head %&gt;% datatable\n\n\n\n\n# query변수 생성\nadae &lt;- adae %&gt;%\n  derive_vars_query(dataset_queries = queries)\n\nhead(adae) %&gt;% select(c(\"USUBJID\", \"AEDECOD\", \"CQ01NAM\",\"SMQ02NAM\")) %&gt;% datatable()\n\n\n\n\n\nAdd ADSL variables\n\nadae &lt;- adae %&gt;%\n  derive_vars_merged(\n    dataset_add = select(adsl, !!!negate_vars(adsl_vars)),  # adsl_vars에 없는 변수만 선택\n    by_vars = exprs(STUDYID, USUBJID)  # 키 변수\n  )\n\nAssociated check perform, Create eSub XPT  - metatools & xportr package 이용.\n\ndir &lt;- tempdir()  # 임시 디렉토리 지정\n\nadae %&gt;%\n  drop_unspec_vars(metacore) %&gt;%  # 사양서(metacore)에 없는 변수 제거\n  check_variables(metacore) %&gt;%   # 필수 변수 존재 여부 검증\n  check_ct_data(metacore, na_acceptable = TRUE) %&gt;%  # CT(Controlled Terminology) 준수 확인\n  order_cols(metacore) %&gt;%        # 컬럼 순서 사양서에 맞춤\n  sort_by_key(metacore) %&gt;%       # 키 변수(SORT KEYS)로 정렬\n  xportr_type(metacore, domain = \"ADAE\") %&gt;%  # 변수 타입 강제 변환 (예: 문자 → 숫자)\n  xportr_length(metacore) %&gt;%     # SAS 길이 지정 (예: 문자열 200자)\n  xportr_label(metacore) %&gt;%      # 변수 라벨 할당 (예: \"Age at Baseline\")\n  xportr_df_label(metacore) %&gt;%   # 데이터셋 라벨 할당 (예: \"Adverse Events Analysis Dataset\")\n  xportr_write(file.path(dir, \"adae.xpt\"), metadata = metacore, domain = \"ADAE\") %&gt;%  # XPT 파일 저장\n  datatable()"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#adpc",
    "href": "posts/2025-04-08-ADaM/index.html#adpc",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "ADPC",
    "text": "ADPC\n약물동력학(PK) 데이터를 분석하기 위한 핵심 데이터셋으로, 시간에 따른 혈중 약물 농도를 기록한다.\n목적:  - 시간에 따른 약물 농도(PK 데이터)를 분석하여 약물 흡수, 분포, 대사, 배설(ADME) 평가.  구조:  - 각 행은 특정 시간점(NTIM)에서의 환자별 농도(DV)를 기록하며, USUBJID와 시간점으로 구분.  주요 활용:  - PK 파라미터(예: AUC, Cmax, Tmax) 계산 및 모델링. \n주요 변수 (Columns)\n\n\n\n\n\n\n\nCategory\nVariables\nDescription\n\n\n\nIdentifiers\n• USUBJID• SUBJID• VISIT• VISITNUM\n\n• 고유 환자 ID• 대상자 ID• 방문 이름• 방문 번호\n\n\nTime Info\n• NTIM• TIME• NOM_*\n\n• 실제 시간(hr)• 계획 시간• 명목 시간(일/주 단위)\n\n\nConcentration\n• DV• LLOQ• BLQ\n\n• 관측 농도(ng/mL)• 정량 한계• 한계 미만 여부(Y/N)\n\n\nDose Info\n• DOSE• DOSEUNIT• ROUTE\n\n• 투여 용량• 단위(mg/kg)• 투여 경로(IV/PO)\n\n\nPK Parameters\n• AUC• CMAX• TMAX\n\n• 농도-시간 곡선 아래 면적• 최대 농도• 최대 농도 도달 시간\n\n\nStandards\nAll variables\nCDISC ADaM 표준 준수"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#adppk",
    "href": "posts/2025-04-08-ADaM/index.html#adppk",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "ADPPK",
    "text": "ADPPK\nDPPK는 약물동력학(PK) 파라미터(예: AUC, Cmax, Tmax)를 분석하기 위한 데이터셋으로, ADPC에서 계산된 PK 파라미터를 주로 다룬다.\n목적:  - PK 파라미터를 조직화하여 약물 노출량, 반감기, 청소율 등을 분석.  구조:  - 각 행은 환자별 PK 파라미터를 포함하며, USUBJID와 파라미터 종류(PARAM/PARAMCD)로 구분.  차이점:  - ADPC: 시간별 농도 데이터 (Raw PK 데이터).  - ADPPK: 파생된 PK 파라미터 (예: AUCINF, CL). \n주요 변수 (Columns) \n\n\n\n\n\n\n\nCategory\nVariables\nDescription\n\n\n\nIdentifiers\n• USUBJID• SUBJID• STUDYID\n\n• 고유 환자 ID• 대상자 ID• 연구 ID\n\n\nPK Parameters\n• PARAM/PARAMCD• AVAL• AVALU\n\n• 파라미터 이름/코드• 실제 값• 단위\n\n\nDose Info\n• DOSE• DOSU\n\n• 투여 용량• 용량 단위\n\n\nAnalysis\n• ANALYZT• METHOD\n\n• 분석 방법• 계산 알고리즘\n\n\nStandards\nAll variables\nCDISC ADaM 표준 준수"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#demopragphic-table",
    "href": "posts/2025-04-08-ADaM/index.html#demopragphic-table",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "demopragphic table",
    "text": "demopragphic table\n\n\nADSL 데이터를 이용하여 작성한다.\n\ntern: 임상시험 보고용 테이블 및 그래프 생성을 위한 패키지\n\n\nlibrary(tern)   \n\n# 1. 결측값 명시적 처리\nadsl2 &lt;- adsl  %&gt;%  df_explicit_na()\n\n# 2. 테이블 레이아웃 정의\nlyt &lt;- basic_table(show_colcounts = TRUE)  %&gt;%  \n  split_cols_by(var = \"ACTARM\")  %&gt;%   # 치료군별 컬럼 분할\n  add_overall_col(\"All Patients\") %&gt;%   # 전체 집단 컬럼 추가\n  analyze_vars(\n    vars = c(\"AGE\", \"AGEGR1\", \"SEX\", \"RACE\"),\n    var_labels = c(\"Age (yr)\", \"Age group\", \"Sex\", \"Race\")\n  )\n\n# 3. 테이블 빌드\nresult &lt;- build_table(lyt, adsl2) %&gt;% print\n\n                                       Placebo     Screen Failure   Xanomeline High Dose   Xanomeline Low Dose   All Patients\n                                       (N=86)          (N=52)              (N=72)                (N=96)            (N=306)   \n—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\nAge (yr)                                                                                                                     \n  n                                      86              52                  72                    96                306     \n  Mean (SD)                          75.2 (8.6)      75.1 (9.7)          73.8 (7.9)            76.0 (8.1)         75.1 (8.5) \n  Median                                76.0            76.0                75.5                  78.0               77.0    \n  Min - Max                          52.0 - 89.0    50.0 - 89.0         56.0 - 88.0            51.0 - 88.0       50.0 - 89.0 \nAge group                                                                                                                    \n  n                                      86              52                  72                    96                306     \n  &gt;64                                72 (83.7%)      43 (82.7%)          61 (84.7%)            88 (91.7%)        264 (86.3%) \n  18-64                              14 (16.3%)      9 (17.3%)           11 (15.3%)             8 (8.3%)          42 (13.7%) \nSex                                                                                                                          \n  n                                      86              52                  72                    96                306     \n  F                                  53 (61.6%)      36 (69.2%)          35 (48.6%)            55 (57.3%)        179 (58.5%) \n  M                                  33 (38.4%)      16 (30.8%)          37 (51.4%)            41 (42.7%)        127 (41.5%) \nRace                                                                                                                         \n  n                                      86              52                  72                    96                306     \n  AMERICAN INDIAN OR ALASKA NATIVE        0           1 (1.9%)            1 (1.4%)                  0              2 (0.7%)  \n  ASIAN                                   0           2 (3.8%)               0                      0              2 (0.7%)  \n  BLACK OR AFRICAN AMERICAN           8 (9.3%)       6 (11.5%)           9 (12.5%)              6 (6.2%)          29 (9.5%)  \n  WHITE                              78 (90.7%)      43 (82.7%)          62 (86.1%)            90 (93.8%)        273 (89.2%)"
  },
  {
    "objectID": "posts/2025-04-08-ADaM/index.html#adverse-event-table",
    "href": "posts/2025-04-08-ADaM/index.html#adverse-event-table",
    "title": "ADaM Compliant ADSL Dataset Generation",
    "section": "adverse event table",
    "text": "adverse event table\n\n\nADSL, ADAE 데이터를 이용하여 작성한다.\n\n입력 데이터:     adsl (기본 환자 정보)     adae (이상반응 데이터)  출력:     치료군별/신체계별 AE 발생 현황을 보여주는 분석용 테이블 \n\nlibrary(pharmaverseadam);library(tern)\n\n# 데이터 전처리\n  # 결측값 처리\n  adsl &lt;- adsl %&gt;% df_explicit_na()\n  adae &lt;- adae %&gt;% df_explicit_na()\n  \n  # 라벨 추가 및 필터링\n  adae &lt;- adae %&gt;%\n    var_relabel(\n      AEBODSYS = \"MedDRA System Organ Class\",  # 신체계 라벨\n      AEDECOD = \"MedDRA Preferred Term\"        # PT 라벨\n    ) %&gt;%\n    filter(SAFFL == \"Y\")  # 안전성 평가 대상만 선택\n        # 분할 함수 정의\n          split_fun &lt;- drop_split_levels\n\n# 테이블 레이아웃 정의.\n  # 테이블 구조 설계\n  lyt &lt;- basic_table(show_colcounts = TRUE) %&gt;%\n    \n    # 1. 컬럼 분할 (치료군 기준)\n    split_cols_by(var = \"ACTARM\") %&gt;%  \n    add_overall_col(label = \"All Patients\") %&gt;%\n    \n    # 2. 전체 환자/이벤트 수 요약\n    analyze_num_patients(\n      vars = \"USUBJID\",\n      .stats = c(\"unique\", \"nonunique\"),\n      .labels = c(\n        unique = \"Total number of patients with at least one adverse event\",\n        nonunique = \"Overall total number of events\"\n      )\n    ) %&gt;%\n    \n    # 3. 행 분할 (신체계 기준)\n    split_rows_by(\n      \"AEBODSYS\",\n      child_labels = \"visible\",\n      nested = FALSE,\n      split_fun = split_fun,\n      label_pos = \"topleft\",\n      split_label = obj_label(adae$AEBODSYS)\n    ) %&gt;%\n    \n    # 4. 환자/이벤트 수 요약\n    summarize_num_patients(\n      var = \"USUBJID\",\n      .stats = c(\"unique\", \"nonunique\"),\n      .labels = c(\n        unique = \"Total number of patients with at least one adverse event\",\n        nonunique = \"Total number of events\"\n      )\n    ) %&gt;%\n    \n    # 5. 특정 반응(PT) 발생 횟수 계산\n    count_occurrences(\n      vars = \"AEDECOD\",\n      .indent_mods = -1L\n    ) %&gt;%\n    \n    # 6. 라벨 추가\n    append_varlabels(adae, \"AEDECOD\", indent = 1L)\n\nresult &lt;- build_table(lyt, df = adae, alt_counts_df = adsl); result\n\nMedDRA System Organ Class                                              Placebo     Xanomeline High Dose   Xanomeline Low Dose   All Patients\n  MedDRA Preferred Term                                                 (N=86)            (N=72)                (N=96)            (N=306)   \n————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\nTotal number of patients with at least one adverse event              69 (80.2%)        70 (97.2%)            86 (89.6%)        225 (73.5%) \nOverall total number of events                                           301               436                    454               1191    \nCARDIAC DISORDERS                                                                                                                           \n  Total number of patients with at least one adverse event            13 (15.1%)        15 (20.8%)            16 (16.7%)         44 (14.4%) \n  Total number of events                                                  27                30                    34                 91     \n  ATRIAL FIBRILLATION                                                  1 (1.2%)          2 (2.8%)              2 (2.1%)           5 (1.6%)  \n  ATRIAL FLUTTER                                                          0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  ATRIAL HYPERTROPHY                                                   1 (1.2%)             0                      0              1 (0.3%)  \n  ATRIOVENTRICULAR BLOCK FIRST DEGREE                                  1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  ATRIOVENTRICULAR BLOCK SECOND DEGREE                                 2 (2.3%)          1 (1.4%)              2 (2.1%)           5 (1.6%)  \n  BRADYCARDIA                                                          1 (1.2%)             0                      0              1 (0.3%)  \n  BUNDLE BRANCH BLOCK LEFT                                             1 (1.2%)             0                      0              1 (0.3%)  \n  BUNDLE BRANCH BLOCK RIGHT                                            1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  CARDIAC DISORDER                                                        0              1 (1.4%)                  0              1 (0.3%)  \n  CARDIAC FAILURE CONGESTIVE                                           1 (1.2%)             0                      0              1 (0.3%)  \n  MYOCARDIAL INFARCTION                                                4 (4.7%)          4 (5.6%)              2 (2.1%)          10 (3.3%)  \n  PALPITATIONS                                                            0                 0                  2 (2.1%)           2 (0.7%)  \n  SINUS ARRHYTHMIA                                                     1 (1.2%)             0                      0              1 (0.3%)  \n  SINUS BRADYCARDIA                                                    2 (2.3%)         8 (11.1%)              7 (7.3%)          17 (5.6%)  \n  SUPRAVENTRICULAR EXTRASYSTOLES                                       1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  SUPRAVENTRICULAR TACHYCARDIA                                            0                 0                  1 (1.0%)           1 (0.3%)  \n  TACHYCARDIA                                                          1 (1.2%)             0                      0              1 (0.3%)  \n  VENTRICULAR EXTRASYSTOLES                                               0              1 (1.4%)              2 (2.1%)           3 (1.0%)  \n  VENTRICULAR HYPERTROPHY                                              1 (1.2%)             0                      0              1 (0.3%)  \n  WOLFF-PARKINSON-WHITE SYNDROME                                          0                 0                  1 (1.0%)           1 (0.3%)  \nCONGENITAL, FAMILIAL AND GENETIC DISORDERS                                                                                                  \n  Total number of patients with at least one adverse event                0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \n  Total number of events                                                  0                 2                      1                 3      \n  VENTRICULAR SEPTAL DEFECT                                               0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \nEAR AND LABYRINTH DISORDERS                                                                                                                 \n  Total number of patients with at least one adverse event             1 (1.2%)          1 (1.4%)              2 (2.1%)           4 (1.3%)  \n  Total number of events                                                  2                 1                      3                 6      \n  CERUMEN IMPACTION                                                       0                 0                  1 (1.0%)           1 (0.3%)  \n  EAR PAIN                                                             1 (1.2%)             0                      0              1 (0.3%)  \n  TINNITUS                                                                0                 0                  1 (1.0%)           1 (0.3%)  \n  VERTIGO                                                                 0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \nEYE DISORDERS                                                                                                                               \n  Total number of patients with at least one adverse event             4 (4.7%)          1 (1.4%)              2 (2.1%)           7 (2.3%)  \n  Total number of events                                                  8                 2                      2                 12     \n  CONJUNCTIVAL HAEMORRHAGE                                                0                 0                  1 (1.0%)           1 (0.3%)  \n  CONJUNCTIVITIS                                                       2 (2.3%)             0                      0              2 (0.7%)  \n  EYE ALLERGY                                                          1 (1.2%)             0                      0              1 (0.3%)  \n  EYE PRURITUS                                                         1 (1.2%)             0                      0              1 (0.3%)  \n  EYE SWELLING                                                         1 (1.2%)             0                      0              1 (0.3%)  \n  GLAUCOMA                                                             1 (1.2%)             0                      0              1 (0.3%)  \n  VISION BLURRED                                                          0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \nGASTROINTESTINAL DISORDERS                                                                                                                  \n  Total number of patients with at least one adverse event            17 (19.8%)        20 (27.8%)            16 (16.7%)         53 (17.3%) \n  Total number of events                                                  26                35                    26                 87     \n  ABDOMINAL DISCOMFORT                                                    0              1 (1.4%)                  0              1 (0.3%)  \n  ABDOMINAL PAIN                                                       1 (1.2%)          1 (1.4%)              3 (3.1%)           5 (1.6%)  \n  CONSTIPATION                                                         1 (1.2%)             0                      0              1 (0.3%)  \n  DIARRHOEA                                                           9 (10.5%)          3 (4.2%)              6 (6.2%)          18 (5.9%)  \n  DYSPEPSIA                                                            1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  DYSPHAGIA                                                               0                 0                  1 (1.0%)           1 (0.3%)  \n  FLATULENCE                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  GASTROINTESTINAL HAEMORRHAGE                                            0              1 (1.4%)                  0              1 (0.3%)  \n  GASTROOESOPHAGEAL REFLUX DISEASE                                     1 (1.2%)             0                      0              1 (0.3%)  \n  GLOSSITIS                                                            1 (1.2%)             0                      0              1 (0.3%)  \n  HIATUS HERNIA                                                        1 (1.2%)             0                      0              1 (0.3%)  \n  NAUSEA                                                               3 (3.5%)          6 (8.3%)              3 (3.1%)          12 (3.9%)  \n  RECTAL HAEMORRHAGE                                                      0                 0                  1 (1.0%)           1 (0.3%)  \n  SALIVARY HYPERSECRETION                                                 0              4 (5.6%)                  0              4 (1.3%)  \n  STOMACH DISCOMFORT                                                      0              1 (1.4%)                  0              1 (0.3%)  \n  VOMITING                                                             3 (3.5%)          6 (8.3%)              4 (4.2%)          13 (4.2%)  \nGENERAL DISORDERS AND ADMINISTRATION SITE CONDITIONS                                                                                        \n  Total number of patients with at least one adverse event            21 (24.4%)        36 (50.0%)            51 (53.1%)        108 (35.3%) \n  Total number of events                                                  48               118                    126               292     \n  APPLICATION SITE BLEEDING                                               0                 0                  1 (1.0%)           1 (0.3%)  \n  APPLICATION SITE DERMATITIS                                          5 (5.8%)          7 (9.7%)              9 (9.4%)          21 (6.9%)  \n  APPLICATION SITE DESQUAMATION                                           0                 0                  1 (1.0%)           1 (0.3%)  \n  APPLICATION SITE DISCHARGE                                              0              1 (1.4%)                  0              1 (0.3%)  \n  APPLICATION SITE DISCOLOURATION                                         0                 0                  1 (1.0%)           1 (0.3%)  \n  APPLICATION SITE ERYTHEMA                                            3 (3.5%)         14 (19.4%)            13 (13.5%)         30 (9.8%)  \n  APPLICATION SITE INDURATION                                          1 (1.2%)             0                      0              1 (0.3%)  \n  APPLICATION SITE IRRITATION                                          3 (3.5%)         9 (12.5%)              9 (9.4%)          21 (6.9%)  \n  APPLICATION SITE PAIN                                                   0              2 (2.8%)                  0              2 (0.7%)  \n  APPLICATION SITE PERSPIRATION                                           0              2 (2.8%)                  0              2 (0.7%)  \n  APPLICATION SITE PRURITUS                                            6 (7.0%)         21 (29.2%)            23 (24.0%)         50 (16.3%) \n  APPLICATION SITE REACTION                                            1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  APPLICATION SITE SWELLING                                               0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \n  APPLICATION SITE URTICARIA                                              0              1 (1.4%)              2 (2.1%)           3 (1.0%)  \n  APPLICATION SITE VESICLES                                            1 (1.2%)          5 (6.9%)              5 (5.2%)          11 (3.6%)  \n  APPLICATION SITE WARMTH                                                 0                 0                  1 (1.0%)           1 (0.3%)  \n  ASTHENIA                                                             1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  CHEST DISCOMFORT                                                        0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  CHEST PAIN                                                              0              2 (2.8%)                  0              2 (0.7%)  \n  CHILLS                                                               1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  CYST                                                                    0                 0                  1 (1.0%)           1 (0.3%)  \n  FATIGUE                                                              1 (1.2%)          5 (6.9%)              5 (5.2%)          11 (3.6%)  \n  FEELING ABNORMAL                                                        0              1 (1.4%)                  0              1 (0.3%)  \n  FEELING COLD                                                            0              1 (1.4%)                  0              1 (0.3%)  \n  INFLAMMATION                                                            0                 0                  1 (1.0%)           1 (0.3%)  \n  MALAISE                                                                 0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \n  OEDEMA                                                                  0                 0                  2 (2.1%)           2 (0.7%)  \n  OEDEMA PERIPHERAL                                                    2 (2.3%)          2 (2.8%)              1 (1.0%)           5 (1.6%)  \n  PAIN                                                                    0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  PYREXIA                                                              2 (2.3%)             0                  1 (1.0%)           3 (1.0%)  \n  SECRETION DISCHARGE                                                     0                 0                  1 (1.0%)           1 (0.3%)  \n  SUDDEN DEATH                                                            0                 0                  1 (1.0%)           1 (0.3%)  \n  SWELLING                                                                0                 0                  1 (1.0%)           1 (0.3%)  \n  ULCER                                                                   0                 0                  1 (1.0%)           1 (0.3%)  \nHEPATOBILIARY DISORDERS                                                                                                                     \n  Total number of patients with at least one adverse event             1 (1.2%)             0                      0              1 (0.3%)  \n  Total number of events                                                  1                 0                      0                 1      \n  HYPERBILIRUBINAEMIA                                                  1 (1.2%)             0                      0              1 (0.3%)  \nIMMUNE SYSTEM DISORDERS                                                                                                                     \n  Total number of patients with at least one adverse event                0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  Total number of events                                                  0                 1                      2                 3      \n  HYPERSENSITIVITY                                                        0                 0                  1 (1.0%)           1 (0.3%)  \n  SEASONAL ALLERGY                                                        0              1 (1.4%)                  0              1 (0.3%)  \nINFECTIONS AND INFESTATIONS                                                                                                                 \n  Total number of patients with at least one adverse event            16 (18.6%)        13 (18.1%)            10 (10.4%)         39 (12.7%) \n  Total number of events                                                  35                20                    18                 73     \n  BRONCHITIS                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  CELLULITIS                                                              0                 0                  1 (1.0%)           1 (0.3%)  \n  CERVICITIS                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  CYSTITIS                                                             1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  EAR INFECTION                                                        2 (2.3%)             0                      0              2 (0.7%)  \n  GASTROENTERITIS VIRAL                                                1 (1.2%)             0                      0              1 (0.3%)  \n  HORDEOLUM                                                               0              1 (1.4%)                  0              1 (0.3%)  \n  INFLUENZA                                                            1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  LOCALISED INFECTION                                                  1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  LOWER RESPIRATORY TRACT INFECTION                                       0              1 (1.4%)                  0              1 (0.3%)  \n  NASOPHARYNGITIS                                                      2 (2.3%)          6 (8.3%)              4 (4.2%)          12 (3.9%)  \n  ONYCHOMYCOSIS                                                           0                 0                  1 (1.0%)           1 (0.3%)  \n  PNEUMONIA                                                               0                 0                  1 (1.0%)           1 (0.3%)  \n  RHINITIS                                                                0              1 (1.4%)                  0              1 (0.3%)  \n  UPPER RESPIRATORY TRACT INFECTION                                    6 (7.0%)          3 (4.2%)              1 (1.0%)          10 (3.3%)  \n  URINARY TRACT INFECTION                                              2 (2.3%)          1 (1.4%)                  0              3 (1.0%)  \n  VAGINAL MYCOSIS                                                      1 (1.2%)             0                      0              1 (0.3%)  \n  VIRAL INFECTION                                                         0                 0                  1 (1.0%)           1 (0.3%)  \nINJURY, POISONING AND PROCEDURAL COMPLICATIONS                                                                                              \n  Total number of patients with at least one adverse event             4 (4.7%)          5 (6.9%)              5 (5.2%)          14 (4.6%)  \n  Total number of events                                                  9                 8                     12                 29     \n  CONTUSION                                                            1 (1.2%)          2 (2.8%)              1 (1.0%)           4 (1.3%)  \n  EXCORIATION                                                          2 (2.3%)          1 (1.4%)              1 (1.0%)           4 (1.3%)  \n  FACIAL BONES FRACTURE                                                   0              1 (1.4%)                  0              1 (0.3%)  \n  FALL                                                                 1 (1.2%)          1 (1.4%)              2 (2.1%)           4 (1.3%)  \n  HIP FRACTURE                                                         1 (1.2%)          2 (2.8%)                  0              3 (1.0%)  \n  JOINT DISLOCATION                                                       0                 0                  1 (1.0%)           1 (0.3%)  \n  SKIN LACERATION                                                      1 (1.2%)             0                  2 (2.1%)           3 (1.0%)  \n  WOUND                                                                   0                 0                  1 (1.0%)           1 (0.3%)  \nINVESTIGATIONS                                                                                                                              \n  Total number of patients with at least one adverse event            10 (11.6%)         5 (6.9%)              8 (8.3%)          23 (7.5%)  \n  Total number of events                                                  19                6                     15                 40     \n  BIOPSY                                                                  0              1 (1.4%)                  0              1 (0.3%)  \n  BIOPSY PROSTATE                                                         0              1 (1.4%)                  0              1 (0.3%)  \n  BLOOD ALKALINE PHOSPHATASE INCREASED                                 1 (1.2%)             0                      0              1 (0.3%)  \n  BLOOD CHOLESTEROL INCREASED                                             0              1 (1.4%)                  0              1 (0.3%)  \n  BLOOD CREATINE PHOSPHOKINASE INCREASED                               1 (1.2%)             0                      0              1 (0.3%)  \n  BLOOD GLUCOSE INCREASED                                                 0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  BLOOD URINE PRESENT                                                  1 (1.2%)             0                      0              1 (0.3%)  \n  BODY TEMPERATURE INCREASED                                              0                 0                  1 (1.0%)           1 (0.3%)  \n  CYSTOSCOPY                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  ELECTROCARDIOGRAM ST SEGMENT DEPRESSION                              4 (4.7%)             0                  1 (1.0%)           5 (1.6%)  \n  ELECTROCARDIOGRAM T WAVE AMPLITUDE DECREASED                         1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  ELECTROCARDIOGRAM T WAVE INVERSION                                   2 (2.3%)          1 (1.4%)              1 (1.0%)           4 (1.3%)  \n  HEART RATE INCREASED                                                 1 (1.2%)             0                      0              1 (0.3%)  \n  HEART RATE IRREGULAR                                                 1 (1.2%)             0                      0              1 (0.3%)  \n  NASAL MUCOSA BIOPSY                                                     0                 0                  1 (1.0%)           1 (0.3%)  \n  NEUTROPHIL COUNT INCREASED                                              0                 0                  1 (1.0%)           1 (0.3%)  \n  URINE ANALYSIS ABNORMAL                                                 0                 0                  1 (1.0%)           1 (0.3%)  \n  WEIGHT DECREASED                                                        0                 0                  1 (1.0%)           1 (0.3%)  \n  WHITE BLOOD CELL COUNT INCREASED                                        0                 0                  1 (1.0%)           1 (0.3%)  \nMETABOLISM AND NUTRITION DISORDERS                                                                                                          \n  Total number of patients with at least one adverse event             6 (7.0%)          3 (4.2%)              1 (1.0%)          10 (3.3%)  \n  Total number of events                                                  8                 5                      1                 14     \n  DECREASED APPETITE                                                   1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  DEHYDRATION                                                          1 (1.2%)             0                      0              1 (0.3%)  \n  DIABETES MELLITUS                                                    1 (1.2%)             0                      0              1 (0.3%)  \n  FOOD CRAVING                                                         1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  HYPERCHOLESTEROLAEMIA                                                   0              1 (1.4%)                  0              1 (0.3%)  \n  HYPONATRAEMIA                                                        1 (1.2%)             0                      0              1 (0.3%)  \n  INCREASED APPETITE                                                   1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \nMUSCULOSKELETAL AND CONNECTIVE TISSUE DISORDERS                                                                                             \n  Total number of patients with at least one adverse event             5 (5.8%)         8 (11.1%)              7 (7.3%)          20 (6.5%)  \n  Total number of events                                                  8                 11                    10                 29     \n  ARTHRALGIA                                                           1 (1.2%)          1 (1.4%)              2 (2.1%)           4 (1.3%)  \n  ARTHRITIS                                                            1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  BACK PAIN                                                            1 (1.2%)          3 (4.2%)              1 (1.0%)           5 (1.6%)  \n  FLANK PAIN                                                              0              2 (2.8%)                  0              2 (0.7%)  \n  MUSCLE SPASMS                                                           0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  MUSCULAR WEAKNESS                                                       0                 0                  1 (1.0%)           1 (0.3%)  \n  MYALGIA                                                                 0              1 (1.4%)                  0              1 (0.3%)  \n  PAIN IN EXTREMITY                                                    1 (1.2%)             0                      0              1 (0.3%)  \n  SHOULDER PAIN                                                        1 (1.2%)             0                  2 (2.1%)           3 (1.0%)  \nNEOPLASMS BENIGN, MALIGNANT AND UNSPECIFIED (INCL CYSTS AND POLYPS)                                                                         \n  Total number of patients with at least one adverse event                0              1 (1.4%)              2 (2.1%)           3 (1.0%)  \n  Total number of events                                                  0                 1                      3                 4      \n  COLON CANCER                                                            0                 0                  1 (1.0%)           1 (0.3%)  \n  MALIGNANT FIBROUS HISTIOCYTOMA                                          0                 0                  1 (1.0%)           1 (0.3%)  \n  PROSTATE CANCER                                                         0              1 (1.4%)                  0              1 (0.3%)  \nNERVOUS SYSTEM DISORDERS                                                                                                                    \n  Total number of patients with at least one adverse event            12 (14.0%)        25 (34.7%)            22 (22.9%)         59 (19.3%) \n  Total number of events                                                  16                43                    42                101     \n  AMNESIA                                                                 0              1 (1.4%)                  0              1 (0.3%)  \n  BALANCE DISORDER                                                        0                 0                  1 (1.0%)           1 (0.3%)  \n  BURNING SENSATION                                                       0              2 (2.8%)                  0              2 (0.7%)  \n  COGNITIVE DISORDER                                                      0              1 (1.4%)                  0              1 (0.3%)  \n  COMPLEX PARTIAL SEIZURES                                                0                 0                  1 (1.0%)           1 (0.3%)  \n  COORDINATION ABNORMAL                                                   0                 0                  1 (1.0%)           1 (0.3%)  \n  DIZZINESS                                                            2 (2.3%)         11 (15.3%)             9 (9.4%)          22 (7.2%)  \n  HEADACHE                                                             7 (8.1%)          6 (8.3%)              3 (3.1%)          16 (5.2%)  \n  HEMIANOPIA HOMONYMOUS                                                   0                 0                  1 (1.0%)           1 (0.3%)  \n  HYPERSOMNIA                                                             0              1 (1.4%)                  0              1 (0.3%)  \n  LETHARGY                                                                0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  PARAESTHESIA                                                            0              1 (1.4%)                  0              1 (0.3%)  \n  PARAESTHESIA ORAL                                                       0                 0                  1 (1.0%)           1 (0.3%)  \n  PARKINSON'S DISEASE                                                  1 (1.2%)             0                      0              1 (0.3%)  \n  PAROSMIA                                                                0              1 (1.4%)                  0              1 (0.3%)  \n  PARTIAL SEIZURES WITH SECONDARY GENERALISATION                          0              1 (1.4%)                  0              1 (0.3%)  \n  PSYCHOMOTOR HYPERACTIVITY                                            1 (1.2%)             0                      0              1 (0.3%)  \n  SOMNOLENCE                                                           2 (2.3%)          1 (1.4%)              3 (3.1%)           6 (2.0%)  \n  STUPOR                                                                  0                 0                  1 (1.0%)           1 (0.3%)  \n  SYNCOPE                                                                 0              2 (2.8%)              5 (5.2%)           7 (2.3%)  \n  SYNCOPE VASOVAGAL                                                       0              1 (1.4%)                  0              1 (0.3%)  \n  TRANSIENT ISCHAEMIC ATTACK                                              0              1 (1.4%)              2 (2.1%)           3 (1.0%)  \nPSYCHIATRIC DISORDERS                                                                                                                       \n  Total number of patients with at least one adverse event            10 (11.6%)        8 (11.1%)             11 (11.5%)         29 (9.5%)  \n  Total number of events                                                  14                11                    15                 40     \n  AGITATION                                                            2 (2.3%)             0                  3 (3.1%)           5 (1.6%)  \n  ANXIETY                                                              1 (1.2%)             0                  3 (3.1%)           4 (1.3%)  \n  COMPLETED SUICIDE                                                    1 (1.2%)             0                      0              1 (0.3%)  \n  CONFUSIONAL STATE                                                    2 (2.3%)          1 (1.4%)              3 (3.1%)           6 (2.0%)  \n  DELIRIUM                                                                0              1 (1.4%)                  0              1 (0.3%)  \n  DELUSION                                                             1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  DEPRESSED MOOD                                                          0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  DISORIENTATION                                                       1 (1.2%)             0                      0              1 (0.3%)  \n  HALLUCINATION                                                           0              1 (1.4%)                  0              1 (0.3%)  \n  HALLUCINATION, VISUAL                                                   0              1 (1.4%)                  0              1 (0.3%)  \n  INSOMNIA                                                             2 (2.3%)          2 (2.8%)                  0              4 (1.3%)  \n  IRRITABILITY                                                         1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  LIBIDO DECREASED                                                        0              1 (1.4%)                  0              1 (0.3%)  \n  LISTLESS                                                                0              1 (1.4%)                  0              1 (0.3%)  \n  NIGHTMARE                                                               0              1 (1.4%)                  0              1 (0.3%)  \n  RESTLESSNESS                                                            0                 0                  1 (1.0%)           1 (0.3%)  \nRENAL AND URINARY DISORDERS                                                                                                                 \n  Total number of patients with at least one adverse event             4 (4.7%)          3 (4.2%)              4 (4.2%)          11 (3.6%)  \n  Total number of events                                                  5                 4                      4                 13     \n  CALCULUS URETHRAL                                                       0              1 (1.4%)                  0              1 (0.3%)  \n  DYSURIA                                                              1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  ENURESIS                                                                0                 0                  1 (1.0%)           1 (0.3%)  \n  INCONTINENCE                                                            0                 0                  1 (1.0%)           1 (0.3%)  \n  MICTURITION URGENCY                                                  1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  NEPHROLITHIASIS                                                      1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  POLLAKIURIA                                                          1 (1.2%)             0                      0              1 (0.3%)  \nREPRODUCTIVE SYSTEM AND BREAST DISORDERS                                                                                                    \n  Total number of patients with at least one adverse event             2 (2.3%)          1 (1.4%)                  0              3 (1.0%)  \n  Total number of events                                                  4                 1                      0                 5      \n  BENIGN PROSTATIC HYPERPLASIA                                         1 (1.2%)          1 (1.4%)                  0              2 (0.7%)  \n  PELVIC PAIN                                                          1 (1.2%)             0                      0              1 (0.3%)  \nRESPIRATORY, THORACIC AND MEDIASTINAL DISORDERS                                                                                             \n  Total number of patients with at least one adverse event            10 (11.6%)        10 (13.9%)            10 (10.4%)         30 (9.8%)  \n  Total number of events                                                  15                22                    16                 53     \n  ALLERGIC GRANULOMATOUS ANGIITIS                                         0              1 (1.4%)                  0              1 (0.3%)  \n  COUGH                                                                3 (3.5%)          5 (6.9%)              6 (6.2%)          14 (4.6%)  \n  DYSPHONIA                                                               0                 0                  1 (1.0%)           1 (0.3%)  \n  DYSPNOEA                                                             1 (1.2%)          1 (1.4%)              1 (1.0%)           3 (1.0%)  \n  EMPHYSEMA                                                            1 (1.2%)             0                      0              1 (0.3%)  \n  EPISTAXIS                                                               0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \n  HAEMOPTYSIS                                                          1 (1.2%)             0                      0              1 (0.3%)  \n  NASAL CONGESTION                                                     3 (3.5%)          3 (4.2%)              1 (1.0%)           7 (2.3%)  \n  PHARYNGEAL ERYTHEMA                                                     0              1 (1.4%)                  0              1 (0.3%)  \n  PHARYNGOLARYNGEAL PAIN                                                  0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  POSTNASAL DRIP                                                       1 (1.2%)             0                      0              1 (0.3%)  \n  PRODUCTIVE COUGH                                                        0              1 (1.4%)                  0              1 (0.3%)  \n  RALES                                                                1 (1.2%)             0                      0              1 (0.3%)  \n  RESPIRATORY TRACT CONGESTION                                            0              1 (1.4%)                  0              1 (0.3%)  \n  RHINORRHOEA                                                             0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \nSKIN AND SUBCUTANEOUS TISSUE DISORDERS                                                                                                      \n  Total number of patients with at least one adverse event            21 (24.4%)        42 (58.3%)            42 (43.8%)        105 (34.3%) \n  Total number of events                                                  47               111                    118               276     \n  ACTINIC KERATOSIS                                                       0              1 (1.4%)                  0              1 (0.3%)  \n  ALOPECIA                                                             1 (1.2%)             0                      0              1 (0.3%)  \n  BLISTER                                                                 0              1 (1.4%)              5 (5.2%)           6 (2.0%)  \n  COLD SWEAT                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  DERMATITIS ATOPIC                                                    1 (1.2%)             0                      0              1 (0.3%)  \n  DERMATITIS CONTACT                                                      0                 0                  1 (1.0%)           1 (0.3%)  \n  DRUG ERUPTION                                                        1 (1.2%)             0                      0              1 (0.3%)  \n  ERYTHEMA                                                            9 (10.5%)         14 (19.4%)            15 (15.6%)         38 (12.4%) \n  HYPERHIDROSIS                                                        2 (2.3%)         8 (11.1%)              4 (4.2%)          14 (4.6%)  \n  PRURITUS                                                             8 (9.3%)         26 (36.1%)            23 (24.0%)         57 (18.6%) \n  PRURITUS GENERALISED                                                    0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \n  RASH                                                                 5 (5.8%)         11 (15.3%)            13 (13.5%)         29 (9.5%)  \n  RASH ERYTHEMATOUS                                                       0                 0                  2 (2.1%)           2 (0.7%)  \n  RASH MACULO-PAPULAR                                                     0              1 (1.4%)                  0              1 (0.3%)  \n  RASH PAPULAR                                                            0              1 (1.4%)                  0              1 (0.3%)  \n  RASH PRURITIC                                                           0              2 (2.8%)              1 (1.0%)           3 (1.0%)  \n  SKIN EXFOLIATION                                                        0                 0                  1 (1.0%)           1 (0.3%)  \n  SKIN IRRITATION                                                      3 (3.5%)          5 (6.9%)              6 (6.2%)          14 (4.6%)  \n  SKIN ODOUR ABNORMAL                                                     0              1 (1.4%)                  0              1 (0.3%)  \n  SKIN ULCER                                                           1 (1.2%)             0                      0              1 (0.3%)  \n  URTICARIA                                                               0              1 (1.4%)              1 (1.0%)           2 (0.7%)  \nSOCIAL CIRCUMSTANCES                                                                                                                        \n  Total number of patients with at least one adverse event                0              1 (1.4%)                  0              1 (0.3%)  \n  Total number of events                                                  0                 1                      0                 1      \n  ALCOHOL USE                                                             0              1 (1.4%)                  0              1 (0.3%)  \nSURGICAL AND MEDICAL PROCEDURES                                                                                                             \n  Total number of patients with at least one adverse event             2 (2.3%)          2 (2.8%)              1 (1.0%)           5 (1.6%)  \n  Total number of events                                                  2                 2                      1                 5      \n  ACROCHORDON EXCISION                                                    0              1 (1.4%)                  0              1 (0.3%)  \n  CATARACT OPERATION                                                   1 (1.2%)             0                  1 (1.0%)           2 (0.7%)  \n  EYE LASER SURGERY                                                    1 (1.2%)             0                      0              1 (0.3%)  \n  SKIN LESION EXCISION                                                    0              1 (1.4%)                  0              1 (0.3%)  \nVASCULAR DISORDERS                                                                                                                          \n  Total number of patients with at least one adverse event             3 (3.5%)          1 (1.4%)              4 (4.2%)           8 (2.6%)  \n  Total number of events                                                  7                 1                      5                 13     \n  HOT FLUSH                                                               0                 0                  1 (1.0%)           1 (0.3%)  \n  HYPERTENSION                                                         1 (1.2%)             0                  2 (2.1%)           3 (1.0%)  \n  HYPOTENSION                                                          2 (2.3%)             0                  1 (1.0%)           3 (1.0%)  \n  ORTHOSTATIC HYPOTENSION                                              1 (1.2%)             0                      0              1 (0.3%)  \n  WOUND HAEMORRHAGE                                                       0              1 (1.4%)                  0              1 (0.3%)"
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html",
    "href": "source_code/Custom_Search_Zarathu.html",
    "title": "Depedency",
    "section": "",
    "text": "!pip install simplejson\n\nRequirement already satisfied: simplejson in c:\\users\\zarathu09\\anaconda3\\envs\\zarathu\\lib\\site-packages (3.18.3)\nfrom datetime import datetime\nimport os\nimport sys\nimport urllib.request\nimport pandas as pd \nimport json\nimport re \nimport requests\nimport simplejson"
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "href": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "title": "Depedency",
    "section": "API KEY 발급받는 방법은 링크를 참조해주세요",
    "text": "API KEY 발급받는 방법은 링크를 참조해주세요\n\n네이버 https://zerosecu.tistory.com/18\n\n일 허용 한도 25000건\n\n카카오 https://kadosholy.tistory.com/25\n구글 https://gomgomi.tistory.com/3\n\n일일 검색어 제한 10,000개\n\n# Naver_client_id = \n# Naver_client_secret = \n# Kakao_API_key= \n# Google_SEARCH_ENGINE_ID = \n# Google_API_KEY ="
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "href": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "title": "Depedency",
    "section": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!",
    "text": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!\n추가하실 도메인을 넣어주세요\n\nTrash_Link = [\"tistory\", \"kin\", \"youtube\", \"blog\", \"book\", \"news\", \"dcinside\", \"fmkorea\", \"ruliweb\", \"theqoo\", \"clien\", \"mlbpark\", \"instiz\", \"todayhumor\"]"
  }
]