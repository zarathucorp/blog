{
  "hash": "e7de3501ee93eabd517f85cf20d231d8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring Regression Models for Regression Analysis (1): Regression Analysis, Linear Regression, HC Standard Errors\"\ndescription: \"Regression Analysis의 기본 Model인 Linear regression과 이의 robust한 (Co)variance Matrix Estimator인 HC Standard Errors(HCCCM), Wild Bootstrap에 대해서 공부합니다.\"\nimage: img/reg1.jpg\ncategories: [statistics]\nauthor:\n  - name: Lee Seungjun\n    url: https://github.com/aiseungjun\nfig_width: 400\ndate: \"2025-02-28\"\nformat: html\nexecute:\n  freeze: true\ndraft: false\ncitation: true\nlicense: CC BY-NC\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## 들어가며\n\n\n\n차라투 블로그 “Exploring Regression Models for Regression\nAnalysis”에서는 세(네) 장에 걸쳐서 통계에서의 Regression Analysis를 위한\n여러 Regression Model들을 수학적으로 깊게 탐구합니다. 1장에서는\nRegression Analysis의 개념, (simple, multiple or general) linear\nregression에 대한 개념 및 Analysis 과정에 대한 소개로 시작해서 data의\n특성 중 하나인 Homo, Heteroskedasticity의 개념, Heteroskedasticity로\n추정되는 경우 linear regression 모델의 robust한 (co)variance를 구하는\n방법 중 하나이자 필수로 고려해야 하는 Heteroskedasticity-consistent\nstandard errors (heteroskedasticity-robust standard errors)와 Wild\nBootstrap를 살펴볼 것입니다. 2장에서는 1장에서 다룬 기본 linear\nregression에서 link function을 도입하여 regression의 개념을 outcome of\nsingle yes/no, outcome of single K-way, count 등 non-normal한 종속변수로\n확장한 Generalized linear model의 개념을 Exponential Family, Link\nFunction와 같은 핵심 개념과 함께 깊게 살펴보며, model의 parameter를\nestimate하는 알고리즘인 IRLS(Fisher scoring), HC standard errors의\nclustered data 버전인 Cluster-robust standard error를 다룹니다.\n3장에서는 GLM에서 여전히 남아있는 observations의 independent\n조건(오차항의 독립성)을 극복하여 clustered(panel, longitudinal..) data를\n고려할 수 있는 모델인 GEE, GLMM의 근본적인 원리를 M-estimator, robust\n(sandwich) estimator와 같은 수학적 개념들과 함께 탐구할 것입니다.\n**{4장은 계획 중에 있습니다.}**\n\n결국 “Exploring Regression Models for Regression Analysis”는 Regression\nAnalysis을 논리적으로 수행하기 위해 어떤 Regression Model 들이 있으며,\n이들의 수학적인 원리가 무엇인지를 deep dive 하는 글입니다. 영문 글들\n중에서도 이 내용들을 이어지게 다뤄주는 글은 거의 없고, 아주 친절하게\n수식을 전개하였기 때문에 근본적인 Regression Models의 수학적 원리를\n공부하기 좋은 글들이라고 생각하며, R 코드는 간단하게만 주석 형태로\n제공하기 때문에 로컬에서 주석 해제 후 돌려보시고, 실제 의학 연구에 관련\n분석이 필요하시다면 해당 연구를 [차라투](https://www.zarathu.com/en)로\n문의해주시길 바랍니다.\\\n\n## 1. Regression Analysis\n\n\n\n**Regression Analysis**는 통계학에서 가장 널리 사용되는 방법론 중\n하나로, 어떤 종속변수(dependent variable)와 하나 이상의\n독립변수(independent variables) 간의 관계를 추정하고 해석하는 분석\n기법입니다. (종속변수 또한 벡터, 즉 여러 개일 수 있습니다.)\n\n의학 분야에서는 예후를 예측하는 모델을 만들거나(예: 특정 약물 투여 후\n혈압, outcome 등의 변화를 예측), 특정 위험인자(risk factor)가 결과에\n유의미한 영향을 미치는지(예: 어떤 치료법이 환자의 생존율에 유의미한\n차이를 주는지) 등을 살펴보기 위해 **Regression Analysis**가 필수적으로\n활용됩니다. 이러한 Regression Analysis을 완벽하게 수행하기 위해서는\n본인의 *data가 어떤 특성을 갖고 있는지 파악하고*, 이에 대해 *어떠한\nRegression Model을 고려해야 하는지* 알고, 이 모델이 *어떻게 data에\n적합(fit)하는지*에 대한 대략적인 수학 또는 알고리즘의 원리와, 적합된\n모델이 *통계적으로 유의미한지 검정(test)*하고 예측 성능이나 해석력을\n평가하는 방법까지 전 과정을 이해해야 합니다. 본 1장에서는 가장 간단한,\n대신 많은 가정이 필요한 Regression Model인 (General) Linear Regression에\n대해서 위 과정과 함께 살펴볼 것이며, 이후의 장들에서는 data의 가정이\n조금씩 깨질 때 (data의 특성이 변할 때) 어떤 Regression Model을 고려해야\n하는지와 이 Model들의 parameters는 어떻게 수학적으로 추정하는지 살펴볼\n것입니다.\\\n\n## 2. Linear Regression (LM)\n\n### 2.1. (Simple, Multiple) Linear Regression 정의\n\n------------------------------------------------------------------------\n\n**(General) Linear Regression**은 의학뿐만 아니라 다양한 분야에서 가장\nbasic한 Regression Model입니다. **Linear Regression**은 독립변수 $X$ 와\n종속변수 $Y$ 간의 **선형(linear) 관계**를 가정하고, 이를 통해 종속변수를\n설명하거나 예측합니다. 독립변수가 1개일 때는 **Simple Linear\nRegression**, 2개 이상일 때는 **Multiple Linear Regression**라고\n부릅니다. 의학 연구에서 예를 들면, 혈압(종속변수)을 나이, 체중,\n성별(독립변수) 등으로 설명하거나 예측하는 과정을 생각할 수 있습니다.\n이러한 Linear regression은 아래 네 가지 가정을 기반으로 추정하며, 이\n가정들에 대해서 잘 이해하는 것은 매우 중요하고, 이들 중 특정 가정들이\n위배될 경우 이후에 다룰 Regression Model들을 고려해야 함을 명심해주시면\n좋을 것 같습니다.\n\n-   **선형성(Linearity) 가정**\n\n    독립변수와 종속변수가 선형 관계에 있다고 가정합니다. 산점도(scatter\n    plot)를 통해 대략적인 선형 관계 여부를 확인할 수 있습니다.\n\n-   **오차항의 정규성(Normality) 가정**\n\n    주어진 독립변수의 값에서 종속변수의 확률분포는 정규분포를 따른다고\n    가정합니다. 이는 오차항 ε이 정규분포를 따른다고도 표현할 수\n    있습니다.(종속변수가 정규분포를 따른다는 뜻은 Linear Model이\n    종속변수의 mean을 예측하므로, 이 둘의 차인 오차항은 평균이 0이고\n    분산은 종속변수와 같은 정규분포를 따르게 되기 때문입니다.) 이는 정규\n    P-P plot 혹은 Q-Q plot 등을 통해 가정 위배 여부를 대략적으로 확인할\n    수 있습니다.\n\n-   **오차항의 독립성(Independence) 가정**\n\n    각 관측치(Observations, Data(set)) 또는 잔차(residual)가 서로\n    독립이라고 가정합니다. 즉, data간의 상관관계가 없다고 가정하는\n    것이고, 잔차산점도(residual plot), Durbin-Watson 통계량 등을 통해\n    자기상관(autocorrelation)이 있는지 살펴볼 수 있습니다.\n\n-   **오차항의 등분산성(Homoscedasticity) 가정**\n\n    모든 독립변수의 값에서 종속변수의 분산이 동일하다고 가정합니다.\n    잔차산점도 등을 통해 잔차가 일정한 분산을 가지는지 대략적으로 확인할\n    수 있습니다.\n\n이후 Linear Regression Model은 다음과 같은 과정으로 분석을 수행하게\n됩니다; (1) 최소제곱법(Least Squares)를 통해 model parameters를\n추정(estimate), (2) 결정계수($R^2$)를 통해 모델이 종속변수를 얼마나 잘\n설명하는지 확인, (3) F-검정(F-test)으로 전체 회귀식의 유의성을 검정, (4)\n검정(t-test)으로 각 회귀계수(regression coefficient)가 유의미한지 확인.\n특히, 독립변수 각각의 Model coefficient(parameter)를 검정 함으로써\n종속변수와의 상관성을 분석하는 (4) 과정은 유의성을 판단하는 가장 중요한\n검정 과정으로, 고전적으로 **Wald Test, Likelihood Ratio Test, Score\nTest**가 자주 사용됩니다.\n\n### 2.2. Linear Regression 수학적 표현 및 추정\n\n------------------------------------------------------------------------\n\nLinear Regression은 다음과 같은 형태를 가집니다:\n\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_k X_{ik} + \\varepsilon_i\n$$\n\n혹은 Matrix 형태로 간단히 쓰면,\n\n$$\n    \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\\\\n    where, \\quad \\mathbf{y} \\in \\mathbb{R}^n, \\mathbf{X} \\in \\mathbb{R}^{n \\times p}, \\boldsymbol{\\beta} \\in \\mathbb{R}^p, \\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\n$$\n\n-   $\\mathbf{y} \\in \\mathbb{R}^n$: 관측된 종속변수들의 벡터\n\n-   $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$: 설계 행렬, 각 행은 하나의\n    관측치(Observation)이며 ($n$), 각 열은 하나의 독립변수에 ($p$) 해당\n\n-   $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$: 추정하고자 하는 회귀 계수(or\n    모수 or parameters) 벡터 (여기선 intercept $\\beta_0$ 제외)\n\n-   $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$: 랜덤 오차 항 벡터, 위\n    **네 가정**에 따라 랜덤 오차항은 평균이\n    $E(\\boldsymbol{\\varepsilon}) = 0$이고, 분산이 모든 관측치에서 같으며\n    독립인 정규분포\n\n입니다. Multiple Linear Regression의 모델 $\\boldsymbol{\\beta}$의 분산은\nscalar variance가 아니라 (co)variance matrix 형태가 될 것입니다.\n(simple도 intercept를 고려하면 matrix) 앞으로 세 장에 걸쳐 이를 분산\n행렬이라고 부르겠지만, 정확히는 **분산-공분산 행렬**이라고도 부릅니다.\n또한, 위 matrix form 수식처럼 intercept를 제외하는 경우도 많은데, 항상\n있다고 가정하며 가독성을 위해 제외한 것이니 혼동할 필요 없이 자연스럽게\n읽으시면 됩니다. Covariance matrix에서는 회귀계수 서로 간의 공분산이\n들어있으므로, 각 **회귀계수(독립변수의 유의성)에 대한 검정**은\n**Covariance Matrix에 있는 각 diagonal 원소들을 사용하여 수행**하게\n됩니다. (**회귀계수 각각에서 스스로의 대한 공분산 = 회귀계수의 분산**)\n예를 들어 $\\beta_2$의 분산은 $p$가 5 (독립변수가 4개, intercept 포함)일\n때 Covariance Matrix의 3행 3열 값이 될 것입니다. (intercept가 없다면 2행\n2열) 여기서 확인할 수 있듯 Regression Model의 **parameter 추정 역시\n중요**하지만, **parameter의 Covariance Matrix를 추정하는 것 또한 유의성\n판단에서 매우 중요**하며, 이후의 장들 또한 자연스럽게 Model의 소개,\nparamater 추정법, parameter의 (Co)variance Matrix를 추정법으로\n이루어지게 될 것입니다.\n\n또한, Linear Model 식은 선형대수학에서 **Hyper Plane**으로 정의하는\n형태가 되는데, 쉽게 설명하자면 독립변수가 하나일 경우 종속변수와의 2차원\n평면에서 직선(2차원의 Hyper Plane)을 띄고, 독립변수가 두 개일 경우\n종속변수와의 3차원 공간에서 평면을 띄는(3차원의 Hyper Plane), 선형적으로\n공간을 두 부분으로 가르는 공간이라고 생각하시면 될 것 같습니다.\n(마찬가지로 쉽게 생각하면, 비선형적일 경우 직선이 아니라 곡선, 평면이\n아니라 곡면일 것입니다.)\n\n이제 어떻게 Linear Model을 regression, 즉 fit(parameter를 추정)할 것인지\n설명하겠습니다. 가장 기본적으로 알려진 Model의 통계학적 parameter\n추정법으로는 **Ordinary Least Squares(OLS), Maximum Likelihood\nEstimation(MLE), Method of Moment(MOM)**가 있습니다. 이후의 모델들은\n대부분 MLE로 모델을 추정하지만, Linear Model은 특별하게도 MLE와 OLS의\n추정 결과가 같고, OLS의 추정 결과는 BLUE(Best Linear Unbiased Estimator)\n입니다. **최소제곱법(OLS, Ordinary Least Squares) 추정량(estimator)**은\n**오차 제곱합(SSR, Sum of Squared Residuals)**, 즉 모든 데이터에서 실제\n종속변수 값과 모델의 예측 값의 차이를 제곱한 수들의 합을 최소화하는\n$\\boldsymbol{\\beta}$를 찾는 것이며, 다음의 closed-form solution을\n갖습니다: $$\n\\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n$$\n\n그리고, 고전적 가정(특히 등분산성, 독립성, 정상성)이 모두 충족된다고 할\n때, 이 $\\hat{\\boldsymbol{\\beta}}$의 추정오차의 공분산행렬(covariance\nmatrix)은 $$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}\n$$ 이며, 실제 계산할 때는 데이터에서 추정한 $\\hat{\\sigma}^2$를 통해 $$\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1}, \\quad where \\; \\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}, \\; e_i = y_i - x_i \\hat{\\boldsymbol{\\beta}}\n$$\n\n로 추정합니다. (분모에서 K를 뺀 이유는 degree of freedom에 의한 것이며,\n에러 term이 정확히는 차원에 맞춰 $x_i^\\top \\hat{\\boldsymbol{\\beta}}$\n지만, 앞으로 이정도 표기는 가독성을 위해 넘어가겠습니다.) 이렇게 구한\n모델 추정량과 추정오차의 공분산행렬을 통해 각 회귀계수에 대한 검정을\n수행하거나 신뢰구간(CI)을 계산할 수 있게 됩니다.\n\n다음으로 넘어가기 전에, Linear Regression에서 (1) $\\hat{\\beta}$이\nclosed-form solution\n$(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$을 갖게\n되는 과정과, (2) 위에서 언급하였듯 model의 parameter ($\\hat{\\beta}$) OLS\n추정량이 MLE(Maximum likelihood estimation) 추정량과 같음을\n증명하겠습니다.\n\n**(1) Prove**\n$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$**.**\n\n우선, OLS 추정량은 오차 제곱합(SSR)을 최소화하는 식이므로 오차 SSR을\n표현하는 식 $J(\\beta)$를 적으면 다음과 같습니다.\n\n$$\nJ(\\beta) = \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) = \\frac{1}{2} \\sum_{i=1}^{n} \\big( x_i \\beta - y^{(i)} \\big)^2\n$$\n\n이제 위 $J(\\beta)$를 $\\beta$에 대해 미분하였을 때 0이 나오는\n$\\hat{\\beta}$이 SSR이 최소가 되는 OLS 추정량입니다.(이에 대한 증명은\n안하겠지만, 간단하게 볼록한 2차 함수에서는 미분값이 0인 점이 최소점인\n것과 같은 원리라고 생각하시면 되겠습니다.) 식을 풀어보면,\n\n$$ \\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big) $$\n입니다. $\\nabla$는 미분하는 변수가 scalar가 아니라 vector이기 때문에\n사용되는 기호이며(gradient), 그냥 $\\beta$로 미분한다고만 생각하시면\n됩니다. 이후 미분하는 과정 또한 크게 다르지 않습니다. 식을 계속\n진행해보면, $y^\\top y$는 식에 $\\beta$가 없고 $(X\\beta)^\\top y$와\n$y^\\top (X\\beta)$ 둘다 단순히 두 벡터의 내적인 똑같은 식이므로\n$$ = \\frac{1}{2} \\nabla_\\beta \\big( \\beta^\\top (X^\\top X) \\beta - 2 (X^\\top y)^\\top \\beta \\big) $$이고,\n이제 $\\beta$로 미분하면, 우항은 그냥 미분, 좌항은 두 번 나오므로\n곱미분을 수행하여\\\n$$ = \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big) $$\\\n$$ = X^\\top X \\beta - X^\\top y = 0 $$\n\n입니다. 결국 OLS 추정량 $\\hat{\\beta}$는\n\n$$ X^\\top X \\hat{\\beta} = X^\\top y $$ 이고, 양변 앞에 역행렬을 곱해주면\\\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$가 됩니다.\n\n**(2) Prove OLS estimator is same as MLE estimator in Linear Model\n(Regression).**\n\n\\\n위 Linear Regression Model 식을 다음과 같이 작성할 수 있습니다. $$\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n$$\n\n식이 어색할 수 있지만 정확히 이전의 수학적 표현과 동일하며,\n$\\boldsymbol{\\varepsilon}$ term이 복잡해 보이는 이유는, 이전의 네 가지\n조건들을 수식으로 반영하여 모델을 확률 기반으로 해석하였기 때문입니다.\n식을 설명드리자면, 선형성(Linearity) 가정에 의해 error의 mean이\n(기댓값이) 0이 되고, 오차항의 정규성 가정에 의해 정규 분포를 따르며,\n오차항의 독립성 및 등분산성 가정에 의해 분산(Covariance Matrix)이 값이\n모두 $\\sigma^2$인 대각행렬, 즉 다른 observations간의 correlation은\n0이고(없고), 각 observations의 분산은 $\\sigma^2$로 일정함을 표현한\n것입니다.\n\n**최대가능도방법** 또는 **최대우도법**(Maximum Likelihood Method)은 어떤\n모수와 표집한 값들이 주어졌을 때, 표집한 값들이 나올 가능도(확률)을\n최대로 만드는 모수를 선택하는 아주 general한 모델의 점 추정 방법이며,\n처음 들어보셨다면 대표님께서 더욱 자세하게 설명하신 블로그 글을 읽으시면\n좋을 것 같습니다. 철학적으로는 종속변수의 확률 분포를 데이터에 맞게\n가정한 후, 모든 observations(data) 각각이 나올 확률(분포)을 모두 곱한\n식이 최대가 되도록 하는 모수값이 바로 MLE를 통해 추정한 parameter라고\n생각하시면 됩니다. 간단한 예시를 들어보자면, 동전을 던져 앞면이 7번,\n뒷면이 3번 나온 데이터에서 동전이 앞면이 나올 확률을 모수로 두고\n종속변수의 분포를 베르누이 분포로 가정하면, 각각의 동전을 던져 얻은\n관측치들 10개의 data의 Likelihood(확률)를 모두 곱하면 모수가 하나였으니\n변수가 하나인 하나의 식(함수)가 나오고, 이 식(함수)을 최대화 하는 모수를\n찾는 method가 MLE라고 간단하게 정리할 수 있을 것 같습니다. (이때 추정된\n결과는 0.7일 것이고, 모수로 식을 미분한 함수(score function)이 0인 값을\n찾음으로써 모수를 추정하게 되며, MLE 이외에도 MAP, 베이지안 등 다른 추정\n기법들도 있지만 여기에서는 MLE 정도를 이해하면 충분할 것 같습니다.)\n\n이제 돌아와서 LM에서는 OLS estimator와 MLE estimator가 상응함을\n보이겠습니다. 이후의 식 전개는 dim이 1일 때로 증명하겠습니다.\nMulti-dimension에서는 분포식과 term이 더 복잡하지만 meaning은 1차원과\n정확히 동일하기 때문입니다. 오차 항 $\\boldsymbol{\\varepsilon}$는\n정규분포를 따르므로, n개의 관측치에 대한 Likelihood는 $$\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n$$\n\n이며, 로그를 씌운 log Likelihood는 $$\n\\ell(\\boldsymbol{\\beta}) = \\log \\mathcal{L}(\\boldsymbol{\\beta})\n$$\n\n입니다. 식이 복잡해 보이지만 데이터 n개에 대하여 정규(가우시안)분포를\n따르는 종속변수가 모델의 예측값을 가질 확률을 단순히 모두 곱한 식입니다.\n결국 **얻어진 표본 data 자체가 나올 확률에 대한 식을 세운 후, 이를\n최대화 하는 parameter를 찾는 과정이 MLE estimation**이라고 직관적으로\n해석할 수 있습니다.\n\n돌아와서, log를 씌운 이유를 생각해보겠습니다. log의 그래프를\n생각해보시면 log함수는 직관적으로도 monotonically increase하기 때문에\nlikelihood를 최대화 하는 parameter와 log likelihood를 최대화 하는\nparameter는 같으며, log는 곱셈을 덧셈으로, 지수 term을 아래로 만들어주어\nlikelihood 식이 매우 간단해지기 때문에 (아래에서 실제로 그런지\n확인해보세요.) MLE에서는 항상 log likelihood를 최대화하는 parameter를\n찾는 방향으로 parameter를 추정합니다. 이어서 전개하면,\\\n\\\n$$\n= \\log \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2 \\sigma^2} \\right)\n$$\\\n$$\n= \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2}{2 \\sigma^2}\n$$\\\n$$\n= n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y_i - \\mathbf{x_i^\\top} \\boldsymbol{\\beta})^2\n$$\n\n입니다. Log likelihood를 최대화 하는 것은 negative Log likelihood를\n최소화 하는 것과 같으므로 양변에 -를 곱하면\\\n$$\n- \\ell(\\boldsymbol{\\beta}) = -n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} + \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n$$\n\n이고, 상수 $-n \\log \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}$는\n$\\boldsymbol{\\beta}$에 영향을 주지 않으므로 제거하면, $$\n- \\log \\mathcal{L}(\\boldsymbol{\\beta}) \\approx \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n$$\n\n가 됩니다. 위에서 보았듯 OLS estimator는 SSR을 최소화하는 아래 문제로\n정의되므로, $$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} (y^{(i)} - \\mathbf{x}^{(i)^\\top} \\boldsymbol{\\beta})^2\n$$\n\n결국 위 두 식, OLS estimator와 MLE estimator의 목적식이 같고, 따라서\n추정량 또한 같습니다. $$\n\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}\n$$\n\n위 증명은 MLE로 LM의 parameter를 추정한 것이 아니라 식 전개를 통해 OLS와\n목적식이 같음을 보인 것이고, 실제 MLE로 parameter를 추정하는 과정은\n2장부터 자세히 보게 될 것입니다. 한 가지 첨언하자면, Linear model에서는\n이 두 추정량이 같지만, 정규성 가정이 깨진 경우 두 추정량은 같지 않을 수\n있습니다. 이유를 생각해보면, 정규분포는 확률이 가장 큰, 즉 mode가\nmean이며 mean을 기준으로 양 옆이 symmetric한 분포이기 때문에 두 추정량이\n같으며, 정규분포가 아닐 경우 mean을 추정하려고 하는 MLE는 절대적인\n거리를 좁히려는 OLS와 다른 값을 추정하게 된다고 직관적으로 생각해볼 수\n있습니다.\\\n\n## 3. Linear Regression in Homo vs. Heteroskedasticity\n\n\n\n앞서 언급한 Linear Regression 모형은 “오차항의\n등분산성(homoscedasticity)”을 가정합니다. 그러나 실제 연구 상황에서는 이\n가정이 위배되는 경우가 흔합니다. **Heteroskedasticity**는 이러한\n오차항은 독립 변수에 따라 같지 않을 수 있다는 뜻으로, 등분산성 가정이\n깨진 경우를 의미합니다. 즉,\n\n-   **Homoscedasticity**: 모든 관측값에 대해 오차항(또는 종속변수)의\n    분산이 동일\n-   **Heteroskedasticity**: 오차항(또는 종속변수)의 분산이 관측값에 따라\n    다름\n\n입니다. 즉 이전 수식 중 오차 항을 $\\boldsymbol{\\varepsilon}$로 두었다면,\nobservations의 분산은 더이상 $\\sigma^2$로 일정하지 않고 observations마다\n다른 값을 갖을 수 있다는 뜻입니다. 예를 들어, 임상 연구에서 나이(age)에\n따라 혈압(blood pressure)의 분산이 달라질 수 있습니다. 이런 상황에선\n등분산성을 가정하는 것이 부적절해집니다. 등분산성 가정이 깨진 상황에서\nLinear regression을 수행하면, $\\hat{\\boldsymbol{\\beta}}$ 자체가 편향되진\n않더라도(즉, 일관성(consistency)은 여전히 유지),\n$\\hat{\\boldsymbol{\\beta}}$ 의 분산 추정치\n$\\hat{\\sigma}^2 ( \\mathbf{X}^\\top \\mathbf{X} )^{-1}$ 가 bias를 갖게 되어\n잘못된 표준오차, 잘못된 유의성 검정 결과로 이어질 수 있습니다. 따라서,\n**Heteroskedasticity가 의심되는 상황에서**는 모델의 분산을 **안정적으로\n추정**해야 하며, 이때 1번으로 고려되는 방법 중 하나가\n**Heteroskedasticity-consistent(heteroskedasticity-robust) standard\nerrors**, 줄여서 HC standard errors를 통한 (Co)variance Matrix\nestimation입니다.\\\n\n## 4. Heteroskedasticity-Consistent Standard Errors (HC Standard Errors)\n\n### 4.1. HC Standard Errors 정의\n\n------------------------------------------------------------------------\n\n**HC(Heteroskedasticity-Consistent) standard errors**는, 고전적 선형회귀\n가정 중 **등분산성**만 깨졌을 때(나머지 *선형성, 정규성, 독립성 가정 in\nLM* 은 그대로 유지) **회귀계수 추정치의 분산**을 “강건(robust)”하게\n추정하기 위한 method입니다. 이는 heteroskedasticity-robust standard\nerrors 또는 HCCME(Heteroskedasticity-Consistent Covariance Matrix\nEstimation) 라고도 부르며, 이전에로 구한 $\\hat{\\beta}$를 그대로\n사용하되, 그 공**분산행렬 추정만 새롭게(robust하게) 구하는 방식**입니다.\n다만, 독립성(오차항들이 서로 독립), 선형성, 정규성 등의 다른 가정이 또\n깨져 있다면 HC SE만으로는 대응할 수 없음을 명심하시길 바랍니다. 예를\n들어, 오차항이 서로 상관되어 있는 clustered data에서는 더 이상 추정량이\nconsist하지 않기 때문에, 이보다 한 단계 더 나아간 cluster-robust\nstandard errors, 혹은 GEE, GLMM 등의 모델을 고려해야 합니다. 즉,\n“등분산성 가정만 깨졌을 때” 쓰는 표준오차(or covariance matrix)\n추정량이라고 생각하면 됩니다. 또한, 모델이 실제로 크게 잘못 설정되어\n있다면(*선형성조차 안 맞는 경우, 모델의 estimator가 크게 편향, HC se가\n모델에서 구한 se와 크게 차이가 나는 경우 등*), 그때는 variance(standard\nerror)만 “robust standard errors”로 바꾼다고 해서 문제가 해결되지 않고,\n모델을 수정하거나 data를 다시 확인하고 분석을 다른 방식으로 수행해야\n합니다.\n\n즉 Greene의 말처럼,\n\n> “Simply computing a robust covariance matrix for an otherwise\n> inconsistent estimator does not give it redemption.”\n\n이라는 점을 늘 유의해야 합니다. 정리하자면, 모델을 잘 구성하고, 모델의\n분산과 큰 차이가 없을 경우에는 *모델의 parameter의 분산 효율성이 최대가\n아니거나 MLE 추정을 통해 얻은 모델의 분산이 더 이상 불편향이 아닐 때도*\nheteroskedasticity-robust standard errors를 통해서 모델의 분산을\nrobust하게 estimate할 수 있으며, 분석 결과 편향이 너무 크지 않다면\n이후의 검정에서 이 HC se를 사용함으로써 이후의 통계 분석에서 설득력을\n높일 수 있습니다. 추가로 스포하자면, 아래에서 다룰\nHeteroskedasticity-robust Standard Errors 식은 Linear Model에서\nrobust하게 분산을 추정하는 **Robust(or Sandwich) Estimation**이며,\n이후의 모델에서도 해당 모델에 대한 Sandwich Estimator를 고려함으로써\n안정적인 분산 추정 방법에 대해 다룰 것입니다.\n\n### 4.2. HC(CME)0 수학적 표현\n\n------------------------------------------------------------------------\n\n이제 위에서 설명한 Heteroskedasticity한 data에서 robust하게 standard\nerrors, 혹은 Covariance Matrix를 estimate 하는\nheteroskedasticity-consistent standard errors의 수식을 유도하겠습니다.\n\n이전에 선형 회귀 모델을 다음과 같이 정의한 적이 있습니다.\\\n$$\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$ 이때, 위에서는 등분산성을 가정하였기 때문에\n$\\boldsymbol{\\varepsilon}$를 원소값이 모두 $\\sigma^2$ 인 대각행렬로\n가정하여 각 observations의 분산은 $\\sigma^2$로 일정함을 가정했다면,\n이번에 정의한 $\\boldsymbol{\\varepsilon}$은\n$E(\\boldsymbol{\\varepsilon}) = 0$,\n$E(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^\\top) = \\Phi$\n입니다. 이때 $\\Phi$는 $\\operatorname{diag}(\\varepsilon_i^2)$로 정의되는\n대각행렬이며(대각 성분을 제외하면 모두 0인 행렬), 식을 통해서 여전히\nerror term의 mean이 0이고 observations들은\nindependent하지만(대각행렬이므로), 더 이상 observations의 분산이 서로\n일치하지 않음을 표현한 것입니다. 즉, 이제 Homoscedasticity에서\nHeteroskedasticity을 고려하기 시작했다는 것을 수식을 해석함으로써 알 수\n있습니다.\n\n#### Variance estimatation\n\n위 식의 OLS 추정량은 여전히 $$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n$$\n\n입니다. (still consistent) 이제 모델의 variance 식을 전개해보면, $$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\operatorname{Var} \\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\right )$$\n\n이며, $(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$는\ndeterminant한 값으로, variance는 식 내의 determinant한 salar는 Var\n밖으로 제곱과 함께 나오는 것처럼\n($\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X)$), 행렬 또는 벡터에 대해도\n비슷하게 $$\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} ) \\left ((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\right ) ^ \\top \\\\\n= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{Var} ( \\mathbf{y} )  \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}$$\n가 됩니다. (벡터와 행렬을 다룰 때에는 위처럼 제곱한다고 생각하시면 될 것\n같습니다.) 참고로,\n$$\\left ( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\right )^ \\top = \\left ( (\\mathbf{X}^\\top \\mathbf{X})^\\top \\right )^ {-1} =  (\\mathbf{X}^\\top \\mathbf{X})^ {-1}$$\n입니다.\n\n따라서, 처음 설정한대로 $\\operatorname{Var} ( \\mathbf{y} ) = \\Phi$를\n넣으면, 최종적으로 추정된 consistent model $\\hat{\\boldsymbol{\\beta}}$의\n(co)variance matrix $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}})$는\n다음과 같습니다. $$\n(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n하나 확인할 수 있는 사실은, 등분산성 가정($\\Phi = \\sigma^2 \\mathbf{I}$)\n하에선 위 식이 아래처럼 단순화되어 이전 (co)variance matrix가 나왔던,\n가정하의 특수한 결과라는 것입니다.\n$$ \\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X} ^ \\top \\mathbf{X})^ {-1} \\mathbf{X} ^ \\top \\Phi \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\sigma ^ 2 \\mathbf{I} \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\mathbf{X} ^ \\top \\mathbf{X} (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} \\\\ = \\sigma ^ 2 (\\mathbf{X} ^ \\top \\mathbf{X}) ^ {-1} $$\n또한, 이 때의 (OLS 추정량)의 분산 $\\sigma^2$의 식도 그냥 보여드리고\n넘어갔었는데, 이는 대각 성분이 모두 같기 때문에 단순히 **잔차 제곱합**\n$\\sum e_i^2$을 자유도($N-K$)로 나눈 값($s^2$)으로 추정하여\n$\\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{N-K}$ 로 한번에 표현한 식이었다는\n것도 확인할 수 있습니다.\n\n#### HC0's Robustness\n\n이분산성이 존재할 때 $\\Phi$는 대각행렬이지만 **대각원소가 서로\n다릅니다**. White는 1980년에 이 heteroskedasticity에 robust하게 분산을\n추정하기 위해 위 식에서 $\\Phi = \\sigma^2 \\mathbf{I}$로 term이 소거되게\n하는 대신, $\\Phi_{ii}$를 **잔차 제곱** $e_i^2$로 추정하는 HC0을\n제안했습니다: $$\n\\hat{\\Phi}_{ii} = e_i^2 \\quad \\Rightarrow \\quad \\hat{\\Phi} = \\operatorname{diag}(e_i^2)\n$$\n\n이를 위 (co)variance 식에 대입하면 HC0 분산 추정량이 얻어집니다: $$\n\\operatorname{HC0} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n이는 variance 식으로부터 시작해서 얻은 식이고, 데이터로부터 얻은 matrix\n$\\hat{\\Phi} = \\operatorname{diag}(e_i^2)$를 사용하여 이분산성의 구체적\n형태를 가정하지 않았기 때문에 안정적으로 heteroskedasticity를\n고려합니다. 이처럼 **앞 뒤가 같은 모양 사이에 데이터로부터 얻은 분산\n추정 식이 들어간 형태가 샌드위치 같다고 해서 Sandwich Estimator**라고도\n부르며, 여전히 **가운데 matrix 항이 diagonal 함**을 통해 오차(또는\n관측치)간의 **상관관계는 없다**는 것도 기억해야 합니다.\n\n### 4.3. HC(CME) 1\\~3 수학적 표현\n\n------------------------------------------------------------------------\n\n이 HC Standard Error가 나온 이후, 위 식으로부터 구한 분산은 소표본에서는\n**모델의 overfitting(과적합, 딥러닝과 머신러닝에서도 자주 대두되는\n문제로 데이터가 적어 학습한 데이터에서는 오차가 적지만 보지 못한\n데이터에 대해서는 오차가 크게 나오는 상황, 통계 분석은 test data를 따로\n두지 않기 때문에 overfitting 대처가 더욱 중요.)**에 의해 residual이 작게\n나오고, 이에 따라 **모델의 covariance가 과소평가**될 가능성이 크다는\n문제가 제시되었습니다. 이에, 소표본에서도 안정적으로 추정하기 위해\nWhite(1980)의 HC Standard Errors를 HC0로 두고, 여러 버전의 HC Standard\nErrors가 최근까지 다양한 학자들에 의해 연구되고 있습니다. 보통\nHC0\\~HC5까지를 HC Standard Errors로 부르며, HC 오른쪽 숫자가 버전을\n뜻하여 이 수가 커질수록 최근에 나온 것이고, 각각은 simulation 실험을\n통해 소표본에서 더욱 강건함을 보이는 식으로 논문이 나옵니다. 중요한\n점은, 첫 번째로 이 **HC Standard Errors들은 소표본에서는 값이 다르지만\n표본의 크기가 무수히 커질수록 asymptotically하게 같으며** 소표본에서의\n고려를 위해 값을 더 크게 만드는 추가 term을 넣어준다는 점(물론\n수식적으로 필요성을 증명하지만), 둘 째로 R의 sandwich 패키지에서는 4까지\n지원하고, 3을 넘는 HC 시리즈가 쓰이는 경우는 거의 볼 수 없기 때문에\n3까지의 식 정도만 다루어도 충분하다는 점, 마지막으로 위 직관적인\n이유로나 수식적으로나 식을 해석해 보면 거의 모든 표본 cases에 대해\n**버전이 클수록 분산을 더욱 크게 추정한다는 점**입니다. ~~따라서\n유의성을 보이기 위해서는 버전이 작은게 유리할 것입니다.\\^\\^~~ 아래에서는\nHC1부터 3까지의 수식을 살펴보겠습니다. 각각의 철학에 대한 자세한 증명\n과정 또한 다룰까 했지만 중요하지 않기 때문에 간단히 소개한 후 로컬에서\n돌려 보실 R 예시 코드를 보여드릴 것입니다.\n\n#### HC1: 소표본 편향 보정\n\nHC1은 소표본에서 자유도 조정 인자 $\\frac{n}{n-k}$를 도입하여 편향을\n줄인다는 철학에서 출발하여, 잔차 $e_i^2$의 기대값이\n$\\sigma_i^2(1-h_{ii})$이므로, $E(e_i^2) \\approx \\sigma_i^2$이 되도록\nHC0에 $\\frac{N}{N-K}$을 나누어서(1보다 크며, n이 커짐에 따라 1로\n수렴하는 term 추가) 스케일링합니다. 즉, 식은 다음과 같습니다: $$\n\\operatorname{HC1} = \\frac{N}{N-K} \\cdot \\operatorname{HC0} \\\\\n= \\frac{N}{N-K} \\cdot (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n#### HC2: Leverage 보정\n\n위 HC1에서 $E(e_i^2) = \\sigma_i^2(1-h_{ii})$ 라고 이야기 하였습니다.\n일단 그렇다고 하면, 기존에는 $\\sigma_i^2$를 $e_i^2$으로 추정하였다면,\n사실 $\\frac{e_i^2}{1-h_{ii}}$의 기대값이 $\\sigma_i^2$이기 때문에 HC1처럼\n앞에 scalar term을 추가하는 대신, HC2는 이를 직접적으로 반영하여\n$\\hat{\\Phi}$를 $\\operatorname{diag}(e_i^2)$ 대신\n$\\operatorname{diag}( \\frac{e_i^2}{1-h_{ii}})$로 추정합니다. 이 때\n$h_{ii}$은 **Leverage**(래버리지), $H(or \\;h)$는 Hat Matrix라고 불리고,\n식은\n$\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top$이며,\n위 $h_{ii}$은 이 matrix의 대각원소를 뜻합니다. Leverage에 대한 이야기\n또한 길지만 간단하게 설명하면, $\\mathbf{H}$가 Hat Matrix라고 부르는\n이유는 이 $\\mathbf{H}$ term에 $\\mathbf{y}$를 곱하면, $$\n\\mathbf{Hy} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y} \\\\\n= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n=\\hat{\\mathbf{y}}\n$$ 이 되어 $\\mathbf{y}$에 hat을(추정치) 씌운다는 의미에서 비롯되었으며,\n이는 기하학적으로 더욱 복잡하게 설명될 수 있지만 간단한 의미는 **각\n관측치가 얼마나 모델의 estimation에 영향을 주었는지를 보여주는\nmatrix**입니다. 이 matrix의 diagonal값인 레버리지를 보며 관측치\n하나하나의 영향력을 볼 수 있고, **이 값이 큰** **관측치에 대해서는 아래\n식에서처럼 분모를 작게 함으로써 분산을 크게 추정**한다고 직관적으로\n생각할 수 있습니다. $$\n\\operatorname{HC2} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{1-h_{ii}} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n넘어가기 전에, 증명 없이 사용해왔던 수식\n$E(e_i^2) = \\sigma_i^2(1-h_{ii})$ 를 간단하게만 증명해보겠습니다. 예측\n오차는 실제값 $y_i$와 예측값 $\\hat{y}_i$의 차이입니다: $$\ne_i = y_i - \\hat{y}_i.\n$$ 잔차 벡터 $\\mathbf{e}$는 다음과 같이 표현됩니다: $$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{y}.\n$$\n\n에러 제곱의 기댓값은 잔차의 분산을 의미합니다. 잔차 벡터 $\\mathbf{e}$의\n공분산 행렬은 다음과 같이 계산됩니다: $$\n\\text{Cov}(\\mathbf{e}) = \\text{Cov}((\\mathbf{I} - \\mathbf{H}) \\mathbf{y}) = (\\mathbf{I} - \\mathbf{H}) \\text{Cov}(\\mathbf{y}) (\\mathbf{I} - \\mathbf{H})^T.\n$$ $\\text{Cov}(\\mathbf{y}) = \\sigma_i^2 \\mathbf{I}$이므로 (당연히\n$\\sigma$는 관측치 $i$마다 다를 수 있습니다.): $$\n\\text{Cov}(\\mathbf{e}) = (\\mathbf{I} - \\mathbf{H}) \\sigma_i^2 \\mathbf{I} (\\mathbf{I} - \\mathbf{H})^T = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n$$ 여기서 $\\mathbf{H}$는 대칭 행렬($\\mathbf{H} = \\mathbf{H}^T$)이고 멱등\n행렬($\\mathbf{H}^2 = \\mathbf{H}$)이므로 (이 부분은 기하학적인 설명이\n많이 필요해서 증명하지 않겠다만, 이들은 단지 특정 행렬들의 성질이며 이를\n만족한다고 하고 넘기겠습니다.): $$\n\\text{Cov}(\\mathbf{e}) = \\sigma_i^2 (\\mathbf{I} - \\mathbf{H}).\n$$\n\n$i$번째 잔차 $e_i$의 분산은 공분산 행렬의 $i$번째 대각 성분이고, $$\n\\text{Var}(e_i) = [\\text{Cov}(\\mathbf{e})]_{ii} = \\sigma^2 (1 - h_{ii}).\n$$ 에러 제곱의 기댓값은 잔차의 분산과 동일하므로 (에러의 mean이\n0이므로), $$\nE[e_i^2] = \\text{Var}(e_i) = \\sigma^2 (1 - h_{ii}).\n$$ 가 증명됩니다.\n\n#### HC3: 강화된 Leverage 보정\n\nHC3은 **Jackknife 접근법**에서 영감을 받아 HC2에서 $\\Phi$의 분모 term을\n더 강하게 $(1-h_{ii})^2$로 둡니다. 직관적으로, 같은 $h_{ii}$에 대해 더\n크게 분산을 추정 ($(1-h_{ii})^2 < 1-h_{ii}$)하여, 고레버리지, 즉\noutlier에 대해 더 안정적으로 소표본의 분산을 추정할 수 있습니다. $$\n\\operatorname{HC3} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}\\left( \\frac{e_i^2}{(1-h_{ii})^2} \\right) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n#### HC(CME)s 비교\n\n위 내용을 간단하게 표로 정리하면 다음과 같습니다.\n\n| 유형 | 수식                              | 보정 요소    | 사용 시나리오       |\n|------|-----------------------------------|--------------|---------------------|\n| HC0  | $\\operatorname{diag}(e_i^2)$      | 없음         | 대표본              |\n| HC1  | $\\frac{n}{n-k}\\operatorname{HC0}$ | 자유도       | 소표본 기본         |\n| HC2  | $\\frac{e_i^2}{1-h_{ii}}$          | 레버리지 1차 | 고레버리지 존재 시  |\n| HC3  | $\\frac{e_i^2}{(1-h_{ii})^2}$      | 레버리지 2차 | 소표본 + 고레버리지 |\n\n정리하자면, 동분산성 검정을 검정하여 이분산성이 확인되면 HC se 시리즈의\n적용을 고려해야 하고, 표본 크기가 $n < 50$인 경우는 HC3 또는 아래에서\n다룰 bootstrap기반의 (Clustered) Bootstrap Covariance Matrix\nEstimation를, $n > 500$인 경우는 HC0/HC1로 충분하다고 간단하게 생각해볼\n수 있습니다. (물론 이는 정해진 규칙이 아니고, 시작은 항상 HC0부터\n검정해보는 것이 맞습니다.)\\\n\n## 5. (Wild) Bootstrap\n\n마지막으로, sandwich estimator는 아니지만, 모든 모델에서 data로부터\n안정적으로 분산을 추정하여 의학 연구에서 인정하는 기법 중 하나인\nBootstraping을 통한 분산 term 추정 기법을 간단하게 소개드리겠습니다. 이\nmethod는 특정 가정으로부터 수학적인 추정식을 세워 분산을 추정하는 대신,\nBootstraping 통계 기법을 사용하여 data 만으로 경험적인 분산을\n추정합니다. 때문에 이번에 소개하는 (Wild) Bootstrap 은 어떤 가정이나\n모델이든 사용할 수 있는, 심지어 2장에서 다룰 clustered(grouped) data\n에서도 사용할 수 있는 방법입니다.(가정이 전혀 들어가지 않았기\n떄문입니다.) 이후에 다룰 **샌드위치 추정량(Sandwich Estimator)**과, 그의\nOLS 버전인 **HC(Heteroskedasticity-Consistent)** 시리즈가 개발되었지만,\n이들은 결국 대표본 점근적 성질로부터 얻은 수식이고, 극단적인 **쇼포본**\n같은 작은 클러스터 수에서는 이러한 부트스트랩이 더 강력한 대안이 될 수도\n있습니다. 이를 소개하기 전에 간단히 중요한 통계학적 resampling 기법인\n잭나이프(Jackknife)와 부트스트랩(Bootstrap)을 얘기해보겠습니다. 이 둘은\n모두 표본을 계속해서 얻기 힘든 상황 (현실에서는 대부분 그럴\n것입니다.)에서 모수를 추정하기 위해, 이미 갖고 있는 표본의 data로부터\nresampling을 하여 추정하는 통계학적 방법론입니다.\n\n#### 잭나이프(Jackknife)\n\n**Jackknife** method는 한 번에 하나의 클러스터를 제외하고 모델을 추정\n하는 것을 모든 클러스터에 대해 반복하여 분산을 계산합니다:\n\n$$\n\\mathrm{Var}(\\hat{\\beta}_c) = \\frac{n-1}{n} \\sum_{c=1}^C \\left(\\hat{\\beta}_c - \\bar{\\hat{\\beta}}\\right)^2,\n$$\n\n여기서 $\\hat{\\beta}_c$는 c번째 클러스터를 제외한 추정치,\n$\\bar{\\hat{\\beta}}$는 추정치의 평균이고, 이는 resampling 기법이지만\n위에서 설명드렸던 HC3 추정량이 이 식으로부터 도출된 결과입니다. 물론,\n이는 이론적인 결과이므로 각각이 서로 다른 결과를 도출할 수도 있을\n것입니다. 단, resampling 기법들은 그 특성상 클러스터나 그 data point의\n수가 많을 경우 계산 부하가 큽니다.\n\n#### 부트스트랩(Bootstrap)\n\n**Bootstrap** method는 데이터를 무작위로 resampling하여 여러 버전의\n데이터셋을 생성하고, 각각에서 모델을 추정한 후 추정값의 변동성을\n분산으로 사용합니다. 대표적인 예시로 **와일드 부트스트랩 (Wild\nBootstrap)**은 잔차(residual)에 랜덤 가중치를 곱해 인위적 으로 데이터의\n분산을 크게 보정하고, **케이스 기반 (XY/Pairs 부트스트랩)**은 기본적인\n방법으로 클러스터 자체를 복원 추출하여 새 data를 생성한 뒤 구하는 과정을\n반복하며, **잔차 기반 (Residual 부트스트랩)**은 잔차 만을 리샘플링하고\n재추정하는 방법입니다다.\n\n#### Wild Bootstrap\n\n위에서 설명드렸듯, 잔차 $e_i$에 “랜덤 가중치” $w_i$를 곱해 새로운 반응\n변수를 생성하는 방법으로 다음과 같을 것입니다:\n\n$$\ny^* = \\hat{y} + w_i \\hat{e}.\n$$\n\n이때 **가중치 분포** 로 사용되는 대표적인 예시는 Rademacher:\n$w_i \\in \\{-1, 1\\}$, Mammen: $w_i = \\frac{\\sqrt{5} \\pm 1}{2}$, Webb: 6점\n분포($w_i \\in \\left\\{ -\\sqrt{\\frac{3}{2}}, -\\sqrt{\\frac{2}{2}}, -\\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{1}{2}}, \\sqrt{\\frac{2}{2}}, \\sqrt{\\frac{3}{2}} \\right\\}$)로\n클러스터별 정규분포 등 다른 적용도 가능합니다. 위에서 언급하였듯,\n이외에도 **케이스 기반 (XY) 부트스트랩**, **잔차 부트스트랩**,\n**프랙셔널 부트스트랩(Fractional Bootstrap)**등의 방법론이 있지만,\ndata를 크게 왜곡하지 않는 Wild가 이러한 분산 추정에서 디폴트하게\n고려됩니다.\n\n## 6. R 예시: HC0\\~3 및 부트스트랩\n\n\n\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. 위 내용 중 \"시리즈가 클\n수록 강건하게 추정한다\"는 점을 기억하시고 해석하면 됩니다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## mtcars 데이터에서 mpg(연비)를 wt(차량 무게), hp(마력)으로 설명하는 회귀모형\n#data(mtcars)\n#model <- lm(mpg ~ wt + hp, data = mtcars)\n#\n## 기본 OLS 표준오차 (등분산 가정)\n#summary(model)  \n#\n## HC 표준오차 계산\n#library(sandwich)\n#library(lmtest)\n#\n## HC 유형별 분산-공분산 행렬\n#cov_hc0 <- vcovHC(model, type = \"HC0\")  # 기본 White 추정량\n#cov_hc1 <- vcovHC(model, type = \"HC1\")  # 자유도 보정\n#cov_hc2 <- vcovHC(model, type = \"HC2\")  # 잔차 스케일링\n#cov_hc3 <- vcovHC(model, type = \"HC3\")  # 작은 표본 보정\n#\n## 계수 테스트 결과 비교\n#coeftest(model, vcov = cov_hc0)  \n#coeftest(model, vcov = cov_hc1)  \n#\n## 부트스트랩 분산-공분산 행렬 (기본 100회 복제)\n#cov_wild <- vcovBS(fit, cluster = ~cluster_id, type = \"wild\", R = 1000)\n#cov_xy <- vcovBS(model, R = 200, type = \"xy\")  # xy-쌍 부트스트랩\n#\n## 결과 비교\n#coeftest(model, vcov = cov_wild)  \n#coeftest(model, vcov = cov_xy)  \n#\n## 주의: 부트스트랩 결과는 실행 횟수 R, 실행 시마다 다를 수 있음\n```\n:::\n\n\n\n## 마무리하며\n\n------------------------------------------------------------------------\n\n이번 1장에서는 **Regression Analysis**의 기본 개념과 **(simple, multiple\nor General) Linear Regression**의 이론적 배경, 그리고 등분산성 가정이\n깨진(Heteroskedasticity) 상황에서 유용하게 쓰이는\n**Heteroskedasticity-Consistent Standard Errors(HC standard errors)**에\n대해 살펴보았습니다.\n\n**HC standard errors**를 사용하면, Linear Regression 모델에서 등분산성\n가정이 위배되더라도 Standard Errors(or Covariance Matrix)를 좀 더\n타당하게 추정할 수 있습니다. 하지만, “**오차항의 독립성**, **선형성**,\n**정규성**” 등 나머지 가정이 크게 어긋난다면, 단순히 HC SE로 분산을\n보정하는 것만으로는 해결되지 않습니다. HC SE가 기본 OLS 분산추정치와\n크게 다르다면, “정말 모델이 맞는지”를 다시 고민하고, 필요하다면 모델을\n재설정하거나 다른 방법을 모색해야 합니다.\n\n어쨌든, Heteroskedasticity가 의심되는 상황에서 가장 먼저 고려할 만한\n접근인 **HC(Heteroskedasticity-Consistent) standard errors**는 모델의\n유의성 검정을 위해 안정적으로 분산을 추정할 때 널리 쓰이며, 의학\n연구에서도 일상적으로 활용되고 있습니다. 다음 2장에서는 이런 Linear\nRegression을 더 확장한 **Generalized Linear Model(GLM)**의 개념을\n본격적으로 다루고, HC standard errors의 **clustered data** 버전인\n**Cluster-robust standard error**에 대해서도 다룰 예정입니다.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}