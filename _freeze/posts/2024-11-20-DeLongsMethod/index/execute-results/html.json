{
  "hash": "7cf3bf4042448086b63ff9dc3dec3414",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"DeLong's Method; for ROC AUC\"\ndescription: | \n  ROC AUC와 관련된 통계; SE 추정과 이의 CI 구하기.\ncategories:\n  - R\n  - statistics\nauthor:\n  name: \"Hojun LEE\"\n  url: https://github.com/21-HJ\nimage: img/logo.png\nfig_width: 400\ndate: 2024-11-20\nformat: html\nexecute:\n  freeze: true\ndraft: false\nlicense: CC BY-NC\n---\n\n\n  \n# Introduction\n  \n  우리는 **ROC** 곡선에서 **AUC** 값을 구할 수 있다. **AUC** 값이 1에 가까우면 모델의 성능이 좋다라고 하며, 0.5에 가까워질 수록 성능이 나쁘다- 라고 한다. 그렇다면, **ROC**에서 신뢰구간은 어떻게 구할까? 특정 **AUC**값과 비교하여 **p value**를 구하려면 어떻게 해야 할까? \n  \n  이를 위해 **DeLong**의  **AUC 표준 오차** 구하는 방법을 소개하려 한다. (E. DeLong, 1988) \n\n## ROC curve (Recevier Operating Characteristic Curve)\n \n-   **ROC** 곡선은 이진 분류 문제에서 모델의 성능을 평가하는데 사용되는 시각적 도구이다.\n-   다양한 임계값에서의 민감도와 False Positive Rate의 관계를 시각화한 것으로,\n-   **민감도(Sensitivity)**를 **Y**축으로, **1 - 특이도(False Positive Rate)**을 **X**축으로 하여 관계를 그린 그래프이다. \n\n**민감도(Sensitivity, True Positive Rate)**\n\n-   실제 양성 사례를 얼마나 잘 분류하는지/ 실제 양성인 샘플을 양성으로 올바르게 분류한 비율\n$$\nSens = \\frac{True Positives(TP)}{True Positives(TP) + False Negatives(FN)}\n$$\n\n**특이도(Specificity, True Negative Rate)**\n\n-   실제 음성 사례를 얼마나 잘 분류하는지/ 실제 음성인 샘플을 음성으로 올바르게 분류한 비율\n-   **False Positive Rate(FPR)** : 1-Spec, 실제 음성인 샘플을 잘못 양성으로 분류한 비율\n\n$$\nSpec = \\frac{True Negatives(TN)}{True\\ Negatives(TN) + False \\ Positives(FP)}\n$$\n\n\n**이상적인 ROC 곡선**\n\n-   이상적인 분류는 **ROC**가 (0,1)을 지날 때, 즉 **FPR**이 0이고, **TPR**이 1인 경우.\n-   무작위 분류는 **ROC**가 **y=x**일 때, **TPR = FPR**인 경우.\n\n**AUC(Area Under the Curve)**\n\n-   **AUC**는 **ROC** 곡선 아래 면적을 의미한다.\n-   **AUC**가 1에 가까울수록 성능이 좋은 모델. 0.5에 가까울수록 성능이 안좋은 모델.\n\n ![ROC](img/logo.png){#fig-1 width=\"250\" fig-align=\"center\"}\n\n## Empirical ROC Curve\n-   아래와 같이 **ROC Curve**를 추정할 수 있다.\n-   test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 $C_1$($X_i$,n=m), 발생하지 않은 집단을 $C2$($Y_i$,n=n)이라 하자. \n\n\n$$\n\\text{for any real number z,} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n$$\n\n$$\nSens(z) = \\frac{1}{m}\\sum_{i=1}^{m}{I(X_i \\geq z)}, \n$$\n\n$$\nSpec(z) = \\frac{1}{n}\\sum_{j=1}^{n}{I(Y_i < z)}, \n$$\n\n$$\nI(A) =\n\\begin{cases} \n1, &  A \\text{ is true} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n\n-   이때 실수 z가 variable 내 가능한 값들 내에서 움직인다면, **ROC curve**는 **[1 - spec(z)]**를 **X**로, **Sens(z)**를 **Y**로 갖는 plot이라 할 수 있다. 만약 **z**가 가능한 최대값보다 크다면 curve는 (0,0)을, 최솟값보다 작다면(1,1)을 지날 것이다. \n    **Sens(z) = 1 - Spec(z)**라면 y=x 위, 45도 선 위에 놓일 것이다.\n    \n ![AUC](img/auc.png){#fig-2 width=\"250\" fig-align=\"center\"}\n\n## AUC\n-   **ROC curve**는 위와 같이 구할 수 있다. 그럼 이의 넓이: **AUC** 값은 어떻게 구할까? \n  보통, 곡선 아래 넓이는 *trapezoidal rule*을 통해 구한다. 고등학교 때 배운 적분을 떠올리면 된다. 수많은 사다리꼴로 쪼개어 넓이를 근사하던 기억을 되살려 보자. \n\n ![Trapezes](img/trapezes.png){#fig-3 width=\"250\" fig-align=\"center\"}\n \n \n  여기서, *Mann-Whitney two sample statistic*에 따르면, ROC curve 아래 넓이를 구할때, *trapezodial rule*로 구한 넓이는 *Mann-Whitney two sample statistic*으로 구한 넓이로 대체할 수 있다.\n  \n-   **Mann-Whitney statistic**는 확률$\\theta$를 예측한다. $C_2$에서 무작위 추출한 값이 $C_1$의 값보다 같거나 작을 확률을 추정한다. \n*(test를 시행한 총 집단의 수를 N이라 할 때, 실제로 이벤트가 발생한 집단을 $C_1$($X_i$,n=m), 발생하지 않은 집단을 $C2$($Y_i$,n=n)이라 하자)*\n\n$$\n\\hat{\\theta} = \\frac{1}{mn}\\sum_{j=1}^{n}\\sum_{i=1}^{m}{\\psi(X_i, Y_j)} \n$$\n\n$$\n\\psi(X,Y) = \\begin{cases} \n1 &  \\ Y < X \\\\\n1/2 & \\  Y=X \\\\\n0 & \\ Y>X\n\\end{cases}\n$$\n\n- 모든 (X, Y) 쌍에 대해 X > Y이면 1을, X = Y이면 $\\frac{1}{2}$, X < Y 이면 0을 부여하여 확률을 구한다. \n- 직관적으로, **ROC curve**와 **AUC** 값은 곧 모델의 추정이 옳을 확률이며, 이의 성능의 최고값은 1, 최저값은 1/2이라는 점을 고민하면 위의 추정은 그럴듯 하다. \n\n$$\nE(\\hat{\\theta}) = Pr(X>Y) + \\frac{1}{2}Pr(X= Y)\n$$\n\n- 그럼, **확률(AUC)**은 위와 같이 정리할 수 있다. \n\n## Standard Error\n이제, **AUC**값의 신뢰성을 측정하기 위해서는 **SE(standard Error)**를 계산하는 것이 필요하다. \n이는 **DeLong(1988)**이 제시한 방법을 참고하자. \n\n\nξ를 각각의 집단 간 공분산이라 하자.\n\nξ₁₀은 $C_1$의 $X_i$와 $C_2$의 $Y_j, Y_k$간 공분산,\n\nξ₀₁은 $C_2$의 $Y_j$와 $C_1$의 $X_i, Y_k$간 공분산,\n\nξ₁₁은 $C_1$의 $X_i$와 $C_2$의 $Y_j$간 자기공분산이다.\n\nξ₁₀, ξ₀₁, ξ₁₁의 기대값은 아래와 같다. \n\n$$\nξ_{10} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_k)]-\\theta^2, \n$$\n\n$$\nξ_{01} = E[\\psi(X_i, Y_j) \\psi(X_k,Y_j)]-\\theta^2, \n$$\n\n$$\nξ_{11} = E[\\psi(X_i, Y_j) \\psi(X_i,Y_j)]-\\theta^2, \n$$\n\n기댓값들을 이용하여 **AUC** 추정치의 분산을 계산할 수 있다.\n\n$$\nvar(\\hat{\\theta}) = \\frac{(n-1)ξ_{10}+(m-1)ξ_{01}}{mn} + \\frac{ξ_{11}}{mn}\n$$\n\n이와 같은 방법으로 단일 표본 집합에서의 **AUC**의 **표준 오차**를 구할 수 있다. \n\n-   이제, 단일 표본 집합이 아닌 다른 표본집합 r과 s에 대해 다뤄보자. 여러 표본 집합이 있을 경우, 각 표본 간의 **상호 공분산** 또한 고려되어야한다. 두 표본 집합 r과 s에 대해 AUC의 공분산 계산은 아래와 같다. \n\n\n$$\nξ_{10}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n$$\n\n$$\nξ_{01}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s,\n$$\n\n$$\nξ_{11}^{rs} = E[\\psi(X_i^r, Y_j^r) \\psi(X_i^s,Y_k^s)]-\\theta^r \\theta^s, \n$$\n\n이제, 아래 식을 통해 표본 집합의 **AUC** 값 간의 **공분산**을 계산할 수 있다.\n\n$$\ncov(\\hat{\\theta^r},\\hat{\\theta^s}) = \\frac{(n-1)ξ_{10}^{rs}+(m-1)ξ_{01}^{rs}}{mn} + \\frac{ξ_{11}^{rs}}{mn}\n$$\n이 수식을 통해 여러 표본 집합 간의 공분산을 반영하여 **AUC의 표준 오차**를 더 정확하게 추정할 수 있다.\n\n\n* 이를 바탕으로 우리가 궁금한 값 **\"표준 오차\"**에 접근해 보자.\n**(Hoeffding_1948, Bamber_1975, Sen_1960)**과 같은 분들 덕분에, 우리는 **AUC 표준 오차**를 보다 정확히 추정할 수 있다.\n\n$$\nV^r_{10}(X_i) = \\frac{1}{n} \\sum_{j=1}^n\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (i= 1,2,...,m)\n$$\n\n$$\nV^r_{01}(Y_j) = \\frac{1}{m} \\sum_{i=1}^m\\psi(X_i^r,Y_j^r)\\ \\ \\ \\ \\ (j= 1,2,...,m)\n$$\n\n*   $V^r_{10}(X_i)$은 집합r에서 값을 기반으로 한 분산이다.\n\n$\\psi(X_i^r,Y_j^r)$는 $X_i^r,Y_j^r$의 관계를 나타내며, 이를 통해 분산을 구한다.\n$V^r_{01}(Y_j)$은 집합 $Y^r_j$에서의 분산이다. \n\n두 값을 통해 각 표본 집합에 대한 분산을 따로 계산한 후, 이를 결합하여 최종적으로 **표준 오차**를 추정할 수 있다. \n각 분산 값이 표본 크기에 따라 가중평균되며, **AUC 표준 오차 S**는 아래와 같다.\n\n$$\n\\\\ \nS = \\frac{1}{m}S_{10} + \\frac{1}{n}S_{01}\n\\\\\n$$\n\n> 표준 오차를 통해 우리는 AUC 값이 얼마나 신뢰할 수 있는지 평가할 수 있으며, 이는 모델의 성능을 명확히 이해하는데 중요한 역할을 한다. \n\n  \n# Code\n\n이를 R에서는 **pROC package**를 통해 이용할 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC);library(dplyr);library(data.table)\ndata(aSAH)\nSAH <- as.data.table(aSAH)\nhead(SAH)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    gos6 outcome gender   age  wfns s100b  ndka\n   <ord>  <fctr> <fctr> <int> <ord> <num> <num>\n1:     5    Good Female    42     1  0.13  3.01\n2:     5    Good Female    37     1  0.14  8.54\n3:     5    Good Female    42     1  0.10  8.09\n4:     5    Good Female    27     1  0.04 10.42\n5:     1    Poor Female    42     3  0.13 17.40\n6:     1    Poor   Male    48     2  0.10 12.75\n```\n\n\n:::\n:::\n\n\n\n\n>데이터는 pROC 패키지 내장 데이터인 aSAH 데이터를 사용하였다. aSAH는 뇌동맥류 파열로 인해 발생하는 SAH에 대한 데이터이다. n=113명이며, 변수는 아래와 같다.\n\n```\n  gos6        (Glasgow Outcome scale), \n  outcome     (clinical outcome: Good/Poor)\n  gender      (성별:  Male/female)       \n  age         (나이)        \n  wfns        (신경학적 평가 점수: 1~5, 1= 가장 양호), \n  s100b       (혈액 내 S100B 단백질 농도: 뇌손상 biomarker)\n  ndka        (혈액 내 neuron-specific enoalse 농도: 신경 손상 biomarker)\n```\n\n위 변수 중 100b를 사용하여 outcome을 예측하는 **ROC curve**와 **AUC** 값을 구해보자.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_result <- roc(response = SAH$outcome, \n                  predictor = SAH$s100b, \n                  levels = c(\"Good\", \"Poor\"),  # outcome의 순서 지정\n                  direction = \"<\")  \n\nprint(roc_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = SAH$outcome, predictor = SAH$s100b, levels = c(\"Good\",     \"Poor\"), direction = \"<\")\n\nData: SAH$s100b in 72 controls (SAH$outcome Good) < 41 cases (SAH$outcome Poor).\nArea under the curve: 0.7314\n```\n\n\n:::\n:::\n\n\n\n위와 같이 지정한 변수 및 **AUC** 값을 확인할 수 있다.\n\n여기서 **ROC curve**를 직접 보고싶으면 아래와 같이 확인할 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(\n  roc_result,\n  col = \"blue\",    # curve 색깔 지정\n  lwd = 2,         # curve 두께 지정\n  main = \"ROC Curve for s100b\"      #그래프 제목 지정\n  # xlim = c(1, 0),                 # x축 범위, 일반적으로 1-spec(FPR)   \n  # ylim = c(0, 1)                  # y축 범위, 일반적으로 Sensitivity       \n  # type = \"1\"                      # 그래프의 형태, 기본값은 선그래프\n  # xlab/ylab = \"1-Specificity\"     # x/y축 레이블 지정.\n)\ntext(             # curve와 함께 AUC값 표기.\n  x = 0.6, y = 0.4, \n  labels = paste(\"AUC =\", round(auc(roc_result), 3)), \n  col = \"red\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n이처럼 **ROC curve, AUC**값을 구할 수 있다. \n이제 **AUC** 값에 대해 추가적으로 알아보자.\n\n-   **pROC package**을 이용해 **AUC**의 **표준오차(SE)와 신뢰구간(CI)**를 구할 수 있다.\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nauc_result <- auc(roc_result)   #이전에 구한 roc_result 사용\n\nse_auc <- sqrt(var(roc_result))  \n\nci_result <- ci.auc(roc_result)\n\nprint(auc_result);print(ci_result);print(se_auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.7314\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI: 0.6301-0.8326 (DeLong)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05165929\n```\n\n\n:::\n:::\n\n\n\n-   위처럼 **SE/CI**를 구하는 것에 더해, ROC curve에서 **특정 threshold(0.7)**와 비교한 **p-value**를 구할 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz_value <- (auc_result - 0.7)/se_auc\np_value <- 2*(1- pnorm(abs(z_value)))\ncat(\"p-value for AUC > 0.7:\", p_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value for AUC > 0.7: 0.5437048 \n```\n\n\n:::\n:::\n\n\n\n>  위 경우는 유의하지 않다. \n\n\n# Example\n\n-   위와 같은 방법으로 aSAH의 다양한 변수들 중, **outcome 예측**에 가장 좋은 변수를 확인하자.\n또한, **age/gender**를 기준으로 **subgroup analysis**도 진행하자. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC);library(data.table);library(dplyr)\n#data\ndata(aSAH)\nSAH <- as.data.table(aSAH)\n\n#subgroup\nsubgroups <- list(\n  data = SAH,\n  age_over_50 = SAH[age > 50],\n  age_50_or_below = SAH[age <= 50],\n  male = SAH[gender == \"Male\"],\n  female = SAH[gender == \"Female\"]\n)\n\n# ROC 및 성능 평가 함수(threshold: 0.8)\nevaluate_roc <- function(data, predictor_var) {\n  #predictor_var는 numeric variable이어야.\n   if (!is.numeric(data[[predictor_var]])) {\n    data[[predictor_var]] <- as.numeric(data[[predictor_var]])\n   }\n  \n  # outcome variable은 factor variable이어야.\n  if (!is.factor(data$outcome)) {\n    data$outcome <- factor(data$outcome, levels = c(\"Poor\", \"Good\"))\n  }\n  \n  #ROC\n  roc_result <- roc(data$outcome, data[[predictor_var]], levels = c(\"Poor\", \"Good\"))\n  \n  #AUC, CI\n  auc_value <- auc(roc_result) %>% round(3)\n  auc_CI <- ci.auc(roc_result) %>% round(3)\n  \n      # Handle AUC == 1\n      if (auc_value == 1) {\n        warning(\"AUC is 1. SE and p-value may be undefined.\")\n        se_auc <- NA\n        p_value <- NA\n      } else {\n        # SE \n        se_auc <- sqrt(var(roc_result))\n        # P-value\n        z_value <- (auc_value - 0.8) / se_auc\n        p_value <- 2 * (1 - pnorm(abs(z_value)))\n      }\n  \n    p_value_display <- ifelse(p_value < 0.001, \"<0.001\", round(p_value, 3))\n  \n    \n    # 비교하고픈 cutoff가 있다면 비교하여 Sensitivity, Specificity, PPV, NPV, Accuracy등 다양한 값을 구할 수 있다. \n      #ex. median으로 cutoff를 지정하여 비교할 수 있다. (결과에는 포함하지 않음.)\n      #median cutoff.\n      median_cutoff <- median(data[[predictor_var]], na.rm = TRUE)\n      \n      # \n      data$predicted <- data[[predictor_var]] >= median_cutoff\n      table1 <- table(data$outcome, data$predicted)\n\n  \n        # 구조 점검.\n        row_levels <- rownames(table1)\n        col_levels <- colnames(table1)\n        \n        if (!(\"Good\" %in% row_levels)) table1 <- rbind(table1, \"Good\" = c(0, 0))\n        if (!(\"Poor\" %in% row_levels)) table1 <- rbind(table1, \"Poor\" = c(0, 0))\n        if (!(\"TRUE\" %in% col_levels)) table1 <- cbind(table1, \"TRUE\" = c(0, 0))\n        if (!(\"FALSE\" %in% col_levels)) table1 <- cbind(table1, \"FALSE\" = c(0, 0))\n        \n        TP <- table1[\"Good\", \"TRUE\"]\n        FP <- table1[\"Poor\", \"TRUE\"]\n        FN <- table1[\"Good\", \"FALSE\"]\n        TN <- table1[\"Poor\", \"FALSE\"]\n        \n        Sensitivity <- (TP / (TP + FN)) %>% round(3)\n        Specificity <- (TN / (TN + FP)) %>% round(3)\n        PPV <- (TP / (TP + FP)) %>% round(3)\n        NPV <- (TN / (TN + FN)) %>% round(3)\n        Accuracy <- ((TP + TN) / (TP + FP + TN + FN)) %>% round(3)\n    \n  list(\n    AUC = auc_value,\n    AUC_CI = auc_CI,\n    `P value` = p_value_display\n  )\n}\n\n\n# ROC 분석\npredictor_vars <- c(\"gos6\", \"age\", \"wfns\", \"s100b\", \"ndka\")\n\nresults <- lapply(subgroups, function(group) {\n  lapply(predictor_vars, function(var) evaluate_roc(group, var))\n})\n\nresult_tables <- rbindlist(lapply(names(results), function(group_name) {\n  \n  #subgroup 가져오기\n  group_results <- results[[group_name]]\n  # 예측 변수 가져오기\n  group_data <- rbindlist(lapply(seq_along(group_results), function(i) {\n    # 인덱스(i)\n    data <- group_results[[i]]\n    \n    if (length(data) > 0) {\n      as.data.table(data) %>%\n        mutate(\n          Group = group_name,               \n          Predictor = paste(\"Predictor\", i), \n          `AUC(95% CI)` = paste0(min(AUC_CI), \"-\", max(AUC_CI)), \n          `P value` = ifelse(`P value` < 0.001, \"<0.001\", round(`P value`, 3)) \n        ) %>%\n        select(\n          Group, Predictor, AUC, `AUC(95% CI)`, `P value`\n        ) \n    }\n  }))\n  \n  return(group_data)\n}), use.names = TRUE, fill = TRUE) \n\nresult_tables[, Predictor := case_when(\n  Predictor == \"Predictor 1\" ~ \"gos6\",\n  Predictor == \"Predictor 2\" ~ \"age\",\n  Predictor == \"Predictor 3\" ~ \"wfns\",\n  Predictor == \"Predictor 4\" ~ \"s100b\",\n  Predictor == \"Predictor 5\" ~ \"ndka\"\n)]\n\n# cause CI, 3 row created. rm. \nresult_tables_filtered <- result_tables[seq(1, nrow(result_tables), by = 3), ]\nprint(result_tables_filtered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Group Predictor   AUC AUC(95% CI) P value\n             <char>    <char> <num>      <char>  <char>\n 1:            data      gos6 1.000         1-1    <NA>\n 2:            data       age 0.615 0.508-0.722  <0.001\n 3:            data      wfns 0.824 0.749-0.899   0.531\n 4:            data     s100b 0.731  0.63-0.833   0.182\n 5:            data      ndka 0.612 0.501-0.723  <0.001\n 6:     age_over_50      gos6 1.000         1-1    <NA>\n 7:     age_over_50       age 0.517  0.36-0.675  <0.001\n 8:     age_over_50      wfns 0.738 0.613-0.864   0.334\n 9:     age_over_50     s100b 0.725 0.588-0.861    0.28\n10:     age_over_50      ndka 0.626 0.473-0.778   0.025\n11: age_50_or_below      gos6 1.000         1-1    <NA>\n12: age_50_or_below       age 0.541 0.387-0.695  <0.001\n13: age_50_or_below      wfns 0.910 0.837-0.983   0.003\n14: age_50_or_below     s100b 0.702 0.529-0.874   0.266\n15: age_50_or_below      ndka 0.612 0.453-0.771   0.021\n16:            male      gos6 1.000         1-1    <NA>\n17:            male       age 0.680 0.514-0.845   0.154\n18:            male      wfns 0.876 0.773-0.979   0.148\n19:            male     s100b 0.773 0.632-0.914   0.707\n20:            male      ndka 0.552 0.371-0.734   0.007\n21:          female      gos6 1.000         1-1    <NA>\n22:          female       age 0.635   0.49-0.78   0.026\n23:          female      wfns 0.779  0.67-0.887   0.705\n24:          female     s100b 0.720   0.57-0.87   0.296\n25:          female      ndka 0.667 0.526-0.808   0.064\n              Group Predictor   AUC AUC(95% CI) P value\n```\n\n\n:::\n:::\n\n\n\n>  데이터 상 크게 유의미한 변수는 없다. \n\n\n# Conclusion\n\n-   **ROC curve**를 그리고, **AUC** 값을 구할 수 있다.\n-   **AUC** 값의 **SE, CI**를 구할 수 있다. \n-   threshold와 비교하여 **p-value**를 구할 수 있다.\n-   이를 실제 데이터에 적용/연습해 보았다.\n\n\n# Reference\n[1]   E. DeLong, D. DeLong, and D. Clarke-Pearson, “Comparing the areasunder two or more correlated receiver operating characteristic curves: anonparametric approach,” Biometrics, pp. 837–845, 1988",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}