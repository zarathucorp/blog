---
title: "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)"
description: |
  아인슈타인의 일반상대성이론은 태양 근처에서 빛이 휘어지는 현상을 빛이 아닌 시공간이 휘어지는 것으로 해석합니다. 비슷한 아이디어를 통계학에 적용하여 U-shape 관계를 휘어진 다차원 공간에서의 선형모형으로 재해석하였습니다.  
categories:
  - article
  - statistics
  - R 
  - rpackage
author:
  - name: Jinseob Kim
    url: https://github.com/jinseob2kim
    affiliation: ANPANMAN Co.,Ltd
    affiliation_url: https://www.anpanman.co.kr
date: 11-08-2018
bibliography: VecLinear.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,fig.align="center",message=FALSE, warning=FALSE,cache=T,dpi=300)
library(knitr);library(reshape2);library(ggplot2);require(grid);library(gridExtra);library(parallel);library(readxl);library(data.table)
```

본 연구는 [김진섭](https://github.com/jinseob2kim) 대표가 박사학위 논문으로 계획했던 연구로, 결과적으로 학술지 게재와 심사통과에 실패했다는 것을 미리 알려드립니다. 계산법은 [R package](https://github.com/jinseob2kim/MDLM) 로 만들었습니다. 


## Abstract 

선형모형을 적용하기 어려운 $J,U$-shape같은 curved linear relationship을 비선형모형으로 분석하면 선형모형에 비해 해석이 어려워진다. 이에 본 연구에서는 선형관계의 컨셉은 유지하면서 $J,U$-shape 같은 curved linear relationship을 표현할 수 있는 모형을 제안한다. 이것은 선형모형의 무대를 1차원에서 휘어진 다차원 공간으로 확장함으로서 가능하며, curved linear relationship을 다차원공간에서의 선형관계로 재해석할 수 있다. 시뮬레이션 결과 선형관계는 기존의 선형모형과 동등한 성능으로 추정하면서 더 우수한 성능으로 curved relationship를 추정할 수 있었고, 실제 $U$-shape을 보이는 관계를 다차원공간에서의 선형관계로 쉽게 설명할 수 있었으며 $U$-shape의 cut-off값도 쉽게 계산할 수 있었다. 선형모형을 완벽히 포함하여 확장한 본 연구의 제안이 건강연구의 새로운 표준으로 자리잡을 수 있으리라 자신한다. 


## Introduction
Multivariable Linear Model은 분석결과의 해석이 간단하면서도 여러 독립변수들을 동시에 고려할 수 있는 장점이 있어 Health Science에서 널리 이용된다[@schneider2010linear]. 그러나 모든 관계가 선형관계인 것은 아니며 흔한 non-linear relationship으로  $J,U$-shape같은 curved linear relationship이 있다[@calabrese2001u;@power1998u;@de2009depression;@knutson2006u]. 이런 관계를 단순히 선형모형으로 분석하게 되면 간단하긴 하나 정확한 추정을 할 수 없으며 exponential, Log나 제곱, 루트를 이용해 변수를 치환하여 선형모형을 이용할 수 있다[@jagodzinski1981testing]. 그러나 치환으로 선형관계를 만들 수 있는 경우는 극히 일부분에 지나지 않아 많은 경우에 비선형모형(non-linear model)을 활용하는데, 대표적인 방법으로는 독립변수의 고차항을 모형에 추가하거나(Polynomial Model) 비모수적인 방법으로 곡선을 추정하는 Additive Model, 그리고 Multi-layer를 이용한 neural network이 있다[@jagodzinski1981testing;@buja1989linear;@hornik1989multilayer]. 그러나 이런 비선형모형들은 휘어진 모양을 해석하기 때문에 직선으로 해석하는 선형모형에 비해 해석이 복잡할 수 밖에 없다. 


이와 비슷한 문제가 20세기 초 물리학에서도 있었는데 태양 주위에서 빛이 휘는 문제가 바로 그것이다. 이 현상은 뉴턴의 물리학으로 설명되지 않았었는데, 아인슈타인(*Albert Einstein*)은 빛이 휘는 것이 아니라 태양 근처의 4차원 시공간(spacetime)이 휘어진 것이라는 발상의 전환을 통해 이 문제를 설명하였다[@coles2001einstein]. 이것이 유명한 일반상대성이론으로 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 의미한다[@verlinde2011origin].  


이에 저자는 아인슈타인의 아이디어와 비슷하게 선형모형의 무대를 휘어진 다차원 공간으로 확장함으로서 $J,U$-shape을 선형관계로 해석할 수 있는 Multi-dimensional Linear Model(MDLM)을 제안한다. 이것은 기존의 선형모형에 차원(dimension)의 개념을 추가하여 일반화한 것으로 모든 독립변수(independent variable)들이 같은 dimension의 정보라면 기존의 Linear Model과 일치한다. 먼저 개념을 수식으로 정리한 후 계수들을 추정하는 방법을 설명할 것이며 다양한 시나리오를 시뮬레이션하여 MDLM의 유용성을 살펴보겠다. 마지막으로 실제 $U$-shape을 갖는 데이터에 본 모형을 적용하여 유용성을  평가할 것이다.   


## Formula
음이 아닌 실수 $Y$를 종속변수로 실수 $X_1$, $X_2$,$\cdots$, $X_n$들을 독립변수라 하자. 

### 2 independent variables, 2 dimensions
$Y$와 $X_1$, $X_2$의 선형관계를 2차원 벡터공간에서 표현하면 아래와 같다. 

$$
\begin{aligned}
\boldsymbol{\vec{Y}} &= (\beta_{01} + \beta_1X_1)\boldsymbol{\vec{g}}_1 + (\beta_{02} + \beta_2X_2)\boldsymbol{\vec{g}}_2 
\end{aligned}
$$
$\boldsymbol{\vec{g}}_i$들은 $X_i$방향으로의 단위벡터로서 크기는 모두 1이며 Figure \@ref(fig:fig1)에 그림으로 표현되어 있다. 


```{r fig1,fig.cap="MDLM with 2 variables, 2 dimensions"}
d=data.frame(x=c(0,0,0), y=c(0,0,0), vx=c(1,0.5,1.5), vy=c(0,sqrt(3)/2,sqrt(3)/2))

ggplot() + geom_segment(data=d, mapping=aes(x=x, y=y, xend=x+vx, yend=y+vy), arrow=arrow(length=unit(0.3,"cm")), size=1, color="black") + geom_point(data=d, mapping=aes(x=x, y=y), size=1, shape=21, fill="black") + annotate(geom="text", x=c(1,0.55,1.6), y=c(0.05,sqrt(3)/2,sqrt(3)/2), label=c(as.character(expression(beta[0][1]+beta[1]*X[1])),as.character(expression(beta[0][2]+beta[2]*X[2])),as.character(expression(Y))),color="black",parse=T)+xlab("")+ylab("")+xlim(c(0,2))+ylim(c(0,1))

```

 이것은 방향의 개념을 제외하면 기존의 선형모형과 같으며, 만일 $\boldsymbol{\vec{g}}_1$과 $\boldsymbol{\vec{g}}_2$이 같은 방향이라면 아래와 같이 기존 선형모형과 일치하게 된다.

$$
\begin{aligned}
Y &= (\beta_{01} + \beta_1X_1) + (\beta_{02} + \beta_2X_2) \\
  &= \beta_{0} + \beta_1X_1 + \beta_2X_2
\end{aligned}
$$
($\beta_0 = \beta_{01}+\beta_{02}$)



해석은 기존의 선형모형과 같이 변화량을 이용하며 $Y$와 $X_1$, $X_2$의 변화량에 대해서 식을 재구성하면 아래와 같다. 

$$
\begin{aligned}
d\boldsymbol{\vec{Y}} &= \beta_1dX_1\boldsymbol{\vec{g}}_1 +\beta_2dX_2\boldsymbol{\vec{g}}_2 
= \beta_1d\boldsymbol{\vec{X}_1} + \beta_2d\boldsymbol{\vec{X}_2} 
\end{aligned}
$$
즉, $X_2$가 고정되어 있을 때 $Y$는 $X_1$의 방향으로 $\beta_1$만큼 증가한다고 할 수 있으며, $X_1$이 고정되어 있다면 $Y$는 $X_2$의 방향으로 $\beta_2$만큼 증가한다고 볼 수 있다.



 벡터로 표현된 위 식을 $\boldsymbol{\vec{g}}_1$과 $\boldsymbol{\vec{g}}_2$의 내적값인 $g_{12}$를 이용해 스칼라로 표현하면 아래와 같다.

$$Y^2 = (\beta_{01} + \beta_1X_1)^2 + (\beta_{02} + \beta_2X_2)^2 + 2g_{12}(\beta_{01} + \beta_1X_1)(\beta_{02} + \beta_2X_2)$$


만약 $g_{12}=0$ 즉, $X_1, X_2$가 독립된 차원을 갖는다면 $X_2$가 고정되었을 때 $X_1$과 $Y$의 관계는 $X_1=-\frac{\beta_{01}}{\beta_1}$에서 최소값을 갖는 $U$-shape을 보이며 $X_2$와 $Y$의 관계도 마찬가지이다. 일반적으로 $Y^2 = (\beta_{01} + \beta_1X_1 + g_{12}(\beta_{02} + \beta_2X_2))^2+ (1-g_{12}^2)(\beta_{02} + \beta_2X_2)^2$로 식을 변형하면 $X_2$가 고정되었을 때 $X_1$과 $Y$의 관계는 $X_1 = -\frac{\beta_{01}+g_{12}(\beta_{02} + \beta_2X_2)}{\beta_1}$에서 최소값을 갖는 $U$-shape을 보임을 확인할 수 있다. 




### $p$ independent variable, 2 dimensions

일반적으로 독립변수가 $p$개인 경우 $X_1, \cdot, X_l$이 같은 차원, $X_{l+1}, \cdot, X_p$가 같은 차원에 있다고 가정하면 다음과 같이 벡터식과 스칼라식을 표현할 수 있다. 

$$
\begin{aligned}
\boldsymbol{\vec{Y}} &= (\beta_{01} + \beta_1X_1 + \cdots + \beta_lX_l)\boldsymbol{\vec{g}}_1 + (\beta_{02} + \beta_{l+1}X_{l+1} \cdots + \beta_pX_p )\boldsymbol{\vec{g}}_2 \\\\
Y^2 &= (\beta_{01} + \beta_1X_1 + \cdots + \beta_lX_l)^2 + (\beta_{02} + \beta_{l+1}X_{l+1} \cdots + \beta_pX_p)^2 \\ 
+ & 2g_{12}(\beta_{01} + \beta_1X_1 + \cdots + \beta_lX_l)(\beta_{02} + \beta_{l+1}X_{l+1} \cdots + \beta_pX_p)
\end{aligned}
$$

### $p$ independent variable, $p$ dimensions

마지막으로 독립변수들이 전부 다른 방향을 갖고 있다고 가정한다면 아래와 같은 벡터식과 스칼라식을 얻는다. 

$$
\begin{aligned}
\boldsymbol{\vec{Y}} &= (\beta_{01} + \beta_1X_1)\boldsymbol{\vec{g}}_1 + (\beta_{02} + \beta_{2}X_2 )\boldsymbol{\vec{g}}_2 + \cdots (\beta_{0p} + \beta_pX_p)\boldsymbol{\vec{g}}_p \\
&= \sum_{i=1}^{p}{(\beta_{0i} + \beta_iX_i)\boldsymbol{\vec{g}}_i} \\\\
Y^2 &= \sum_{i=1}^{p}{(\beta_{0i} + \beta_iX_i)\boldsymbol{\vec{g}}_i} \cdot
\sum_{i=1}^{p}{(\beta_{0i} + \beta_iX_i)\boldsymbol{\vec{g}}_i} \\
&= \sum_{i=1}^{p}{(\beta_{0i} + \beta_iX_i)^2} + 2\sum_{i < j}{g_{ij}(\beta_{0i} + \beta_iX_i)(\beta_{0j} + \beta_jX_j)}
\end{aligned}
$$

여기서 $g_{ij}$는 $X_i$방향의 단위벡터 $\boldsymbol{\vec{g}}_i$와 $X_j$방향의 단위벡터 $\boldsymbol{\vec{g}}_j$의 내적값으로 두 벡터의 dependency를 나타낸다. 위의 경우와 마찬가지로 모든 $g_{i}$들의 방향이 같다면 아래와 같이 기존의 선형모형과 같은 관계를 얻는다. 

$$Y= \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p$$
(단, ($\beta_0 = \beta_{01}+\beta_{02}+\cdots + \beta_{0p}$)
)


## Estimation 
$\beta = (\beta_1, \beta_2, \cdots, \beta_p, \beta_{01}, \beta_{02}, \cdots, \beta_{0p})$ estimation을 위해 최소화해야 할 cost function은 아래와 같이 오차제곱의 합(Sum of Squared Error: SSE)으로 하면 자연스럽게 기존 선형모형의 최소제곱 추정을 일반화 할 수 있다.     

$$SSE(\beta) = \sum_{k=1}^N (Y_k - \sqrt{\sum_{i=1}^n(\beta_iX_{ki}+\beta_{i0})^2 + 2\sum_{i<j}g_{ij}(\beta_iX_{ki}+\beta_{i0})(\beta_jX_{kj}+\beta_{j0})})^2$$

($Y_k, X_{ki}$: $k$th individual's $Y, X_{i}$ value)

위 식에서 $g_{ij}$가 전부 1이라면 $SSE(\beta) = \sum_{k=1}^N (Y_k- \beta_0 -\beta_1X_{k1} - \beta_2X_{k2} - \cdots - \beta_pX_{kp})^2$로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다. 



### $\beta$ Estimation
기존 선형모형과 달리 $SSE(\beta)$를 최소로 하는 $\beta$값은 직접 계산하기 어려워, optimization technique을 이용하며 Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다 [@nelder1965simplex;@fletcher1964function;@byrd1995limited]. 위의 방법들을 활용하여 우리는 초기 $\beta$값들부터 시작해서 반복적인 계산을 통해 수렴값을 얻게 된다. 한편 초기값이 바뀌면 $SSE(\beta)$의 값은 같더라도 $\beta$들의 부호가 다른 결과를 얻을 가능성이 있는데 이는 $SSE(\beta)$가 $\beta$들의 2차식으로만 구성되어 있기 때문이며, $\beta$들의 부호가 바뀌더라도 $SSE(\beta)$가 같다면 해석은 동일하다.


추정된 $\beta$값들의 standard error들은 $SSE(\beta)$의 hessian matrix($H$)로 부터 구할 수 있다. 어떤 함수 $f(\theta)$의 hessian matrix란 $f(\theta)$를 2번 미분한 성분들로 이루어진 행렬인데, 일반적으로 $\theta$의 variance와 반비례함이 알려져 있다[@Dov__1991]. 여기에서는 1변수 함수의 예를 통해 직관적으로 이해해보도록 하자. $f(\theta)$가 $\theta_0$에서 최소값을 갖을 때 $f$를 1번 미분한 값은 0임을 이용, $f$를 $\theta_0$ 부근에서 2차항까지만 테일러 급수전개를 하면

$$
\begin{aligned}
SSE(\hat{\theta} + d\theta) &= SSE(\hat{\theta}) + H \cdot\dfrac{(d\theta)^2}{2} 
\end{aligned}
$$

이고 $d\theta$에 대해 정리하면

$$(d\theta)^2 = 2\cdot H^{-1}\cdot (SSE(\hat{\theta}+d\theta)-SSE(\hat{\theta}))$$


이 된다. 즉 hessian이 커질수록 $\theta$의 분산에 해당하는 $(d\theta)^2$ 이 감소함을 알 수 있다. 

일반적으로 $SSE(\beta)$를 최소로 하는 $\hat{\beta}$들의 variance-covariance matrix는 아래와 같이 표현되며, 대각성분에 루트를 취하면 $\beta$들의 standard error 값이 되어 p-value와 confidence interval(CI)을 계산할 수 있다[@Dov__1991]. 

$$\text{vcov}(\hat{\beta}) = 2\cdot H^{-1}\cdot MSE(\hat{\beta})$$

($MSE$: Mean Squared Error)


### Estimation of $g_{ij}$
위에 설명한 추정은 $g_{ij}$가 고정되었을 때를 가정한 것인데, $g_{ij}$를 데이터에서 직접 구할 수도 있으며 이것은 Generalized Estimating Equation(GEE)에서 working correlation matrix를 직접 계산할 수 있는 것과 마찬가지이다[@pan2002selecting]. 이 때는 $g_{ij}$들은 $\beta$들과 달리 -1에서 1까지의 값을 갖는다는 제한조건이 있어 constrained optimization technique를 이용해야 하며 나머지는 앞서와 동일하다[@rios2013derivative]. 



## Simulation
$Y$와 $X_1$, $X_2$의 여러가지 관계에 대한 Simulation을 이용해 MDLM의 유용성을 살펴볼 것이며 구체적으로 다음의 5개 모형을 비교하겠다. 

1. Linear Model (LM)

2. MDLM with fixed $g_{12}=0$ (MDLM 1)

3. MDLM with non-fixed $g_{12}$ (MDLM 2)

4. Quadratic Model: Polynomial model with 2nd degree (Quadratic)

5. Generalized Additive Model (GAM)


모형비교는 Root Mean Square Error(RMSE)와 Akaike Information Criterion(AIC)을 이용하였으며 GAM의 경우는 effective degree of freedom(edf)를 AIC 계산에 활용하였다[@wood2001mgcv]. 


편의상 $X_1$과 $X_2$는 각각 1부터 10까지의 자연수를 갖는 것으로 가정하였으며 따라서 샘플수는 100이다. 모든 계산은 `R` 3.5.1의 **optim**, **constrOptim** 함수를 이용하였다.


```{r}
set.seed(222)
library(MASS);library(mgcv)
source('gradient_function.R')
ybar="y"
xbar=c("x1","x2")
#b.start=c(0,0,0,0)
#r.start=0
#t.start=c(b.start,r.start) # beta1, beta2, beta01, beta02, g12


rCor=function(corr){
  return(matrix(c(1,corr,corr,1),nrow=2,byrow=T))
}

ui=matrix(c(0,0,0,0,1,0,0,0,0,-1),nrow=2,byrow=T)
ci=c(-1,-1)



simdata=CJ(x1=1:10,x2=1:10)

SampleOur=function(ds=simdata,sd=1,beta.gap=5){
  n=nrow(ds)
  beta=c(runif(4,0,beta.gap),runif(1,-1,1))
  ds[,y:=sqrt((beta[1]+beta[2]*x1)^2 + (beta[3]+beta[4]*x2)^2 + 2*beta[5]*(beta[1]+beta[2]*x1)*(beta[3]+beta[4]*x2))+ rnorm(n,sd)]
  #yvar= (beta[1]+beta[2]*ds[,1])^2 + (beta[3]+beta[4]*ds[,2])^2 + 2*beta[5]*(beta[1]+beta[2]*ds[,1])*(beta[3]+beta[4]*ds[,2])
  #ds$y=sqrt(yvar) + rnorm(n,sd)
  return(data.frame(ds))
}


SampleOurFix=function(ds=simdata,sd=1){
  n=nrow(ds)
  ds[,y:=sqrt(x1^2+x2^2)+rnorm(n,sd=sd)]
  #data.frame(x1=rep(-5:5,11),x2=rep(-5:5,each=10))
  #ds$y=sqrt(ds$x1^2+ds$x2^2)+rnorm(n,sd=sd)
  #colnames(ds)=c("x1","x2","y")
  return(data.frame(ds))
}


SampleLinearFix=function(ds=simdata,sd=1){
  n=nrow(ds)
  ds[,y:=x1+x2+rnorm(n,sd=sd)]
  #ds=data.frame(x1=rep(1:10,10),x2=rep(1:10,each=10))
  #ds$y=ds$x1+ds$x2+rnorm(n,sd=sd)
  #colnames(ds)=c("x1","x2","y")
  return(data.frame(ds))
}


#SampleQuadFix=function(ds=simdata,sd=1){
#  n=nrow(ds)
#  ds[,y:=x1^2+x2^2+rnorm(n,sd=sd)]
  #ds=data.frame(x1=rep(1:10,10),x2=rep(1:10,each=10))
  #ds$y= ds$x1^2 + ds$x2^2 + rnorm(n,sd=sd)
  #colnames(ds)=c("x1","x2","y")
#  return(data.frame(ds))
#}



CompareMethods=function(dsample=dsample){
  nsample = nrow(dsample)
  nx=ncol(dsample)-1
  b.start = rep(0, 2*nx)
  r.start = rep(0.5, nx*(nx-1)/2)
  rmse=rep(0,5)
  aic=c(0,5)
  rmse[1]=sqrt(1/nsample * sum((lm(y~x1+x2,data=dsample)$residuals)^2))
  
  f2=function(x){CostFun(bs=x[1:length(b.start)], ybar,xbar,dsample,LowerToMatrix(rep(0, nx*(nx-1)/2)))}
  rmse[2]=sqrt(1/nsample * optim(c(b.start),f2,NULL)$value)
  
  f3=function(x){CostFun(bs=x[1:length(b.start)], ybar,xbar,dsample,LowerToMatrix(x[-(1:length(b.start))]))}
  rmse[3]=sqrt(1/nsample * constrOptim(c(b.start,r.start),f3,NULL,ui=ui,ci=ci)$value)
  
  dsample$x1_2=dsample$x1^2
  dsample$x2_2=dsample$x2^2
  rmse[4]=sqrt(1/nsample * sum((lm(y ~ x1 + x1_2 + x2 + x2_2, dsample)$residuals)^2))
  bb=gam(y~s(x1)+s(x2),data=dsample)
  gam.edf=sum(bb$edf)+1
  rmse[5]=sqrt(1/nsample * sum((bb$residuals)^2))
  #nn=neuralnet(y~x1+x2,hidden=2,linear.output=T,data=dsample,likelihood = T)
  #rmse[5]=sqrt(1/nsample * 2*nn$result.matrix[1,])
  aic = nsample* (log(2*pi)+1+2*log(rmse)) + 2*c(4,5,6,6,gam.edf)
  return(list(rmse,aic,gam.edf))
}


#zz=CompareMethods(dsample=sample.lm(100))
#xx=CompareMethods(dsample=sample.our(100))
#yy=CompareMethods(dsample=sample.quad(100))



#s1=mclapply(1:3,function(x)CompareMethods(dsample=SampleOurFix(100,sd=0.5)))

#mclapply(1:3,function(x)CompareMethods(dsample=SampleLinearFix(100,sd=2)))
#mclapply(1:3,function(x)CompareMethods(dsample=SampleQuadFix(100,sd=1)))


row.summary=function(reslist,resnum=1,dec=1){
    tb=t(sapply(1:length(reslist),function(x)reslist[[x]][[resnum]]))
    tb.summary = paste(round(colMeans(tb),dec),"±",round(apply(tb,2,sd),dec))
    if(resnum==3){
      tb.summary = paste(round(mean(tb),dec),"±",round(sd(tb),dec))
    }
    return(tb.summary)
  }

ScenarioResTable=function(dsample=SampleOurFix(simdata,sd=0.5),ntrial=100,dec=1){
  s1=mclapply(1:ntrial,function(x)CompareMethods(dsample))
  df=c(4,5,6,6,row.summary(s1,3,dec))
  out=rbind(row.summary(s1,1,dec),df,row.summary(s1,2,dec))
  colnames(out)= c("LM","MDLM(1)","MDLM(2)","Quadratic","GAM")
  rownames(out)=c("RMSE","DF","AIC")
  return(out)
}


ntry=3
s1=ScenarioResTable(SampleLinearFix(simdata,sd=1),ntry,1)
s2=ScenarioResTable(SampleOurFix(simdata,sd=1),ntry,1)
s3=ScenarioResTable(SampleOur(simdata,sd=1,beta.gap=5),ntry,1)

```


### Scenario 1: $Y = X_1 + X_2$
$Y \sim N(X_1 + X_2, 1)$ 로 샘플링해 데이터를 생성하였으며 100회의 시뮬레이션을 수행해 RMSE와 AIC를 비교하였다(Table 1). 

```{r}
kable(s1,align="c",caption = "Results: $Y = X_1 + X_2$")
```


비교 결과 선형모형이 적은 parameter로 효율적인 추정을 하고 있음을 알 수 있었으며 $g_{12}$를 고정하지 않은 MDLM(2)가 선형모형과 비슷한 성능을 보이는데 이는 MDLM이 선형모형을 포함한 개념임을 생각했을 때 자연스러운 결과이다.  


### Scenario 2: $Y^2 = X_1^2 + X_2^2$ 
이번엔 $Y \sim N(\sqrt{X_1^2 + X_2^2}, 1)$로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 2).

```{r}
kable(s2,align="c",caption = "Results: $Y^2 = X_1^2 + X_2^2$")
```

이번엔 MDLM(1)이 선형모형보다 확실히 우수한 추정값을 보이는것을 확인할 수 있으며, $g_{12}$를 고정하지 않은 MDLM(2) 또한 MDLM(1)과 비슷한 성능을 보이는 것을 확인할 수 있다. 


### Scenario 3: $\boldsymbol{\vec{Y}} = (\beta_{01} + \beta_1X_1)\boldsymbol{\vec{g}}_1 + (\beta_{02} + \beta_2X_2)\boldsymbol{\vec{g}}_2$

마지막으로 Scenario2를 일반화한 경우를 시뮬레이션하였다. $\beta$들은 -5~5, $g_{12}$는 -1~1의 값을 임의로 선택하여 $\boldsymbol{\vec{Y}} = (\beta_{01} + \beta_1X_1)\boldsymbol{\vec{g}}_1 + (\beta_{02} + \beta_2X_2)\boldsymbol{\vec{g}}_2$를 만족하는 $Y$를 샘플링하였다. 즉, $Y \sim N(\sqrt{(\beta_{01} + \beta_1X_1)^2 + (\beta_{02} + \beta_2X_2)^2 + 2g_{12}(\beta_{01} + \beta_1X_1)(\beta_{02} + \beta_2X_2)}, 1)$로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 3).

```{r}
kable(s3,align="c",caption = "Results: $\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2$")
```

시뮬레이션 결과 $g_{12}$를 추정할 수 있는 MDLM(2)가 다른 모형들보다 압도적으로 우수한 성능을 보이는 것을 확인할 수 있었다. 


## Apply to Real data

이번에는 실제 $U$-shape을 보이는 데이터를 MDLM으로 분석해보겠다. http://biostat.mc.vanderbilt.edu/dupontwd/wddtext/data/3.25.2.SUPPORT.csv의 데이터에는 응급실 내원 당시 평균 동맥압(mean arterial pressure, MAP)과 재실기간(length of stay,LOS)의 정보가 있는데 MAP와 LOS의 natural logarithm값의 관계가 $U$-shape이다. 이제 log(LOS)와 MAP의 관계를 아래와 같이 모델링 하였다.


$$\boldsymbol{\vec{\text{log(LOS)}}} = \beta_{00}\boldsymbol{\vec{g_{1}}} + (\beta_{01}+ \beta_1\cdot \text{MAP})\boldsymbol{\vec{g_{2}}}$$

즉 log(LOS)를 intercept와 MAP의 독립된 2차원으로 바라보는 관점으로 스칼라로 표현한 모형은 아래와 같다. 


$$(\text{log(LOS)})^2 = \beta_{00}^2 + (\beta_{01}+ \beta_1\cdot \text{MAP})^2$$


```{r}
#a=fread("http://biostat.mc.vanderbilt.edu/dupontwd/wddtext/data/3.25.2.SUPPORT.csv")
a <- fread("3.25.2.SUPPORT.csv")
a[,":="(intcpt=1,loglos=log(los))]
a=data.frame(a)
```


```{r}
#source('/home/secondmath/Dropbox/Survival_new/gradient_function.R')
ybar="loglos"
xbar=c("map","intcpt")
nx=2
nsample=nrow(a)
b.start=c(0,0,0,0)
r.start=0.1
t.start=c(b.start,r.start) # beta1, beta2, beta01, beta02, g12

ui=matrix(c(0,0,0,0,1,0,0,0,0,-1),nrow=2,byrow=T)
ci=c(-1,-1)

rmse=rep(0,3)
lmd=lm(loglos~map,data=a)
rmse[1]=sqrt(1/nsample * sum(lmd$residuals^2))
f2=function(x){CostFun(bs=x[1:length(b.start)], ybar,xbar,a,LowerToMatrix(rep(0, nx*(nx-1)/2)))}
mdlm1=optim(c(b.start),f2,NULL)  
rmse[2]=sqrt(1/nsample * mdlm1$value)
a$map2=(a$map)^2
qd=lm(loglos~map+map2,data=a)
rmse[3]=sqrt(1/nsample * sum(qd$residuals^2))
aic = nsample* (log(2*pi)+1+2*log(rmse)) + 2*c(4,5,5)
qd.coef=qd$coefficients

lmd_coef=round(lmd$coefficients,4)
mdlm1_coef=round(c(mdlm1$par[2]+mdlm1$par[4],mdlm1$par[3],mdlm1$par[1]),4)
qd_coef=round(qd.coef,4)
```

MDLM을 이용하여 Intercept와 MAP의 2차원공간으로 log(LOS)를 $\text{log(LOS)}^2 = `r abs(mdlm1_coef[1])`^2 + (`r -mdlm1_coef[2]` + `r -mdlm1_coef[3]`\cdot\text{MAP})^2$의 모형으로 추정할 수 있었고 AIC값은 $`r round(aic[2])`$였다. 이것은 선형모형으로 추정한 $\text{log(LOS)} = `r lmd_coef[1]` + `r lmd_coef[2]` \cdot \text{MAP}$(AIC $`r round(aic[1])`$), quadratic항을 추가한 $\text{log(LOS)} = `r qd_coef[1]` `r qd_coef[2]` \cdot \text{MAP} + `r qd_coef[3]` \cdot \text{MAP}^2$(AIC $`r round(aic[3])`$)보다 좋은 추정 결과이다(Figure \@ref(fig:fig2)). 또한 U shape의 cutoff값을 $\dfrac{`r abs(mdlm1_coef[2])`}{`r abs(mdlm1_coef[3])`} = `r round(abs(mdlm1_coef[2]/mdlm1_coef[3]),2)`$로 간단히 계산할 수 있었다.



```{r fig2,fig.cap="Relation between mean arterial pressure(mmHg) and length of stay day(log scale)"}
ggplot(a,aes(map,loglos))+geom_point()+geom_smooth(aes(map,loglos,color="black"),method="lm",se=F) +stat_function(aes(map,color="blue"),fun = function(x) sqrt((mdlm1$par[3]+mdlm1$par[1]*x)^2+ (mdlm1$par[2]+mdlm1$par[4])^2))+stat_function(aes(map,color="red"),fun = function(x) (qd.coef[1]+qd.coef[2]*x+qd.coef[3]*x^2))+scale_colour_manual(name = 'Model', values =c('black'='black',"blue"="blue", 'red'='red'), labels = c('Linear','MDLM','Quadratic'))+xlab("Mean arterial pressure")+ylab("log(Length of stay)")

```


## Discussion
MDLM을 이용해서 기존의 선형모형을 완벽히 포괄하면서 선형모형의 개념을 휘어진 다차원 공간으로 확장할 수 있었고, 이를 통해 $J,U$-shape같은 curved linear relationship을 잘 추정하고 cut-off값도 쉽게 확인할 수 있는 장점을 확인할 수 있었다. 기존 선형모형의 틀은 유지하면서 그것만이 진실은 아닐 수 있다는 것을 보여줬다는 점, 이를 통해 선형관계가 아닌 것을 선형관계로 해석할 수 있는 방법을 제시하였다는 점에서 큰 의미가 있다. 향후 연구자들이 평면에서의 선형관계에 국한되지 않고 가설을 검증할 수 있을 것이며, 이를 토대로 Health science 연구에서 MDLM이 기존 선형모형을 포괄하는 새로운 표준으로 자리잡을 수 있으리라 예상한다. 


MDLM의 추정식은 제곱근이 포함되어 있어 $Y$가 음수값을 갖고 있는 경우에는 적용하기 어렵다는 한계가 있다. 그러나 Health Science 분야에서 음수값을 갖는 지표는 별로 많지 않아 큰 문제는 아닐 것으로 생각하며, 변수변환을 통해 (+)로만 이루어진 새로운 변수를 만들어 해결할 수도 있다. 이 문제는 물리학자 *Paul Dirac*이 특수상대성이론을 고려한 양자역학의 방정식을 만들 때 겪었던 문제와 비슷한데, 그는 방정식의 계수가 꼭 숫자일 필요가 없고 행렬일 수도 있다는 기발한 아이디어로 이를 해결했다[@dirac1928quantum]. 예를 들어 $Y = \sqrt{\beta_0^2 + \beta_1^2x_1^2 + \beta_2^2x_2^2 }$ 일 때, $\beta_0, \beta_1, \beta_2$가 숫자일 필요가 없다는 것이다.  $\beta$들을 행렬로 간주한다면  $Y = \sqrt{\beta_0^2 + \beta_1^2x_1^2 + \beta_2^2x_2^2 } = \beta_0 + \beta_1x_1 + \beta_2x_2$ 꼴이 되어 제곱근을 없애는 것이 가능하다. 이 행렬들은 최소 $4\times 4$ 이상의 정방행렬이어야 함이 알려져 있으며, 대표적인 예로 Cliford Algebra를 만족하는 Dirac matrices 있는데 $\beta$들의 예를 하나 들면 아래와 같다[@deTraubenberg2009]. 

$$
\beta_0 = \alpha_0 \times \begin{pmatrix}
  1 & 0 &  0 &  0 \\
  0 & 1 &  0 &  0 \\ 
  0 & 0 &  -1 &  0 \\
  0 & 0 &  0 & -1
\end{pmatrix}
$$


$$
\beta_1 = \alpha_1 \times \begin{pmatrix}
   0 &  0 & 0 & 1 \\
   0 &  0 & 1 & 0 \\
   0 & -1 & 0 & 0 \\
  -1 &  0 & 0 & 0
\end{pmatrix}
$$


$$
\beta_2 = \alpha_2 \times \begin{pmatrix}
   0 & 0 & 1 &  0 \\
   0 & 0 & 0 & -1 \\
  -1 & 0 & 0 &  0 \\
   0 & 1 & 0 &  0
\end{pmatrix}
$$

($\alpha_0, \alpha_1, \alpha_2$: 실수)


회귀계수가 숫자가 아닌 행렬이 가능하다는 이 아이디어가 향후 본 연구의 제곱근 문제를 극복하는 열쇠가 될 수 있을 것이라 생각한다. 


$SSE(\beta)$를 최소화하는$\beta$를 구하기 위해 optimization tequnique을 활용한 것도 문제가 될 수 있는데, 얻은 $SSE(\beta)$값이 진짜 최소값(global minimum)인지 보장할 수 없기 때문이다. 이를 local minima problem이라 한다. 그러나 machine learning의 유행과 더불어 optimization technique도 빠르게 발전되고 있어 조만간 이 문제가 해결되리라 예상한다. 게다가 최근 연구에서 high-dimensional space인 경우 local minima problem은 매우 희귀한 것으로 나타났는데, 모든 차원에서 local minima일 가능성은 매우 낮기 때문으로 여겨진다[@dauphin2014identifying]. 


Introduction에서 언급했듯이 아인슈타인(*Albert Einstein*)은 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 설명하였는데, 수식으로 살펴보면 3차원 공간에서 기술된 뉴턴의 중력장 방정식 $\nabla^2\Phi = 4 \pi G\rho_0$를 휘어진 4차원에서의 방정식 $\boldsymbol{R}_{uv}-\dfrac{1}{2}\boldsymbol{g}_{uv} = \dfrac{8\pi G}{c^4}\boldsymbol{T}_{uv}$로 확장한 것이다[@verlinde2011origin]. 본 연구의 MDLM을 통해 선형공간의 무대를 다차원으로 확장하여 $U$-shape같은 curved linear relationship을 선형관계로 바라볼 수 있게 되었다는 점에서 물리학에서의 아인슈타인 방정식과 비슷한 의미를 가진다고 감히 주장해 본다. 실제로 $\beta_i^2$ 를 $g_{ii}$, $g_{ij}\beta_i\beta_j$를 합쳐서 $g_{ij}$라 놓으면 $g_{ij}$는 아인슈타인 중력장 방정식을 표현하는데 쓰이는 계량텐서(metric tensor) $g_{uv}$와 같은 의미를 갖게 되는데, $(dY)^2$를 표현하는 식은 $\sum_{i,j}g_{ij}(dX_i)(dX_j)$로 휘어진 시공간에서 두 지점 사이의 거리를 나타내는 방법과 정확히 일치한다. 뉴턴의 방정식으로도 일상적인 운동을 잘 설명할 수 있으나 우주 공간같은 거시적인 스케일에서는 아인슈타인 방정식이 필요해지는데, 이와 마찬가지로 다차원 공간에서 기술된 본 연구의 MDLM이 population level에서 기존 선형모형보다 더 정확히 건강관련 현상을 설명할 수 있으리라 예상한다. 


한편 일반상대성이론은 원자 이하의 미시세계의 현상을 잘 설명하지 못한다는 문제점이 있으며,불확정성의 원리(uncertainty principle)와 슈뢰딩거 방정식(Schrödinger equation)으로 대표되는 양자역학(quantum mechanics)의 논리가 이곳을 지배한다. 슈뢰딩거 방정식은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용으로 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특이한 점이다. 복소수는 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현하게 되고 놀라운 정확도로 미시세계의 현상을 설명할 수 있다. 이것은 Health science에도 중요한 시사점이 될 수 있는데, Health science에서 가장 큰 문제점 중 하나가 population level의 연구결과가 개인의 건강상태를 잘 설명하지 못한다는 것이다. 상태를 확률로 기술한다는 점에서는 베이지안 접근법(bayesian approach)이 양자역학의 접근과 비슷하지만 복소수를 활용할 수 없다는 점에서 차이가 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 개인의 건강상태를 설명하는 새로운 방법이 될 것이라 과감히 추측해 본다.  
