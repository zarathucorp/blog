---
title: "Kappa 분석 이해하기"
description: |
  여러 종류의 Kappa 분석 방법을 설명하고, R 코드 예제를 통해 실전에서 활용하는 방법을 정리하였습니다.
author: 
  name: YeJi Kang
  url: https://github.com/YejiKang63
date: "03-18-2025"
image: logo.png
format: 
  html: 
    toc-depth: 3
    toc-expand: true
    toc-location: left
    toc-title: "Kappa 분석 알아보기"
---

# 시작하기 전에

[Kappa 통계](https://en.wikipedia.org/wiki/Cohen%27s_kappa)는 **두 명 이상의 평가자(rater)가 범주형 데이터를 얼마나 일관되게 평가하는지를 측정하는 방법**이다. 단순한 일치율과 달리, Kappa 계수는 무작위로 일치할 가능성을 보정하여 보다 신뢰할 수 있는 평가 일치도를 제공한다.이는 평가자의 주관적인 판단이 개입되는 연구에서 필수적이라고 볼 수 있다.

Kappa 분석에는 여러 가지 변형이 있으며, 상황에 따라 적절한 방법을 선택해야 한다. 대표적인 Kappa 분석 방법은 다음과 같다:

1.  **Cohen’s Kappa (κ)**: 두 명의 평가자가 범주형 데이터를 평가할 때 사용
2.  **Cohen's Weighted Kappa**: 평가자 간의 불일치 정도를 가중치로 고려할 때 사용 (순위형 변수)
3.  **Fleiss’ Kappa**: 두 명 이상의 평가자가 있을 때 사용 (범주형 변수)
4.  **Generalized Fleiss' Kappa**: Fleiss' Kappa의 확장판으로, 순위형 데이터를 다룰 때 사용
5.  **Krippendorff’s Alpha**: 범주형, 순위형, 연속형 데이터 모두 적용 가능

Cohen’s와 Cohen's Weighted는 평가자가 두 명일때 적용되고, Fleiss’와 Generalized Fleiss'는 두 명 이상일 때 사용된다.

이 글에서는 R을 활용하여 다양한 Kappa 분석 방법을 구현하는 방법을 설명한다.

## 단순 일치율 vs. Kappa 계수

단순한 일치율(Percent Agreement)은 평가자 간의 동일한 판단이 나온 비율을 계산하는 방식이다. 하지만, 이는 무작위로 일치한 경우도 포함하기 때문에 신뢰도가 낮을 수 있다.

예를 들어, 두 평가자가 100개의 사례를 평가했을 때, 70개에서 동일한 판단을 내렸다면 단순 일치율은 70%이다. 하지만, 무작위로도 70%의 일치가 나올 가능성이 있다면, 실제 평가자의 일치 정도를 과대평가할 수 있다.

이를 보완하기 위해 Kappa 계수($κ$)는 무작위 일치율(Expected Agreement)을 고려하여 조정된 값을 제공한다. 즉, Kappa 계수는 실제 일치율과 무작위 일치율 간의 차이를 기반으로 계산되며, 0\~1 사이의 값으로 표현된다.

-   Kappa 계수($κ$) 해석:
    -   $κ$ = 1: 완벽한 일치
    -   $κ$ = 0: 무작위 일치 수준
    -   $κ$ \< 0: 평가자가 오히려 무작위보다 더 불일치
    -   0.6 ≤ $κ$ ≤ 0.8: 상당한 일치
    -   0.4 ≤ $κ$ \< 0.6: 중간 수준의 일치

따라서 $κ$의 값은 크면 클수록 좋은 것이라고 본다.

이제 Kappa 분석이 왜 중요한지를 이해했으므로, 다음 섹션에서는 각 Kappa 분석 방법을 살펴보고, R을 이용하여 실제 데이터를 분석하는 방법을 설명한다.

# 1. Cohen’s Kappa

Cohen’s Kappa($κ$)는 **두 명의 평가자가 범주형 데이터를 평가할 때** 사용되는 가장 기본적인 Kappa 계수이다. 여기서 측정되는 범주형 변수에서는 서로 다른 특성을 구분할 수 있지만, 이 특성 간의 순위나 서열 관계는 존재하지 않는다.

## 1.1 Cohen’s Kappa 공식

Cohen's Kappa는 다음과 같은 공식으로 계산된다:

$$
κ = \frac{P_o - P_e}{1 - P_e}
$$

-   $P_o$: 평가자간 일치 확률 (Observed Accuracy)
-   $P_e$: 우연히 일치된 평가를 받을 비율 (Expected Accuracy)

여기서 $P_e$는 '우연히 일치할 확률'을 나타낸다. 예를 들어, 두 명의 상담사가 환자에게 우울증이 있는지 없는지에 대해 완전히 무작위로 판단했다고 가정한다. 마치 동전을 던지는 것처럼 말이다. 그럼에도 불구하고 두 상담사가 우연히 같은 결론을 내릴 가능성이 어느 정도 존재하게 되는데, 이 우연에 의한 일치 확률을 나타내는 값이 바로 $P_e$가 된다. 즉, $P_e$는 평가자들이 실제로 동의한 정도가 아니라, 순전히 우연으로 평가 결과가 같아질 가능성을 나타내는 가상의 확률이라고 이해하면 된다. $P_e$의 비율이 높을 수록 우연하게 일치한다는 것이고, 이 값이 최소에 가까워질수록 높은 $κ$의 값을 얻을 수 있게 된다.

### Observed Accuracy $P_o$

$$
P_0 = \frac{1}{n} \sum_{i=1}^{g} f_{ii}
$$

-   $n$: 전체 평가 개수
-   $g$: 평가 범주의 개수 (예: 3개의 등급, 5개의 점수 등)
-   $f_{ii}$: 평가자 두 명이 동일한 범주를 선택한 횟수

즉, $P_0$는 전체 평가 중에서 평가자들이 동일한 범주를 선택한 비율을 의미한다. 이는 실제 데이터에서 평가자들이 얼마나 일치했는지를 보여주는 값이다.

### Expected Accuracy $P_e$

$$
P_e = \frac{1}{n^2} \sum_{i=1}^{g} f_{i+} f_{+i}
$$

-   $f_{i+}$: 특정 범주의 행 합 (첫 번째 평가자가 해당 범주를 선택한 횟수)
-   $f_{+i}$: 특정 범주의 열 합 (두 번째 평가자가 해당 범주를 선택한 횟수)

$P_e$는 평가자들이 무작위로 평가했을 때 동일한 범주를 선택할 확률을 의미한다. 이는 두 평가자가 특정 범주를 선택할 확률을 각각 곱하여 계산되며, 모든 범주에 대해 합산하여 전체적인 기대 일치도를 구하는 방식이다.

### Binary Classifications

Cohen’s Kappa는 이진 분류에서도 모델의 예측 신뢰도를 평가하는 중요한 지표로 활용될 수 있다. 이는 다음과 같은 공식으로 표현된다.

$$
κ = \frac{2 \times (TP \times TN - FN \times FP)}
{(TP + FP) \times (FP + TN) + (TP + FN) \times (FN + TN)}
$$

여기서 각 항목은 다음과 같은 의미를 가진다:

-   $TP$ (True Positives): 실제로 긍정(positive)인 경우를 정확하게 예측한 수
-   $FP$ (False Positives): 실제로는 부정(negative)이지만, 긍정으로 잘못 예측한 수
-   $TN$ (True Negatives): 실제로 부정인 경우를 정확하게 예측한 수
-   $FN$ (False Negatives): 실제로는 긍정이지만, 부정으로 잘못 예측한 수

이진 분류에서 Cohen's Kappa는 단순한 정확도보다는 무작위 예측과 비교하여 모델이 얼마나 신뢰할 만한지를 측정하는 데 유용하다. 예를 들어, 불균형한 데이터에서 단순한 정확도는 높은 값이 나올 수 있지만, Kappa 값이 낮게 나오는 경우가 있을 수 있다. 이는 모델이 특정 클래스를 과도하게 예측하고 있음을 나타낼 수 있다.

## 1.2 Example 1

첫 번째 예시에서는 두 평가자가 다섯 개 항목을 각각 평가한 뒤, Cohen's Kappa 통계량을 이용해 두 평가자 간의 일치도를 분석했다. R에서 Cohen's Kappa를 구하기 위해서는 irr 패키지를 사용한다 (Desc Tools, psych 등 다른 패키지도 존재).

```         
library(irr)

# 두 평가자가 5개의 항목을 평가
ratings <- data.frame(
  rater1 = c("A", "A", "B", "A", "C"),
  rater2 = c("A", "B", "B", "A", "C")
)

# Cohen's Kappa 계산
result <- kappa2(ratings, weight = "unweighted")
print(result)
```

**출력:**

```         
 Cohen's Kappa for 2 Raters (Weights: unweighted)

 Subjects = 5 
   Raters = 2 
    Kappa = 0.688 

        z = 2.28 
  p-value = 0.0224
```

이 예시의 결과는 Kappa 값이 0.688로 나타났고, z 통계량은 2.28, p값은 0.0224로 나타나 통계적으로 유의미한 일치도를 보였다. 이는 두 평가자가 단순히 우연히 일치하는 것을 넘어 실제로 상당히 일치된 평가를 내렸다는 의미로 해석할 수 있다.

## 1.3 Example 2

다음 예시는 실제 의료 환경에서 자주 발생하는 사례를 바탕으로 Cohen's Kappa를 적용한 것이다. 두 명의 영상의학 전문의가 CT, MRI, PET, X-ray 총 4가지 영상진단 방식으로 환자를 각각 독립적으로 평가했을 때, 두 전문의 간 평가가 얼마나 일치하는지를 분석한다.

``` r
library(ggplot2); library(officer)

imaging.modalities <- c("CT", "MRI", "PET", "Xray")

kappa.results <- sapply(imaging.modalities, function(modality){
  var1 <- paste0(modality, "_Rater1")
  var2 <- paste0(modality, "_Rater2")

  kappa_calc <- irr::kappa2(diagnosis_data[, .SD, .SDcols = c(var1, var2)], weight = "unweighted")
  standard_error <- kappa_calc$value / kappa_calc$statistic
  conf_interval <- c(kappa_calc$value - qnorm(0.975) * standard_error,
                     kappa_calc$value + qnorm(0.975) * standard_error)

  return(paste0(round(kappa_calc$value, 3), " (95% CI: ", round(conf_interval[1], 3),
                "-", round(conf_interval[2], 3), ")"))
})
```

이 예시에서 계산된 Kappa 값은 각 영상진단 방식별로 제공되며, 각 값에 대한 표준오차와 95% 신뢰구간도 함께 계산된다. irr::kappa2 함수를 통해 두 평가자 간의 Cohen's Kappa 값을 계산했고, weight = "unweighted" 옵션으로 평가 항목 간의 차이에 동일한 가중치를 부여한다.

이와 같이 Cohen's Kappa는 두 평가자가 같은 값을 측정했는지 여부를 고려하지만, 불일치의 정도는 고려하지 않는다. Example 1을 보면 Cohen's Kappa는 평가자가 A와 C를 선택했을 때와 A와 B를 선택했을 때를 동일한 불일치로 간주하고 있는 것을 확인할 수 있다. 그렇다면 범주형 변수가 아닌 순위형 변수가 있다면 어떨까?

# 2. Weighted Kappa

순위형 변수, 즉 특성을 정렬할 수 있는 변수가 있다면 그 순서 또한 고려해야 한다.

예를 들어, '불만족', '중립'과 '만족'이 있다고 하자. 불만족과 중립 사이에는 불만족과 만족 사이보다 작은 차이가 있다. 이러한 차이를 고려하는 것이 바로 Weighted Kappa다.

다시 말해, Weighted Kappa는 **순위형 데이터를 평가하는 두 명의 평가자 간 일치도를 측정하는 방법**이다. Cohen’s Kappa은 일치 vs. 불일치를 1과 0으로 구분하는 반면, Weighted Kappa는 평가자의 불일치 정도에 가중치를 부여하여 더 세밀한 분석이 가능하다.

## 2.1 Weighted Kappa 공식

$$
\kappa_w = \frac{P_o - P_e}{1 - P_e}
$$

이 식은 일반적인 Cohen's Kappa의 공식과 같지만, Weighted Kappa의 경우 Observed Accuracy $P_o$와 Expected Accuracy $P_e$을 계산할 때 가중치를 적용한 값을 사용한다는 점에서 차이가 있다. 각 항목은 다음과 같이 계산한다.

### Observed Accuracy ($P_o$)

$P_o$는 두 평가자의 실제 평가 결과를 바탕으로 각 범주 간에 가중치를 적용하여 계산한 값이다.

$$
P_{o} = \sum_{i}\sum_{j} W_{ij}P_{ij}
$$

-   $W_{ij}$ : 각 범주(i,j) 간의 가중치
-   $P_{ij}$ : 두 평가자가 범주 (i,j)를 선택한 관측 비율

### Expected Accuracy ($P_e$)

$P_e$는 각 평가자의 범주별 평가 확률의 곱에 가중치를 곱하여 계산된다.

$$
P_e = \sum_{i}\sum_{j} W_{ij}P_{i+}P_{+j}
$$

-   $P_{i+}$ : 평가자 1이 범주 i를 선택한 전체 비율 (행 방향)
-   $P_{+j}$ : 평가자 2가 범주 j를 선택한 전체 비율 (열 방향)

## 2.2 가중치($W$)의 종류

Weighted Kappa에서 주로 사용하는 대표적인 가중치 부여 방식은 두 가지다: 선형(linear)과 제곱(quadratic)

### 선형 가중치 (Linear weights)

선형 가중치는 **Cicchetti-Allison weights**라고도 하며, 평가 항목 간의 불일치 정도에 따라 **일정한 간격으로** 가중치를 부여한다. 즉, 두 평가자 간의 평가가 한 단계씩 멀어질 때마다 일정한 비율로 일치도가 감소한다. 특징은 각 평가 간의 차이에 비례하여(선형적으로) 가중치를 부여한다는 것이다.

**수식 표현:**

$$
W_{ij}^{linear} = 1 - \frac{|i - j|}{k - 1}
$$

-   $i$, $j$: 두 평가자가 선택한 범주(단계)
-   $k$: 범주의 전체 개수

**예시:**\
범주가 1\~4단계로 구성된 경우 ($k$=4):

-   평가자가 각각 1단계와 2단계를 선택했다면:

$$
W_{12}^{linear} = 1 - \frac{|1 - 2|}{4 - 1} = \frac{2}{3} \approx 0.67
$$

이는 평가자들이 약 67% 정도로 일치하고 있다고 볼 수 있으며, 33%는 불일치한다고 해석할 수 있다. 이 결과를 제곱 가중치를 적용했을 때와 비교해 본다.

### 제곱 가중치 (Quadratic weights)

제곱 가중치는 **Fleiss-Cohen weights**라고도 하며, 평가 항목 간의 불일치가 클수록 가중치를 **제곱에 비례하여(비선형적으로)** 부여한다. 특히 평가자 간의 작은 불일치는 가볍게, 큰 불일치는 더 무겁게 여긴다. 즉, 불일치의 정도가 증가할수록 가중치는 비선형적으로(제곱의 비율로) 감소하게 된다.

**수식 표현:**

$$
W_{ij}^{quadratic} = 1 - \frac{(i - j)^2}{(k - 1)^2}
$$

-   $i$, $j$: 두 평가자가 선택한 범주(단계) - ($k$): 범주의 전체 개수

**예시:**\
Linear의 예시와 마찬가지로 범주가 1\~4단계로 구성된 경우($k$=4):

-   평가자가 각각 1단계와 2단계를 선택했다면:

$$
W_{12}^{quadratic} = 1 - \frac{(1 - 2)^2}{(4 - 1)^2} = 1 - \frac{1}{9} = \frac{8}{9} \approx 0.89
$$

이는 평가자들이 약 89%로 상당히 높은 일치도를 나타내며, 한 단계만 차이가 나기에 작은 불일치 정도에 높은 가중치를 부여한 것이다. 그러나 두 단계 이상의 큰 차이가 발생하면 가중치가 급격히 감소하여, 큰 불일치로 인식을 한다.

이와 같이 Quadratic 방식에서는 큰 불일치를 더욱 엄격하게 평가한다.

### 가중치 선택 방법

-   **선형 가중치(Linear weights)**는 모든 단계 간의 차이를 동일한 중요도로 평가할 때 사용한다.
    -   진단 결과가 1단계에서 2단계로 바뀌는 것과, 2단계에서 3단계로 바뀌는 것의 중요도가 같은 경우
-   **제곱 가중치(Quadratic weights)**는 작은 차이보다 큰 차이에 더 큰 중요성을 부여할 때 적합하다.
    -   1단계와 2단계 간의 차이는 그리 크지 않지만, 2단계와 3단계 간의 차이는 매우 큰 의미를 가지는 경우

결국, 분석하려는 데이터와 평가 기준의 특성에 따라 적절한 가중치를 선택하면 된다.

## 2.3 Example 1

Weighted Kappa는 R코드를 활용해 수월하게 구할 수 있다.

아래는 두 명의 영상의학 전문의가 MRI 영상을 보고 병변의 심각도를 5단계 척도(1: 정상, 2: 경미, 3: 중등도, 4: 심함, 5: 매우 심함)로 평가한 경우를 예로 든 것이다. 두 평가자 간의 일치도를 Weighted Kappa를 사용하여 분석하며, 선형(linear) 가중치를 적용하여 평가 간의 차이를 부분적으로 반영한다.

``` r
library(irr)

# 평가 데이터 예시 (랜덤하게 생성)
set.seed(123)
lesion_assessment <- data.frame(
  Rater1 = sample(1:5, 100, replace = TRUE),
  Rater2 = sample(1:5, 100, replace = TRUE)
)

# 두 평가자 간의 Weighted Cohen's Kappa
kap <- irr::kappa2(lesion_assessment, weight = "equal")
se <- kap$value / kap$statistic
ci_lower <- kap$value - qnorm(0.975) * se
ci_upper <- kap$value + qnorm(0.975) * se

weighted_kappa_result <- paste0(round(kap$value, 3), " (95% CI: ", round(ci_lower, 3), "~", round(ci_upper, 3), ")")
weighted_kappa_result
```
이 예시를 통해 두 평가자 간의 의견 일치 수준을 파악할 수 있다.위 코드 실행 결과로 선형 가중치를 사용한 Weighted Kappa 값을 확인할 수 있으며, 범주 간의 불일치 정도를 반영한 평가자 간 일치도를 평가할 수 있다. 여기서 quadratic 가중치를 부여하기 위해서는 weight = "squared"로 수정하면 된다.

**출력:**

``` r
[1] "0.048 (95% CI: -0.089~0.185)"
```

처음에 설명했던 것 처럼, Cohen’s Kappa 값이 0.048이라는 것은 두 평가자가 거의 무작위로 평가를 했거나, 일치도가 매우 낮다는 것을 의미한다. Example의 결과 값으로 0.048로 나타났기 때문에 이런 경우 평가자 간의 의견이 거의 일치하지 않는다고 결론을 내린다.

신뢰구간을 살펴보면 -0.089에서 0.185 사이에 위치하고 있다. 95% 신뢰구간은 실제 모집단에서의 Kappa 값이 이 범위 안에 있을 확률이 95%라는 뜻이다. 하지만 이 신뢰구간에는 음수 값(-0.089)이 포함되어 있으므로 일반적으로 평가자 간의 일치도가 단순한 우연보다도 낮다는 뜻이다. 이는 평가자들이 무작위로 답변한 것보다도 더 일치도가 낮을 가능성이 있다는 것이며, 신뢰구간이 넓다는 것은 데이터가 불안정하거나, 평가자 간의 일치도가 일정하지 않다는 것을 나타낸다.

예시에서 이러한 결과가 나오는 이유는 평가 데이터 자체가 랜덤하게 생성되었기 때문이다. 그러므로 당연히 통계적으로 신뢰하기 어려운 결과가 나온다. 실제 상황에서 이러한 Kappa 값이 나온다면, 평가 기준을 명확하게 정리하고, 데이터의 질을 개선하는 것이 필요할 것이다.

# 3. Fleiss' Kappa

앞서 살펴본 Cohen's와 Weighted Kappa는 평가자 또는 연구자가 정확히 두 명일 때만 적용이 가능하다. 그렇다면 평가자 수가 세 명 이상일 경우에는 어떻게 해야 하는가. 이런 상황을 해결하기 위해 다양한 접근이 제안되었다. 평가자들을 두 명씩 묶어 모든 조합의 Cohen's kappa 값을 구한 뒤 그 평균을 사용하는 방법이 존재하기도 하나, 현재 학계에서 가장 폭넓게 사용되고 인정받는 방식은 Fleiss Kappa이다.

**Fleiss' Kappa**($\kappa$)는 세 명 이상의 평가자가 범주형 데이터를 평가할 때, 평가자 간의 전반적인 **일치도(agreement)** 를 측정하는 통계 지표다. Cohen’s Kappa가 두 명의 평가자에 대해서만 사용되는 반면, Fleiss’ Kappa는 다수의 평가자가 있는 경우에도 적용이 가능하다.

또한, Fleiss' Kappa는 한 명의 평가자가 같은 항목을 **두 번 이상 서로 다른 시점에서 반복하여 평가**할 때도 사용할 수 있다. 이 경우 Fleiss' Kappa는 동일한 평가자가 여러 번 평가했을 때, 각각의 평가가 얼마나 일관성 있게 일치하는지를 나타내는 지표가 된다. 즉, 동일 평가자의 시간에 따른 평가 일관성을 측정하는 데에도 Fleiss' Kappa가 활용될 수 있는 것이다.

## 3.1 Fleiss' Kappa 공식

Fleiss' Kappa($\kappa$)를 정의하는 수식은 다음과 같다.

$$
\kappa = \frac{\bar{P_o} - \bar{P_e}}{1 - \bar{P_e}}
$$

-   $\bar{P_o}$: 관찰된 평균 일치율 (Observed agreement)
-   $\bar{P_e}$: 우연에 의한 평균 일치율 (Expected agreement)

이와 같이 Fleiss' Kappa는 관찰된 평균 일치율($\bar{P}_0$)과 우연에 의한 평균 일치율($\bar{P}_e$)을 사용하여 계산한다. 각각의 공식은 아래와 같다.

### Observed agreement $\bar{P}_o$:

$\bar{P}_o$는 평가자들이 실제로 얼마나 의견이 일치했는지를 측정하는 값인데, 이는 실제 평가에서 동일한 범주를 선택한 평가자들의 비율을 정량적으로 나타내는 값이며, 다음과 같은 수식으로 계산된다.

$$
\bar{P}_o = \frac{1}{N n (n - 1)}\left(\sum_{i=1}^{N}\sum_{j=1}^{k}n_{ij}^{2} - N n\right)
$$ - $N$: 평가 항목의 총 개수 - $n$: 각 대상당 평가자의 수 - $k$: 평가 범주의 수 - $n_{ij}$: $i$번째 대상에서 $j$번째 카테고리를 선택한 평가자의 수

이 공식에서 첫 번째 항인 $\sum_{i=1}^{N}\sum_{j=1}^{k}n_{ij}^{2}$는 특정 범주를 선택한 평가자 수를 제곱하고 합산한다. 동일한 범주를 선택한 평가자가 많을수록 그 값이 더 커지며, 평가자들이 일관된 결정을 내릴수록 이 항의 값이 증가하게 된다. 이 식에서 $Nn$을 빼는 이유는, $\sum n_{ij}^2$ 항에는 평가자가 한 명만 특정 범주를 선택한 경우도 포함되기 때문이다. 평가자 수가 많을수록 이 값이 증가하므로, 이를 보정하기 위해 전체 평가 대상 개수($N$)와 평가자 수($n$)를 곱한 값을 빼준다. 이를 통해, 평가자가 많을수록 발생할 수 있는 단순한 일치 효과를 제거하고, 실제로 의미 있는 평가자 간의 일치도를 측정할 수 있도록 조정한다.

마지막으로, 이 값을 $N n (n - 1)$로 나누어 정규화한다. 이는 전체 평가 수에 대해 평균을 구하는 과정이며, 결과적으로 Observed Agreement는 "평균적인 일치도"를 나타내는 값이 된다. 이 값이 클수록 평가자들이 동일한 평가를 내린 비율이 높다는 것을 의미하며, 평가자 간의 의견이 더욱 일관되게 나타난다는 것을 보여준다. 이 값은 Expected Agreement($\bar{P}_e$)와 비교하여 Fleiss' Kappa 값을 계산하는 핵심 요소다.

### Expected agreement by chance $\bar{P_e}$

$\bar{P_e}$은 평가자들이 평가를 무작위로 진행했을 때 결과가 우연히 일치하게 될 확률을 나타낸다.

$$
\bar{P}_e = \sum_{j=1}^{k} P_j^{2}
$$

여기서 $P_j$는 모든 대상과 평가자를 통틀어 $j$번째 카테고리에 선택된 횟수의 합을 전체 평가 횟수($N \times n$)로 나눈 값이다. 이를 식으로 표현할 수 있다.

$$
P_j = \frac{1}{Nn} \sum_{i=1}^{N} n_{ij}
$$ 이 공식에서 $\sum_{i=1}^{N} n_{ij}$는 모든 평가 대상에서 특정 범주 $j$가 선택된 총 횟수를 의미하며, 이를 전체 평가 횟수($N \times n$)로 나누어 특정 범주 $j$가 선택될 확률을 구한다.

무작위로 평가를 했다고 가정하면, 한 평가자가 특정 범주 $j$를 선택할 확률은 $P_j$이고, 또 다른 평가자도 동일한 범주를 선택할 확률 역시 $P_j$이므로, 이 두 확률을 곱한 $P_j^2$이 해당 범주에서 평가가 우연히 일치할 확률이 된다. 따라서 모든 범주 $j=1$부터 $k$까지에 대해 이러한 우연 일치 확률을 더하면, 전체적으로 평가자들이 무작위로 평가했을 때 기대되는 평균적인 일치 확률 $\bar{P}_e$을 계산할 수 있다.

## 3.2 Example

다음 예시 데이터로 Fleiss' Kappa의 개념을 살펴본다.

| 항목 | 범주1 | 범주2 | 범주3 |
|------|-------|-------|-------|
| 1    | 0     | 0     | 5     |
| 2    | 0     | 1     | 4     |
| 3    | 1     | 0     | 4     |
| 4    | 0     | 2     | 3     |
| 5    | 0     | 1     | 4     |

-   평가 항목 수: $N = 5$
-   평가자 수: $n = 5$ (각 항목마다 평가자가 5명)
-   평가 범주 수: $k = 3$

**R 코드로 구하는 방식:**

```         
# `irr` 패키지의 `kappam.fleiss()` 함수를 사용한다
install.packages("irr")
library(irr)

# 데이터 행렬 생성
ratings <- matrix(c(
  0, 0, 5,
  0, 1, 4,
  1, 0, 4,
  0, 2, 3,
  0, 1, 4), 
  nrow = 5, byrow = TRUE)

# Fleiss' Kappa 계산
fleiss_kappa <- kappam.fleiss(ratings)
print(fleiss_kappa)
```

**출력:**

```         
 Fleiss' Kappa for m Raters

 Subjects = 5 
   Raters = 3 
    Kappa = -0.25 

        z = -1.83 
  p-value = 0.067 
```

Cohen's Kappa와 같이 Fleiss' Kappa는 평가 값이 동일한 경우 1, 다르면 0으로 단순 비교한다. 순위형 변수로 Weight를 부여해야 할 경우, Generalized Fleiss' Kappa를 사용하면 된다.

# 4. Generalized Fleiss' Kappa

**Generalized Fleiss' Kappa**는 Fleiss' Kappa를 일반화한 지표로, 여러 명의 평가자가 평가하는 경우 사용된다. 각 평가자가 평가하는 항목 수가 다르거나 평가 범주 간 순서가 중요한 경우에도 사용할 수 있는 지표다. 이는 평가 값 사이의 거리를 반영하여 평가자 간의 불일치 정도를 측정한다.

기존 Fleiss' Kappa와 달리 평가자가 모든 항목을 평가하지 않아도 되며, 항목마다 평가자 수가 달라도 적용할 수 있다. 또한 항목 간, 평가자 간의 부분적인 데이터 결측이 있더라도 신뢰도 높은 일치도를 구할 수 있다.

## 4.1 Generalized Fleiss' Kappa 공식

Generalized Fleiss' Kappa ($\kappa_G$)의 수식은 다음과 같다.

$$
\kappa_{G} = \frac{P_o - P_e}{1 - P_e}
$$

-   $P_o$: 관찰된 가중 평균 일치율 (Observed weighted agreement)
-   $P_e$: 우연히 기대되는 가중 평균 일치율 (Expected weighted agreement by chance)

각 항목의 계산법은 아래와 같다.

#### Observed weighted agreement $P_o$

$$
P_o = \frac{1}{\sum_{i=1}^{N} n_i(n_i - 1)} \sum_{i=1}^{N}\sum_{j=1}^{k}\sum_{l=1}^{k} W_{jl} \cdot n_{ij}(n_{il}-\delta_{jl})
$$ - $N$: 평가된 항목(대상)의 총 개수 - $k$: 평가 범주의 수 - $n_i$: $i$번째 항목을 평가한 평가자의 수 - $n_{ij}$: $i$번째 항목을 $j$번째 범주로 평가한 평가자의 수 - $W_{jl}$: 범주 간 가중치 - $\delta_{jl}$: 범주가 같으면 1, 다르면 0인 값

Generalized Fleiss' Kappa에서 Observed Weighted Agreement $P_o$는 평가자들이 실제로 얼마나 일치했는지를 측정하는 값이다. 기존 Fleiss' Kappa가 단순한 평가 일치율을 계산하는 방식이라면, Generalized Fleiss' Kappa는 평가 값 사이의 차이를 반영하여 가중치를 적용하는 방식으로 평가자 간의 일치도를 보다 정교하게 측정한다.

$P_o$는 평가자들이 동일한 평가를 내린 정도를 가중(weighted) 방식으로 계산하며, 전체 평가 대상에서 발생한 모든 평가 쌍에 대한 가중 평균을 구하는 방식으로 정의된다. 먼저, 전체 평가자들이 내린 평가 쌍의 총 개수는 $\sum_{i=1}^{N} n_i(n_i - 1)$로 계산된다. 여기서 $N$은 평가 대상의 개수이며, $n_i$는 특정 평가 대상에 대해 평가를 수행한 평가자 수이다. 평가 대상마다 평가자 수가 다를 수 있으므로 이를 반영하여 전체적인 합을 계산한다.

식에서 그 외의 부분은 평가자 간의 일치도를 계산한다. $W_{jl}$은 범주 $j$와 범주 $l$ 사이의 가중치로, 두 평가 값이 얼마나 다른지를 수치적으로 반영하는 역할을 하는데, 앞서 설명한 linear 또는 quadratic 방식으로 설정된다. $n_{ij}$는 평가 대상 $i$에서 범주 $j$를 선택한 평가자의 수를 의미하며, $n_{il}$은 동일한 평가 대상에서 범주 $l$을 선택한 평가자의 수를 나타낸다. 또한 $\delta_{jl}$은 Kronecker Delta로, $j$와 $l$이 동일한 경우 1, 다르면 0을 반환하는 지표이다.

$P_o = 1$이면 평가자들이 완벽하게 동일한 평가를 내린 경우이며, $P_o = 0$이면 평가자 간 평가가 완전히 무작위로 이루어진 경우를 의미한다.

#### Expected weighted agreement by chance $P_e$

$$
P_e = \frac{1}{\left(\sum_{i=1}^{N} n_i\right)^2 - \sum_{i=1}^{N} n_i} \sum_{j=1}^{k}\sum_{l=1}^{k} W_{jl}\left(\sum_{i=1}^{N}n_{ij}\right)\left(\sum_{i=1}^{N}n_{il}-\delta_{jl}\right)
$$ $P_e$\*\*는 Fleiss' Kappa에서 Expected agreement by chance ($P_e$)를 확장한 개념으로, 평가 값들 사이의 거리를 고려하여 가중(weighted) 방식으로 측정한다.

$\left(\sum_{i=1}^{N} n_i\right)^2 - \sum_{i=1}^{N} n_i$는 전체 평가 데이터에서 발생할 수 있는 모든 평가 쌍의 개수를 나타낸다. 여기서 $\sum_{i=1}^{N} n_i$는 전체 평가자가 수행한 총 평가 개수이며, 이를 제곱한 값에서 자기 자신과의 비교를 제외하기 위해 $\sum_{i=1}^{N} n_i$를 빼준다. 이는 평가자들이 임의로 범주를 선택했을 때 가능한 모든 평가 쌍의 개수를 정규화하는 역할을 한다.

$\sum_{i=1}^{N} n_{ij}$와 $\sum_{i=1}^{N} n_{il}$은 특정 범주 $j$와 $l$이 전체 평가에서 각각 몇 번 선택되었는지를 나타낸다. 이 값에 가중치 행렬 $W_{jl}$을 곱하는데, $W_{jl}$은 범주 $j$와 범주 $l$ 사이의 거리를 반영하는 값으로, linear 또는 quadratic 가중치로 결정된다.마지막으로, $\delta_{jl}$는 두 범주가 동일할 경우 1, 다를 경우 0을 갖는 함수이다.

결과적으로 $P_e$를 $P_o$와 비교하여 Kappa 값이 계산되며, $P_o$가 $P_e$보다 클수록 평가자 간의 신뢰도가 높다는 것을 의미한다.

## 4.2 Example

다음과 같은 예시 데이터를 통해 계산 방식을 간단히 이해해 본다. 평가 범주는 1\~3으로 순서형이며, 일부 평가자는 특정 항목을 평가하지 않았다.

| 항목 | 평가자1 | 평가자2 | 평가자3 | 평가자4 |
|------|---------|---------|---------|---------|
| 1    | 1       | 2       | 2       | \-      |
| 2    | 2       | 2       | 3       | 2       |
| 3    | 3       | 3       | \-      | \-      |
| 4    | 1       | 1       | 1       | 2       |

Generalized Fleiss' Kappa는 R의 `irrCAC` 패키지에 있는 `fleiss.kappa.raw` 함수를 사용하여 쉽게 계산할 수 있다.

``` r
library(irrCAC)

# 여러 평가자의 병변 평가 데이터
ratings <- data.frame(
  rater1 = c(1, 2, 3, 1),
  rater2 = c(2, 2, 3, 1),
  rater3 = c(2, 3, NA, 1),
  rater4 = c(NA, 2, NA, 2)
)

# Linear Weighted Fleiss Generalized Kappa
kappa_linear <- fleiss.kappa.raw(ratings, weights = "linear")
print(kappa_linear)

# Quadratic Weighted Fleiss Generalized Kappa
kappa_quadratic <- fleiss.kappa.raw(ratings, weights = "quadratic")
print(kappa_quadratic)
```

# 5. Krippendorff’s Alpha

Krippendorff’s Alpha는 여러 평가자가 동일한 항목을 평가할 때, 평가자 간의 신뢰도를 측정하는 지표이다. 가장 큰 특징은 범주형, 순서형, 연속형 데이터를 모두 처리할 수 있으며, 평가자 수에 제한이 없고, 불완전한 데이터(NA 값 포함)도 분석할 수 있다는 점이다. 이러한 특성 덕분에 결측값이 포함된 데이터에서도 안정적인 신뢰도 평가가 가능하고, Cohen’s Kappa나 Fleiss Kappa보다 범용성이 뛰어나다.

범주형, 순서형, 연속형 데이터: - **Nominal (명목형)**: 범주 간 서열이 없는 경우 - **Ordinal (순위형)**: 서열이 있는 경우 (예: 경미, 중등, 심함) - **Interval (구간형)**: 연속형 데이터 (예: 온도, IQ 점수 등)

## 5.1 신뢰도 데이터

먼저 Krippendorff’s Alpha를 계산하기 위해서는 평가자들이 동일한 단위(unit)에 대해 내린 평가 데이터를 분석해야 한다. 평가자들은 독립적으로 하나 이상의 값을 할당할 수 있으며, 이러한 데이터를 **m × N 행렬**로 표현할 수 있다. 여기서 m은 평가자의 수, N은 평가된 항목의 개수이다.

평가자가 특정 단위 $u_j$에 할당한 값 $v_{ij}$를 포함하는 행렬을 다음과 같이 정의할 수 있다.

$$
\begin{array}{c|cccc}
    & u_1 & u_2 & u_3 & \cdots & u_N \\\hline
c_1 & v_{11} & v_{12} & v_{13} & \cdots & v_{1N} \\
c_2 & v_{21} & v_{22} & v_{23} & \cdots & v_{2N} \\
c_3 & v_{31} & v_{32} & v_{33} & \cdots & v_{3N} \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
c_m & v_{m1} & v_{m2} & v_{m3} & \cdots & v_{mN}
\end{array}
$$

-   $m$: 평가자의 수
-   $N$: 평가된 항목(unit)의 개수
-   $v_{ij}$: 평가자 $c_i$가 특정 단위 $u_j$에 대해 부여한 값
-   $m_j$: 단위 $u_j$에 대해 평가된 개수

여기서 $m_j$는 특정 단위 $u_j$에 대해 평가된 개수이다. 일부 평가자가 특정 단위에 대한 평가를 하지 않을 수도 있기 때문에, $m_j$는 평가자가 동일하지 않은 경우 $m$보다 작을 수도 있다.

Krippendorff’s Alpha를 계산하려면 평가된 값들이 **서로 비교 가능(pairable)** 해야 하므로, $m_j \geq 2$의 조건이 필요하다. 즉, 특정 단위에서 최소 두 명 이상의 평가자가 값을 할당해야 한다. 전체 데이터에서 가능한 쌍의 개수는 다음과 같다.

$$
\sum_{j=1}^{N} m_j = n \leq mN
$$

이러한 행렬 표현은 Krippendorff’s Alpha에서 관찰된 불일치 $D_o$와 기대되는 불일치 $D_e$를 계산하는 기본적인 데이터 구조를 나타낸다.

## 5.1 Krippendorff’s Alpha 공식

Krippendorff’s Alpha는 평가자 간의 일치도를 분석하기 위해 관찰된 불일치($D_o$)와 우연히 예상된 불일치($D_e$)를 비교하여 계산된다. 평가자들이 응답할 수 있는 모든 가능한 값들의 집합을 $R$이라고 하고, 평가자들이 특정 예제에 대해 내린 응답을 하나의 단위(unit)라고 할 때, Krippendorff’s Alpha는 다음과 같이 정의된다.

$$
\alpha = 1 - \frac{D_o}{D_e}
$$

-   $D_o$: 실제로 관찰된 불일치 (Observed Disagreement)
-   $D_e$: 우연히 예상된 불일치 (Expected Disagreement by chance)

### Observed Disagreement $D_o$

$D_o$는 다음과 같이 정의된다.

$$
D_o = \frac{1}{n} \sum_{c \in R} \sum_{k \in R} \delta(c, k) \sum_{u \in U} m_u \frac{n_{cku}}{P(m_u, 2)}
$$

-   $\delta(c, k)$: 두 개의 평가 값 $c$와 $k$ 사이의 차이
-   $n$: 총 가능한 쌍(pair)의 개수 - $m_u$: 특정 단위 $u$에 포함된 평가 수
-   $n_{cku}$: 단위 $u$에서 평가 값 $(c, k)$ 쌍이 나타난 횟수
-   $P$: 순열(permutation)

이 식은 평가자들이 특정 단위에서 얼마나 불일치했는지를 개념적으로 가중 평균(weighted average)한 것으로 해석할 수 있다. 여기서 중요한 역할을 하는 $\delta(c, k)$는 평가 값 $c$와 평가 값 $k$ 사이의 차이를 의미하는데, 순위형 데이터의 경우 제곱을 취하여 거리의 크기를 강조하는 방식으로 계산한다. 즉, 두 평가 값이 동일하면 $\delta(c, k) = 0$이 되고, 평가 값 간 차이가 크면 불일치도가 더 커지는 방식이다.

-   Krippendorff’s Alpha는 데이터의 유형에 따라 다른 $\delta(c, k)$의 정의를 사용한다. 이 값을 기반으로 평가자 간의 불일치를 측정하므로, 이 함수가 어떻게 정의가 되느냐에 따라 Alpha $α$의 값이 달라진다.

또한, $P(m_u, 2)$는 특정 단위 $u$에서 평가된 값들 중에서 2개의 값을 선택하여 순서를 고려한 쌍의 개수를 의미한다. 이는 순열 함수로 표현되며, Krippendorff’s Alpha에서 관찰된 불일치도를 계산할 때 중요한 역할을 한다. 공식은 다음과 같다.

$$
P(m_u, 2) = \frac{m_u (m_u - 1)}{2}
$$

이러한 계산이 필요한 이유는, Krippendorff’s Alpha에서 평가자 간의 불일치를 측정할 때 단순한 개별 평가 값을 비교하는 것이 아니라, 각 단위에서 이루어진 모든 평가 값들 간의 관계를 분석해야 하기 때문이다.

이 식은 평가 값 자체의 범주를 기반으로 전체적인 불일치도를 직접적으로 계산하는 방식이라면, 단위별 불일치도를 먼저 계산한 후 전체 평균을 구하는 방식도 존재한다. $D_o$에 대한 단위 중심의 접근법은 다음과 같다.

$$
D_o = \frac{1}{n} \sum_{j=1}^{N} m_j E(\delta_j)
$$

여기서 $E(\delta_j)$는 모든 가능한 쌍에 대해 평균적인 거리이며, 아래와 같은 식으로 표현할 수 있다.

$$
E(\delta_j) = \frac{ \sum_{i>i'} \delta(v_{ij}, v_{i'j}) }{\binom{m_j}{2}}
$$

-   $v_{ij}$: 평가자 $i$가 단위 $j$에 대해 부여한 평가 값
-   $\delta(v_{ij}, v_{i'j})$: 두 평가 값 $v_{ij}$와 $v_{i'j}$ 간의 거리
-   $\sum_{i>i'}$: 단위 $j$에서 모든 평가자 간의 가능한 쌍을 고려한 합 - $\binom{m_j}{2}$: 단위 $j$에서 가능한 모든 평가 쌍의 개수. $\frac{m_j(m_j - 1)}{2}$로 계산된다.

이 수식은 단위 $j$에서 평가자들이 부여한 모든 값들을 비교하여 평균적인 불일치도를 구하는 과정이다. 평가자들이 같은 값을 부여했다면 $\delta(v_{ij}, v_{i'j}) = 0$이 되어 $E(\delta_j)$ 값이 작아지고, 평가 값이 크게 차이 날수록 $E(\delta_j)$ 값이 증가한다.

또한, 만약 모든 단위에서 평가자 수가 일정하다면, $D_o$는 전체 평가 단위에서 가능한 모든 평가 쌍에 대한 평균적인 거리로 해석할 수 있다. 이는 일반적으로 평가 행렬에서 대각선에서의 평균적인 거리로 볼 수 있으며, 평가자들이 특정 경향을 가지고 평가했는지 또는 무작위로 평가했는지를 판단하는 중요한 지표가 된다.

### Expected Disagreement by chance $D_e$

우연히 예상된 불일치 $D_e$는 평가자들이 무작위로 응답했다고 가정할 때 예상되는 불일치도를 의미하는데, 모든 가능한 평가 값 쌍(c, k)의 발생 확률을 이용해 불일치를 추정한다. 이는 Krippendorff’s Alpha에서 평가자 간의 일치도를 측정할 때, 실제 관찰된 불일치도($D_o$)와 비교하는 기준이 된다.

$$
D_e = \frac{1}{P(n,2)} \sum_{c \in R} \sum_{k \in R} \delta(c,k) P_{ck}
$$ 
- $P(n,2)$: 전체 가능한 평가 쌍(pair)의 개수 
- $\delta(c,k)$: 두 개의 평가 값 $c$와 $k$ 사이의 차이 
- $P_{ck}$: 특정 평가 값 $(c,k)$ 쌍이 발생할 확률

$$
P_{ck} = \begin{cases} 
  n_c n_k & \text{if } c \neq k \\
  n_c (n_c - 1) & \text{if } c = k 
\end{cases}
$$

이 식에서 $n_c$와 $n_k$는 각각 평가 값 $c$와 $k$가 전체 데이터에서 나타난 횟수이다. 만약 $c \neq k$이면, 서로 다른 두 개의 평가 값이 선택될 확률은 $n_c n_k$로 표현된다. 반면, $c = k$이면 동일한 평가 값이 두 번 선택될 확률은 $n_c (n_c - 1)$로 계산된다. 이러한 확률을 고려하여 평가 값들이 랜덤하게 분포되었을 때 기대되는 불일치도를 구할 수 있다.

즉, $D_e$는 평가자들이 일관된 기준 없이 평가한 경우 예상되는 불일치도의 평균적인 크기를 나타낸다.

Krippendorff’s Alpha는 개념적으로 직관적이지만, 계산적으로는 다소 복잡할 수 있다. 그러나 이 지표는 다양한 데이터 유형에 적용할 수 있으며, 평가자의 수가 일정하지 않거나 결측값이 포함된 경우에도 안정적인 신뢰도 분석을 수행할 수 있는 장점이 있다.

### Krippendorff’s Alpha 값 해석

$α$에 대한 해석은 다음과 같다.

-   **α = 1**: 완벽한 평가자 간 일치 (모든 평가자가 동일한 응답)
-   **0.8 ≤ α ≤ 1**: 높은 신뢰도를 의미하며 연구 결과로 활용 가능
-   **0.67 ≤ α \< 0.8**: 신뢰할 수 있는 수준이지만 엄격한 연구에서는 보완이 필요함
-   **0 ≤ α \< 0.67**: 신뢰도가 낮아 추가적인 평가 기준 수정 또는 평가자 교육이 필요함
-   **α \< 0**: 평가자 간 일치도가 우연보다도 낮음 (평가 기준이 모호하거나 데이터에 문제 가능성)

## 5.2 Example

아래는 `irrCAC` 패키지를 사용하여 Krippendorff’s Alpha를 계산하는 R 코드 예제이다.

``` r
library(irrCAC)

# 여러 평가자의 병변 평가 데이터
ratings <- data.frame(
  rater1 = c(1, 2, 3, 1, 2, NA, 4, 3, NA, 2),
  rater2 = c(2, 2, 3, 1, 3, 2, 4, 3, 2, 1),
  rater3 = c(2, 3, NA, 1, 4, 2, NA, 3, 2, NA),
  rater4 = c(NA, 2, NA, 2, 3, 1, 4, NA, 3, 2)
)

# Krippendorff’s Alpha 계산 (순위형 데이터)
alpha_result <- krippen.alpha.raw(ratings, weights = "ordinal")
print(alpha_result)
```

이 코드는 순위형 데이터를 기반으로 Krippendorff’s Alpha를 계산하는 방법을 보여준다. 평가자 간 일치도를 보다 유연하게 측정할 수 있으며, Fleiss Kappa나 Weighted Cohen’s Kappa보다 데이터 특성에 덜 제한을 받는다는 장점이 있다. 또한, 결측값이 포함된 데이터를 그대로 분석할 수 있다는 점에서 다른 신뢰도 측정법보다 강력한 활용성을 가진다.

Krippendorff’s Alpha는 평가자가 많을수록, 그리고 평가 기준이 명확할수록 신뢰도가 높게 나타난다. 하지만 평가자 간 의견 차이가 크다면 신뢰도 값이 낮아질 수 있으며, 이런 경우 평가 기준을 조정하거나 추가적인 훈련이 필요할 수 있다.

# 마치며

마지막으로 유의할 점은 Kappa가 두 평가자 사이에 평가 결과가 얼마나 비슷한지, 즉 평가자들이 서로 얼마나 일치하는지만 측정할 수 있다는 것이다. 이것을 '신뢰도(Reliability)'라고 한다. 반면, 평가자들이 실제로 올바른 평가를 하고 있는지 여부, 즉 '타당도(Validity)'에 대해서는 알려주지 못한다. 평가가 옳고 정확한지 판단하는 것은 타당도의 영역이며, Kappa는 오직 신뢰도를 측정할 때만 사용할 수 있다는 점을 기억하자.
